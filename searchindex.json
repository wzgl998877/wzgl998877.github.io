{"categories":[{"title":"git","uri":"https://wzgl998877.github.io/categories/git/"},{"title":"Java","uri":"https://wzgl998877.github.io/categories/java/"},{"title":"spring","uri":"https://wzgl998877.github.io/categories/spring/"},{"title":"中间件","uri":"https://wzgl998877.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"title":"云原生","uri":"https://wzgl998877.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"title":"常用框架","uri":"https://wzgl998877.github.io/categories/%E5%B8%B8%E7%94%A8%E6%A1%86%E6%9E%B6/"},{"title":"数据库","uri":"https://wzgl998877.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"日常总结","uri":"https://wzgl998877.github.io/categories/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"},{"title":"算法","uri":"https://wzgl998877.github.io/categories/%E7%AE%97%E6%B3%95/"},{"title":"系统设计","uri":"https://wzgl998877.github.io/categories/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"title":"网络编程","uri":"https://wzgl998877.github.io/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"posts":[{"content":"[TOC]\n高性能Mysql 字段类型 整数类型 有两种类型的数字：整数(whole number)和实数（real number，带有小数部分的数字）。如果存储整数，可以使用这几种整数类型：TINYINT、SMALLINT、MEDIUMINT、INT或BIGINT。它们分别使用8、16、24、32和64位存储空间。可以存储的值的范围从-2(N-1)到2(N-1)-1，其中N是存储空间的位数。整数类型有可选的UNSIGNED属性，表示不允许负值，这大致可以使正数的上限提高一倍。例如，TINYINT UNSIGNED可以存储的值的范围是0～255，而TINYINT的值的存储范围是-128～127。有符号和无符号类型使用相同的存储空间，并具有相同的性能，因此可以根据数据实际范围选择合适的类型。MySQL可以为整数类型指定宽度，例如，INT(11)，这对大多数应用毫无意义：它不会限制值的合法范围，只是规定了MySQL的一些交互工具（例如，MySQL命令行客户端用来显示字符的个数。对于存储和计算来说，INT(1)和INT(20)是相同的。\n实数类型 FLOAT和DOUBLE类型支持使用标准的浮点运算进行近似计算。如果你需要知道浮点运算是怎么计算的，则需要研究平台的浮点数的具体实现方式。有两种方式可以指定浮点列所需的精度，这可能会导致MySQL以静默方式选择不同的数据类型，或者在存储值时对其进行近似处理。这些精度说明符是非标准的，因此我们建议只指定数据类型，不指定精度。浮点类型通常比DECIMAL使用更少的空间来存储相同范围的值。FLOAT列使用4字节的存储空间。DOUBLE占用8字节，比FLOAT具有更高的精度和更大的值范围。与整数类型一样，你只能选择存储类型；MySQL会使用DOUBLE进行浮点类型的内部计算。\n由于额外的空间需求和计算成本，应该尽量只在对小数进行精确计算时才使用ECIMAL——例如，存储财务数据。但在一些大容量的场景，可以考虑使用BIGINT代替DECIMAL，将需要存储的货币单位根据小数的位数乘以相应的倍数即可。假设要存储财务数据并精确到万分之一分，则可以把所有金额乘以一百万，然后将结果存储在BIGINT里，这样可以同时避免浮点存储计算不精确和DECIMAL精确计算代价高的问题。\n字符串类型 MySQL支持多种字符串数据类型，每种类型还有许多变体。每个字符串列可以有自己的字符集和该字符集的排序规则集。VARCHAR和CHAR是两种最主要的字符串类型。下面的描述假设使用的存储引擎是InnoDB。先来看看VARCHAR和CHAR值通常是如何存储在磁盘上的。请注意，存储引擎在内存中存储CHAR或VARCHAR值的方式可能与在磁盘上存储该值的方式不同，并且服务器在从存储引擎检索该值时可能会将其转换为另一种存储格式。下面是关于两种类型的一些比较。\n  VARCHAR用于存储可变长度的字符串，是最常见的字符串数据类型。它比固定长度的类型更节省空间，因为它仅使用必要的空间（即，更少的空间用于存储更短的值）。VARCHAR需要额外使用1或2字节记录字符串的长度：如果列的最大长度小于或等于255字节，则只使用1字节表示，否则使用2字节。假设采用latin1字符集，一个VARCHAR(10)的列需要11字节的存储空间。VARCHAR(1000)的列则需要1002个字节，因为需要2字节存储长度信息。VARCHAR节省了存储空间，所以对性能也有帮助。但是，由于行是可变长度的，在更新时可能会增长，这会导致额外的工作。如果行的增长使得原位置无法容纳更多内容，则处理行为取决于所使用的存储引擎。例如，InnoDB可能需要分割页面来容纳行。其他一些存储引擎也许不在原数据位置更新数据。下面这些情况使用VARCHAR是合适的：字符串列的最大长度远大于平均长度；列的更新很少，所以碎片不是问题；使用了像UTF-8这样复杂的字符集，每个字符都使用不同的字节数进行存储。InnoDB更为复杂，它可以将过长的VARCHAR值存储为BLOB。我们稍后再讨论\n  CHAR是固定长度的：MySQL总是为定义的字符串长度分配足够的空间。当存储CHAR值时，MySQL删除所有尾随空格。如果需要进行比较，值会用空格填充。\n  与CHAR和VARCHAR类似的类型还有BINARY和VARBINARY，它们存储的是二进制字符串。二进制字符串与常规字符串非常相似，但它们存储的是字节而不是字符。填充也不同：MySQL填充BINANRY用的是\\0（零字节）而不是空格，并且在检索时不会去除填充值。当需要存储二进制数据，并且希望MySQL将值作为字节而不是字符进行比较时，这些类型非常有用。字节比较的优势不仅仅是大小写不敏感。MySQL比较BINARY字符串时，每次按一个字节，并且根据该字节的数值进行比较。因此，二进制比较比字符比较简单得多，因此速度更快。\n 使用VARCHAR(5)和VARCHAR(200)存储\u0026rsquo;hello\u0026rsquo;的空间开销是一样的。那么使用更短的列有什么优势吗？事实证明有很大的优势。较大的列会使用更多的内存，因为MySQL通常会在内部分配固定大小的内存块来保存值。这对于使用内存临时表的排序或操作来说尤其糟糕。在利用磁盘临时表进行文件排序时也同样糟糕。最好的策略是只分配真正需要的空间。\n   BLOB和TEXT类型BLOB和TEXT都是为存储很大的数据而设计的字符串数据类型，分别采用二进制和字符方式存储。实际上，它们分别属于两组不同的数据类型家族：字符类型是TINYTEXT、SMALLTEXT、TEXT、MEDIUMTEXT和LONGTEXT；二进制类型是TINYBLOB、SMALLBLOB、BLOB、MEDIUMBLOB、LONGBLOB。BLOB是SMALLBLOB的同义词，TEXT是SMALLTEXT的同义词。与其他数据类型不同，MySQL把每个BLOB和TEXT值当作一个具有自己标识的对象来处理。存储引擎通常会专门存储它们。当BLOB和TEXT值太大时，InnoDB会使用独立的“外部”存储区域，此时每个值在行内需要1～4字节的存储空间，然后在外部存储区域需要足够的空间来存储实际的值。BLOB和TEXT家族之间的唯一区别是，BLOB类型存储的是二进制数据，没有排序规则或字符集，但TEXT类型有字符集和排序规则。MySQL对BLOB和TEXT列的排序与其他类型不同：它只对这些列的最前max_sort_length字节而不是整个字符串做排序。如果只需要按前面少数几个字符排序，可以减小max_sort_length服务器变量的值。MySQL不能将BLOB和TEXT数据类型的完整字符串放入索引，也不能使用索引进行排序。\n  有时可以使用ENUM（枚举）列代替常规的字符串类型。ENUM列可以存储一组预定义的不同字符串值。MySQL在存储枚举时非常紧凑，会根据列表值的数量压缩到1或者2字节中。在内部会将每个值在列表中的位置保存为整数\n  日期和时间类型 MySQL中有很多数据类型用以支持各种各样的日期和时间值，比如YEAR和DATE。MySQL可以存储的最小时间粒度是微秒。大多数时间类型都没有其他选择，因此不存在哪一种是最佳选择的问题。唯一的问题是，当需要同时存储日期和时间时该怎么做。MySQL提供了两种非常相似的数据类型来实现这一需求：DATETIME和TIMESTAMP。对于许多应用程序来说，两者都可以，但在某些场景，一个比另一\n DATETIME这种类型可以保存大范围的数值，从1000年到9999年，精度为1微秒。它以YYYYMMDDHHMMSS格式存储压缩成整数的日期和时间，且与时区无关。这需要8字节的存储空间。默认情况下，MySQL以可排序、无歧义的格式显示DATETIME值，例如，2008-01-16 22：37：08。这是ANSI表示日期和时间的标准方式。 TIMESTAMP顾名思义，TIMESTAMP类型存储自1970年1月1日格林尼治标准时间(GMT)午夜以来经过的秒数——与UNIX时间戳相同。TIMESTAMP只使用4字节的存储空间，所以它的范围比DATETIME小得多：只能表示从1970年到2038年1月19日  位压缩数据类型 MySQL有几种使用值中的单个位来紧凑地存储数据的类型。所有这些位压缩类型，不管底层存储和处理方式如何，从技术上来说都是字符串类型。\n BIT可以使用BIT列存储一个或多个true/false值。BIT(1)定义一个包含1位的字段，BIT(2)存储2位的字段，依此类推；BIT列的最大长度为64位。InnoDB将每一列存储为足够容纳这些位的最小整数类型，所以使用BIT列不会节省任何存储空间。MySQL在处理时会将BIT视为字符串类型，而不是数字类型。当检索BIT(1)的值时，结果是一个包含二进制值0或1的字符串，而不是ASCII码的“0”或“1”。但是，如果在数字上下文中检索该值，则会将BIT字符串转换为数字。 SET如果需要存储多个true/false值，可以考虑使用MySQL原生的SET数据类型，可以将多列组合成一列，这在MySQL内部是以一组打包的位的集合来表示的。这样可以更有效地利用存储空间，MySQL具有FIND_IN_SET()和FIELD()等函数，使其易于在查询中使用。  JSON数据类型 MySQL有原生的JSON数据类型，可以方便地直接在表中的JSON结构部分进行操作。纯粹主义者可能会认为，在数据库中存储原始JSON是一种反范式，因为理想情况下，schema应该是JSON中具体字段的表示。新手在查看JSON数据类型时，可能会发现这是避免创建和管理独立字段的捷径。\n总的来说，决定使用原生SQL还是JSON取决于在数据库中存储JSON的便捷性是否大于性能。如果每天访问这些数据数百万次或数十亿次，速度差异就会累加起来\n选择标识符 一般来说，标识符是引用行及通常使其唯一的方式。例如，如果你有一个关于用户的表，可能希望为每个用户分配一个数字ID或唯一的用户名。此字段可能是主键中的部分或全部。为标识符列选择合适的数据类型非常重要。与其他列相比，更有可能将标识符列与其他值（例如，在联接中）进行比较，并使用它们进行查找。标识符列也可能在其他表中作为外键，因此为标识符列选择数据类型时，应该与联接表中的对应列保持一致。（正如我们在本章前面演示的，在关联表中使用相同的数据类型是一个好主意，因为这些列很可能在联接中使用。）\n 整数类型整数通常是标识符的最佳选择，因为它们速度快，并且可以自动递增。AUTO_INCREMENT是一个列属性，可以为新的行自动生成一个整数类型的值。例如，计费系统可能需要为每个客户生成新发票，使用AUTO_INCREMENT意味着生成的第一张发票是1，第二张是2，依此类推。请注意，应该确保选择适合预期数据增长的整数大小，与整数意外耗尽有关的系统停机事故可不止发生一次。 对于标识符来说，ENUM和SET类型通常是糟糕的选择，尽管对某些只包含固定状态或者类型的静态“定义表”来说可能是没有问题的。ENUM和SET列适用于保存 字符串类型如果可能，应避免使用字符串类型作为标识符的数据类型**，因为它们很消耗空间，而且通常比整数类型慢**。对于完全“随机”的字符串要非常小心，如MD5()、SHA1()或UUID()生成的字符串。这些函数生成的新值会任意分布在很大的空间内，这会减慢INSERT和某些类型的SELECT查询的速度：  因为插入的值会写到索引的随机位置，所以会使得INSERT查询变慢。这会导致页分裂、磁盘随机访问，以及对于聚簇存储引擎产生聚簇索引碎片。 SELECT查询也会变慢，因为逻辑上相邻的行会广泛分布在磁盘和内存中。 对于所有类型的查询，随机值都会导致缓存的性能低下，因为它们会破坏引用的局部性，而这正是缓存的工作原理。如果整个数据集都是“热的”，那么将任何特定部分的数据缓存到内存中都没有任何好处，而且如果工作集比内存大，缓存就会出现大量刷新和不命中。果存储通用唯一标识符(UUID)值，则应该删除破折号，或者更好的做法是，使用UNHEX()函数将UUID值转换为16字节的数字，并将其存储在一个BINARY(16)列中。可以使用HEX()函数以十六进制格式检索值。    特殊数据类型 某些类型的数据并不直接对应于可用的内置类型。IPv4地址就是一个很好的例子。人们通常使用VARCHAR(15)列来存储IP地址。然而，它们实际上是32位无符号整数，而不是字符串。用小数点将地址分成四段的表示方法只是为了让人们阅读容易，所以应该将I P地址存储为无符号整数。MySQL提供了INET_ATON()和INET_NTOA()函数来在这两种表示形式之间进行转换。使用的空间从VARCHAR(15)的约16字节缩减到无符号32位整数的4字节。如果你担心数据库的可读性，不想继续使用函数查看行数据，请记住MySQL有视图，可以使用视图来简化数据查看的复杂性。\nMySQL schema设计中的陷阱 尽管设计原则有好有坏，但MySQL的实现方式会带来一些问题，这意味着你也可能会犯MySQL特有的错误。本节讨论我们在MySQL schema设计中观察到的问题。它可能会帮助你避免这些错误，并让你选择更适合MySQL具体实现的替代方案。\n太多的列 MySQL的存储引擎API通过在服务器和存储引擎之间以行缓冲区格式复制行来工作；然后，服务器将缓冲区解码为列。将行缓冲区转换为具有解码列的行数据结构的操作代价是非常高的。InnoDB的行格式总是需要转换的。这种转换的成本取决于列数。当调查一个具有非常宽的表（数百列）的客户的高CPU消耗问题时，我们发现这种转换代价可能会变得非常昂贵，尽管实际上只使用了几列。如果计划使用数百列，请注意服务器的性能特征会有所不同。\n太多的联接 所谓的实体属性值(entity attribute value，EAV)设计模式是一种被普遍认为糟糕的设计模式的典型案例，尤其是在MySQL中效果不佳。MySQL限制每个联接有61个表，而E AV模式设计的数据库需要许多自联接。我们已经看到不少E AV模式设计的数据库最终超过了这个限制。然而，即使联接数远小于61，规划和优化查询的成本对MySQL来说也会成为问题。一个粗略的经验法则是，如果需要以高并发性快速执行查询，那么每个查询最好少于十几个的表。\n全能的枚举 要小心过度使用ENUM。下面是我们看到的一个例子：schema中大量地散布着这种模式。在任何具有枚举值类型的数据库中，这可能是一个值得商榷的设计决策\n变相的枚举 ENUM列允许在列中保存一组已定义值中的单个值。SET列则允许在列中保存一组已定义值中的一个或多个值。有时很容易混淆。这里有一个例子：如果这里真和假两种情况不会同时出现，那么毫无疑问应该使用ENUM列而不是SET列。\nNULL不是虚拟值 我们之前说过避免使用NULL的好处，并且建议尽可能考虑其他选择。即使需要在表中存储事实上的“空值”，也可能不需要使用NULL。也许可以使用0、特殊值或空字符串作为代替。但是遵循这个原则也不要走极端。当需要表示未知值时，不要太害怕使用NULL。在某些情况下，使用NULL比使用某个虚拟常数更好。从受约束类型的域中选择一个值，例如使用-1表示一个未知的整数，可能会使代码复杂化，容易引入bug，并通常会把事情搞得一团糟。处理NULL并不容易，但通常比其他替代方案更好\n小结  尽量避免在设计中出现极端情况，例如，强制执行非常复杂的查询或者包含很多列的表设计（很多的意思是介于有点多和非常多之间）。 使用小的、简单的、适当的数据类型，并避免使用NULL，除非确实是对真实数据进行建模的正确方法。 尝试使用相同的数据类型来存储相似或相关的值，尤其是在联接条件中使用这些值时。 注意可变长度字符串，它可能会导致临时表和排序的全长内存分配不乐观。 如果可能的话，尝试使用整数作为标识符。 避免使用一些传统的MySQL技巧，例如，指定浮点数的精度或整数的显示宽度。 小心使用ENUM和SET类型。它们很方便，但也可能被滥用，有时还很棘手。另外最好避免使用BIT类型。  创建高性能的索引 索引，在MySQL中也叫作键(key)，是存储引擎用于快速找到记录的一种数据结构。本章将讨论索引的一些有用的特性。要想获得好的性能，索引至关重要。尤其是当表中的数据量越来越大时，索引对性能的影响愈发重要。在数据量较小且负载较低时，缺少合适的索引对性能的影响可能还不明显，但当数据量逐渐增大时，性能会急剧下降。不过，索引却经常被忽略，有时候甚至被误解，所以在实际案例中，经常会遇到由糟糕索引导致的问题。这也是为什么我们把本章放在了全书靠前的位置，甚至比查询优化还要靠前。索引优化应该是对查询性能优化最有效的手段了。索引能够轻易将查询性能提高几个数量级，“最优”的索引有时比一个“好的”索引性能要好两个数量级。创建一个真正“最优”的索引经常需要重写查询\nInnoDB存储引擎有一个被称为自适应哈希索引的特性。当InnoDB发现某些索引值被非常频繁地被访问时，它会在原有的B-tree索引之上，在内存中再构建一个哈希索引。这就让B-tree索引也具备了一些哈希索引的优势，例如，可以实现非常快速的哈希查找。这个过程是完全自动化的，用户无法进行控制或者配置。不过，可以通过参数彻底关闭自适应哈希索引这个特性\n 全值匹配 全值匹配指的是和索引中的所有列匹配，例如，前面提到的索引可用于查找姓名为Cuba Allen、出生于1960-01-01的人。 匹配最左前缀例如，前面提到的索引可用于查找所有姓为Allen的人，即只使用索引的第一列。 匹配列前缀 也可以只匹配某一列的值的开头部分。例如，前面提到的索引可用于查找所有姓以J开头的人。这里也只使用了索引的第一列。 匹配范围值例如，前面提到的索引可用于查找姓在Allen和Barrymore之间的人。这里也只使用了索引的第一列。精确匹配某一列而范围匹配另外一列前面提到的索引也可用于查找所有姓为Allen，并且名字是字母K开头（比如Kim、Karl等）的人，即第一列last_name是全匹配，第二列first_name是范围匹配。 只访问索引的查询 B-tree索引通常可以支持“只访问索引的查询”，即查询只需要访问索引，而无须访问数据行。后面我们将单独讨论这种“覆盖索引”的优化。因为索引树中的节点是有序的，所以除了按值查找，索引还可以用于查询中的ORDER BY操作（按顺序查找）。 一般来说，如果B-tree可以按照某种方式查找到值，那么也可以按照这种方式去排序。所以，如果ORDER BY子句满足前面列出的几种查询类型，则这个索引也可以用于这类排序场景。下面是一些关于B-tree索引的限制。  不能使用索引  如果不是按照索引的最左列开始查找，则无法使用索引。例如，上面例子中的索引无法用于查找名字为Bill的人，也无法查找某个特定生日的人，因为这两列都不是最左数据列。类似地，也无法查找姓氏以某个字母结尾的人。 不能跳过索引中的列。也就是说，前面所述的索引无法用于查找姓为Smith并且在某个特定日期出生的人。如果不指定名(first_name)，则MySQL只能使用索引的第一列。 如果查询中有某列的范围查询，则其右边所有列都无法使用索引优化查找。例如，有查询WHERE last_name=\u0026lsquo;Smith\u0026rsquo;AND first_name LIKE\u0026rsquo;J%\u0026lsquo;AND dob=\u0026lsquo;1976-12-23\u0026rsquo;，这个查询只能使用索引的前两列，因为这里LIKE是一个范围条件（不过，MySQL可以把其余列用于其他目的）。如果范围查询列值的数量有限，那么可以通过使用多个等于条件来代替范围条件。现在可以看到前面提到的索引列的顺序是多么重要：这些限制都和索引列的顺序有关。在优化性能的时候，可能需要使用相同的列但顺序不同的索引来满足不同类型的查询需求。也有些限制并不是B-tree本身导致的，而是MySQL查询优化器和存储引擎使用索引的方式导致的，这部分限制在未来的版本中可能就不再是限制了。  全文索引 FULLTEXT是一种特殊类型的索引，它查找的是文本中的关键词，而不是直接比较索引中的值。全文索引和其他几类索引的匹配方式完全不一样。它有许多需要注意的细节，如停用词、词干、复数、布尔搜索等。全文索引更类似于搜索引擎做的事情，而不是简单的WHERE条件匹配。在相同的列上同时创建全文索引和基于值的B-tree索引并不会有冲突，全文索引适用于MATCH AGAINST操作，而不是普通的WHERE条件操作。\n高性能的索引策略 前缀索引和索引的选择性 有时候为了提升索引的性能，同时也节省索引空间，可以只对字段的前一部分字符进行索引，这样做的缺点是，会降低索引的选择性。索引的选择性是指，不重复的索引值（也称为基数，cardinality）和数据表的记录总数(T)的比值，范围从1/T到1之间。索引的选择性越高则查询效率越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。\n一般情况下，列前缀的选择性也是足够高的，足以满足查询性能。对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL并不支持对这些列的完整内容进行索引。这里的关键点在于，既要选择足够长的前缀以保证较高的选择性，同时又不能太长（以便节约空间）。前缀应该足够长，以使得前缀索引的选择性接近于索引整列。换句话说，前缀的“基数”应该接近于完整列的“基数”。\n前缀索引是一种能使索引更小、更快的有效办法，但它也有缺点：MySQL无法使用前缀索引做ORDER BY和GROUP BY操作，也无法使用前缀索引做覆盖扫描。\n多列索引 很多人对多列索引的理解都不够。一个常见的错误就是，为每列创建独立的索引，或者按照错误的顺序创建多列索引。\n在多列上独立地创建多个单列索引，在大部分情况下并不能提高MySQL的查询性能。MySQL引入了一种叫**“索引合并”(index merge)的策略**，它在一定程度上可以使用表中的多个单列索引来定位指定的行。在这种情况下，查询能够同时使用两个单列索引进行扫描，并将结果进行合并。这种算法有三个变种：OR条件的联合(union)，AND条件的相交(intersection)，组合前两种情况的联合及相交。MySQL会使用这类技术来优化复杂查询，所以在某些语句的Extra列中还可以看到嵌套操作。索引合并策略有时候效果非常不错，但更多的时候，它说明了表中的索引建得很糟糕：\n 当优化器需要对多个索引做相交（相交操作是使用“索引合并”的一种情况，另一种是做联合操作）操作时（通常有多个AND条件），通常意味着需要一个包含所有相关列的多列索引，而不是多个独立的单列索引。 当优化器需要对多个索引做联合操作时（通常有多个OR条件），通常需要在算法的缓存、排序和合并操作上耗费大量CPU和内存资源，尤其是当其中有些索引的选择性不高，需要合并扫描返回的大量数据的时候。 更重要的是，优化器不会把这些操作计算到“查询成本”(cost)中，优化器只关心随机页面读取。这会使得查询的成本被“低估”，导致该执行计划还不如直接进行全表扫描。这样做不但会消耗更多的CPU和内存资源，还可能会影响并发的查询，但如果单独运行这样的查询则往往会忽略对并发性的影响。通常来说，使用UNION改写查询，往往是最好的办法。  如果在EXPLAIN中看到有索引合并，那么就应该好好检查一下查询语句的写法和表的结构，看是不是已经是最优的。也可以通过参数optimizer_switch来关闭索引合并功能，还可以使用IGNORE INDEX语法让优化器强制忽略掉某些索引，从而避免优化器使用包含索引合并的执行计划。\n选择合适的索引列顺序 最容易让人感到困惑的问题之一就是索引列的顺序。正确的顺序依赖于使用该索引的查询语句，同时还需要考虑如何更好地满足排序和分组操作的需要。\n对于如何选择索引的列顺序有一个重要的经验法则：将选择性最高的列放到索引最前列。这个建议准确吗？在很多场景中可能有帮助，但是要全面地考虑各种场景的话，考虑如何避免大量随机I/O和排序可能更重要。（场景不同则选择不同，没有一个放之四海皆准的法则。这里只是说明，这个经验法则可能没有你想象中那么重要。）\n当不需要考虑排序和分组时，将选择性最高的列放在前面通常是很好的。这时索引的作用只是优化查询语句中的WHERE条件。在这种情况下，按这个原则设计的索引确实能够最快地过滤出需要的行，对于在WHERE子句中只使用了索引部分前缀列的查询来说，选择性也更高。然而，性能不只依赖于所有索引列的选择性（整体基数），也和查询条件的具体值有关，也就是和值的分布有关。这和前面介绍的选择前缀的长度需要考虑的因素一样。可能需要根据那些运行频率最高的查询来调整索引列的顺序，让这种情况下索引的选择性最高。\n聚簇索引 聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。具体的细节依赖于其实现方式，但InnoDB的聚簇索引实际上在同一个结构中保存了B-tree索引和数据行。当表有聚簇索引时，它的数据行实际上存放在索引的叶子页(leaf page)中。术语“聚簇”表示数据行和相邻的键值紧凑地存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引（不过，覆盖索引可以模拟多个聚簇索引的情况，本章后面将详细介绍）。因为是存储引擎负责实现索引，因此，不是所有的存储引擎都支持聚簇索引。本节我们主要关注InnoDB，但是这里讨论的原理对于任何支持聚簇索引的存储引擎都是适用的。\n如果你没有定义主键，InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。这样做的缺点在于，所有需要使用这种隐藏主键的表都依赖一个单点的“自增值”，这可能会导致非常高的锁竞争，从而出现性能问题。\n聚集的数据有一些重要的优点：\n 你可以把相互关联的数据保存在一起。例如，在实现电子邮箱应用时，可以根据用户ID来聚集数据，这样只需要从磁盘读取少数的数据页就能获取某个用户的全部邮件。如果没有使用聚簇索引，则每封邮件都可能导致一次磁盘I/O。 数据访问更快。聚簇索引将索引和数据保存在同一个B-tree中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找要快。 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。如果在设计表和查询时能充分利用上面的优点，那么就能极大地提升性能。  同时，聚簇索引也有一些缺点：\n 聚簇数据最大限度地提高了I/O密集型应用的性能，但如果数据全部都放在内存中，则访问的顺序就没那么重要了，聚簇索引也就没什么优势了。 插入速度严重依赖于插入顺序。按照主键的顺序插入行是将数据加载到InnoDB表中最快的方式。但如果不是按照主键的顺序加载数据，那么在加载完成后最好使用OPTIMIZE TABLE命令重新组织一下表。 更新聚簇索引列的代价很高，因为它会强制InnoDB将每个被更新的行移动到新的位置。 基于聚簇索引的表在插入新行，或者主键被更新导致需要移动行的时候，可能面临页分裂(page split)的问题。当行的主键值要求必须将这一行插入某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次页分裂操作。页分裂会导致表占用更多的磁盘空间。 聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。 二级索引（非聚簇索引）可能比想象中的要更大，因为二级索引的叶子节点包含了引用行的主键列。 二级索引访问需要两次索引查找，而不是一次。为什么二级索引需要两次索引查找？答案是，二级索引中保存的是“行指针”。要记住，**二级索引叶子节点保存的不是指向行的物理位置的指针，而是行的主键值。这意味着通过二级索引查找行，存储引擎需要找到二级索引的叶子节点，以获得对应的主键值，然后根据这个值去聚簇索引中查找对应的行。**这里做了双倍工作：两次B-tree查找而不是一次。对于InnoDB，自适应哈希索引（参考本章前面的“B-tree索引”一节）能够减少这样的重复工作。  InnoDB的数据分布 聚簇索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回滚指针，以及所有的剩余列。\nInnoDB的二级索引的叶子节点中存储的是主键值，并以此作为指向行的“指针”。这样的策略减少了当出现行移动或者数据页分裂时二级索引的维护工作。使用主键值作为指针会让二级索引占用更多的空间，换来的好处是，InnoDB在移动行时无须更新二级索引中的这个“指针”。\n使用顺序递增的主键 因为主键的值是顺序的，所以InnoDB把每一条记录都存储在上一条记录的后面。当达到页的最大填充因子时（InnoDB默认的最大填充因子是页大小的15/16，留出部分空间用于以后修改），下一条记录就会被写入新的页中。一旦数据按照这种顺序写入，主键页就会近似于被顺序的记录填满，这也正是所期望的结果（然而，二级索引页可能有所不同）。\n使用非顺序递增的主键 因为新写入的记录的主键值不一定比之前插入的大，所以InnoDB无法简单地总是把新记录插到索引的最后，而是需要为新记录寻找合适的位置——通常是已有数据的中间位置——并且分配空间。这会增加很多额外工作，并导致数据分布不够优化。下面是总结的一些缺点：\n 写入的目标页可能已经刷到磁盘上并从缓存中移除，或者还没有被加载到缓存中，InnoDB在插入之前不得不先找到，并从磁盘将目标页读取到内存中。这将导致大量的随机I/O。 因为写入是乱序的，所以InnoDB不得不频繁地做页分裂操作，以便为新记录分配空间。页分裂会导致移动大量数据，一次插入最少需要修改三个页而不是一个。 由于频繁的页分裂，页会变得稀疏并被不规则地填充，所以最终数据会有碎片。在把这些随机值载入聚簇索引后，最好做一次OPTIMIZE TABLE来重建表并优化页的填充情况。  使用InnoDB时应该尽可能地按主键顺序插入数据，并且尽可能地按照单调增加的聚簇键的值顺序插入新记录。\n什么时候按主键顺序插入反而会更糟 对于高并发的工作负载，在InnoDB中按主键顺序插入可能会造成明显的写入竞争。主键的上界会成为“热点”。因为所有的插入都发生在这里，所以并发插入可能导致间隙锁竞争。如果使用自增主键，还会有AUTO_INCREMENT锁机制；\n覆盖索引 大家通常都会根据查询的WHERE条件来创建合适的索引，不过这只是索引优化的一个方面。设计优秀的索引应该考虑到整个查询，而不单是WHERE条件部分。索引的确是一种高效找到数据的方式，但是如果MySQL还可以使用索引直接获取列的数据，这样就不再需要读取数据行了。如果索引的叶子节点中已经包含要查询的数据，那么还有什么必要再回表查询呢？如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为覆盖索引。需要注意的是，只有B-tree索引可以用于覆盖索引。\n覆盖索引是非常有用的工具，能够极大地提高性能。试想一下，如果查询只需要扫描索引而无须回表，会带来多少好处：\n 索引条目通常远小于数据行大小，所以如果只需要读取索引，那么MySQL就会极大地减少数据访问量。这对缓存型的应用负载非常重要，因为在这种情况下，响应时间大部分花费在数据拷贝上。覆盖索引对于I/O密集型的应用也有帮助，因为索引比数据更小，更容易全部放入内存中。 因为索引是按照列值的顺序存储的（至少在单页内如此），所以对于I/O密集型的范围查询会比随机从磁盘读取每一行数据的I/O要少得多。可以通过OPTIMIZE命令使得索引完全实现顺序排列，这让简单的范围查询能使用完全顺序的索引访问。 由于InnoDB的聚簇索引的特点，覆盖索引对InnoDB表特别有用。InnoDB的二级索引在叶子节点中保存了记录的主键值，所以如果二级索引能够覆盖查询，则可以避免对主键索引的二次查询。  使用索引扫描来做排序 MySQL有两种方式可以生成有序的结果：通过排序操作，或者按索引顺序扫描。如果在EXPLAIN的输出结果中，type列的值为“index”，则说明MySQL使用了索引扫描来做排序。\n扫描索引本身是很快的，因为只需要从一条索引记录移动到紧接着的下一条记录。但如果索引不能覆盖查询所需的全部列，那么就不得不每扫描一条索引记录都回表查询一次对应的记录。这基本上都是随机I/O，因此按索引顺序读取数据的速度通常要比顺序地全表扫描慢，尤其是在I/O密集型的应用负载上。\nMySQL可以使用同一个索引既满足排序，又用于查找行。因此，如果可能，设计索引时应该尽可能地同时满足这两项任务，这样是最好的。只有当索引的顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向（倒序或正序）都一样时，MySQL才能使用索引来对结果做排序。如果查询需要联接多张表，则只有当ORDER BY子句引用的字段全部在第一个表中时，才能使用索引做排序。ORDER BY子句和查找型查询的限制是一样的：需要满足索引的最左前缀的要求，否则，MySQL需要执行排序操作，而无法利用索引排序。\n有一种特殊情况，如果前导列为常量的时候，ORDER BY子句中的列也可以不满足索引的最左前缀的要求。如果在WHERE子句或者JOIN子句中将这些列指定为了常量，就可以“填补”索引字段的间隙了。\n-- 索引为idx_merch_id_shop_id(merch_id, shop_id)\rselect *\rfrom t_material\rwhere merch_id = '123456'\rorder by shop_id;\r 因为merch_id已经指定了，所以也会走索引。\n下面是一些不能使用索引做排序的查询：   查询使用了两种不同的排序方向，但是索引中的列都是按正序排序的。\n  查询的ORDER BY子句中，引用了一个不在索引中的列\n  查询的WHERE和ORDER BY中的列无法组合成索引的最左前缀\n  查询在索引列的第一列上是范围条件，所以MySQL无法使用索引的其余列\n-- 大于或者等于\rselect *\rfrom t_material\rwhere merch_id \u0026gt; '123456'\rorder by shop_id;\r-- in\rselect *\rfrom t_material\rwhere merch_id in ('123456', '1234567')\rorder by shop_id;\r-- or\rselect *\rfrom t_material\rwhere merch_id = '123456' or merch_id = '1234567'\rorder by shop_id;\r   冗余和重复索引 不幸的是，MySQL允许在相同列上创建多个相同的索引。虽然MySQL会抛出一个警告，但是并不会阻止你这么做。MySQL需要单独维护重复的索引，优化器在优化查询的时候也需要逐个地进行评估，这会影响性能，同时也浪费磁盘空间。重复索引是指在相同的列上按照相同顺序创建的相同类型的索引。应该避免创建这样的重复索引，发现以后应该立即移除。\n冗余索引和重复索引有一些不同。如果创建了索引(A，B)，再创建索引(A)就是冗余索引，因为这只是前一个索引的前缀索引，因此，索引(A，B)也可以当作索引(A)来使用（这种冗余只是对B-tree索引来说的）。但是如果再创建索引(B，A)，则不是冗余索引，索引(B)也不是，因为B不是索引(A，B)的最左前缀列。另外，如果新建的是其他不同类型的索引（例如，哈希索引或者全文索引），那么无论覆盖了哪些索引列，也不会是B-tree索引的冗余索引。\n表中的索引越多，插入的速度越慢。一般来说，增加新索引会导致INSERT、UPDATE、DELETE等操作的速度变慢，特别是当新增索引后达到了内存瓶颈的时候。\n解决冗余索引和重复索引的方法很简单，删除这些索引就可以了，但在删除或扩展索引的时候要非常小心。回忆一下，在前面的InnoDB的示例表中，因为二级索引的叶子节点包含了主键值，所以在列(A)上的索引就相当于在(A，ID)上的索引。如果有像WHERE A=5 ORDER BY ID这样的查询，这个索引会很有用。但如果将索引扩展为(A，B)，则实际上就变成了(A，B，ID)，那么上面查询的ORDERBY子句就无法使用该索引做排序，而只能用文件排序了。所以，建议使用Percona工具箱中的pt-upgrade工具来仔细检查计划中的索引变更。对于上述的两种情况，都可以考虑使用MySQL 8.0的不可见索引特性，而不是直接删除索引。要使用这个特性，可以通过ALTER TABLE语句，改变索引的一个标志位，使得优化器在确定执行计划时，忽略该索引。如果你发现计划删除的索引依旧有非常重要的作用，可以直接把索引改成可见，而不需要重新构建该索引。\n未使用的索引 除了冗余索引和重复索引，可能还会有一些服务器永远不用的索引。这样的索引完全是累赘，建议删除。\n找到未使用索引的最好办法就是使用系统数据库 performance_schema和sys。在sys数据库中，在table_io_waits_summary_by_index_usage视图中可以非常简单地知道哪些索引从来没有被使用过\n减少索引和数据的碎片 B-tree索引可能会产生碎片化，这会降低查询的效率。碎片化的索引可能会以很差或者无序的方式存储在磁盘上。根据设计，B-tree索引需要随机磁盘访问才能定位到叶子页，所以随机访问总是不可避免的。然而，如果叶子页在物理分布上是顺序且紧密的，那么查询的性能就会更好。否则，对于范围查询、索引覆盖扫描等操作来说，速度可能会降低很多；对于索引覆盖扫描，这一点会表现得更加明显。表的数据存储也可能发生碎片化。然而，数据存储的碎片化比索引更加复杂。有三种类型的数据碎片。\n 行碎片(Row fragmentation)这种碎片指的是数据行被存储在多个地方的多个片段中。即使查询只从索引中访问一行记录，行碎片也会导致性能下降。 行间碎片(Intra-row fragmentation)行间碎片是指逻辑上顺序的页或者行，在磁盘上不是顺序存储的。行间碎片对诸如全表扫描和聚簇索引扫描之类的操作有很大的影响，因为这些操作原本能够从磁盘上顺序存储的数据中获益。 剩余空间碎片(Free space fragmentation)剩余空间碎片是指数据页中有大量的空余空间。这会导致服务器读取大量不需要的数据，从而造成浪费。  可以通过执行OPTIMIZE TABLE或者导出再导入的方式来重新整理数据。这对多数存储引擎都是有效的。\n小结 在选择索引和编写利用这些索引的查询时，有如下三个原则始终需要记住：\n 单行访问是很慢的，特别是在机械硬盘中存储（SSD的随机I/O要快很多，不过这一点仍然成立）。如果服务器从存储中读取一个数据块只是为了获取其中一行，那么就浪费了很多工作。最好读取的块中能包含尽可能多的所需要的行。 按顺序访问范围数据是很快的，有两个原因。第一，顺序I/O不需要多次磁盘寻道，所以比随机I/O要快很多（特别是对于机械硬盘）。第二，如果服务器能够按需顺序读取数据，那么就不再需要额外的排序操作，并且GROUP BY查询也无须再做排序和将行按组进行聚合计算了。 索引覆盖查询是很快的。如果一个索引包含了查询需要的所有列，那么存储引擎就不需要再回表查找行。这避免了大量的单行访问，而上面的第一点已经写明单行访问是很慢的。  总的来说，编写查询语句时应该尽可能选择合适的索引以避免单行查找，尽可能地使用数据内部顺序从而避免额外的排序操作，并尽可能地使用索引覆盖查询。这与本章开头提到的Lahdenmaki和Leach的书中介绍的“三星”评价系统是一致的。\n那么如何判断一个系统创建的索引是合理的呢？一般来说，我们建议按响应时间来对查询进行分析。找出那些消耗最长时间的查询或者那些给服务器带来最大压力的查询，然后检查这些查询的schema、SQL语句和索引结构，判断是否有查询扫描了太多的行，是否做了很多额外的排序或者使用了临时表，是否使用随机I/O访问数据，或者是否有太多回表查询查询那些不在索引中的列的操作。如果一个查询无法从所有可能的索引中获益，则应该看看是否可以创建一个更合适的索引来提升性能。如果不行，还可以看看是否可以重写该查询，将其转化成一个能够高效利用现有索引或者新创建索引的查询。\n查询性能优化 在前面的章节中，我们介绍了如何设计最优的库表结构、如何建立最好的索引，这些对于提高性能来说是必不可少的。但这些还不够——还需要合理地设计查询。如果查询写得很糟糕，即使库表结构再合理、索引再合适，也无法实现高性能。查询优化、索引优化、库表结构优化需要齐头并进，一个不落。\n为什么查询速度会慢 在尝试编写快速的查询之前，需要清楚一点，真正重要的是响应时间。如果把查询看作一个任务，那么它由一系列子任务组成，每个子任务都会消耗一定的时间。如果要优化查询，实际上要优化其子任务，要么消除其中一些子任务，要么减少子任务的执行次数，要么让子任务运行得更快。通常来说，查询的生命周期大致可以按照如下顺序来看：从客户端到服务器，然后在服务器上进行语法解析，生成执行计划，执行，并给客户端返回结果。其中，“执行”可以被认为是整个生命周期中最重要的阶段，这其中包括大量为了检索数据对存储引擎的调用以及调用后的数据处理，包括排序、分组等。在完成这些任务的时候，查询需要在不同的地方花费时间，包括网络、CPU计算、生成统计信息和执行计划、锁等待（互斥等待）等操作，尤其是向底层存储引擎检索数据的调用操作，这些调用需要在内存操作、CPU操作和内存不足时导致的I/O操作上消耗时间。根据存储引擎不同，可能还会产生大量的上下文切换以及系统调用。优化查询的目的就是减少和消除这些操作所花费的时间。再次声明一点，对于一个查询的全部生命周期，上面列得并不完整。这里我们只是想说明：了解查询的生命周期和清楚查询的时间消耗情况对于优化查询有很大意义。\n慢查询基础：优化数据访问 一条查询，如果性能很差，最常见的原因是访问的数据太多。某些查询可能不可避免地需要筛选大量数据，但这并不常见。大部分性能低下的查询都可以通过减少访问的数据量的方式进行优化。对于低效的查询，我们发现通过下面两个步骤来分析总是很有效：\n  确认应用程序是否在检索大量且不必要的数据。这通常意味着访问了太多的行，但有时候也可能是访问了太多的列。\n  确认MySQL服务器层是否在分析大量不需要的数据行。\n  是否向数据库请求了不需要的数据 有些查询会请求超过实际需要的数据，然后这些多余的数据会被应用程序丢弃。这会给MySQL服务器带来额外的负担，并增加网络开销，另外，这也会消耗应用服务器的CPU和内存资源\n  查询了不需要的记录。先使用SELECT语句查询大量的结果，然后获取前面的N行后关闭结果集（例如，在新闻网站中取出100条记录，但是只是在页面上显示前面10条）。他们认为MySQL会执行查询，并只返回他们需要的10条数据，然后停止查询。实际情况是，MySQL会查询出全部的结果集，客户端的应用程序会接收全部的结果集数据，然后抛弃其中大部分数据。**最简单有效的解决方法就是在这样的查询后面加上LIMIT子句。**这个理解是有问题的，**mysql的查询本来就有这个问题-深分页问题，limit m n工作原理就是先读取前面m+n条记录，然后抛弃前m条，读后面n条想要的，所以m越大，偏移量越大，性能就越差。**具体可以看https://juejin.cn/post/7012016858379321358\n  多表联接时返回全部列\n  总是取出全部列，每次看到SELECT*的时候都需要用怀疑的眼光审视，是不是真的需要返回全部的列，很可能不是必需的。取出全部列，会让优化器无法完成索引覆盖扫描这类优化，还会为服务器带来额外的I/O、内存和CPU的消耗。\n  重复查询相同的数据\n  MySQL是否在扫描额外的记录 在确定查询只返回需要的数据以后，接下来应该看看查询为了返回结果是否扫描了过多的数据。对于MySQL，最简单的衡量查询开销的三个指标如下：\n 响应时间 扫描的行数 返回的行数  没有哪个指标能够完美地衡量查询的开销，但它们大致反映了MySQL在内部执行查询时需要访问多少数据，并可以大概推算出查询运行的时间。这三个指标都会被记录到MySQL的慢日志中，所以检查慢日志记录是找出扫描行数过多的查询的好办法。\n响应时间 响应时间只是一个表面上的值。响应时间是两部分之和：服务时间和排队时间。服务时间是指数据库处理这个查询真正花了多长时间。排队时间是指服务器因为等待某些资源而没有真正执行查询的时间——可能是等I/O操作完成，也可能是等待行锁，等等。遗憾的是，我们无法把响应时间细分到上面这些部分，除非有什么办法能够逐个测量这些消耗，这很难做到。最常见和重要的是I/O等待和锁等待，但是实际情况更加复杂。实际上，I/O等待和锁等待非常重要，因为它们对于性能有着至关重要的影响。所以在不同类型的应用压力下，响应时间并没有一致的规律或者公式。诸如存储引擎的锁（表锁、行锁）、高并发资源竞争、硬件响应等诸多因素都会影响响应时间。所以，响应时间既可能是一个问题的结果也可能是一个问题的原因，不同案例情况不同。\n当你看到一个查询的响应时间时，首先需要问问自己，这个响应时间是否是一个合理的值。实际上可以使用“快速上限估计”法来估算查询的响应时间。概括地说，了解这个查询需要哪些索引以及它的执行计划是什么，然后计算大概需要多少个顺序和随机I/O，再用其乘以在具体硬件条件下一次I/O的消耗时间。最后把这些消耗都加起来，就可以获得一个大概参考值来判断当前响应时间是不是一个合理的值。\n扫描的行数和返回的行数 分析查询时，查看该查询扫描的行数是非常有帮助的。这在一定程度上能够说明该查询找到需要的数据的效率高不高。对于找出那些“糟糕”的查询，这个指标可能还不够完美，因为并不是所有行的访问代价都是相同的。较短的行的访问速度更快，内存中的行比磁盘中的行的访问速度要快得多。理想情况下扫描的行数和返回的行数应该是相同的，但实际中这种“美事”并不多。例如，在做一个联接查询时，服务器必须要扫描多行才能生成结果集中的一行。扫描的行数与返回的行数的比率通常很低，一般在1：1到10：1之间，不过有时候这个值也可能非常非常大。\n扫描的行数和访问类型 在评估查询开销的时候，需要考虑从表中找到某一行数据的成本。MySQL有好几种访问方式可以查找并返回一行结果。有些访问方式可能需要扫描很多行才能返回一行结果，也有些访问方式可能无须扫描就能返回结果。EXPLAIN语句中的type列反映了访问类型。访问类型有很多种，从全表扫描到索引扫描、范围扫描、唯一索引查询、常数引用等。这里列出的这些，速度从慢到快，扫描的行数从多到少。你不需要记住这些访问类型，但需要明白扫描表、扫描索引、范围访问和单值访问的概念。如果你没办法找到合适的访问类型，那么最好的解决办法通常就是增加一个合适的索引，这也正是我们前一章讨论过的问题。现在应该明白为什么索引对于查询优化如此重要了吧。索引让MySQL以最高效、扫描行数最少的方式找到需要的记录。\n一般地，MySQL能够使用如下三种方式应用WHERE条件，从好到坏依次为：\n 在索引中使用WHERE条件来过滤不匹配的记录。这是在存储引擎层完成的。 使用索引覆盖扫描（在Extra列中出现了Using index）来返回记录，直接从索引中过滤不需要的记录并返回命中的结果。这是在MySQL服务器层完成的，但无须再回表查询记录。 从数据表中返回数据，然后过滤不满足条件的记录（在Extra列中出现Usingwhere）。这在MySQL服务器层完成，MySQL需要先从数据表中读出记录然后过滤。  如果发现查询需要扫描大量的数据但只返回少数行，那么通常可以尝试下面的技巧去优化它：\n 使用索引覆盖扫描，把所有需要用的列都放到索引中，这样存储引擎无须回表获取对应行就可以返回结果了 改变库表结构。例如，使用单独的汇总表。 重写这个复杂的查询，让MySQL优化器能够以更优化的方式执行这个查询  重构查询的方式 在优化有问题的查询时，目标应该是找到获得实际需要的结果的替代方法——但这并不一定意味着从MySQL返回完全相同的结果集。有时候，可以将查询转换为返回相同结果的等价形式，以获得更好的性能。但是，如果可以获得更好的效率，还应该考虑重写查询以检索不同的结果。通过修改应用代码和查询，最终达到一样的目的。\n一个复杂查询还是多个简单查询 设计查询的时候，一个需要考虑的重要问题是，是否需要将一个复杂的查询分成多个简单的查询。在传统实现中，总是强调需要数据库层完成尽可能多的工作，这样做的逻辑在于以前人们总是认为网络通信、查询解析和优化是一件代价很高的事情。但是这样的想法对于MySQL并不适用，因为MySQL从设计上让连接和断开连接都很轻量，在返回一个小的查询结果方面很高效。现代的网络速度比以前要快很多，能在很大程度上降低延迟。在某些版本的MySQL中，即使在一台通用服务器上，也能够运行每秒超过10万次的简单查询，即使是一个千兆网卡也能轻松满足每秒超过2000次的查询。所以运行多个小查询现在已经不是大问题了。在MySQL内部，每秒能够扫描内存中上百万行的数据，相比之下，MySQL响应数据给客户端就慢得多了。在其他条件都相同的时候，使用尽可能少的查询当然是更好的。但是有时候，将一个大查询分解为多个小查询是很有必要的。\n切分查询 有时候对于一个大查询，我们需要“分而治之”，将大查询切分成小查询，每个查询的功能完全一样，只完成一小部分，每次只返回一小部分查询结果。\n删除旧的数据就是一个很好的例子。定期清除大量数据时，如果用一个大的语句一次性完成的话，则可能需要一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。将一个大的DELETE语句切分成多个较小的查询可以尽可能小地影响MySQL的性能，同时还可以降低MySQL复制的延迟。\n分解联接查询 很多高性能的应用都会对联接查询进行分解。简单地说，可以对每一个表进行一次单表查询，然后将结果在应用程序中进行联接。\n分解为\n用分解联接查询的方式重构查询有如下优势：\n 让缓存的效率更高。许多应用程序可以方便地缓存单表查询对应的结果对象。例如，上面查询中的tag mysql已经被缓存了，那么应用就可以跳过第一个查询。再例如，应用中已经缓存了ID为123、567、9098的内容，那么第三个查询的IN()中就可以少几个ID。 将查询分解后，执行单个查询可以减少锁的竞争。 在应用层做联接，可以更容易对数据库进行拆分，更容易做到高性能和可扩展。 查询本身的效率也可能会有所提升。在这个例子中，使用IN()代替联接查询，可以让MySQL按照ID顺序进行查询，这可能比随机的联接要更高效。 可以减少对冗余记录的访问。在应用层做联接查询，意味着对于某条记录应用只需要查询一次，而在数据库中做联接查询，则可能需要重复地访问一部分数据。从这点看，这样的重构还可能会减少网络和内存的消耗。  在有些场景下，在应用程序中执行联接操作会更加有效。比如，当可以缓存和重用之前查询结果中的数据时、当在多台服务器上分发数据时、当能够使用IN()列表替代联接查询大型表时、当一次联接查询中多次引用同一张表时。\n查询执行的基础 当希望MySQL能够以更高的性能运行查询时，最好的办法就是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，很多查询优化工作实际上就是遵循一些原则让优化器能够按照预想的合理的方式运行。\n当向MySQL发送一个请求的时候，MySQL到底做了些什么：\n 客户端给服务器发送一条SQL查询语句。 服务器端进行SQL语句解析、预处理，再由优化器生成对应的执行计划。 MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询。 将结果返回给客户端。  MySQL的客户端/服务器通信协议 一般来说，不需要去理解MySQL通信协议的内部实现细节，只需要大致理解通信协议是如何工作的。MySQL的客户端和服务器之间的通信协议是“半双工”的，这意味着，在任何时刻，要么是由服务器向客户端发送数据，要么是由客户端向服务器发送数据，这两个动作不能同时发生。所以，我们无法也无须将一个消息切成小块来独立发送。这种协议让MySQL通信变得简单快速，但是也从很多地方限制了MySQL。一个明显的限制是，这意味着没法进行流量控制。一旦一端开始发送消息，另一端要接收完整个消息才能响应它。这就像来回抛球的游戏：在任何时刻，只有一个人能控制球，而且只有控制球的人才能将球抛回去（发送消息）。客户端用一个单独的数据包将查询传给服务器。这也是为什么当查询的语句很长的时候，参数max_allowed_packet就特别重要了。一旦客户端发送了请求，它能做的事情就只是等待结果了。然而，一般的服务器响应给用户的数据通常很多，由多个数据包组成。当服务器开始响应客户端请求时，客户端必须完整地接收整个返回结果，而不能简单地只取前面几条结果，然后让服务器停止发送数据。在这种情况下，客户端若接收完整的结果，然后取前面几条需要的结果，或者接收完几条结果后就“粗暴”地断开连接，都不是好主意。这也是在必要的时候一定要在查询中加上LIMIT限制的原因。\n换一种方式解释这种行为：当客户端从服务器取数据时，看起来是一个拉数据的过程，但实际上是MySQL在向客户端推送数据的过程。客户端不断地接收从服务器推送的数据，客户端也没法让服务器停下来。客户端像是“从消防水管喝水”。多数连接MySQL的库函数都可以获得全部结果集并将结果缓存到内存里，还可以逐行获取需要的数据。默认一般是获得全部结果集并将它们缓存到内存中。MySQL通常需要等所有的数据都已经发送给客户端才能释放这条查询所占用的资源，所以接收全部结果并缓存通常可以减少服务器的压力，让查询能够早点结束、早点释放相应的资源。当使用多数连接MySQL的库函数从MySQL获取数据时，其结果看起来都像是从MySQL服务器获取数据，而实际上都是从这个库函数的缓存获取数据。多数情况下这没什么问题，但是在需要返回一个很大的结果集的时候，这样做并不好，因为库函数会花很多时间和内存来存储所有的结果集。如果能够尽早开始处理这些结果集，就能大大减少内存的消耗，在这种情况下可以不使用缓存来记录结果而是直接处理。这样做的缺点是，对于服务器来说，需要查询完成后才能释放资源，所以在和客户端交互的整个过程中，服务器的资源都是被这个查询所占用的。\n查询状态 对于一个MySQL连接，或者一个线程，任何时刻都有一个状态，该状态表示了MySQL当前正在做什么。有很多种方式能查看当前的状态，最简单的是使用SHOWFULL PROCESSLIST命令（该命令返回结果中的Command列，其就表示当前的状态）。在一个查询的生命周期中，状态会变化很多次。MySQL官方手册中对这些状态值的含义有最权威的解释，下面将这些状态列出来，并做一个简单的解释。\n  Sleep 线程正在等待客户端发送新的请求。\n  Query 线程正在执行查询或者正在将结果发送给客户端。\n  Locked 在MySQL服务器层，该线程正在等待表锁。在存储引擎级别实现的锁，例如 InnoDB的行锁，并不会体现在线程状态中。\n  Analyzing and statistics 线程正在检查存储引擎的统计信息，并优化查询。\n  Copying to tmp table [on disk] 线程正在执行查询，并且将其结果集复制到一个临时表中，这种状态一般要么是在做GROUP BY操作，要么是在进行文件排序操作，或者是在进行UNION操作。如果这个状态后面还有“on disk”标记，那表示MySQL正在将一个内存临时表放到磁盘上。\n  Sorting result 线程正在对结果集进行排序。\n  了解这些状态的基本含义非常有用，这可以让你很快地了解当前“谁正在持球”。在一个繁忙的服务器上，可能会看到大量的不正常的状态，例如，statistics正占用大量的时间。这通常表示，某个地方有异常了。\n查询优化处理 查询的生命周期的下一步是将一个SQL查询转换成一个执行计划，MySQL再依照这个执行计划和存储引擎进行交互。这包括多个子阶段：解析SQL、预处理、优化SQL执行计划。这个过程中产生的任何错误（例如，语法错误）都可能终止查询。这里不打算详细介绍MySQL的内部实现，而只是选择性地介绍其中几个独立的部分，在实际执行中，这几部分可能一起执行也可能单独执行。\n语法解析器和预处理  首先，MySQL通过关键字将SQL语句进行解析，并生成一棵对应的“解析树”。MySQL解析器将使用MySQL语法规则验证和解析查询。例如，它将验证是否使用了错误的关键字，使用关键字的顺序是否正确，或者它还会验证引号是否能前后正确匹配。 然后，预处理器检查生成的解析树，以查找解析器无法解析的其他语义，例如，这里将检查数据表和数据列是否存在，还会解析名字和别名，看看它们是否有歧义。 下一步预处理器会验证权限。这通常很快，除非服务器上有非常多的权限配置。  查询优化器 现在解析树被认为是合法的了，并且由优化器将其转化成查询执行计划。一条查询可以有很多种执行方式，最后都返回相同的结果。优化器的作用就是找到这其中最好的执行计划。MySQL使用基于成本的优化器，它将尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。最初，成本的最小单位是随机读取一个4KB数据页的成本，后来成本计算公式变得更加复杂，并且引入了一些“因子”来估算某些操作的代价，如执行一次WHERE条件比较的成本。可以通过查询当前会话的Last_query_cost的值来得知MySQL计算的当前查询的成本\nshow status like 'Last_query_cost';\r 这是根据一系列的统计信息计算得来的：每个表或者索引的页面个数、索引的基数（索引中不同值的数量）、索引和数据行的长度、索引分布情况。优化器在评估成本的时候并不考虑任何层面的缓存带来的影响，它假设读取任何数据都需要一次磁盘I/O。有很多种原因会导致MySQL优化器选择错误的执行计划，如下所示：\n 统计信息不准确。MySQL服务器依赖存储引擎提供的统计信息来评估成本，但是有的存储引擎提供的信息是准确的，有的偏差可能非常大。例如，InnoDB因为其MVCC的架构，并不能维护一个数据表的行数的精确统计信息。 成本指标并不完全等同于运行查询的实际成本，因此即使统计数据是准确的，查询的成本也可能超过或者低于MySQL估算的近似值。例如，有时候某个执行计划虽然需要读取更多的页面，但是它的成本却更低。因为如果这些页面都是顺序读或者这些页面都已经在内存中的话，那么它的访问成本将很低。MySQL并不知道哪些页面在内存中、哪些在磁盘中，所以查询在实际执行过程中到底需要多少次物理I/O是无法得知的。 MySQL的最优可能和你想的最优不一样。你可能希望执行时间尽可能短，但是MySQL只是基于其成本模型选择最优的执行计划，而有些时候这并不是最快的执行方式。所以，这里我们看到的根据执行成本来选择执行计划并不是完美的模型。 MySQL从不考虑其他并发执行的查询，这可能会影响到当前查询的速度。 MySQL也并不是任何时候都是基于成本的优化。它有时也会基于一些固定的规则，例如，如果存在全文搜索的MATCH()子句，则在存在FULLTEXT索引的时候就使用全文索引。即使有时候使用其他索引和WHERE条件可以远比这种方式要快，MySQL也仍然会使用对应的全文索引。 MySQL不会考虑不受其控制的操作的成本，例如，执行存储函数或者用户自定义函数的成本。 优化器有时候无法估算所有可能的执行计划，所以它可能错过实际上最优的执行计划。  MySQL的查询优化器是一个非常复杂的软件，它使用了很多优化策略来生成一个最优的执行计划。优化策略可以简单地分为两种，一种是静态优化，一种是动态优化。静态优化可以直接对解析树进行分析，并完成优化。例如，优化器可以通过一些简单的代数变换将WHERE条件转换成另一种等价形式。静态优化不依赖于特别的数值，如WHERE条件中带入的一些常数等。静态优化在第一次完成后就一直有效，即使使用不同的参数重复执行查询也不会发生变化，可以认为这是一种“编译时优化”。然而，动态优化则和查询的上下文有关，也可能和很多其他因素有关，例如WHERE条件中的取值、索引中条目对应的数据行数等。这需要在每次查询的时候都重新评估，可以认为这是“运行时优化”。在执行绑定变量和存储过程的时候，动态优化和静态优化的区别非常重要。MySQL对查询的静态优化只需要做一次，但对查询的动态优化则在每次执行时都需要重新评估。有时候甚至在查询的执行过程中也会重新优化。下面是一些MySQL能够处理的优化类型。\n  重新定义联接表的顺序，数据表的联接并不总是按照在查询中指定的顺序进行。决定联接的顺序是优化器很重要的一个功能。\n  将外联接转化成内联接，并不是所有的OUTER JOIN语句都必须以外联接的方式执行。诸多因素，例如WHERE条件、库表结构都可能会让外联接等价于一个内联接。MySQL能够识别这一点并重写查询，让其可以调整联接顺序。\n  使用代数等价变换规则，MySQL可以使用一些代数等价变换规则来简化并规范表达式。它可以合并和减少一些比较，还可以移除一些恒成立和一些恒不成立的判断。例如，(5=5 AND a\u0026gt;5)将被改写为a\u0026gt;5）这些规则对于编写条件语句很有用。\n  优化COUNT()、MIN()和MAX()，索引和列是否可为空通常可以帮助MySQL优化这类表达式。例如，要找到某一列的最小值，只需要查询对应B-tree索引最左端的记录，MySQL可以直接获取索引的第一行记录。在优化器生成执行计划的时候就可以利用这一点，在B-tree索引中，优化器会将这个表达式作为一个常数对待。类似地，如果要查找一个最大值，也只需读取B-tree索引的最后一条记录。如果MySQL使用了这种类型的优化，那么在EXPLAIN中就可以看到“Select tables optimized away”。从字面意思可以看出，它表示优化器已经从执行计划中移除了该表，并以一个常数代替。\n  预估并转化为常数表达式， 当MySQL检测到一个表达式可以转化为常数的时候，就会一直把该表达式作为常数进行优化处理。\n  覆盖索引扫描，当索引中的列包含所有查询中需要使用的列的时候，MySQL就可以使用索引返回需要的数据，而无须查询对应的数据行。\n  子查询优化，MySQL在某些情况下可以将子查询转换为一种效率更高的形式，从而减少多个查询多次对数据进行访问。\n  提前终止查询，在发现已经满足查询需求的时候，MySQL总是能够立刻终止查询。一个典型的例子就是当使用了LIMIT子句的时候。除此之外，MySQL在其他几类情况下也会提前终止查询，例如发现了一个不成立的条件，这时MySQL可以立刻返回一个空结果。除此之外，MySQL在执行过程中，如果发现某些特殊的条件，则会提前终止查询。当查询执行引擎需要检索“不同取值”或者判断存在性的时候，MySQL都可以使用这类优化。例如，我们现在需要找到没有演员的所有电影：这个查询将会过滤掉所有有演员的电影。每一部电影可能都会有很多的演员，但是上面的查询一旦找到任何一个演员，就会停止并立刻判断下一部电影，因为只要有一名演员，那么WHERE条件就会过滤掉这部电影。类似这种“不同值/不存在”的优化一般可用于DISTINCT、NOT EXIST()或者LEFT JOIN类型的查询。\n  等值传播，如果两列的值可通过等式联接，那么MySQL能够把其中一列的WHERE条件传递到另一列上。例如，我们看下面的查询：\n因为这里使用了film_id字段进行等值联接，MySQL知道这里的WHERE子句不仅适用于film表，而且对于film_actor表同样适用。\n  列表IN()的比较,在很多数据库服务器中，IN()完全等同于多个OR条件的子句，因为这两者是完全等价的。在MySQL中这点是不成立的，MySQL将IN()列表中的数据先进行排序，然后通过二分查找的方式来确定列表中的值是否满足条件，这是一个O(logn)复杂度的操作，等价地转换成OR查询的复杂度为O(n)，对于IN()列表中有大量取值的时候，MySQL的处理速度将会更快。\n  上面列举的远不是MySQL优化器的全部，MySQL还会做大量其他的优化，但上面的这些例子已经足以让大家明白优化器的复杂性和智能性了。如果说从上面这段讨论中我们应该学到什么，那就是不要自以为比优化器更聪明。最终你可能会占一些便宜，但是有可能会使查询变得更加复杂而难以维护，而最终的收益为零。让优化器按照它的方式工作就可以了。当然，虽然优化器已经很智能了，但是有时候也无法给出最优的结果。有时候你可能比优化器更了解数据，例如，由于应用逻辑使得某些条件总是成立；还有时候，优化器缺少某种功能特性，如哈希索引；再如前面提到的，从优化器的执行成本角度评估出来的最优执行计划，在实际运行中可能比其他的执行计划更慢。如果能够确认优化器给出的不是最佳选择，并且清楚背后的原理，那么也可以帮助优化器做进一步的优化。例如，可以在查询中添加hint提示，也可以重写查询，或者重新设计更优的库表结构，或者添加更合适的索引。\nMySQL如何执行联接查询 ySQL中使用的术语“联接”（对应英文为Join）的范围可能比你熟悉的更广泛。总的来说，MySQL认为每一个查询都是联接——不仅是匹配两张表中对应行的查询，而是每一个查询、每一个片段（包括子查询，甚至基于单表的SELECT）都是联接。因此，理解MySQL如何执行联接查询是非常重要的。\n我们先来看一个UNION查询的例子。对于UNION查询，MySQL先将一系列的单个查询结果放到一个临时表中，然后再重新读出临时表中的数据来完成UNION查询。在MySQL的概念中，每个查询都是一次联接，所以读取临时表的结果也是一次联接。\n当前MySQL的联接执行策略很简单：MySQL对任何联接都执行嵌套循环联接操作，即MySQL先在一个表中循环取出单条数据，然后再嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为止。最后根据各个表匹配的行，返回查询中需要的各列。MySQL会尝试在最后一个联接表中找到所有匹配的行，如果最后一个联接表无法找到更多的行，MySQL返回到上一层次的联接表，看是否能够找到更多的匹配记录，依此类推，迭代执行。在MySQL 8.0.20版本之后，已经不再使用基于块的嵌套循环联接操作，取而代之的是哈希联接（参见链接33）。这让联接操作性能变得更好，特别是当数据集可以全部存储在内存时。\n执行计划 和很多其他关系数据库不同，MySQL并不会生成查询字节码来执行查询。MySQL生成查询的一棵指令树，然后通过查询执行引擎执行完成这棵指令树并返回结果。最终的执行计划包含了重构查询的全部信息。如果你对某个查询执行EXPLAINEXTENDED后，再执行SHOW WARNINGS，就可以看到重构出的查询。\n联接查询优化器 MySQL查询优化器最重要的一部分就是联接查询优化器，它决定了多个表联接时的顺序。通常多表联接的时候，可以有多种不同的联接顺序来获得相同的执行结果。联接查询优化器通过评估不同顺序时的成本来选择一个成本最低的联接顺序。下面的查询可以通过不同顺序的联接最后获得相同的结果：\n很容易看出，可以通过一些不同的执行计划来完成上面的查询。例如，MySQL可以从film表开始，使用film_actor表的索引film_id来查找对应的actor_id值，然后再根据actor表的主键找到对应的记录。Oracle用户会用下面的术语描述：“film表作为驱动表先查找file_actor表，然后以此结果为驱动表再查找actor表”。但mysql会自动优化查询，重新定义联接的顺序是优化器非常重要的一项功能。不过有的时候，优化器给出的并不是最优的联接顺序。这时可以使用STRAIGHT_JOIN关键字重写查询，让优化器按照你认为的最优的联接顺序执行，STRAIGHT_JOIN功能同inner join类似，但能让左边的表来驱动右边的表\n不过老实说，人的判断很难那么精准。绝大多数时候，优化器做出的选择都比普通人的判断要更准确。联接优化器会尝试在所有的联接顺序中选择一个成本最低的来生成执行计划树。如果可能，优化器会遍历每一个表，然后逐个做嵌套循环，计算执行每一棵可能的计划树的成本，最后返回一个最优的执行计划。不过，糟糕的是，n个表的联接可能有n的阶乘种联接顺序，我们称之为所有可能的查询计划的“搜索空间”。搜索空间的增长速度非常块，例如，若是10个表的联接，那么共有3628800种不同的联接顺序！当搜索空间非常大的时候，优化器不可能逐一评估每一种联接顺序的成本。这时，优化器选择使用“贪婪”搜索的方式查找“最优”的联接顺序。实际上，当需要联接的表超过optimizer_search_depth的限制的时候，就会选择“贪婪”搜索模式了（optimizer_search_depth参数可以根据需要指定大小）。在MySQL这些年的发展过程中，优化器积累了很多“启发式”的优化策略来加速执行计划的生成。在绝大多数情况下这都是有效的，但因为不会去计算每一种联接顺序的成本，所以偶尔也会选择不是最优的执行计划。有时查询不能重新排序，联接优化器可以利用这一点通过消除选择来减小搜索空间。左联接(LEFT JOIN)和相关子查询都是很好的例子（稍后将详细介绍子查询）。这是因为，一个表的结果依赖于另外一个表中检索的数据，这种依赖关系通常可以帮助联接优化器通过消除选择来减少搜索空间。\n排序优化 无论如何排序都是一个成本很高的操作，所以从性能角度考虑，应尽可能避免排序或者尽可能避免对大量数据进行排序。当不能使用索引生成排序结果的时候，MySQL需要自己进行排序，如果数据量小则在内存中进行，如果数据量大则需要使用磁盘，不过MySQL将这个过程统一称为文件排序(filesort)，即使完全是在内存中排序不需要任何磁盘文件时也是如此。\n如果需要排序的数据量小于“排序缓冲区”，MySQL使用内存进行快速排序操作。如果内存不够排序，那么MySQL会先将数据分块，对每个独立的块使用“快速排序”进行排序，并将各个块的排序结果存放在磁盘上，然后将各个排好序的块进行合并(merge)，最后返回排序结果。MySQL有如下两种排序算法。\n 两次传输排序（旧版本使用）读取行指针和需要排序的字段，对其进行排序，然后再根据排序结果读取所需要的数据行。这需要进行两次数据传输，即需要从数据表中读取两次数据，第二次读取数据的时候，因为是读取排序列进行排序后的所有记录，这会产生大量的随机I/O，所以两次传输排序的成本非常高。 单次传输排序（新版本使用）先读取查询所需要的所有列，然后再根据给定列进行排序，最后直接返回排序结果。因为不再需要从数据表中读取两次数据，对于I/O密集型的应用来说，这样做的效率高了很多。另外，相比两次传输排序，这个算法只需要一次顺序I/O就可读取所有的中每一行所需要的列，而不仅仅是进行排序操作所需要的列。这意味着更少的元组可以放入排序缓冲区，使得文件排序(filesort)操作必须执行更多的排序合并过程。  MySQL在进行文件排序时需要使用的临时存储空间可能会比想象的要大得多。原因在于MySQL在排序时，对每一个排序记录都会分配一个足够长的定长空间来存放。这个定长空间必须足够长才能容纳其中最长的字符串，例如，如果是VARCHAR列，则需要分配其完整长度；如果使用utf8mb4字符集，那么MySQL将会为每个字符预留4字节。我们曾经在一个库表结构设计不合理的案例中看到，排序消耗的临时空间比磁盘上的原表要大很多倍。在联接查询的时候如果需要排序，MySQL会分两种情况来处理这样的文件排序。如果ORDER BY子句中的所有列都来自联接的第一个表，那么MySQL在联接处理第一个表的时候就进行文件排序。如果是这样，那么在MySQL的EXPLAIN结果中可以看到Extra字段会有“Using filesort”字样。除此之外的所有情况，MySQL都会先将联接的结果存放到一个临时表中，然后在所有的联接都结束后，再进行文件排序。在这种情况下，在MySQL的EXPLAIN结果的Extra字段可以看到“Usingtemporary；Using filesort”字样。如果查询中有LIMIT的话，LIMIT也会在文件排序之后应用，所以即使需要返回较少的数据，临时表和需要排序的数据量仍然会非常大。\n查询执行引擎 在解析和优化阶段，MySQL将生成查询对应的执行计划，MySQL的查询执行引擎会根据这个执行计划来完成整个查询。这里的执行计划是一个数据结构，而不是和很多其他的关系数据库那样生成对应的可执行的字节码。\n相对于查询优化阶段，查询执行阶段不是那么复杂：MySQL只是简单地根据执行计划给出的指令逐步执行。在根据执行计划逐步执行的过程中，有大量的操作需要通过调用存储引擎实现的接口来完成，这些接口也就是我们称为“handler API”的接口。查询中的每一个表都由一个handler的实例表示。如果一个表在查询中出现了三次，服务器会创建三个handler对象。前面我们有意忽略了这一点，实际上，MySQL在优化阶段就为每个表创建了一个handler实例，优化器根据这些实例的接口可以获取表的相关信息，包括表的所有列名、索引统计信息，等等。\n存储引擎接口有着非常丰富的功能，但是底层接口却只有几十个，这些接口像“搭积木”一样能够完成查询的大部分操作。例如，有一个查询某个索引的第一行的接口，其还有查询某个索引条目的下一个条目的功能，有了这两个功能就可以完成全索引扫描的操作了。这种简单的接口模式，让MySQL的存储引擎的插件式架构成为可能，但是正如前面的讨论，这也给优化器带来了一定的限制。并不是所有的操作都由handler完成。例如，当MySQL需要进行表锁的时候。handler可能会实现自己的级别的、更细粒度的锁，如InnoDB就实现了自己的行级基本锁，但这并不能代替服务器层的表锁。正如我们在第1章所介绍的，如果是所有存储引擎共有的特性则由服务器层实现，比如时间和日期函数、视图、触发器等。\n将结果返回给客户端 执行查询的最后一个阶段是将结果返回给客户端。即使查询不需要给客户端返回结果集，MySQL仍然会返回这个查询的一些信息，如该查询影响到的行数。MySQL将结果集返回客户端是一个增量且逐步返回的过程。例如，我们回头看看前面的联接操作，一旦服务器处理完最后一个联接表，开始生成第一条结果时，MySQL就可以开始向客户端逐步返回结果集了。这样处理有两个好处：服务器端无须存储太多的结果，也就不会因为要返回太多结果而消耗太多内存。另外，这样的处理也可让MySQL客户端第一时间获得返回的结果。结果集中的每一行都会以一个满足MySQL客户端/服务器通信协议的封包发送，再通过TCP协议进行传输，在TCP传输的过程中，可能对MySQL的封包进行缓存，然后批量传输。\nMySQL查询优化器的局限性 MySQL所实现的查询执行方式并不是对每种查询都是最优的。不过还好，MySQL查询优化器只对少部分查询不适用，而且我们往往可以通过改写查询让MySQL高效地完成工作。\nUNION的限制 有时，MySQL无法将限制条件从UNION的外层“下推”到内层，这使得原本能够限制部分返回结果的条件无法应用到内层查询的优化上。如果希望UNION的各个子句能够根据LIMIT只取部分结果集，或者希望能够先排好序再合并结果集的话，就需要在UNION的各个子句中分别使用这些子句。例如，想将两个子查询结果联合起来，然后再取前20条记录，那么MySQL会将两个表存放到同一个临时表中，然后再取出前20行记录：\n(select mate_id from t_material order by mate_id desc)\runion\r(select mate_id from t_material_unit_rule order by mate_id desc)\rlimit 20;\r 这条查询将会把actor表中的200条记录和customer表中的599条记录存放在一个临时表中，然后再从临时表中取出前20条。可以通过在UNION的两个子查询中分别加上一个LIMIT 20来减少临时表中的数据：\n(select mate_id from t_material order by mate_id desc limit 20)\runion\r(select mate_id from t_material_unit_rule order by mate_id desc limit 20)\rorder by mate_id desc\rlimit 20;\r 现在临时表只包含40条记录了，除了考虑性能之外，在这里还需要注意一点：从临时表中取出数据的顺序并不是一定的，所以如果想获得正确的顺序，还需要在最后的LIMIT操作前加上一个全局的ORDER BY操作。\n等值传递 某些时候，等值传递会带来一些意想不到的额外消耗。例如，考虑一列上的巨大IN()列表，优化器知道它将等于其他表中的一些列，这是由于WHERE、ON或USING子句使列彼此相等。优化器通过将列表复制到所有相关表中的相应列来“共享”列表。通常，因为各个表新增了过滤条件，所以优化器可以更高效地从存储引擎过滤记录。但是如果这个列表非常大，则会导致优化和执行都会变慢。在写作本书的时候，除了修改MySQL源代码，目前还没有什么办法能够绕过该问题（不过这个问题很少会碰到）。\n并行执行 MySQL无法利用多核特性来并行执行查询。很多其他的关系数据库能够提供这个特性，但是MySQL做不到。这里特别指出是想告诉读者不要花时间去尝试寻找并行执行查询的方法。\n在同一个表中查询和更新 MySQL不允许对一张表同时进行查询和更新。\n优化特定类型的查询 这一节，我们将介绍如何优化特定类型的查询。在本书的其他部分会分散介绍这些优化技巧，不过这里将会汇总一下，以便参考和查阅。本节介绍的多数优化技巧都和特定的版本有关，所以对于未来MySQL的版本未必适用。毫无疑问，某一天优化器自己也会实现这里列出的部分或者全部优化技巧。\n优化COUNT()查询 COUNT()聚合函数，以及如何优化使用了该函数的查询，很可能是MySQL中最容易被误解的前10个话题之一。你在网上随便搜索一下就能看到很多错误的理解，可能比我们想象中的要多得多。在做优化之前，先来看看COUNT()函数真正的作用是什么。COUNT()是一个特殊的函数，有两种非常不同的作用：它可以统计某列的值的数量，也可以统计行数。在统计列值时要求列值是非空的（不统计NULL）。如果在COUNT()的括号中指定了列或者列的表达式，则统计的就是这个表达式有值的结果数。因为很多人对NULL理解有问题，所以这里很容易产生误解。如果你想了解更多关于SQL语句中NULL的含义，建议阅读一些关于SQL语句基础的书籍。（关于这个话题，互联网上的一些信息是不够准确的。）COUNT()的另一个作用是统计结果集的行数。当MySQL确认括号内的表达式值不可能为空时，实际上就是在统计行数。最简单的就是当我们使用COUNT( * )的时候，这种情况下通配符 * 并不会像我们猜想的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计所有的行数。我们发现最常见的错误之一是，当需要统计行数时，在COUNT()函数的括号内指定了列名。如果想要知道结果中的行数，应该始终使用COUNT(*)，这样可以更清晰地传达意图，避免糟糕的性能表现。\n  使用近似值\n有时候，某些业务场景并不要求完全精确的统计值，此时可以用近似值来代替。EXPLAIN出来的优化器估算的行数就是一个不错的近似值，执行EXPLAIN并不需要真正地去执行查询，所以成本很低。很多时候，计算精确值非常复杂，而计算近似值则非常简单。曾经有一个客户希望我们统计他的网站的当前活跃用户数是多少，这个活跃用户数保存在缓存中，过期时间为30分钟，所以每隔30分钟需要重新计算并放入缓存。这个活跃用户数本身就不是精确值，所以使用近似值代替是可以接受的。另外，如果要精确统计在线人数，使用WHERE条件会很复杂，一方面需要剔除当前非活跃用户，另一方面还要剔除系统中某些特定ID的“默认”用户，去掉这些约束条件对总数的影响很小，但却可能提升该查询的性能。更进一步的优化则可以尝试删除DISTINCT这样的约束来避免文件排序。这样重写过的查询比原来精确统计的查询快很多，而返回的结果则几乎相同。\n  更复杂的优化\n通常来说，COUNT()查询需要扫描大量的行（意味着要访问大量数据）才能获得精确的结果，因此是很难优化的。除了前面提到的方法，在MySQL层面还能做的就只有索引覆盖扫描了。如果这还不够，那就需要考虑修改应用的架构，可以增加类似Memcached这样的外部缓存系统。不过，可能很快你就会陷入一个熟悉的困境：“快速、精确和实现简单”。三者永远只能满足其二，必须舍掉一个。\n  优化联接查询 需要特别提到以下几点。\n 确保ON或者USING子句中的列上有索引。在创建索引的时候就要考虑到联接的顺序。当表A和表B用列c联接的时候，如果优化器的联接顺序是B、A，那么就不需要在B表的对应列上建索引。没有用到的索引只会带来额外的负担。一般来说，除非有其他理由，否则只需在联接顺序中的第二个表的相应列上创建索引。 确保任何GROUP BY和ORDER BY中的表达式只涉及一个表中的列，这样MySQL才有可能使用索引来优化这个过程。 当升级MySQL的时候需要注意：联接语法、运算符优先级等其他可能会发生变化的地方。因为以前是普通联接的地方可能会变成笛卡儿积，不同类型的联接可能会生成不同的结果，甚至会产生语法错误。  使用WITH ROLLUP优化GROUP BY 分组查询的一个变种就是要求MySQL对返回的分组结果再做一次超级聚合。可以使用WITH ROLLUP子句来实现这种逻辑，但可能优化得不够。可以通过EXPLAIN来观察其执行计划，特别要注意分组是否是通过文件排序或者临时表实现的。然后再去掉WITH ROLLUP子句来看执行计划是否相同。也可以通过本节前面介绍的优化器提示来强制执行计划。很多时候，如果可以，在应用程序中做超级聚合是更好的，虽然这需要给客户端返回更多的结果。也可以在FROM子句中嵌套使用子查询，或者是通过一个临时表存放中间数据，然后和临时表执行UNION来得到最终结果。最好的办法是尽可能地将WITH ROLLUP功能转移到应用程序中处理。\n优化LIMIT和OFFSET子句 在系统中需要进行分页操作的时候，我们通常会使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY子句。如果有对应的索引，通常效率会不错，否则，MySQL需要做大量的文件排序操作。一个非常常见又令人头疼的问题是，在偏移量非常大的时候，例如，可能是LIMIT 1000，20这样的查询，这时MySQL需要查询10020条记录然后只返回最后20条，前面10 000条记录都将被抛弃，这样的代价非常高。如果所有的页面被访问的频率都相同，那么这样的查询平均需要访问半个表的数据。要优化这种查询，要么是在页面中限制分页的数量，要么是优化大偏移量的性能。优化此类分页查询的一个最简单的办法就是尽可能地使用索引覆盖扫描，而不是查询所有的行。然后根据需要做一次联接操作再返回所需的列。在偏移量很大的时候，这样做的效率会有非常大的提升。考虑下面的查询：\n如果这个表非常大，那么这个查询最好改写成下面的样子：\n这种“延迟联接”之所以有效，是因为它允许服务器在不访问行的情况下检查索引中尽可能少的数据，然后，一旦找到所需的行，就将它们与整个表联接，以从该行中检索其他列。类似的技术也适用于带有LIMIT子句的联接。**有时候也可以将LIMIT查询转换为已知位置的查询，让MySQL通过范围扫描获得对应的结果。**例如，如果在一个位置列上有索引，并且预先计算出了边界值，上面的查询就可以改写为：\nLIMIT和OFFSET的问题，其实是OFFSET的问题，它会导致MySQL扫描大量不需要的行然后再抛弃掉。如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET。例如，若需要按照租借记录做翻页，那么可以根据最新一条租借记录向回追溯，这种做法可行是因为租借记录的主键是单调增长的。首先使用下面的查询获得第一组结果：\n假设上面的查询返回的是主键为16，049到16，030的租借记录，那么下一页查询就可以从16，030这个点开始：\n其他优化办法还包括使用预先计算的汇总表，或者联接到一个冗余表，冗余表只包含主键列和需要做排序的数据列。\n优化SQL CALC FOUND ROWS 分页的时候，另一个常用的技巧是在LIMIT语句中加上SQL_CALC_FOUND_ROWS提示(hint)，这样就可以获得去掉LIMIT以后满足条件的行数，因此可以作为分页的总数。看起来，MySQL做了一些非常“高深”的优化，像是通过某种方法预测了总行数。但实际上，MySQL只有在扫描了所有满足条件的行以后，才会知道行数，所以加上这个提示以后，不管是否需要，MySQL都会扫描所有满足条件的行，然后再抛弃掉不需要的行，而不是在满足LIMIT的行数后就终止扫描。所以该提示的代价可能非常高。一个更好的设计是将具体的页数换成“下一页”按钮，假设每页显示20条记录，那么我们每次查询时都是用LIMIT返回21条记录并只显示20条，如果第21条存在，那么就显示“下一页”按钮，否则就说明没有更多的数据，也就无须显示“下一页”按钮了。另一种做法是先获取并缓存较多的数据——例如，缓存1000条——然后每次分页都从这个缓存中获取。这样做可以让应用程序根据结果集的大小采取不同的策略，如果结果集小于1000，就可以在页面上显示所有的分页链接，因为数据都在缓存中，所以这样做不会对性能造成影响。如果结果集大于1000，则可以在页面上设计一个额外的“找到的结果多于1000条”之类的按钮。这两种策略都比每次生成全部结果集再抛弃不需要的数据的效率高很多。\n优化UNION查询 **MySQL总是通过创建并填充临时表的方式来执行UNION查询，因此很多优化策略在UNION查询中都没法很好地被使用。**经常需要手工地将WHERE、LIMIT、ORDER BY等子句“下推”到UNION的各个子查询中，以便优化器可以充分利用这些条件进行优化（例如，直接将这些子句冗余地写一份到各个子查询）。除非你确实需要服务器消除重复的行，否则一定要使用UNION ALL，这一点很重要。如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致对整个临时表的数据做唯一性检查。这样做的代价非常高。即使有ALL关键字，MySQL仍然会使用临时表存储结果。事实上，MySQL总是将结果放入临时表，然后再读出，再返回给客户端，虽然很多时候这样做是没有必要的（例如，MySQL可以直接把这些结果返回给客户端）。\n小结 如果把创建高性能应用程序比作一个环环相扣的“难题”，除了前面介绍的schema、索引和查询语句设计之外，查询优化应该是解开“难题”的最后一步。要想写一个好的查询，你必须理解schema设计、索引设计等，反之亦然。理解查询是如何被执行的以及时间都消耗在哪些地方，依然是前面我们介绍的响应时间的一部分。如果再加上一些诸如解析和优化过程的知识，就可以更进一步地理解我们在第7章中讨论的MySQL如何访问表和索引的内容了。这也可从另一个维度帮助你理解MySQL在访问表和索引时查询和索引的关系。优化通常需要三管齐下：不做、少做、快速地做。\n","id":0,"section":"posts","summary":"[TOC] 高性能Mysql 字段类型 整数类型 有两种类型的数字：整数(whole number)和实数（real number，带有小数部分的数字）。如果存","tags":["mysql"],"title":"高性能Mysql","uri":"https://wzgl998877.github.io/2024/09/%E9%AB%98%E6%80%A7%E8%83%BDmysql/","year":"2024"},{"content":"[TOC]\nSpring Cloud 学习 springcloud 自己觉得就是利用了spring可插拔的特性，引入了很多框架，解决了服务治理（负载均衡、熔断降级、配置管理等）的一套模板（框架），Spring Cloud Netflix 组件库和 Spring Cloud Alibaba 组件库这两个库是两种流行的方案，国内倾向于用阿里的这套，以下就是学习阿里这套的记录。\n配置中心、注册中心-Nacos 微服务间的内部调用 服务间的调用主要有两种方式：RPC 与 HTTP\n RPC：主要是dubbo HTTP：OpenFeign、Ribbon+RestTemplate  Ribbon+RestTemplate OpenFeign dubbo ","id":1,"section":"posts","summary":"[TOC] Spring Cloud 学习 springcloud 自己觉得就是利用了spring可插拔的特性，引入了很多框架，解决了服务治理（负载均衡、熔断降级、配置管理等）的一套模板（框架），","tags":["spring"],"title":"SpringCloud","uri":"https://wzgl998877.github.io/2022/11/springcloud/","year":"2022"},{"content":"[TOC]\n容器 容器- Container，容器其实是一种沙盒技术。沙盒就是能够像一个集装箱一样，把你的应用“装”起来的技术。这样，应用与应用之间，就因为有了边界而不至于相互干扰；而被装进集装箱的应用，也可以被方便地搬来搬去\nnamespace 命名空间。Namespace 是 Linux 内核的一项功能，该功能对内核资源(进程 ID、主机名、用户 ID、文件名、网络访问和进程间通信等相关资源)进行隔离，使得进程都可以在单独的命名空间中运行，并且只可以访问当前命名空间的资源。创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。容器，其实是一种特殊的进程而已。相当于给进程盖了一间小板房，这样就实现了系统全局资源和进程局部资源的隔离。\ncgroups 全称是 Linux Control Group， 用来实现对进程的 CPU、内存等资源的优先级和配额限制。限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。相当于给进程的小板房加了一个天花板。\nchroot chroot 它可以更改进程的根目录，也就是限制访问文件系统，创建一份完全独立的文件系统，相当于给进程的小板房铺上了地砖。\n创建一个容器  启用 Linux Namespace 配置。 设置指定的 Cgroups 参数。 切换进程的根目录（Change Root）。  与虚拟机对比 虚拟机的工作原理，名为 Hypervisor 的软件是虚拟机最主要的部分。它通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS。这样，用户的应用进程就可以运行在这个虚拟的机器中，它能看到的自然也只有 Guest OS 的文件和目录，以及这个机器里的虚拟设备。这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用。\n镜像 其实我们在其他场合中也曾经见到过“镜像”这个词，比如最常见的光盘镜像，重装电脑时使用的硬盘镜像，还有虚拟机系统镜像。这些“镜像”都有一些相同点：只读，不允许修改，以标准格式存储了一系列的文件，然后在需要的时候再从中提取出数据运行起来。 容器技术里的镜像也是同样的道理。容器的镜像封装了应用程序以及它运行所需要的所有依赖。它还有一个更为专业的名字，叫作：rootfs（根文件系统）：挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统。rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。同一台机器上的所有容器，都共享宿主机操作系统的内核\n 由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性。打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。\n 镜像是容器静态的定义，容器是镜像运行时的实体UnionFS 联合文件系统(Union File System)，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件：\n$ tree\r.\r├── A\r│ ├── a\r│ └── x\r└── B\r├── b\r└── x\r 然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录 C 上：\n$ mkdir C\r$ mount -t aufs -o dirs=./A:./B none ./C\r 这时，我再查看目录 C 的内容，就能看到目录 A 和 B 下的文件被合并到了一起：\n$ tree ./C\r./C\r├── a\r├── b\r└── x\r 文件联合系统是docker实现镜像的基本原理，太复杂了还没搞懂\nDocker Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u0026rsquo;s methodologies for shipping, testing, and deploying code, you can significantly reduce the delay between writing code and running it in production. Docker 是一个用于开发、发布和运行应用程序的开放平台。Docker 使您能够将应用程序与基础架构分离，以便快速交付软件。借助 Docker，您可以像管理应用程序一样管理基础架构。通过利用 Docker 的方法来交付、测试和部署代码，可以显著减少编写代码和在生产环境中运行代码之间的延迟。\nDocker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security lets you run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you don\u0026rsquo;t need to rely on what\u0026rsquo;s installed on the host. You can share containers while you work, and be sure that everyone you share with gets the same container that works in the same way. Docker 提供了在容器中打包和运行应用程序的功能。隔离和安全性允许您在给定主机上同时运行多个容器。容器是轻量级的，包含运行应用程序所需的一切，因此无需依赖主机上安装的内容。您可以在工作时共享容器，并确保与您共享的每个人都能获得以相同方式工作的相同容器。\nDocker是一种开源平台，用于开发、交付和运行应用程序。它利用容器化技术，使得开发者能够将应用程序及其所有依赖项打包到一个称为容器的独立单元中。这个容器包含了应用程序的代码、运行时、系统工具、库以及设置，确保应用程序能够在不同的环境中以一致的方式运行。Docker 是一种容器技术的实现，提供了工具和平台，使容器更易于创建和管理\nDocker架构  Docker client：客户端，用户使用的 Docker daemon：守护进程，负责构建、运行和分发Docker 容器等，管理docker容器的 Registry：镜像仓库  Docker 使用客户端-服务端架构 客户端与 Docker 守护程序通信，后者负责构建、运行和分发 Docker 容器的繁重工作。Docker 客户端和守护程序可以在同一系统上运行，也可以将 Docker 客户端连接到远程 Docker 守护程序。Docker 客户端和守护程序使用 REST API、UNIX 套接字或网络接口进行通信。\nDocker命令 Docker 提供了许多命令，用于管理容器、镜像、网络等方面的各种操作。以下是一些常用的 Docker 命令：\n  容器生命周期管理：\n docker run: 运行一个容器。 docker start: 启动已停止的容器。 docker stop: 停止一个运行中的容器。 docker restart: 重启一个容器。 docker pause: 暂停容器的所有进程。 docker unpause: 恢复容器的所有进程。 docker rm: 删除一个或多个容器。    容器信息查看：\n docker ps: 列出运行中的容器。 docker ps -a: 列出所有容器，包括停止的。 docker inspect: 查看容器详细信息。    镜像管理：\n docker images 或 docker image ls: 列出本地镜像。 docker pull: 从远程仓库拉取镜像。 docker rmi: 删除一个或多个本地镜像。    构建和提交镜像：\n docker build: 根据 Dockerfile 构建镜像。 docker commit: 提交容器为新的镜像。    容器日志和执行：\n docker logs: 查看容器的日志。 docker exec: 在运行中的容器中执行命令。    网络：\n docker network ls: 列出 Docker 网络。 docker network inspect: 查看网络详细信息。    其他常用命令：\n  docker version: 显示 Docker 版本信息。\n  docker info: 显示 Docker 系统信息。\n  docker-compose: 使用 Docker Compose 工具。\n    使用Docker运行一个应用 将一个应用运行在docker中，需要两步\n 构建docker镜像 使用docker运行  Docker中制作容器镜像使用的是Dockerfile\nDockerfile介绍 Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.\nDocker 可以通过读取“Dockerfile”中的指令自动构建镜像。 “Dockerfile”是一个文本文档，其中包含用户可以在命令行上调用来构建镜像的所有命令。Dockerfile 是用于构建 Docker 镜像的脚本文件,语法的简单介绍如下：\n  FROM： 指定基础镜像，即构建新镜像所基于的镜像。通常是操作系统或者包含运行时环境的基础镜像。\nDockerfileCopy code\rFROM openjdk:11\r   WORKDIR： 设置容器内的工作目录，即后续指令执行的基准目录。\nDockerfileCopy code\rWORKDIR /app\r   COPY 或 ADD： 将本地文件或目录复制到容器中。COPY 用于复制本地文件，而 ADD 不仅复制文件，还支持 URL 和解压缩。\nDockerfileCopy code\rCOPY src/ /app/src\r   RUN： 在容器内执行命令，通常用于安装软件包、更新系统、配置环境等。\nDockerfileCopy code\rRUN apt-get update \u0026amp;\u0026amp; apt-get install -y some-package\r   EXPOSE： 指定容器监听的网络端口。该指令并不会实际映射或打开端口，仅作为文档描述。\nDockerfileCopy code\rEXPOSE 8080\r   CMD 或 ENTRYPOINT： 定义容器启动时执行的命令。CMD 用于指定默认的执行命令，ENTRYPOINT 用于指定容器启动时运行的可执行文件。\nDockerfileCopy code\rCMD [\u0026quot;java\u0026quot;, \u0026quot;-jar\u0026quot;, \u0026quot;app.jar\u0026quot;]\r   ENV： 设置环境变量，用于配置容器内部的环境。\nDockerfileCopy code\rENV JAVA_HOME /usr/lib/jvm/java-11-openjdk\r   VOLUME： 创建一个挂载点，用于让容器与主机或其他容器共享数据。\nDockerfileCopy code\rVOLUME /data\r   USER： 设置运行时的用户名或 UID，并切换到该用户。\nDockerfileCopy code\rUSER appuser\r   ARG： 定义构建时传递给镜像的参数。\nDockerfileCopy code\rARG VERSION=latest\r   步骤   建一个springboot项目，随便写一个接口\npackage com.zt.study.docker.study.controller;\rimport org.springframework.web.bind.annotation.RequestMapping;\rimport org.springframework.web.bind.annotation.RestController;\r/**\r* @author zhengtao on 2023/12/2\r*/\r@RestController\rpublic class TestController {\r@RequestMapping(\u0026quot;/hello\u0026quot;)\rpublic String helloWorld() {\rreturn \u0026quot;hello world\u0026quot;;\r}\r}\r   编写Dockerfile文件\n#使用 OpenJDK 8 作为基础镜像\rFROM openjdk:8\r# 设置工作目录\rWORKDIR /java\r# 复制文件\rCOPY *.jar /java/docker-study.jar\r# 暴露端口\rEXPOSE 8080\r# 启动命令\rCMD [\u0026quot;java\u0026quot;,\u0026quot;-jar\u0026quot;,\u0026quot;/java/docker-study.jar\u0026quot;]\r   将jar文件和Dockerfile文件放在linux上同一层级\n使用docker build命令打包镜像  # -t 为命名，.代表在此层目录找Dockerfile\rdocker build -t docker-study .\r  使用docker run命名运行容器\n# -d 代表在后台运行，-p代表暴露端口，即将容器的端口 8080 发布到本机上的8080\rdocker run -dp 127.0.0.1:8080:8080 docker-study\r   使用docker ps看是否启动成功\n也可以使用curl http://localhost:8080/hello验证\n  可以将这个镜像推到docker hub中https://hub.docker.com/\n先在自己的仓库中新建一个docker-study仓库\n# 登录到你自己的docker hub账号，比如我的wzgl998877\rdocker login -u YOUR-USER-NAME\r# 把镜像命名修改为你自己的镜像名称\rdocker tag getting-started YOUR-USER-NAME/getting-started\r# 推送到docker hub\rdocker push YOUR-USER-NAME/getting-started\r   制作一条流水线 Kubernetes 概念 Kubernetes 是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。\nKubernetes 这个名字源于希腊语，意为“舵手”或“飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。\nKubernetes 是一个容器编排平台和集群管理系统\nKubernetes 为你提供：\n  服务发现和负载均衡\nKubernetes 可以使用 DNS 名称或自己的 IP 地址来暴露容器。 如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。\n  存储编排\nKubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。\n  自动部署和回滚\n你可以使用 Kubernetes 描述已部署容器的所需状态， 它可以以受控的速率将实际状态更改为期望状态。 例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。\n  自动完成装箱计算\n你为 Kubernetes 提供许多节点组成的集群，在这个集群上运行容器化的任务。 你告诉 Kubernetes 每个容器需要多少 CPU 和内存 (RAM)。 Kubernetes 可以将这些容器按实际情况调度到你的节点上，以最佳方式利用你的资源。\n  自我修复\nKubernetes 将重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器， 并且在准备好服务之前不将其通告给客户端。\n  密钥与配置管理\nKubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。\n  批处理执行 除了服务外，Kubernetes 还可以管理你的批处理和 CI（持续集成）工作负载，如有需要，可以替换失败的容器。\n  水平扩缩 使用简单的命令、用户界面或根据 CPU 使用率自动对你的应用进行扩缩。\n  IPv4/IPv6 双栈 为 Pod（容器组）和 Service（服务）分配 IPv4 和 IPv6 地址。\n  为可扩展性设计 在不改变上游源代码的情况下为你的 Kubernetes 集群添加功能。\n  Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 而 Kubernetes 包含了一组独立可组合的控制过程，可以持续地将当前状态驱动到所提供的预期状态。 你不需要在乎如何从 A 移动到 C，也不需要集中控制，这使得系统更易于使用且功能更强大、 系统更健壮，更为弹性和可扩展。\n架构 k8s中，有两个主要的组件集合，分别是Control Plane（控制平面）和Data Plane（数据平面）。它们协同工作以管理和运行容器化应用程序。集群里的计算机被称为“节点”（Node），可以是实机也可以是虚机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用，控制面的节点在 Kubernetes 里叫做 Master Node，一般简称为 Master，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。 数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。\n Control Plane（控制平面）:  作用： 控制平面是负责管理整个Kubernetes集群的核心组件集合。做出全局决策（例如，调度），以及检测和响应集群事件。它们决策何时启动、停止和重新调度应用程序容器，以及维护集群的期望状态。控制平面组件确保系统处于所需的状态，并对集群中的各种事件做出响应。 核心组件：  kube-apiserver：是整个 Kubernetes 系统的唯一入口，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信，可以说是 Kubernetes 里的联络员 etcd： 是一个高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态，相当于 Kubernetes 里的配置管理员。注意它只与 apiserver 有直接联系，也就是说任何其他组件想要读写 etcd 里的数据都必须经过 apiserver。 kube-scheduler： 负责容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行，相当于部署人员。 kube-controller-manager：负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能，相当于监控运维人员     Data Plane（数据平面）:  作用： 数据平面负责实际运行应用程序容器，并处理它们之间的网络通信。当应用程序容器接收到请求时，数据平面确保请求得到满足，并将响应返回给请求方。 核心组件：  kubelet：kubelet 是 Node 的代理，在每个Node上运行，负责管理 Node 相关的绝大部分操作，在每个节点上运行，负责管理该节点上的Pod和容器，Node 上只有它能够与 apiserver 通信，实现状态报告、命令下发、启停容器等功能，相当于是 Node 上的一个小管家。 kube-proxy：它是 Node 的网络代理，在每个节点上运行，维护网络规则并处理集群内部的网络通信。它使得Pod能够相互通信，并提供负载均衡。相当于是专职的小邮差。 container-runtime：使 Kubernetes 能够有效运行容器的基本组件。它负责管理 Kubernetes 环境中容器的执行和生命周期。是真正干活的苦力。常见的container-runtime有docker（早期k8s默认）、Containerd（由docker公司开发的，现在广泛使用）、CRI-O （Red Hat 团队开发的容器运行时，专门为 Kubernetes 而设计）   插件：  DNS ：它在 Kubernetes 集群里实现了域名解析服务，能够让我们以域名而不是 IP 地址的方式来互相通信，是服务发现和负载均衡的基础。由于它对微服务、服务网格等架构至关重要，所以基本上是 Kubernetes 的必备插件。 Dashboard ：就是仪表盘，为 Kubernetes 提供了一个图形化的操作界面，非常直观友好，虽然大多数 Kubernetes 工作都是使用命令行 kubectl，但有的时候在 Dashboard 上查看信息也是挺方便的。      总体而言，Control Plane是Kubernetes集群的大脑，负责决策和管理集群状态，而Data Plane是实际运行应用程序容器的地方，负责处理应用程序的网络通信。这种分离使得Kubernetes具有高度的可扩展性和灵活性。\n 容器是一种沙盒技术**。Docker** 是一种容器技术的实现，提供了工具和平台，使容器更易于创建和管理。Kubernetes 是一个容器编排平台和集群管理系统,Kubernetes 又用到了docker来管理容器的执行和生命周期\n 安装k8s 可以通过kubeadm安装k8s\n  安装kubeadm ， yum install -y kubeadm kubelet kubectl\n如果报这种错，error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR FileContent\u0026ndash;proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 [preflight] If you know what you are doing\n编辑 vim /etc/sysctl.conf 加上\nnet.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1\n  启动kubeadm，\n  kubeadm init \u0026ndash;pod-network-cidr=10.244.0.0/16 \u0026ndash;image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers\n   会返回一串信息\nYour Kubernetes control-plane has initialized successfully!\n  ​ To start using your cluster, you need to run the following as a regular user:\nmkdir -p HOME/.kube sudo cp -i /etc/kubernetes/admin.conf ​HOME/.kube/config sudo chown ​(id -u):(id -g) ​HOME/.kube/config\nYou should now deploy a pod network to the cluster. Run \u0026ldquo;kubectl apply -f [podnetwork].yaml\u0026rdquo; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/\nThen you can join any number of worker nodes by running the following on each as root:\nkubeadm join 172.20.21.165:6443 \u0026ndash;token zi2uoy.asuxj73ev826r9fu \u0026ndash;discovery-token-ca-cert-hash sha256:10dfdef0ddb9a2c41b45e0ec5c0fffbfecdf0f45062f04620c07a1124883029c\n   依次执行 mkdir -p HOME/.kube sudo cp -i /etc/kubernetes/admin.conf HOME/.kube/config sudo chown (id -u):(id -g) HOME/.kube/config\n  如果有多台机器，那么去其他的机器上执行kubeadm join 172.20.21.165:6443 \u0026ndash;token zi2uoy.asuxj73ev826r9fu \u0026ndash;discovery-token-ca-cert-hash sha256:10dfdef0ddb9a2c41b45e0ec5c0fffbfecdf0f45062f04620c07a1124883029c\n  如果只有一台机器则执行kubectl taint nodes \u0026ndash;all node-role.kubernetes.io/master-\n    安装网络插件\nkubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n  kubectl version、kubectl get nodes 检测是否安装成功\nkubectl version 后返回 Client Version: version.Info{Major:\u0026ldquo;1\u0026rdquo;, Minor:\u0026ldquo;17\u0026rdquo;, GitVersion:\u0026ldquo;v1.17.3\u0026rdquo;, GitCommit:\u0026ldquo;06ad960bfd03b39c8310aaf92d1e7c12ce618213\u0026rdquo;, GitTreeState:\u0026ldquo;clean\u0026rdquo;, BuildDate:\u0026ldquo;2020-02-11T18:14:22Z\u0026rdquo;, GoVersion:\u0026ldquo;go1.13.6\u0026rdquo;, Compiler:\u0026ldquo;gc\u0026rdquo;, Platform:\u0026ldquo;linux/amd64\u0026rdquo;} Server Version: version.Info{Major:\u0026ldquo;1\u0026rdquo;, Minor:\u0026ldquo;17\u0026rdquo;, GitVersion:\u0026ldquo;v1.17.17\u0026rdquo;, GitCommit:\u0026ldquo;f3abc15296f3a3f54e4ee42e830c61047b13895f\u0026rdquo;, GitTreeState:\u0026ldquo;clean\u0026rdquo;, BuildDate:\u0026ldquo;2021-01-13T13:13:00Z\u0026rdquo;, GoVersion:\u0026ldquo;go1.13.15\u0026rdquo;, Compiler:\u0026ldquo;gc\u0026rdquo;, Platform:\u0026ldquo;linux/amd64\u0026rdquo;}\nkubectl get nodes 后返回，若status为ready代表成功\nNAME STATUS ROLES AGE VERSION devops1722021165 Ready master 17h v1.17.3\n  使用k8s部署一个应用 YAML Kubernetes使用YAML作为主要的配置语言，通过YAML文件来定义和配置各种Kubernetes资源对象，使得用户可以方便地以声明性的方式描述应用程序的结构和部署要求。\n 声明式编程：告诉“机器”你想要的是什么(what)，让机器想出如何去做(how)。如：sql、yaml等 命令式编程：命令“机器”如何去做事情(how)，这样不管你想要的是什么(what)，它都会按照你的命令实现。如：大部分编程语言、Dockerfile等  kubectl 常用命令   创建资源（例如 Pod、Deployment）：\n# 创建或更新\rkubectl apply -f \u0026lt;yaml-file\u0026gt;\r# 只创建\rkubectl create -f \u0026lt;yaml-file\u0026gt;\r   删除资源：\nkubectl delete -f \u0026lt;yaml-file\u0026gt;\r   查看资源\n **查看集群信息：**kubectl cluster-info 查看集群节点：kubectl get nodes **查看所有命名空间：**kubectl get namespaces **查看 Pod 列表：**kubectl get pods **查看 Deployment 列表：**kubectl get deployments **查看 Service 列表：**kubectl get services **查看 ConfigMap 列表：**kubectl get configmaps **查看 Secret 列表：**kubectl get secrets **查看事件：**kubectl get events    查看资源详情\n 查看 Pod 详细信息： kubectl describe pod  查看 Deployment 详细信息：kubectl describe deployment  查看 Service 详细信息：kubectl describe service  查看 ConfigMap 详细信息：kubectl describe configmap  **查看 Secret 详细信息：**kubectl describe secret     执行命令进入 Pod：\n  kubectl exec -it \u0026lt;pod-name\u0026gt; -- /bin/bash\r 在 Pod 中执行命令：  kubectl exec \u0026lt;pod-name\u0026gt; -- \u0026lt;command\u0026gt;\r 例子  先写一个docker-study-deployment.yaml文件  apiVersion: apps/v1 # 使用的 Kubernetes API 版本\rkind: Deployment # 定义的 Kubernetes 资源类型为 Deployment\rmetadata:\rname: docker-study-deployment # Deployment 对象的名称\rspec:\rselector:\rmatchLabels:\rapp: docker-study # 选择器，匹配 Pod 的标签以确定控制哪些 Pod\rreplicas: 2 # 副本数量，指定要运行的 Pod 实例数量\rtemplate:\rmetadata:\rlabels:\rapp: docker-study # Pod 模板的标签\rspec:\rcontainers:\r- name: docker-study # 容器的名称\rimage: wzgl998877/docker-study # 容器的镜像\rports:\r- containerPort: 80 # 容器监听的端口\rvolumeMounts:\r- mountPath: \u0026quot;/usr/share/docker-study/html\u0026quot; # 容器内部的挂载路径\rname: docker-study-vol # 卷的名称\rvolumes:\r- name: docker-study-vol # 卷的名称\remptyDir: {} # 创建一个空目录的卷\r   创建对象 kubectl apply -f docker-study-deployment.yaml\n  使用kubectl get pods\nNAME READY STATUS RESTARTS AGE docker-study-deployment-695f9b8649-5fsnc 1/1 Running 0 8m3s docker-study-deployment-695f9b8649-d5z56 1/1 Running 0 8m2s\n  k8s的API对象 有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod 的多实例管理器；而有了这样一组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。 可是，如果现在两个不同 Pod 之间不仅有“访问关系”，还要求在发起时加上授权信息。最典型的例子就是 Web 应用对数据库访问时需要 Credential（数据库的用户名和密码）信息。那么，在 Kubernetes 中这样的关系又如何处理呢？ Kubernetes 项目提供了一种叫作 Secret 的对象，它其实是一个保存在 Etcd 里的键值对数据。这样，你把 Credential 信息以 Secret 的方式存在 Etcd 里，Kubernetes 就会在你指定的 Pod（比如，Web 应用的 Pod）启动时，自动把 Secret 里的数据以 Volume 的方式挂载到容器里。这样，这个 Web 应用就可以访问数据库了。\npod Pod are the smallest deployable units of computing that you can create and manage in Kubernetes. Pod 是您可以在 Kubernetes 中创建和管理的最小的可部署计算单元。pod 是容器的封装,包含一个或多个共享相同网络命名空间和存储卷的容器,是 Kubernetes 的原子调度单位。\npod的状态\n Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。 Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。 Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。 Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。 Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。  Deployments 多实例控制器，能够让应用永不宕机，多用来发布无状态的应用，用来管理pod。\napiVersion: apps/v1 # 使用的 Kubernetes API 版本\rkind: Deployment # 定义的 Kubernetes 资源类型为 Deployment\rmetadata:\rname: docker-study-deployment # Deployment 对象的名称\rspec:\rselector:\rmatchLabels:\rapp: docker-study # 选择器，匹配 Pod 的标签以确定控制哪些 Pod\rreplicas: 2 # 副本数量，指定要运行的 Pod 实例数量\r# 以上为控制器的定义，以下为被控制的对象\rtemplate:\rmetadata:\rlabels:\rapp: docker-study # Pod 模板的标签\rspec:\rcontainers:\r- name: docker-study # 容器的名称\rimage: wzgl998877/docker-study # 容器的镜像\rports:\r- containerPort: 80 # 容器监听的端口\rvolumeMounts:\r- mountPath: \u0026quot;/usr/share/docker-study/html\u0026quot; # 容器内部的挂载路径\rname: docker-study-vol # 卷的名称\rvolumes:\r- name: docker-study-vol # 卷的名称\remptyDir: {} # 创建一个空目录的卷\r   selector： 它的作用是“筛选”出要被 Deployment 管理的 Pod 对象，下属字段“matchLabels”定义了 Pod 对象应该携带的 label，它必须和“template”里 Pod 定义的“labels”完全相同\n  replicas：副本数量”的意思，指定要在 Kubernetes 集群里运行多少个 Pod 实例，Pod 的个数大于 2 的时候，就会有旧的 Pod 被删除；反之，就会有新的 Pod 被创建，做到了让应用永不宕机\n  template：以下就是被控制的对象\n  ReplicaSet ReplicaSet 是用于维护一组 Pod 副本数量的资源对象。ReplicaSet 的主要目标是确保在集群中始终运行指定数量的相同 Pod 副本，由 Deployment 控制和管理。Deployment操作 ReplicaSet 的个数和属性，进而实现水平扩展 / 收缩和滚动更新。\napiVersion: apps/v1\rkind: ReplicaSet\rmetadata:\rname: nginx-set\rlabels:\rapp: nginx\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rapp: nginx\rtemplate:\rmetadata:\rlabels:\rapp: nginx\rspec:\rcontainers:\r- name: nginx\rimage: nginx:1.7.9\r   水平扩展 / 收缩：修改replicas数量即可，执行命令修改 kubectl scale deployment deployment deployment-name \u0026ndash;replicas=4，或者直接修改yaml文件中的replicas值\n  滚动更新：滚动更新是在不中断服务的情况下逐步替换现有的 Pod，以部署新的应用程序版本或配置。通过修改 ReplicaSet 中的 Pod 模板，可以触发滚动更新。ReplicaSet 会逐步替换当前的 Pod，确保在更新的过程中保持服务可用性。\n当你修改了 Deployment 里的 Pod 定义之后，Deployment Controller 会使用这个修改后的 Pod 模板，创建一个新的 ReplicaSet，这个新的 ReplicaSet 的初始 Pod 副本数是：0。 然后Deployment Controller 开始将这个新的 ReplicaSet 所控制的 Pod 副本数从 0 个变成 1 个，即：“水平扩展”出一个副本。 紧接着，Deployment Controller 又将旧的 ReplicaSet所控制的旧 Pod 副本数减少一个，即：“水平收缩”成两个副本。 如此交替进行，新 ReplicaSet 管理的 Pod 副本数，从 0 个变成 1 个，再变成 2 个，最后变成 3 个。而旧的 ReplicaSet 管理的 Pod 副本数则从 3 个变成 2 个，再变成 1 个，最后变成 0 个。这样，就完成了这一组 Pod 的版本升级过程。 将一个集群中正在运行的多个 Pod 版本，交替地逐一升级的过程，就是“滚动更新”。\n滚动更新的策略：Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。\nspec:\rselector:\rmatchLabels:\rapp: docker-study\rreplicas: 2\rstrategy:\rtype: RollingUpdate\rrollingUpdate:\rmaxSurge: 1\rmaxUnavailable: 1\rminReadySeconds: 60\rprogressDeadlineSeconds: 600\r maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。minReadySeconds，定义了新的 Pod 在被认为是可用之前必须运行的最短时间（以秒为单位）。如果设置了 minReadySeconds，Kubernetes 将等待至少这么长时间，然后开始进行下一个 Pod 的更新。progressDeadlineSeconds 定义了 Deployment 更新的最长时间。如果更新超过指定的时间，Deployment 将被标记为失败，这有助于避免无限期地等待更新完成的情况。\n  回滚版本\n# 查看历史版本\rkubectl rollout history deployment/deployment-name\r# 回滚到对应版本\rkubectl rollout undo deployment/deployment-name --to-revision=2\r# 回滚到上一个版本\rkubectl rollout undo deployment/deployment-name\r   service 实现了负载均衡和服务发现。Kubernetes 会给它分配一个静态 IP 地址，然后它再去自动管理、维护后面动态变化的 Pod 集合，当客户端访问 Service，它就根据某种策略，把流量转发给后面的某个 Pod。\n# 定义一个 Kubernetes Service\rapiVersion: v1\rkind: Service\r# 设置 Service 的元数据，包括名称\rmetadata:\rname: docker-study-svc\r# 指定 Service 的规格\rspec:\r# 选择带有标签 app: docker-study 的 Pod 作为后端\rselector:\rapp: docker-study\r# 定义 Service 的端口映射\rports:\r- # Service 暴露的端口\rport: 80\r# 将流量转发到后端 Pod 的端口\rtargetPort: 80\r# 指定使用的协议（TCP）\rprotocol: TCP\r 使用kubectl apply -f docker-study-svc.yaml\n发现访问不通service对应的ip\n# 2.添加需要加载的模块写入脚本文件\r[root@master ~]# cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/sysconfig/modules/ipvs.modules\r#!/bin/bash\rmodprobe -- ip_vs\rmodprobe -- ip_vs_rr\rmodprobe -- ip_vs_wrr\rmodprobe -- ip_vs_sh\rmodprobe -- nf_conntrack\rEOF\r# 3.为脚本添加执行权限\r[root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules\r# 4.执行脚本文件\r[root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules\r# 5.查看对应的模块是否加载成功\r[root@master ~]# lsmod | grep -e -ip_vs -e nf_conntrack\rnf_conntrack_netlink 49152 0 nfnetlink 20480 3 nf_conntrack_netlink,ip_set\rnf_conntrack 155648 5 xt_conntrack,nf_nat,nf_conntrack_netlink,xt_MASQUERADE,ip_vs\rnf_defrag_ipv6 24576 2 nf_conntrack,ip_vs\rnf_defrag_ipv4 16384 1 nf_conntrack\rlibcrc32c 16384 4 nf_conntrack,nf_nat,xfs,ip_vs\r StatefulSet StatefulSet 是一种控制器（Controller），用于管理有状态应用的部署\nDaemonSet DaemonSet 的主要作用，是让你在 Kubernetes 集群里，运行一个 Daemon Pod。 这个 Pod 有如下三个特征：\n 这个 Pod 运行在 Kubernetes 集群里的每一个节点（Node）上； 每个节点上只有一个这样的 Pod 实例； 当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉。  它通常用于运行类似于日志收集器、监控代理、网络插件等与节点相关的服务\nJob和CornJob ","id":2,"section":"posts","summary":"[TOC] 容器 容器- Container，容器其实是一种沙盒技术。沙盒就是能够像一个集装箱一样，把你的应用“装”起来的技术。这样，应用与应用之间，就因","tags":null,"title":"Docker和k8s","uri":"https://wzgl998877.github.io/2022/09/docker%E5%92%8Ck8s/","year":"2022"},{"content":"[TOC]\n这篇文章主要记录自己学习 Redis 的历程\nRedis 的数据结构  redis中数据结构有 String（字符串）、List（列表）、Hash（哈希）、Set（集合）和 Sorted Set（有序集合）、HyperLogLog(做基数统计的算法)、Geo（存储地理位置信息）、BloomFilter（布隆过滤器）\n底层数据结构一共有 6 种，分别是简单动态字符串、双向链表、压缩列表、哈希表、跳表和整数数组。它们和数据类型的对应关系如下图所示：\n可以看到，String 类型的底层实现只有一种数据结构，也就是简单动态字符串。而 List、Hash、Set 和 Sorted Set 这四种数据类型，都有两种底层实现结构。通常情况下，我们会把这四种类型称为集合类型，它们的特点是一个键对应了一个集合的数据。\n键和值用什么结构组织？  为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。\n一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。所以，我们常说，一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。\n看到这里，你可能会问了：“如果值是集合类型的话，作为数组元素的哈希桶怎么来保存呢？”其实，哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。这也就是说，不管值是 String，还是集合类型，哈希桶中的元素都是指向它们的指针。\n在下图中，可以看到，哈希桶中的 entry 元素中保存了key和value指针，分别指向了实际的键和值，这样一来，即使值是一个集合，也可以通过*value指针被查找到。\n因为这个哈希表保存了所有的键值对，所以，我也把它称为全局哈希表。哈希表的最大好处很明显，就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素。\n你看，这个查找过程主要依赖于哈希计算，和数据量的多少并没有直接关系。也就是说，不管哈希表里有 10 万个键还是 100 万个键，我们只需要一次计算就能找到相应的键。\n为什么哈希表操作变慢了？  当你往哈希表中写入更多数据时，哈希冲突是不可避免的问题。这里的哈希冲突，也就是指，两个 key 的哈希值和哈希桶计算对应关系时，正好落在了同一个哈希桶中。\n毕竟，哈希桶的个数通常要少于 key 的数量，这也就是说，难免会有一些 key 的哈希值对应到了同一个哈希桶中。Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接。\n如下图所示：entry1、entry2 和 entry3 都需要保存在哈希桶 3 中，导致了哈希冲突。此时，entry1 元素会通过一个next指针指向 entry2，同样，entry2 也会通过next指针指向 entry3。这样一来，即使哈希桶 3 中的元素有 100 个，我们也可以通过 entry 元素中的指针，把它们连起来。这就形成了一个链表，也叫作哈希冲突链。\n但是，这里依然存在一个问题，哈希冲突链上的元素只能通过指针逐一查找再操作。如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多，这就会导致某些哈希冲突链过长，进而导致这个链上的元素查找耗时长，效率降低。对于追求“快”的 Redis 来说，这是不太能接受的。\n所以，Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。那具体怎么做呢？\n其实，为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行rehash，这个过程分为三步：\n 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍； 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中； 释放哈希表 1 的空间。  到此，我们就可以从哈希表 1 切换到哈希表 2，用增大的哈希表 2 保存更多数据，而原来的哈希表 1 留作下一次 rehash 扩容备用。这个过程看似简单，但是第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据了。为了避免这个问题，Redis 采用了渐进式 rehash。\n简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。如下图所示：\n这样就巧妙地把一次性大量拷贝的开销，分摊到了多次处理请求的过程中，避免了耗时操作，保证了数据的快速访问。好了，到这里，你应该就能理解，Redis 的键和值是怎么通过哈希表组织的了。对于 String 类型来说，找到哈希桶就能直接增删改查了，所以，哈希表的 O(1) 操作复杂度也就是它的复杂度了。但是，对于集合类型来说，即使找到哈希桶了，还要在集合中再进一步操作。接下来，我们来看集合类型的操作效率又是怎样的。\n集合数据操作效率 和 String 类型不同，一个集合类型的值，第一步是通过全局哈希表找到对应的哈希桶位置，第二步是在集合中再增删改查。那么，集合的操作效率和哪些因素相关呢？\n首先，与集合的底层数据结构有关。例如，使用哈希表实现的集合，要比使用链表实现的集合访问效率更高。其次，操作效率和这些操作本身的执行特点有关，比如读写一个元素的操作要比读写所有元素的效率高。\n接下来，我们就分别聊聊集合类型的底层数据结构和操作复杂度。\n有哪些底层数据结构？  集合类型的底层数据结构主要有 5 种：整数数组、双向链表、哈希表、压缩列表和跳表。其中，哈希表的操作特点我们刚刚已经学过了；整数数组和双向链表也很常见，它们的操作特征都是顺序读写，也就是通过数组下标或者链表的指针逐个元素访问，操作复杂度基本是 O(N)，操作效率比较低；\n压缩列表 压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束。\n在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了。\n跳表 有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位，如下图所示：\n如果我们要在链表中查找 33 这个元素，只能从头开始遍历链表，查找 6 次，直到找到 33 为止。此时，复杂度是 O(N)，查找效率很低。为了提高查找速度，我们来增加一级索引：从第一个元素开始，每两个元素选一个出来作为索引。这些索引再通过指针指向原始的链表。例如，从前两个元素中抽取元素 1 作为一级索引，从第三、四个元素中抽取元素 11 作为一级索引。此时，我们只需要 4 次查找就能定位到元素 33 了。如果我们还想再快，可以再增加二级索引：从一级索引中，再抽取部分元素作为二级索引。例如，从一级索引中抽取 1、27、100 作为二级索引，二级索引指向一级索引。这样，我们只需要 3 次查找，就能定位到元素 33 了。\n可以看到，这个查找过程就是在多级索引上跳来跳去，最后定位到元素。这也正好符合“跳”表的叫法。当数据量很大时，跳表的查找复杂度就是 O(logN)。\n好了，我们现在可以按照查找的时间复杂度给这些数据结构分下类了：\nRedis的IO模型 Redis 是单线程，主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。\n在Linux(UNIX)操作系统中，共有五种IO模型，分别是：阻塞IO模型、非阻塞IO模型、IO复用模型、信号驱动IO模型以及异步IO模型。\n到底什么是IO 我们常说的IO，指的是文件的输入和输出，但是在操作系统层面是如何定义IO的呢？到底什么样的过程可以叫做是一次IO呢？\n拿一次磁盘文件读取为例，我们要读取的文件是存储在磁盘上的，我们的目的是把它读取到内存中。可以把这个步骤简化成把数据从硬件（硬盘）中读取到用户空间中。\n其实真正的文件读取还涉及到缓存等细节，这里就不展开讲述了。关于用户空间、内核空间以及硬件等的关系如果读者不理解的话，可以通过钓鱼的例子理解。\n钓鱼的时候，刚开始鱼是在鱼塘里面的，我们的钓鱼动作的最终结束标志是鱼从鱼塘中被我们钓上来，放入鱼篓中。\n这里面的鱼塘就可以映射成磁盘，中间过渡的鱼钩可以映射成内核空间，最终放鱼的鱼篓可以映射成用户空间。一次完整的钓鱼（IO）操作，是鱼（文件）从鱼塘（硬盘）中转移（拷贝）到鱼篓（用户空间）的过程。\n阻塞IO模型 我们钓鱼的时候，有一种方式比较惬意，比较轻松，那就是我们坐在鱼竿面前，这个过程中我们什么也不做，双手一直把着鱼竿，就静静的等着鱼儿咬钩。一旦手上感受到鱼的力道，就把鱼钓起来放入鱼篓中。然后再钓下一条鱼。\n映射到Linux操作系统中，这就是一种最简单的IO模型，即阻塞IO。 阻塞 I/O 是最简单的 I/O 模型，一般表现为进程或线程等待某个条件，如果条件不满足，则一直等下去。条件满足，则进行下一步操作。\n应用进程通过系统调用 recvfrom 接收数据，但由于内核还未准备好数据报，应用进程就会阻塞住，直到内核准备好数据报，recvfrom 完成数据报复制工作，应用进程才能结束阻塞状态。\n非阻塞IO模型 我们钓鱼的时候，在等待鱼儿咬钩的过程中，我们可以做点别的事情，比如玩一把王者荣耀、看一集《延禧攻略》等等。但是，我们要时不时的去看一下鱼竿，一旦发现有鱼儿上钩了，就把鱼钓上来。\n映射到Linux操作系统中，这就是非阻塞的IO模型。应用进程与内核交互，目的未达到之前，不再一味的等着，而是直接返回。然后通过轮询的方式，不停的去问内核数据准备有没有准备好。如果某一次轮询发现数据已经准备好了，那就把数据拷贝到用户空间中。\n应用进程通过 recvfrom 调用不停的去和内核交互，直到内核准备好数据。如果没有准备好，内核会返回error，应用进程在得到error后，过一段时间再发送recvfrom请求。在两次发送请求的时间段，进程可以先做别的事情。\n信号驱动IO模型 我们钓鱼的时候，为了避免自己一遍一遍的去查看鱼竿，我们可以给鱼竿安装一个报警器。当有鱼儿咬钩的时候立刻报警。然后我们再收到报警后，去把鱼钓起来。\n映射到Linux操作系统中，这就是信号驱动IO。应用进程在读取文件时通知内核，如果某个 socket 的某个事件发生时，请向我发一个信号。在收到信号后，信号对应的处理函数会进行后续处理。\n应用进程预先向内核注册一个信号处理函数，然后用户进程返回，并且不阻塞，当内核数据准备就绪时会发送一个信号给进程，用户进程便在信号处理函数中开始把数据拷贝的用户空间中。\nIO复用模型 我们钓鱼的时候，为了保证可以最短的时间钓到最多的鱼，我们同一时间摆放多个鱼竿，同时钓鱼。然后哪个鱼竿有鱼儿咬钩了，我们就把哪个鱼竿上面的鱼钓起来。\n映射到Linux操作系统中，这就是IO复用模型。多个进程的IO可以注册到同一个管道上，这个管道会统一和内核进行交互。当管道中的某一个请求需要的数据准备好之后，进程再把对应的数据拷贝到用户空间中。\nIO多路转接是多了一个select函数，多个进程的IO可以注册到同一个select上，当用户进程调用该select，select会监听所有注册好的IO，如果所有被监听的IO需要的数据都没有准备好时，select调用进程会阻塞。当任意一个IO所需的数据准备好之后，select调用就会返回，然后进程在通过recvfrom来进行数据拷贝。\n**这里的IO复用模型，并没有向内核注册信号处理函数，所以，他并不是非阻塞的。**进程在发出select后，要等到select监听的所有IO操作中至少有一个需要的数据准备好，才会有返回，并且也需要再次发送请求去进行文件的拷贝。IO复用模型主要有 select、poll、epoll 三种多路复用的网络I/O模型\nselect select 的设计思路是唤醒模式，「通过一个 socket 列表维护所有的 socket」，socket 对应文件列表中的 fd，select 会默认限制最大文件句柄数为 1024，间接控制 fd[] 最大为 1024。\n 如果列表中的 Socket 都**「没有数据」**，就挂起进程。 如果有一个 Socket 收到数据，就唤醒进程，将该线程从等待队列中移除，加入到工作队列，然后准备执行 socket 任务。  select select 的设计思路是唤醒模式，「通过一个 socket 列表维护所有的 socket」，socket 对应文件列表中的 fd，select 会默认限制最大文件句柄数为 1024，间接控制 fd[] 最大为 1024。\n 如果列表中的 Socket 都**「没有数据」**，就挂起进程。 如果有一个 Socket 收到数据，就唤醒进程，将该线程从等待队列中移除，加入到工作队列，然后准备执行 socket 任务。  那么有个问题来了，如果有多个 socket 任务同时唤醒怎么办，也就是说说有多个 socket 任务同时进来，那到底执行哪个 socket 任务？\n所以，当进程被唤醒后，「至少有一个 socket 是有数据的」，这时候只需要**「遍历一遍 socket 列表」**就知道了此次需要执行哪些 socket 了。\n缺点：\n  每次 select 都需要将进程加入到监视 socket 的等待队列，每次唤醒都要将进程从 socket 待队列移除。这里涉及两次遍历操作，而且每次都要将 FDS 列表传递给内核，有一定的开销。\n  进程被唤醒后，只能知道有 socket 接收到了数据，无法知道具体是哪一个 socket 接收到了数据，所以需要用户进程进行遍历，才能知道具体是哪个 socket 接收到了数据。\n  poll poll 其实内部实现基本跟 select 一样，区别在于它们底层组织 fd[] 的数据结构不太一样，poll使用一个 pollfd的指针实现\nstruct pollfd {\rint fd; /* file descriptor */\rshort events; /* requested events to watch */\rshort revents; /* returned events witnessed */\r};\r pollfd结构包含了要监视的event和发生的event，从而实现了 poll 的最大文件句柄数量限制去除了\nepoll 我们前面说到 select 是需要遍历来解决查询就绪 socket 的，效率很低，epoll 就对此做了改进，redis就是采用这个\n 拆分:epoll 将添加等待队列和阻塞进程拆分成两个独立的操作，不用每次都去重新维护等待队列     先用 epoll_ctl 维护等待队列 eventpoll，它通过红黑树存储 socket 对象，实现高效的查找，删除和添加。 再调用 epoll_wait 阻塞进程，底层是一个双向链表。显而易见地，效率就能得到提升。    select 的添加等待队列和阻塞进程是合并在一起的，每次调用select()操作时都得执行一遍这两个操作，从而导致每次都要将fd[]传递到内核空间，并且遍历fd[]的每个fd的等待队列，将进程放入各个fd的等待队列中。\n直接返回有数据的 fd[]:select 进程被唤醒后，是需要遍历一遍 socket 列表，手动获取有数据的 socket，而 epoll 是在唤醒时直接把有数据的 socket 返回给进程，不需要自己去进行遍历查询。  epoll 会先注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知。\nepoll对文件描述符的操作有两种模式：「LT」（level trigger）和 「ET」（edge trigger）默认为 LT :\n  LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。\n  ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时**，不会再次响应应用程序并通知此事件**。\n  异步IO模型 我们钓鱼的时候，采用一种高科技钓鱼竿，即全自动钓鱼竿。可以自动感应鱼上钩，自动收竿，更厉害的可以自动把鱼放进鱼篓里。然后，通知我们鱼已经钓到了，他就继续去钓下一条鱼去了。\n映射到Linux操作系统中，这就是异步IO模型。应用进程把IO请求传给内核后，完全由内核去操作文件拷贝。内核完成相关操作后，会发信号告诉应用进程本次IO已经完成。\n用户进程发起aio_read操作之后，给内核传递描述符、缓冲区指针、缓冲区大小等，告诉内核当整个操作完成时，如何通知进程，然后就立刻去做其他事情了。当内核收到aio_read后，会立刻返回，然后内核开始等待数据准备，数据准备好以后，直接把数据拷贝到用户控件，然后再通知进程本次IO已经完成。\n总结 阻塞IO模型、非阻塞IO模型、IO复用模型和信号驱动IO模型都是同步的IO模型。原因是因为，无论以上那种模型，真正的数据拷贝过程，都是同步进行的。\n信号驱动难道不是异步的么？ 信号驱动，内核是在数据准备好之后通知进程，然后进程再通过recvfrom操作进行数据拷贝。我们可以认为数据准备阶段是异步的，但是，数据拷贝操作是同步的。所以，整个IO过程也不能认为是异步的。\n单线程 Redis 为什么那么快？ 通常来说，单线程的处理能力要比多线程差很多，但是 Redis 却能使用单线程模型达到每秒数十万级别的处理能力，这是为什么呢？其实，这是 Redis 多方面设计选择的一个综合结果。\n一方面，Redis 的大部分操作在内存上完成，再加上它采用了高效的数据结构，例如哈希表和跳表，这是它实现高性能的一个重要原因。另一方面，就是 Redis 采用了多路复用机制使用epoll模型，使其在网络 IO 操作中能并发处理大量的客户端请求。\nAOF 日志 目前，Redis 的持久化主要有两大机制，即 AOF（Append Only File）日志和 RDB 快照\nAOF 日志是如何实现的？ 说到日志，我们比较熟悉的是数据库的写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。不过，AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志，如下图所示：\n那 AOF 为什么要先执行命令再记日志呢？要回答这个问题，我们要先知道 AOF 里记录了什么内容。传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。\n我们以 Redis 收到“set testkey testvalue”命令后记录的日志为例，看看 AOF 日志的内容。其中，“*3”表示当前命令有三个部分，每部分都是由“$+数字”开头，后面紧跟着具体的命令、键或值。这里，“数字”表示这部分中的命令、键或值一共有多少字节。例如，“$3 set”表示这部分有 3 个字节，也就是“set”命令。\n但是，为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。\n而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。\n除此之外，AOF 还有一个好处：它是在命令执行后才记录日志，所以不会阻塞当前的写操作。\n不过，AOF 也有两个潜在的风险。\n首先，如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。如果此时 Redis 是用作缓存，还可以从后端数据库重新读入数据进行恢复，但是，如果 Redis 是直接用作数据库的话，此时，因为命令没有记入日志，所以就无法用日志进行恢复了。\n其次，AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。\n仔细分析的话，你就会发现，这两个风险都是和 AOF 写回磁盘的时机相关的。这也就意味着，如果我们能够控制一个写命令执行完后 AOF 日志写回磁盘的时机，这两个风险就解除了。\n三种写回策略 其实，对于这个问题，AOF 机制给我们提供了三个选择，也就是 AOF 配置项 appendfsync 的三个可选值。\nAlways，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；\nEverysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；\nNo，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。\n针对避免主线程阻塞和减少数据丢失问题，这三种写回策略都无法做到两全其美。我们来分析下其中的原因。\n “同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能； “操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了； “每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。  我把这三种策略的写回时机，以及优缺点汇总在了一张表格里，以方便你随时查看。\n到这里，我们就可以根据系统对高性能和高可靠性的要求，来选择使用哪种写回策略了。总结一下就是：想要获得高性能，就选择 No 策略；如果想要得到高可靠性保证，就选择 Always 策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择 Everysec 策略。\n但是，按照系统的性能需求选定了写回策略，并不是“高枕无忧”了。毕竟，AOF 是以文件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越大。这也就意味着，我们一定要小心 AOF 文件过大带来的性能问题。\n这里的“性能问题”，主要在于以下三个方面：一是，文件系统本身对文件大小有限制，无法保存过大的文件；二是，如果文件太大，之后再往里面追加命令记录的话，效率也会变低；三是，如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。\n所以，我们就要采取一定的控制手段，这个时候，AOF 重写机制就登场了。\n日志文件太大了怎么办？ 简单来说，AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。比如说，当读取了键值对“testkey”: “testvalue”之后，重写机制会记录 set testkey testvalue 这条命令。这样，当需要恢复时，可以重新执行该命令，实现“testkey”: “testvalue”的写入。\n为什么重写机制可以把日志文件变小呢? 实际上，重写机制具有“多变一”功能。所谓的“多变一”，也就是说，旧日志文件中的多条命令，在重写后的新日志中变成了一条命令。\n我们知道，AOF 文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个键值对当前的最新状态，为它生成对应的写入命令。这样一来，一个键值对在重写日志中只用一条命令就行了，而且，在日志恢复时，只用执行这条命令，就可以直接完成这个键值对的写入了。\n下面这张图就是一个例子：\n当我们对一个列表先后做了 6 次修改操作后，列表的最后状态是[“D”, “C”, “N”]，此时，只用 LPUSH u:list “N”, “C”, \u0026ldquo;D\u0026quot;这一条命令就能实现该数据的恢复，这就节省了五条命令的空间。对于被修改过成百上千次的键值对来说，重写能节省的空间当然就更大了。\n不过，虽然 AOF 重写后，日志文件会缩小，但是，要把整个数据库的最新数据的操作日志都写回磁盘，仍然是一个非常耗时的过程\nAOF 重写会阻塞吗? 和 AOF 日志由主线程写回不同，重写过程是由后台子进程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。我把重写的过程总结为“一个拷贝，两处日志”。\n  “一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。\n  “两处日志”，因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。\n  总结来说，每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。\nRDB 快照 上节课，我们学习了 Redis 避免数据丢失的 AOF 方法。这个方法的好处，是每次执行只需要记录操作命令，需要持久化的数据量不大。一般而言，只要你采用的不是 always 的持久化策略，就不会对性能造成太大影响。\n但是，也正因为记录的是操作命令，而不是实际的数据，所以，用 AOF 方法进行故障恢复的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。这当然不是理想的结果。那么，还有没有既可以保证可靠性，还能在宕机时实现快速恢复的其他方法呢？\n当然有了，这就是我们今天要一起学习的另一种持久化方法：内存快照。所谓内存快照，就是指内存中的数据在某一个时刻的状态记录。这就类似于照片，当你给朋友拍照时，一张照片就能把朋友一瞬间的形象完全记下来。\n对 Redis 来说，它实现类似照片记录效果的方式，就是把某一时刻的状态以文件的形式写到磁盘上，也就是快照。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就得到了保证。这个快照文件就称为 RDB 文件，其中，RDB 就是 Redis DataBase 的缩写。和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。听起来好像很不错，但内存快照也并不是最优选项。为什么这么说呢？\n我们还要考虑两个关键问题：\n 对哪些数据做快照？这关系到快照的执行效率问题； 做快照时，数据还能被增删改吗？这关系到 Redis 是否被阻塞，能否同时正常处理请求。  给哪些内存数据做快照？ Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中，这就类似于给 100 个人拍合影，把每一个人都拍进照片里。这样做的好处是，一次性记录了所有数据，一个都不少。\n当你给一个人拍照时，只用协调一个人就够了，但是，拍 100 人的大合影，却需要协调 100 个人的位置、状态，等等，这当然会更费时费力。同样，给内存的全量数据做快照，把它们全部写入磁盘也会花费很多时间。而且，全量数据越多，RDB 文件就越大，往磁盘上写数据的时间开销就越大。\n对于 Redis 而言，它的单线程模型就决定了，我们要尽量避免所有会阻塞主线程的操作，所以，针对任何操作，我们都会提一个灵魂之问：“它会阻塞主线程吗?”RDB 文件的生成是否会阻塞主线程，这就关系到是否会降低 Redis 的性能。\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。\n save：在主线程中执行，会导致阻塞； bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。  好了，这个时候，我们就可以通过 bgsave 命令来执行全量快照，这既提供了数据的可靠性保证，也避免了对 Redis 的性能影响。\n在对内存数据做快照时，这些数据还能“动”吗? 也就是说，这些数据还能被修改吗？ 这个问题非常重要，这是因为，如果数据能被修改，那就意味着 Redis 还能正常处理写操作。否则，所有写操作都得等到快照完了才能执行，性能一下子就降低了。\n快照时数据能修改吗? 在给别人拍照时，一旦对方动了，那么这张照片就拍糊了，我们就需要重拍，所以我们当然希望对方保持不动。对于内存快照而言，我们也不希望数据“动”。\n举个例子。我们在时刻 t 给内存做快照，假设内存数据量是 4GB，磁盘的写入带宽是 0.2GB/s，简单来说，至少需要 20s（4/0.2 = 20）才能做完。如果在时刻 t+5s 时，一个还没有被写入磁盘的内存数据 A，被修改成了 A’，那么就会破坏快照的完整性，因为 A’不是时刻 t 时的状态。因此，和拍照类似，我们在做快照时也不希望数据“动”，也就是不能被修改。\n但是，如果快照执行期间数据不能被修改，是会有潜在问题的。对于刚刚的例子来说，在做快照的 20s 时间里，如果这 4GB 的数据都不能被修改，Redis 就不能处理对这些数据的写操作，那无疑就会给业务服务造成巨大的影响。\n你可能会想到，可以用 bgsave 避免阻塞啊。这里我就要说到一个常见的误区了，避免阻塞和正常处理写操作并不是一回事。此时，主线程的确没有阻塞，可以正常接收请求，但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。\n为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。\n简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。\n此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本（键值对 C’）。然后，主线程在这个数据副本上进行修改。同时，bgsave 子进程可以继续把原来的数据（键值对 C）写入 RDB 文件。\n这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。\n到这里，我们就解决了对“哪些数据做快照”以及“做快照时数据能否修改”这两大问题：Redis 会使用 bgsave 对当前内存中的所有数据做快照，这个操作是子进程在后台完成的，这就允许主线程同时可以修改数据。\n可以每秒做一次快照吗？ 对于快照来说，所谓“连拍”就是指连续地做快照。这样一来，快照的间隔时间变得很短，即使某一时刻发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。但是，这其中的快照间隔时间就很关键了。\n如下图所示，我们先在 T0 时刻做了一次快照，然后又在 T0+t 时刻做了一次快照，在这期间，数据块 5 和 9 被修改了。如果在 t 这段时间内，机器宕机了，那么，只能按照 T0 时刻的快照进行恢复。此时，数据块 5 和 9 的修改值因为没有快照记录，就无法恢复了。\n所以，要想尽可能恢复数据，t 值就要尽可能小，t 越小，就越像“连拍”。那么，t 值可以小到什么程度呢，比如说是不是可以每秒做一次快照？毕竟，每次快照都是由 bgsave 子进程在后台执行，也不会阻塞主线程。\n这种想法其实是错误的。虽然 bgsave 执行时不阻塞主线程，但是，如果频繁地执行全量快照，也会带来两方面的开销。\n 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了（所以，在 Redis 中如果有一个 bgsave 在运行，就不会再启动第二个 bgsave 子进程）。那么，有什么其他好方法吗？  此时，我们可以做增量快照，所谓增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。\n在第一次做完全量快照后，T1 和 T2 时刻如果再做快照，我们只需要将被修改的数据写入快照文件就行。但是，这么做的前提是，我们需要记住哪些数据被修改了。你可不要小瞧这个“记住”功能，它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题。如下图所示：\n如果我们对每一个键值对的修改，都做个记录，那么，如果有 1 万个被修改的键值对，我们就需要有 1 万条额外的记录。而且，有的时候，键值对非常小，比如只有 32 字节，而记录它被修改的元数据信息，可能就需要 8 字节，这样的画，为了“记住”修改，引入的额外空间开销比较大。这对于内存资源宝贵的 Redis 来说，有些得不偿失。\n到这里，你可以发现，虽然跟 AOF 相比，快照的恢复速度快，但是，快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销，那么，还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做到尽量少丢数据呢？\nRedis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。\n这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。\n如下图所示，T1 和 T2 时刻的修改，用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。\n这个方法既能享受到 RDB 文件快速恢复的好处，又能享受到 AOF 只记录操作命令的简单优势，颇有点“鱼和熊掌可以兼得”的感觉，建议你在实践中用起来。\nRedis缓存的淘汰策略 Redis 4.0 之前一共实现了 6 种内存淘汰策略，在 4.0 之后，又增加了 2 种策略。我们可以按照是否会进行数据淘汰把它们分成两类：\n不进行数据淘汰的策略，只有 noeviction 这一种。\n会进行淘汰的 7 种其他策略。\n会进行淘汰的 7 种策略，我们可以再进一步根据淘汰候选数据集的范围把它们分成两类：\n在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis 4.0 后新增）四种。\n在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种。\n我把这 8 种策略的分类，画到了一张图里：\n下面我就来具体解释下各个策略。\n默认情况下，Redis 在使用的内存空间超过 maxmemory 值时，并不会淘汰数据，也就是设定的 noeviction 策略。对应到 Redis 缓存，也就是指，一旦缓存被写满了，再有写请求来时，Redis 不再提供服务，而是直接返回错误。Redis 用作缓存时，实际的数据集通常都是大于缓存容量的，总会有新的数据要写入缓存，这个策略本身不淘汰数据，也就不会腾出新的缓存空间，我们不把它用在 Redis 缓存中。\n我们再分析下 volatile-random、volatile-ttl、volatile-lru 和 volatile-lfu 这四种淘汰策略。它们筛选的候选数据范围，被限制在已经设置了过期时间的键值对上。也正因为此，即使缓存没有写满，这些数据如果过期了，也会被删除。\n例如，我们使用 EXPIRE 命令对一批键值对设置了过期时间后，无论是这些键值对的过期时间是快到了，还是 Redis 的内存使用量达到了 maxmemory 阈值，Redis 都会进一步按照 volatile-ttl、volatile-random、volatile-lru、volatile-lfu 这四种策略的具体筛选规则进行淘汰。\n volatile-ttl 在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。 volatile-random 就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。 volatile-lru 会使用 LRU 算法筛选设置了过期时间的键值对。 volatile-lfu 会使用 LFU 算法选择设置了过期时间的键值对。  可以看到，volatile-ttl 和 volatile-random 筛选规则比较简单，而 volatile-lru 因为涉及了 LRU 算法，所以我会在分析 allkeys-lru 策略时再详细解释。volatile-lfu 使用了 LFU 算法，我会在第 27 讲中具体解释，现在你只需要知道，它是在 LRU 算法的基础上，同时考虑了数据的访问时效性和数据的访问次数，可以看作是对淘汰策略的优化。\n相对于 volatile-ttl、volatile-random、volatile-lru、volatile-lfu 这四种策略淘汰的是设置了过期时间的数据，allkeys-lru、allkeys-random、allkeys-lfu 这三种淘汰策略的备选淘汰数据范围，就扩大到了所有键值对，无论这些键值对是否设置了过期时间。它们筛选数据进行淘汰的规则是：\n allkeys-random 策略，从所有键值对中随机选择并删除数据； allkeys-lru 策略，使用 LRU 算法在所有数据中进行筛选。 allkeys-lfu 策略，使用 LFU 算法在所有数据中进行筛选。  这也就是说，如果一个键值对被删除策略选中了，即使它的过期时间还没到，也需要被删除。当然，如果它的过期时间到了但未被策略选中，同样也会被删除。\nLRU 算法 LRU 算法的全称是 Least Recently UsedLRU(最近最少使用页面置换算法)，从名字上就可以看出，这是按照最近最少使用的原则来筛选数据，最不常用的数据会被筛选出来，而最近频繁使用的数据会留在缓存中。\n那具体是怎么筛选的呢？LRU 会把所有的数据组织成一个链表，链表的头和尾分别表示 MRU 端和 LRU 端，分别代表最近最常使用的数据和最近最不常用的数据。我们看一个例子。\n我们现在有数据 6、3、9、20、5。如果数据 20 和 3 被先后访问，它们都会从现有的链表位置移到 MRU 端，而链表中在它们之前的数据则相应地往后移一位。因为，LRU 算法选择删除数据时，都是从 LRU 端开始，所以把刚刚被访问的数据移到 MRU 端，就可以让它们尽可能地留在缓存中。\n如果有一个新数据 15 要被写入缓存，但此时已经没有缓存空间了，也就是链表没有空余位置了，那么，LRU 算法做两件事：\n数据 15 是刚被访问的，所以它会被放到 MRU 端；\n算法把 LRU 端的数据 5 从缓存中删除，相应的链表中就没有数据 5 的记录了。\n其实，LRU 算法背后的想法非常朴素：它认为刚刚被访问的数据，肯定还会被再次访问，所以就把它放在 MRU 端；长久不访问的数据，肯定就不会再被访问了，所以就让它逐渐后移到 LRU 端，在缓存满时，就优先删除它。\n不过，LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。而且，当有数据被访问时，需要在链表上把该数据移动到 MRU 端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。\n所以，在 Redis 中，LRU 算法被做了简化，以减轻数据淘汰对缓存性能的影响。具体来说，Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构 RedisObject 中的 lru 字段记录）。然后，Redis 在决定淘汰的数据时，第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去。\nRedis 提供了一个配置参数 maxmemory-samples，这个参数就是 Redis 选出的数据个数 N。例如，我们执行如下命令，可以让 Redis 选出 100 个数据作为候选数据集：\nCONFIG SET maxmemory-samples 100\n当需要再次淘汰数据时，Redis 需要挑选数据进入第一次淘汰时创建的候选集合。这儿的挑选标准是：能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值。当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了 maxmemory-samples，Redis 就把候选数据集中 lru 字段值最小的数据淘汰出去。\n这样一来，Redis 缓存不用为所有的数据维护一个大链表，也不用在每次数据访问时都移动链表项，提升了缓存的性能。\n但是，也正是因为只看数据的访问时间，使用 LRU 策略在处理扫描式单次查询操作时，无法解决缓存污染。所谓的扫描式单次查询操作，就是指应用对大量的数据进行一次全体读取，每个数据都会被读取，而且只会被读取一次。此时，因为这些被查询的数据刚刚被访问过，所以 lru 字段值都很大。\n在使用 LRU 策略淘汰数据时，这些数据会留存在缓存中很长一段时间，造成缓存污染。如果查询的数据量很大，这些数据占满了缓存空间，却又不会服务新的缓存请求，此时，再有新数据要写入缓存的话，还是需要先把这些旧数据替换出缓存才行，这会影响缓存的性能。\n为了方便你理解，我给你举个例子。如下图所示，数据 6 被访问后，被写入 Redis 缓存。但是，在此之后，数据 6 一直没有被再次访问，这就导致数据 6 滞留在缓存中，造成了污染。\n所以，对于采用了 LRU 策略的 Redis 缓存来说，扫描式单次查询会造成缓存污染。为了应对这类缓存污染问题\nLFU算法 LFU是最近最不常用页面置换算法(Least Frequently Used),也就是淘汰一定时期内被访问次数最少的页!LFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器，来统计这个数据的访问次数。当使用 LFU 策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。\n和那些被频繁访问的数据相比，扫描式单次查询的数据因为不会被再次访问，所以它们的访问次数不会再增加。因此，LFU 策略会优先把这些访问次数低的数据淘汰出缓存。这样一来，LFU 策略就可以避免这些数据对缓存造成污染了。\n为了避免操作链表的开销，Redis 在实现 LRU 策略时使用了两个近似方法：\n Redis 是用 RedisObject 结构来保存数据的，RedisObject 结构中设置了一个 lru 字段，用来记录数据的访问时间戳； Redis 并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量（例如 10 个）的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选。  在此基础上，Redis 在实现 LFU 策略的时候，只是把原来 24bit 大小的 lru 字段，又进一步拆分成了两部分。\n ldt 值：lru 字段的前 16bit，表示数据的访问时间戳； counter 值：lru 字段的后 8bit，表示数据的访问次数。  当 LFU 策略筛选数据时，Redis 会在候选集合中，根据数据 lru 字段的后 8bit 选择访问次数最少的数据进行淘汰。当访问次数相同时，再根据 lru 字段的前 16bit 值大小，选择访问时间最久远的数据进行淘汰。\nRedis 只使用了 8bit 记录数据的访问次数，而 8bit 记录的最大值是 255，这样可以吗？\n在实际应用中，一个数据可能会被访问成千上万次。如果每被访问一次，counter 值就加 1 的话，那么，只要访问次数超过了 255，数据的 counter 值就一样了。在进行数据淘汰时，LFU 策略就无法很好地区分并筛选这些数据，反而还可能会把不怎么访问的数据留存在了缓存中。\n 假设第一个数据 A 的累计访问次数是 256，访问时间戳是 202010010909，所以它的 counter 值为 255，而第二个数据 B 的累计访问次数是 1024，访问时间戳是 202010010810。如果 counter 值只能记录到 255，那么数据 B 的 counter 值也是 255。\n 此时，缓存写满了，Redis 使用 LFU 策略进行淘汰。数据 A 和 B 的 counter 值都是 255，LFU 策略再比较 A 和 B 的访问时间戳，发现数据 B 的上一次访问时间早于 A，就会把 B 淘汰掉。但其实数据 B 的访问次数远大于数据 A，很可能会被再次访问。这样一来，使用 LFU 策略来淘汰数据就不合适了。\n因此，在实现 LFU 策略时，Redis 并没有采用数据每被访问一次，就给对应的 counter 值加 1 的计数规则，而是采用了一个更优化的计数规则。\n 简单来说，LFU 策略实现的计数规则是：每当数据被访问一次时，首先，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1。\n 下面这段 Redis 的部分源码，显示了 LFU 策略增加计数器值的计算逻辑。其中，baseval 是计数器当前的值。计数器的初始值默认是 5（由代码中的 LFU_INIT_VAL 常量设置），而不是 0，这样可以避免数据刚被写入缓存，就因为访问次数少而被立即淘汰。\ndouble r = (double)rand()/RAND_MAX;\r...\rdouble p = 1.0/(baseval*server.lfu_log_factor+1);\rif (r \u0026lt; p) counter++;  使用了这种计算规则后，我们可以通过设置不同的 lfu_log_factor 配置项，来控制计数器值增加的速度，避免 counter 值很快就到 255 了。为了更进一步说明 LFU 策略计数器递增的效果，你可以看下下面这张表。这是 Redis官网上提供的一张表，它记录了当 lfu_log_factor 取不同值时，在不同的实际访问次数情况下，计数器的值是如何变化的。\n可以看到，当 lfu_log_factor 取值为 1 时，实际访问次数为 100K 后，counter 值就达到 255 了，无法再区分实际访问次数更多的数据了。而当 lfu_log_factor 取值为 100 时，当实际访问次数为 10M 时，counter 值才达到 255，此时，实际访问次数小于 10M 的不同数据都可以通过 counter 值区分出来。\n正是因为使用了非线性递增的计数器方法，即使缓存数据的访问次数成千上万，LFU 策略也可以有效地区分不同的访问次数，从而进行合理的数据筛选。从刚才的表中，我们可以看到，当 lfu_log_factor 取值为 10 时，百、千、十万级别的访问次数对应的 counter 值已经有明显的区分了，所以，我们在应用 LFU 策略时，一般可以将 lfu_log_factor 取值为 10。\n在一些场景下，有些数据在短时间内被大量访问后就不会再被访问了。那么再按照访问次数来筛选的话，这些数据会被留存在缓存中，但不会提升缓存命中率。为此，Redis 在实现 LFU 策略时，还设计了一个 counter 值的衰减机制。\n 简单来说，LFU 策略使用衰减因子配置项 lfu_decay_time 来控制访问次数的衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU 策略再把这个差值除以 lfu_decay_time 值，所得的结果就是数据 counter 要衰减的值。\n 简单举个例子，假设 lfu_decay_time 取值为 1，如果数据在 N 分钟内没有被访问，那么它的访问次数就要减 N。如果 lfu_decay_time 取值更大，那么相应的衰减值会变小，衰减效果也会减弱。所以，如果业务应用中有短时高频访问的数据的话，建议把 lfu_decay_time 值设置为 1，这样一来，LFU 策略在它们不再被访问后，会较快地衰减它们的访问次数，尽早把它们从缓存中淘汰出去，避免缓存污染。\n总结 LRU 和 LFU 两个策略都有应用。LRU 和 LFU 两个策略关注的数据访问特征各有侧重，LRU 策略更加关注数据的时效性，而 LFU 策略更加关注数据的访问频次。通常情况下，实际应用的负载具有较好的时间局部性，所以 LRU 策略的应用会更加广泛。但是，在扫描式查询的应用场景中，LFU 策略就可以很好地应对缓存污染问题了，建议你优先使用。\n优先使用 allkeys-lru 策略。这样，可以充分利用 LRU 这一经典缓存算法的优势，把最近最常访问的数据留在缓存中，提升应用的访问性能。如果你的业务数据中有明显的冷热数据区分，我建议你使用 allkeys-lru 策略。如果业务应用中的数据访问频率相差不大，没有明显的冷热数据区分，建议使用 allkeys-random 策略，随机选择淘汰的数据就行。\n如果你的业务中有置顶的需求，比如置顶新闻、置顶视频，那么，可以使用 volatile-lru 策略，同时不给这些置顶数据设置过期时间。这样一来，这些需要置顶的数据一直不会被删除，而其他数据会在过期时根据 LRU 规则进行筛选。\nRedis中的问题 在实际应用 Redis 缓存时，经常会遇到一些异常问题，概括来说有 4 个方面：==缓存中的数据和数据库中的不一致；缓存雪崩；缓存击穿和缓存穿透==\n数据不一致问题 先删除缓存，再更新数据库 假设线程 A 删除缓存值后，还没有来得及更新数据库（比如说有网络延迟），线程 B 就开始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取。这会带来两个问题：\n 线程 B 读取到了旧值； 线程 B 是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会导致其他线程从缓存中读到旧值。  等到线程 B 从数据库读取完数据、更新了缓存后，线程 A 才开始更新数据库，此时，缓存中的数据是旧值，而数据库中的是最新值，两者就不一致了。\n我用一张表来汇总下这种情况。\n在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除操作。\n之所以要加上 sleep 的这段时间，就是为了让线程 B 能够先从数据库读取数据，再把缺失的数据写入缓存，然后，线程 A 再进行删除。所以，线程 A sleep 的时间，就需要大于线程 B 读取数据再写入缓存的时间。这个时间怎么确定呢？建议你在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算。\n这样一来，其它线程读取数据时，会发现缓存缺失，所以会从数据库中读取最新值。因为这个方案会在第一次删除缓存值后，延迟一段时间再次进行删除，所以我们也把它叫做延迟双删。\n下面的这段伪代码就是“延迟双删”方案的示例。\nredis.delKey(X)\rdb.update(X)\rThread.sleep(N)\rredis.delKey(X)\r 先更新数据库值，再删除缓存值 如果线程 A 删除了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。不过，在这种情况下，如果其他线程并发读缓存的请求不多，那么，就不会有很多请求读取到旧值。而且，线程 A 一般也会很快删除缓存值，这样一来，其他线程再次读取时，就会发生缓存缺失，进而从数据库中读取最新值。所以，这种情况对业务的影响较小。\n我再画一张表，带你总结下先更新数据库、再删除缓存值的情况。\n对比 优先使用先更新数据库再删除缓存的方法，原因主要有两个：\n  先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力；\n  如果业务应用中读取数据库和写缓存的时间不好估算，那么，延迟双删中的等待时间就不好设置。\n  缓存雪崩 缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。\n缓存雪崩一般是由两个原因导致的。\n第一个原因是：缓存中有大量数据同时过期，导致大量请求无法得到处理。\n 当数据保存在缓存中，并且设置了过期时间时，如果在某一个时刻，大量数据同时过期，此时，应用再访问这些数据的话，就会发生缓存缺失。紧接着，应用就会把请求发送给数据库，从数据库中读取数据。如果应用的并发请求量很大，那么数据库的压力也就很大，这会进一步影响到数据库的其他正常业务请求处理。我们来看一个简单的例子，如下图所示：\n 针对大量数据同时失效带来的缓存雪崩问题\n首先，我们可以避免给大量的数据设置相同的过期时间。如果业务层的确要求有些数据同时失效，你可以在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟），这样一来，不同数据的过期时间有所差别，但差别又不会太大，既避免了大量数据同时过期，同时也保证了这些数据基本在相近的时间失效，仍然能满足业务需求。\n除了微调过期时间，我们还可以通过服务降级，来应对缓存雪崩。\n所谓的服务降级，是指发生缓存雪崩时，针对不同的数据采取不同的处理方式。\n 当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息； 当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。  这样一来，只有部分过期数据的请求会发送到数据库，数据库的压力就没有那么大了。下面这张图显示的是服务降级时数据请求的执行情况，你可以看下。\n除了大量数据同时失效会导致缓存雪崩，还有一种情况也会发生缓存雪崩，那就是，Redis 缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。\n一般来说，一个 Redis 实例可以支持数万级别的请求处理吞吐量，而单个数据库可能只能支持数千级别的请求处理吞吐量，它们两个的处理能力可能相差了近十倍。由于缓存雪崩，Redis 缓存失效，所以，数据库就可能要承受近十倍的请求压力，从而因为压力过大而崩溃。\n此时，因为 Redis 实例发生了宕机，我们需要通过其他方法来应对缓存雪崩了。我给你提供两个建议。\n第一个建议，是在业务系统中实现服务熔断或请求限流机制。\n所谓的服务熔断，是指在发生缓存雪崩时，为了防止引发连锁的数据库雪崩，甚至是整个系统的崩溃，我们暂停业务应用对缓存系统的接口访问。再具体点说，就是业务应用调用缓存接口时，缓存客户端并不把请求发给 Redis 缓存实例，而是直接返回，等到 Redis 缓存实例重新恢复服务后，再允许应用请求发送到缓存系统。\n这样一来，我们就避免了大量请求因缓存缺失，而积压到数据库系统，保证了数据库系统的正常运行。\n在业务系统运行时，我们可以监测 Redis 缓存所在机器和数据库所在机器的负载指标，例如每秒请求数、CPU 利用率、内存利用率等。如果我们发现 Redis 缓存实例宕机了，而数据库所在机器的负载压力突然增加（例如每秒请求数激增），此时，就发生缓存雪崩了。大量请求被发送到数据库进行处理。我们可以启动服务熔断机制，暂停业务应用对缓存服务的访问，从而降低对数据库的访问压力，如下图所示：\n服务熔断虽然可以保证数据库的正常运行，但是暂停了整个缓存系统的访问，对业务应用的影响范围大。为了尽可能减少这种影响，我们也可以进行请求限流。这里说的请求限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。\n我给你举个例子。假设业务系统正常运行时，请求入口前端允许每秒进入系统的请求是 1 万个，其中，9000 个请求都能在缓存系统中进行处理，只有 1000 个请求会被应用发送到数据库进行处理。\n一旦发生了缓存雪崩，数据库的每秒请求数突然增加到每秒 1 万个，此时，我们就可以启动请求限流机制，在请求入口前端只允许每秒进入系统的请求数为 1000 个，再多的请求就会在入口前端被直接拒绝服务。所以，使用了请求限流，就可以避免大量并发请求压力传递到数据库层。\n使用服务熔断或是请求限流机制，来应对 Redis 实例宕机导致的缓存雪崩问题，是属于“事后诸葛亮”，也就是已经发生缓存雪崩了，我们使用这两个机制，来降低雪崩对数据库和整个业务系统的影响。\n我给你的第二个建议就是事前预防。\n通过主从节点的方式构建 Redis 缓存高可靠集群。如果 Redis 缓存的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题。\n缓存击穿 缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。缓存击穿的情况，经常发生在热点数据过期失效时，如下图所示：\n为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别频繁的热点数据，我们就不设置过期时间了。这样一来，对热点数据的访问请求，都可以在缓存中进行处理，而 Redis 数万级别的高吞吐量可以很好地应对大量的并发请求访问。\n缓存穿透 缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。此时，应用也无法从数据库中读取数据再写入缓存，来服务后续请求，这样一来，缓存也就成了“摆设”，如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力，如下图所示：\n那么，缓存穿透会发生在什么时候呢？一般来说，有两种情况。\n 业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据； 恶意攻击：专门访问数据库中没有的数据。  为了避免缓存穿透的影响，我来给你提供三种应对方案。\n第一种方案是，缓存空值或缺省值。\n一旦发生缓存穿透，我们就可以针对查询的数据，在 Redis 中缓存一个空值或是和业务层协商确定的缺省值（例如，库存的缺省值可以设为 0）。紧接着，应用发送的后续请求再进行查询时，就可以直接从 Redis 中读取空值或缺省值，返回给业务应用了，避免了把大量请求发送给数据库处理，保持了数据库的正常运行。\n第二种方案是，使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。\n我们先来看下，布隆过滤器是如何工作的。\n布隆过滤器由一个初值都为 0 的 bit 数组和 N 个哈希函数组成，可以用来快速判断某个数据是否存在。当我们想标记某个数据存在时（例如，数据已被写入数据库），布隆过滤器会通过三个操作完成标记：\n首先，使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值。\n然后，我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置。\n最后，我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作。\n如果数据不存在（例如，数据库里没有写入数据），我们也就没有用布隆过滤器标记过数据，那么，bit 数组对应 bit 位的值仍然为 0。\n当需要查询某个数据时，我们就执行刚刚说的计算过程，先得到这个数据在 bit 数组中对应的 N 个位置。紧接着，我们查看 bit 数组中这 N 个位置上的 bit 值。只要这 N 个 bit 值有一个不为 1，这就表明布隆过滤器没有对该数据做过标记，所以，查询的数据一定没有在数据库中保存。为了便于你理解，我画了一张图，你可以看下。\n图中布隆过滤器是一个包含 10 个 bit 位的数组，使用了 3 个哈希函数，当在布隆过滤器中标记数据 X 时，X 会被计算 3 次哈希值，并对 10 取模，取模结果分别是 1、3、7。所以，bit 数组的第 1、3、7 位被设置为 1。当应用想要查询 X 时，只要查看数组的第 1、3、7 位是否为 1，只要有一个为 0，那么，X 就肯定不在数据库中。\n正是基于布隆过滤器的快速检测特性，我们可以在把数据写入数据库时，使用布隆过滤器做个标记。当缓存缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。如果不存在，就不用再去数据库中查询了。这样一来，即使发生缓存穿透了，大量请求只会查询 Redis 和布隆过滤器，而不会积压到数据库，也就不会影响数据库的正常运行。布隆过滤器可以使用 Redis 实现，本身就能承担较大的并发访问压力。\n最后一种方案是，在请求入口的**前端进行请求检测。**缓存穿透的一个原因是有大量的恶意请求访问不存在的数据，所以，一个有效的应对方案是在请求入口前端，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库。这样一来，也就不会出现缓存穿透问题了。\n跟缓存雪崩、缓存击穿这两类问题相比，缓存穿透的影响更大一些，希望你能重点关注一下。从预防的角度来说，我们需要避免误删除数据库和缓存中的数据；从应对角度来说，我们可以在业务系统中使用缓存空值或缺省值、使用布隆过滤器，以及进行恶意请求检测等方法。\n小结 这节课，我们学习了缓存雪崩、击穿和穿透这三类异常问题。从问题成因来看，缓存雪崩和击穿主要是因为数据不在缓存中了，而缓存穿透则是因为数据既不在缓存中，也不在数据库中。所以，缓存雪崩或击穿时，一旦数据库中的数据被再次写入到缓存后，应用又可以在缓存中快速访问数据了，数据库的压力也会相应地降低下来，而缓存穿透发生时，Redis 缓存和数据库会同时持续承受请求压力。\n为了方便你掌握，我把这三大问题的原因和应对方案总结到了一张表格，你可以再复习一下。\n最后，我想强调一下，服务熔断、服务降级、请求限流这些方法都是属于“有损”方案，在保证数据库和整体系统稳定的同时，会对业务应用带来负面影响。例如使用服务降级时，有部分数据的请求就只能得到错误返回信息，无法正常处理。如果使用了服务熔断，那么，整个缓存系统的服务都被暂停了，影响的业务范围更大。而使用了请求限流机制后，整个业务系统的吞吐率会降低，能并发处理的用户请求会减少，会影响到用户体验。\n所以，我给你的建议是，尽量使用预防式方案：\n 针对缓存雪崩，合理地设置数据过期时间，以及搭建高可靠缓存集群； 针对缓存击穿，在缓存访问非常频繁的热点数据时，不要设置过期时间； 针对缓存穿透，提前在入口前端实现恶意请求检测，或者规范数据库的数据删除操作，避免误删除。  ","id":3,"section":"posts","summary":"[TOC] 这篇文章主要记录自己学习 Redis 的历程 Redis 的数据结构 redis中数据结构有 String（字符串）、List（列表）、Hash（哈希）、Set（集合","tags":["Redis"],"title":"Redis学习","uri":"https://wzgl998877.github.io/2022/05/redis%E6%80%BB%E7%BB%93/","year":"2022"},{"content":"[TOC]\ngit是工作中天天用到的东西，但好像也只会几个命令，现在稍微研究下\ngit简介 本地 Git 的三个分区分别是：working directory，stage/index area，commit history。\n working directory是「工作目录」，也就是我们肉眼能够看到的文件，也就是工作区 我们在work dir中执行git add相关命令后，就会把work dir中的修改添加到「暂存区」stage area（或者叫index area）中去，stage 就叫做暂存区。 当stage中存在修改时，我们使用git commit相关命令之后，就会把stage中的修改保存到「提交历史」commit history中，也就是HEAD指针指向的位置。  三个区的状态转移图为\ngit命令 git add git add fileName // 将某个文件加入到暂存区\rgit add . // 将所有修改过的文件加入到暂存区\r 相对应的有 git checkout ，git reset\ngit checkout fileName // 将某个文件从暂存区还原到工作区\rgit checkout . git reset fileName // 将某个文件从暂存区还原到工作区\rgit reset . // 恢复到工作区\r git commit git commit -m '这里写提交原因'\r commit完之后，突然发现一些错别字需要修改，又不想为改几个错别字而新开一个commit到history区，那么就可以使用下面这个命令：\n$ git commit --amend\r 这个场景，我说一个极端一点的例子：比如我从 GitHub 上clone了一个项目，然后乱改了一通代码，结果发现我写的代码根本跑不通，于是后悔了，干脆不改了，我想恢复成最初的模样，怎么办？\n依然是使用checkout命令，但是和之前的使用方式有一些不同：\n$ git checkout HEAD .\rUpdated 12 paths from d480c4f\r 这样，work dir和stage中所有的「修改」都会被撤销，恢复成HEAD指向的那个history commit。\n注意，类似之前通过stage恢复work dir的checkout命令，这里撤销的也只是修改，新增的文件不会被撤销。\n需求一，合并多个commit。\n比如说我本地从17bd20c到HEAD有多个commit，但我希望把他们合并成一个commit推到远程仓库，这时候就可以使用reset命令：\n$ git reset 17bd20c\r$ git add .\r$ git commit -m 'balabala'\r 回顾一下刚才说的reset命令的作用，相当于把 HEAD 移到了17bd20c这个commit，而且不会修改work dir中的数据，所以只要add再commit，就相当于把中间的多个commit合并到一个了。\n需求二，由于HEAD指针的回退，导致有的commit在git log命令中无法看到，怎么得到它们的 Hash 值呢？\n再重复一遍，只要你不乱动本地的.git文件夹，任何修改只要提交到commit history中，都永远不会丢失，看不到某些commit只是因为它们不是我们当前HEAD位置的「历史」提交，我们可以使用如下命令查看操作记录：\n$ git reflog\r 比如reset，checkout等等关键操作都会在这里留下记录，所有commit的 Hash 值都能在这里找到，所以如果你发现有哪个commit突然找不到了，一定都可以在这里找到。\n","id":4,"section":"posts","summary":"[TOC] git是工作中天天用到的东西，但好像也只会几个命令，现在稍微研究下 git简介 本地 Git 的三个分区分别是：working directory，st","tags":null,"title":"git相关","uri":"https://wzgl998877.github.io/2022/03/git%E7%9B%B8%E5%85%B3/","year":"2022"},{"content":"[TOC]\n这篇文章主要对Mysql是怎样运行的进行提炼总结以及补充\nMySQL 的存储引擎 常用的存储引擎有：ARCHIVE、BLACKHOLE、InnoDB、MyISAM等，最常用的就是InnoDB 和MyISAM，其中InnoDB 是MySQL 默认的存储引擎，主要研究的就是 InnoDB\nInnoDB 存储引擎行格式 InnoDB 存储引擎行格式，分别是Compact 、Redundant 、Dynamic 和Compressed，其中mysql5.6 默认使用 Compact，mysql 5.7 默认使用Dynamic\n额外信息   变长字段长度列表\nMySQL 支持一些变长的数据类型，比如VARCHAR(M) 、VARBINARY(M) 、各种TEXT 类型，各种BLOB 类型，把拥有这些数据类型的列称为变长字段，变长字段中存储多少字节的数据是不固定的，所以我们在存储真实数据的时候需要顺便把这些数据占用的字节数也存起来,把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长字段长度列表，各变长字段数据占用的字节数按照列的顺序逆序存放\n  NULL值列表\n把允许为null的列，按照列的顺序逆序排列，每个允许存储NULL 的列对应一个二进制位，二进制位的值为1 时，代表该列的值为NULL 。 二进制位的值为0 时，代表该列的值不为NULL 。 MySQL 规定NULL值列表必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节的高位补0 ，如下图\n  记录头信息\n除了变长字段长度列表、NULL值列表之外，还有一个用于描述记录的记录头信息，它是由固定的5 个字节组成。5 个字节也就是40 个二进制位，不同的位代表不同的意思，如图所示：\n   名称 大小（单位：bit 位） 描述     预留位1 1 没有使用   预留位2 1 没有使用   delete_mask 1 标记该记录是否被删除，值为0 的时候代表记录并没有被删除，为1 的时候代表记录被删除掉了   min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记   n_owned 4 表示当前记录拥有的记录数   heap_no 13 这个属性表示当前记录在本页中的位置   record_type 3 表示当前记录的类型， 0 表示普通记录， 1 表示B+树非叶节点记录， 2 表示最小记录， 3表示最大记录   next_record 16 表示下一条记录的相对位置      真实数据   隐藏数据\n记录的真实数据除了我们自己定义的列的数据以外， MySQL 会为每个记录默认的添加一些列（也称为隐藏列），具体的列如下：DB_ROW_ID （行ID唯一标识一条记录）、DB_TRX_ID（事务ID）、DB_ROLL_PTR（回滚指针）。这里需要提一下InnoDB 表对主键的生成策略：优先使用用户自定义主键作为主键，如果用户没有定义主键，则选取一个Unique 键作为主键，如果表中连Unique 键都没有定义的话，则InnoDB 会为表默认添加一个名为row_id 的隐藏列作为主键 ，因此 DB_TRX_ID（事务ID）、DB_ROLL_PTR（回滚指针）是mysql一定包含，但DB_ROW_ID不一定，生成位置如下：\n    行溢出数据\n我们知道对于VARCHAR(M) 类型的列最多可以占用65535 个字节。其中的M 代表该类型最多存储的字符数量。why？一个可变字段允许存储的最大字节数的长度最多只能用2个字节存储，而两个字节能表示的最大长度就是256*256=65536然后需要减去一个标志位。MySQL 对一条记录占用的最大存储空间是有限制的，除了BLOB 或者TEXT 类型的列之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535 个字节，也就是说2个字节用于存储真实数据的长度（最多）。所以MySQL 服务器建议我们把很长的字符存储类型改为TEXT 或者BLOB 的类型。\n  记录中的数据太多产生的溢出\nMySQL 中磁盘和内存交互的基本单位是页，也就是说MySQL 是以页为基本单位来管理存储空间的，我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB ，也就是16384 字节，而一个VARCHAR(M) 类型的列就最多可以存储65532 个字节，这样就可能造成一个页存放不了一条记录的尴尬情况\n对于Compact 和Reduntant 行格式来说，如果某一列中的数据非常多的话，在本记录的真实数据处只会存储该列的前768 个字节的数据和一个指向其他页的地址，然后把剩下的数据存放到其他页中，这个过程也叫做行溢出，存储超出768 字节的那些页面也被称为溢出页，如图\n  对于Dynamic 和Compressed 行格式 ，这俩行格式和Compact 行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768 个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样：\nInnoDB数据页结构 页是InnoDB 管理存储空间的基本单位，一个页的大小一般是16KB 。InnoDB 为了不同的目的而设计了许多种不同类型的页，比如存放表空间头部信息的页，存放Insert Buffer信息的页，存放INODE 信息的页，存放undo 日志信息的页等。我们聚焦的是那些存放我们表中记录的那种类型的页，官方称这种存放记录的页为索引（ INDEX ）页。数据页代表的这块16KB 大小的存储空间可以被划分为多个部分，不同部分有不同的功能，各个部分如图所示：\n记录在页中的存储 我们自己存储的记录会按照我们指定的行格式存储到User Records 部分，那么行在页中是怎么存储的呢？这时候就用到了行中的记录头信息\n1、heap_no 这个属性表示当前记录在本页中的位置，heap_no 值为0 和1由mysql自动生成，所以有时候也称为伪记录或者虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录\n由于这两条记录不是我们自己定义的记录，所以它们并不存放在页的User Records 部分，他们被单独放在一个称为Infimum + Supremum 的部分。\n2、next_record，它表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。比方说第一条记录的next_record 值为32 ，意味着从第一条记录的真实数据的地址处向后找32 个字节便是下一条记录的真实数据。这就是链表！！！，可以通过一条记录找到它的下一条记录。但是需要注意注意再注意的一点是， 下一条记录指得并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定 Infimum记录（也就是最小记录） 的下一条记录就是本页中主键值最小的用户记录，而本页中主键值最大的用户记录的下一条记录就是 Supremum记录（也就是最大记录）\n显然记录按照主键从小到大的顺序形成了一个单链表\n最大记录的next_record 的值为0 ，这也就是说最大记录是没有下一条记录了，它是这个单链表中的最后一个节点。如果从中删除掉一条记录，这个链表也是会跟着变化的，比如我们把第2条记录删掉：\n从图中可以看出来，删除第2条记录前后主要发生了这些变化：\n  第2条记录并没有从存储空间中移除，而是把该条记录的delete_mask 值设置为1\n  第2条记录的next_record 值变为了0，意味着该记录没有下一条记录了。\n  第1条记录的next_record 指向了第3条记录。\n  还有一点你可能忽略了，就是最大记录的n_owned 值从5 变成了4 ，关于这一点的变化我们稍后会详细说明的。所以，不论我们怎么对页中的记录做增删改操作，InnoDB始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。\n  Page Directory（页目录） 目录的制作过程是这样的：\n 将所有正常的记录（包括最大和最小记录，不包括标记为已删除的记录）划分为几个组。 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned 属性表示该记录拥有多少条记录，也就是该组内共有几条记录。 将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页的尾部的地方，这个地方就是所谓的Page Directory ，也就是页目录 页面目录中的这些地址偏移量被称为槽（英文名： Slot ），所以这个页面目录就是由槽组成的。  比方说现在的page_demo 表中正常的记录共有6条， InnoDB 会把它们分成两组，第一组中只有一个最小记录，第二组中是剩余的5条记录，看下边的示意图：\n从这个图中我们需要注意这么几点：\n 现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组， 槽1 中的值是112 ，代表最大记录 的地址偏移量（就是从页面的0字节开始数，数112个字节）； 槽0 中的值是99 ，代表最小记录的地址偏移 量。 注意最小和最大记录的头信息中的n_owned 属性  最小记录的n_owned 值为1 ，这就代表着以最小记录结尾的这个分组中只有1 条记录，也就是最小记录 本身。 最大记录的n_owned 值为5 ，这就代表着以最大记录结尾的这个分组中只有5 条记录，包括最大记录本 身还有我们自己插入的4 条记录。    用图表示就是：\n为什么最小记录的n_owned 值为1，而最大记录的n_owned 值为5 呢，这里头有什么猫腻么？ 是的，设计InnoDB 的大叔们对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录， 最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。 所以分组是按照下边的步骤进行的：\n 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对 应的记录的n_owned 值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一 个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。  了解了页目录的组成后，那么怎么就加快查找速度的过程呢？看下这个例子\n比方说我们想找主键值为6 的记录，过程是这样的：\n 计算中间槽的位置： (0+4)/2=2 ，所以查看槽2 对应记录的主键值为8 ，又因为8 \u0026gt; 6 ，所以设置 high=2 ， low 保持不变。 重新计算中间槽的位置： (0+2)/2=1 ，所以查看槽1 对应的主键值为4 ，又因为4 \u0026lt; 6 ，所以设置 low=1 ， high 保持不变。 因为high - low 的值为1，所以确定主键值为5 的记录在槽2 对应的组中。此刻我们需要找到槽2 中主键 值最小的那条记录，然后沿着单向链表遍历槽2 中的记录。但是我们前边又说过，每个槽对应的记录都是该 组中主键值最大的记录，这里槽2 对应的记录是主键值为8 的记录，怎么定位一个组中最小的记录呢？别忘 了各个槽都是挨着的，我们可以很轻易的拿到槽1 对应的记录（主键值为4 ），该条记录的下一条记录就 是槽2 中主键值最小的记录，该记录的主键值为5 。所以我们可以从这条主键值为5 的记录出发，遍历槽 2 中的各条记录，直到找到主键值为6 的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以 遍历一个组中的记录的代价是很小的。  这就是典型的二分法哈哈，总结：\n 通过二分法确定该记录所在的槽，并找到该槽中主键值最小的那条记录。 通过记录的next_record 属性遍历该槽所在的组中的各个记录。  Page Header（页面头部） 设计InnoDB 的大叔们为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header 的部分，它是页结构的第二部分，这个部分占用固定的56 个字节，专门存储各种状态信息，具体各个字节都是干嘛的看下\n   名称 占用空间大小 描述     PAGE_N_DIR_SLOTS 2 字节 在页目录中的槽数量   PAGE_HEAP_TOP 2 字节 还未使用的空间最小地址，也就是说从该地址之后就是Free Space   PAGE_N_HEAP 2 字节 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录）   PAGE_FREE 2 字节 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record 也会组成一个单链表，这个单链表中的记录可以被重新利用）   PAGE_GARBAGE 2 字节 已删除记录占用的字节数   PAGE_LAST_INSERT 2 字节 最后插入记录的位置   PAGE_DIRECTION 2 字节 记录插入的方向   PAGE_N_DIRECTION 2 字节 一个方向连续插入的记录数量   PAGE_N_RECS 2 字节 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录）   PAGE_MAX_TRX_ID 8 字节 修改当前页的最大事务ID，该值仅在二级索引中定义   PAGE_LEVEL 2 字节 当前页在B+树中所处的层级   PAGE_INDEX_ID 8 字节 索引ID，表示当前页属于哪个索引   PAGE_BTR_SEG_LEAF 10 字节 B+树叶子段的头部信息，仅在B+树的Root页定义   PAGE_BTR_SEG_TOP 10 字节 B+树非叶子段的头部信息，仅在B+树的Root页定义     PAGE_DIRECTION 假如新插入的一条记录的主键值比上一条记录的主键值大，我们说这条记录的插入方向是右边，反之则是左 边。用来表示最后一条记录插入方向的状态就是PAGE_DIRECTION 。 PAGE_N_DIRECTION 假设连续几次插入新记录的方向都是一致的， InnoDB 会把沿着同一个方向插入记录的条数记下来，这个条 数就用PAGE_N_DIRECTION 这个状态表示。当然，如果最后一条记录的插入方向改变了的话，这个状态的值 会被清零重新统计。  File Header（文件头部） ​\tPage Header 是专门针对数据页记录的各种状态信息，比方说页里头有多少个记录了呀，有多少个槽了呀。我们现在描述的File Header 针对各种类型的页都通用，也就是说不同类型的页都会以File Header 作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页 这个部分占用固定的38 个字节，是由下边这些内容组成的\n   名称 占用空间大小 描述     FIL_PAGE_SPACE_OR_CHKSUM 4 字节 页的校验和（checksum值）   FIL_PAGE_OFFSET 4 字节 页号，InnoDB 通过页号来可以唯一定位一个页   FIL_PAGE_PREV 4 字节 上一个页的页号   FIL_PAGE_NEXT 4 字节 下一个页的页号   FIL_PAGE_LSN 8 字节 页面被最后修改时对应的日志序列位置（英文名是：Log SequenceNumber）   FIL_PAGE_TYPE 2 字节 该页的类型   FIL_PAGE_FILE_FLUSH_LSN 8 字节 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值   FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 字节 页属于哪个表空间    详解：\n  FIL_PAGE_SPACE_OR_CHKSUM\n这个代表当前页面的校验和（checksum）。啥是个校验和？就是对于一个很长很长的字节串来说，我们会 通过某种算法来计算一个比较短的值来代表这个很长的字节串，这个比较短的值就称为校验和。这样在比 较两个很长的字节串之前先比较这两个长字节串的校验和，如果校验和都不一样两个长字节串肯定是不同 的，所以省去了直接比较两个比较长的字节串的时间损耗。\n  FIL_PAGE_TYPE\n这个代表当前页的类型，我们前边说过， InnoDB 为了不同的目的而把页分为不同的类型，我们上边介绍的 其实都是存储记录的数据页，其实还有很多别的类型的页，我们存放记录的数据页的类型其实是FIL_PAGE_INDEX ，也就是所谓的索引页。\n  FIL_PAGE_PREV 和FIL_PAGE_NEXT\n我们前边强调过， InnoDB 都是以页为单位存放数据的，有时候我们存放某种类型的数据占用的空间非常大 （比方说一张表中可以有成千上万条记录）， InnoDB 可能不可以一次性为这么多数据分配一个非常大的存 储空间，如果分散到多个不连续的页中存储的话需要把这些页关联起来， FIL_PAGE_PREV 和FIL_PAGE_NEXT 就分别代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了， 而无需这些页在物理上真正连着\n  总结  InnoDB为了不同的目的而设计了不同类型的页，我们把用于存放记录的页叫做数据页。 一个数据页可以被大致划分为7个部分，分别是   File Header ，表示页的一些通用信息，占固定的38字节。 Page Header ，表示数据页专有的一些信息，占固定的56个字节。 Infimum + Supremum ，两个虚拟的伪记录，分别表示页中的最小和最大记录，占固定的26 个字节。 User Records ：真实存储我们插入的记录的部分，大小不固定。 Free Space ：页中尚未使用的部分，大小不确定。 Page Directory ：页中的某些记录相对位置，也就是各个槽在页面中的地址偏移量，大小不固定，插 入的记录越多，这个部分占用的空间越多。 File Trailer ：用于检验页是否完整的部分，占用固定的8个字节。  每个记录的头信息中都有一个next_record 属性，从而使页中的所有记录串联成一个单链表。 InnoDB 会为把页中的记录划分为若干个组，每个组的最后一个记录的地址偏移量作为一个槽，存放在 Page Directory 中，所以在一个页中根据主键查找记录是非常快的，分为两步：   通过二分法确定该记录所在的槽。 通过记录的next_record属性遍历该槽所在的组中的各个记录。  每个数据页的File Header 部分都有上一个和下一个页的编号，所以所有的数据页会组成一个双链表。 为保证从内存中同步到磁盘的页的完整性，在页的首部和尾部都会存储页中数据的校验和和页面最后修改时 对应的LSN 值，如果首部和尾部的校验和和LSN 值校验不成功的话，就说明同步过程出现了问题。  B+树索引 各个数据页可以组成一个双向链表，而每个数据页中的记录会按照主键值从小到大的顺序组成一个单向链表，每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。页和记录的关系示意图如下：\n新建一个index_demo 表，该表有2个INT 类型的列，1个CHAR(1) 类型的列，而且我们规定了c1 列为主键，这个表使用Compact 行格式来实际存储记录的。为了我们理解上的方便，我们简化了一下index_demo 表的行格式示 意图：\n把一些记录放到页里边的示意图就是：\n一个简单的索引方案 ​\t我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以不得不依次遍历所有的数据页\n​\t所以如果我们想快速的定位到需要查找的记录在哪些数据页中该咋办？还记得我们为根据主键值快速定位一条记录在页中的位置而设立的页目录么？我们也可以想办法为快速定位记录所在的数据页而建立一个别的目录，建这个目录必须完成下边这些事儿：\n  下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。\n我们这里需要做一个假设：假设我们的每个数据页最多能存放3条记录（实际上一个数据页非常大，可以存放下好多记录）。有了这个假设之后我们向index_demo 表插入3条记录：\nmysql\u0026gt; INSERT INTO index_demo VALUES(1, 4, 'u'), (3, 9, 'd'), (5, 3, 'y');\r 那么这些记录已经按照主键值的大小串联成一个单向链表了，如图所示：\n  此时我们再来插入一条记录:\nINSERT INTO index_demo VALUES(4, 4, 'a');\r 因为页10 最多只能放3条记录，所以我们不得不再分配一个新页：\n新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着。它们只是通过维护着上一个页和下一个页的编号而建立了链表关系\n页10 中用户记录最大的主键值是5 ，而页28 中有一条记录的主键值是4 ，因为5\u0026gt;4 ，所以这就不符合下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值的要求，所以在插入主键值为4 的记录的时候需要伴随着一次记录移动，也就是把主键值为5 的记录移动到页28 中，然后再把主键值为4 的记录插入到页10 中，这个过程的示意图如下:\n这个过程表明了在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程我们也可以称为页分裂。\n  给所有的页建立一个目录项\n由于数据页的编号可能并不是连续的，所以在向index_demo 表中插入许多条记录后，可能是这样的效果：\n  ​\t因为这些16KB 的页在物理存储上可能并不挨着，所以如果想从这么多页中根据主键值快速定位某些记录所在的页，我们需要给它们做个目录，每个页对应一个目录项，每个目录项包括下边两个部分：\n 页的用户记录中最小的主键值，我们用key 来表示。 页号，我们用page_no 表示。  所以我们为上边几个页做好的目录就像这样子：\n以页28 为例，它对应目录项2 ，这个目录项中包含着该页的页号28 以及该页中用户记录的最小主键值5 。我们只需要把几个目录项在物理存储器上连续存储，比如把他们放到一个数组里，就可以实现根据主键值快速查找某条记录的功能了。比方说我们想找主键值为20 的记录，具体查找过程分两步：\n 先从目录项中根据二分法快速确定出主键值为20 的记录在目录项3 中（因为 12 \u0026lt; 20 \u0026lt; 209 ），它对应的页是页9 。 再根据前边说的在页中查找记录的方式去页9 中定位具体的记录。  至此，针对数据页做的简易目录就搞定了。不过忘了说了，这个目录有一个别名，称为索引\nInnoDB中的索引方案 上边之所以称为一个简易的索引方案，是因为我们为了在根据主键值进行查找时使用二分法快速定位具体的目录项而假设所有目录项都可以在物理存储器上连续存储，但是这样做有几个问题：\n InnoDB 是使用页来作为管理存储空间的基本单位，也就是最多能保证16KB 的连续存储空间，而随着表中记录数量的增多，需要非常大的连续的存储空间才能把所有的目录项都放下，这对记录数量非常多的表是不现实的。 我们时常会对记录进行增删，假设我们把页28 中的记录都删除了， 页28 也就没有存在的必要了，那意味着目录项2 也就没有存在的必要了，这就需要把目录项2 后的目录项都向前移动一下，这种牵一发而动全身的设计不是什么好主意～  所以，设计InnoDB 的大叔们需要一种可以灵活管理所有目录项的方式。他们灵光乍现，忽然发现这些目录项其实长得跟我们的用户记录差不多，只不过目录项中的两个列是主键和页号而已，所以他们复用了之前存储用户记录的数据页来存储目录项，为了和用户记录做一下区分，我们把这些用来表示目录项的记录称为目录项记录。那InnoDB 怎么区分一条记录是普通的用户记录还是目录项记录呢？别忘了记录头信息里的 record_type 属性，它的各个取值代表的意思如下：\n 0 ：普通的用户记录 1 ：目录项记录 2 ：最小记录 3 ：最大记录 哈哈，原来这个值为1 的record_type 是这个意思呀，我们把前边使用到的目录项放到数据页中的样子就是这样：  从图中可以看出来，我们新分配了一个编号为30 的页来专门存储目录项记录。这里再次强调一遍目录项记录和普通的用户记录的不同点：\n 目录项记录的record_type 值是1，而普通用户记录的record_type 值是0。 目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，可能包含很多列，另外还有InnoDB 自己添加的隐藏列。 还记得我们之前在唠叨记录头信息的时候说过一个叫min_rec_mask 的属性么，只有在存储目录项记录的页中的主键值最小的目录项记录的min_rec_mask 值为1 ，其他别的记录的min_rec_mask 值都是0 。  除了上述几点外，这两者就没啥差别了，它们用的是一样的数据页，页的组成结构也是一样一样的（就是我们前边介绍过的7个部分），都会为主键值生成Page Directory （页目录），从而在按照主键值进行查找时可以使用二分法来加快查询速度。现在以查找主键为20 的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步:\n 先到存储目录项记录的页，也就是页30 中通过二分法快速定位到对应目录项，因为12 \u0026lt; 20 \u0026lt; 209 ，所以定位到对应的记录所在的页就是页9 。 再到存储用户记录的页9 中根据二分法快速定位到主键值为20 的用户记录  虽然说目录项记录中只存储主键值和对应的页号，比用户记录需要的存储空间小多了，但是不论怎么说一个页只有16KB 大小，能存放的目录项记录也是有限的，那如果表中的数据太多，以至于一个数据页不足以存放所有的目录项记录，该咋办呢？\n当然是再多整一个存储目录项记录的页喽～ 为了大家更好的理解新分配一个目录项记录页的过程，我们假设一个存储目录项记录的页最多只能存放4条目录项记录，所以如果此时我们再向上图中插入一条主键值为320 的用户记录的话，那就需要分配一个新的存储目录项记录的页喽：\n从图中可以看出，我们插入了一条主键值为320 的用户记录之后需要两个新的数据页：\n 为存储该用户记录而新生成了页31 。 因为原先存储目录项记录的页30 的容量已满（我们前边假设只能存储4条目录项记录），所以不得不需要一个新的页32 来存放页31 对应的目录项。  现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤，以查找主键值为20 的记录为例：\n 确定目录项记录页 我们现在的存储目录项记录的页有两个，即页30 和页32 ，又因为页30 表示的目录项的主键值的范围是[1, 320) ， 页32 表示的目录项的主键值不小于320 ，所以主键值为20 的记录对应的目录项记录在页30中。 通过目录项记录页确定用户记录真实所在的页。 在真实存储用户记录的页中定位到具体的记录。  那么问题来了，在这个查询步骤的第1步中我们需要定位存储目录项记录的页，但是这些页在存储空间中也可能不挨着，如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个存储目录项记录的页呢？其实也简单，为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子\n如图，我们生成了一个存储更高级目录项的页33 ，这个页中的两条记录分别代表页30 和页32 ，如果用户记录的主键值在[1, 320) 之间，则到页30 中查找更详细的目录项记录，如果主键值不小于320 的话，就到页32中查找更详细的目录项记录。随着表中记录的增加，这个目录的层级会继续增加，如果简化一下，那么我们可以用下边这个图来描述它:\n这他妈的就是B+树了！！！\n聚簇索引 我们上边介绍的B+ 树本身就是一个目录，或者说本身就是一个索引。它有两个特点：\n 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义：   页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。  B+ 树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。  我们把具有这两种特性的B+ 树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL 语句中显式的使用INDEX 语句去创建（后边会介绍索引相关的语句），InnoDB 存储引擎会自动的为我们创建聚簇索引。另外有趣的一点是，在InnoDB 存储引擎中， 聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。\n二级索引 ​\t大家有木有发现，上边介绍的聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+ 树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件该咋办呢？难道只能从头到尾沿着链表依次遍历记录么？不，我们可以多建几棵B+ 树，不同的B+ 树中的数据采用不同的排序规则。比方说我们用c2 列的大小作为数据页、页中记录的排序规则，再建一棵B+ 树，效果如下图所示：\n这个B+ 树与上边介绍的聚簇索引有几处不同：\n 使用记录c2 列的大小进行记录和页的排序，这包括三个方面的含义：  页内的记录是按照c2 列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2 列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2 列大小顺序排成一个双向链表。   B+ 树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。  所以如果我们现在想通过c2 列的值查找某些记录的话就可以使用我们刚刚建好的这个B+ 树了。以查找c2 列的值为4 的记录为例，查找过程如下：\n 确定目录项记录页根据根页面，也就是页44 ，可以快速定位到目录项记录所在的页为页42 （因为2 \u0026lt; 4 \u0026lt; 9 ）。 通过目录项记录页确定用户记录真实所在的页。 在页42 中可以快速定位到实际存储用户记录的页，但是由于c2 列并没有唯一性约束，所以c2 列值为4 的记录可能分布在多个数据页中，又因为2 \u0026lt; 4 ≤ 4 ，所以确定实际存储用户记录的页在页34 和页35 中。 在真实存储用户记录的页中定位到具体的记录。 到页34 和页35 中定位到具体的记录。 但是这个B+ 树的叶子节点中的记录只存储了c2 和c1 （也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。  我们根据这个以c2 列大小排序的B+ 树只能确定我们要查找记录的主键值，所以如果我们想根据c2 列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程也被称为回表。也就是根据c2 列的值查询一条完整的用户记录需要使用到2 棵B+ 树！！！\n因为这种按照非主键列建立的B+ 树需要一次回表操作才可以定位到完整的用户记录，所以这种B+ 树也被称为二级索引（英文名secondary index ），或者辅助索引。由于我们使用的是c2 列的大小作为B+ 树的排序规则，所以我们也称这个B+ 树为为c2列建立的索引。\n联合索引 我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+ 树按照c2和c3 列的大小进行排序，这个包含两层含义：\n 先把各个记录和页按照c2 列进行排序。 在记录的c2 列相同的情况下，采用c3 列进行排序  如图所示，我们需要注意一下几点：\n 每条目录项记录都由c2 、c3 、页号这三个部分组成，各条记录先按照c2 列的值进行排序，如果记录的c2 列相同，则按照c3 列的值进行排序。 B+ 树叶子节点处的用户记录由c2 、c3 和主键c1 列组成。  千万要注意一点，以c2和c3列的大小为排序规则建立的B+树称为联合索引，本质上也是一个二级索引。\nB+树索引的注意事项 根页面万年不动窝 一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建立一个索引，那么它的根节点的页号便会被记录到某个地方，然后凡是InnoDB 存储引擎需要用到这个索引的时候，都会从那个固定的地方取出根节点的页号，从而来访问这个索引。\n内节点中目录项记录的唯一性 我们知道B+ 树索引的内节点中目录项记录的内容是索引列 + 页号的搭配，但是对于二级索引来说，为了让新插入记录能找到自己在哪个页里，我们需要保证在B+树的同一层内节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的：\n 索引列的值 主键值 页号  也就是我们把主键值也添加到二级索引内节点中的目录项记录了，这样就能保证B+ 树每一层节点中各条目录项记录除页号这个字段外是唯一的，所以我们为c2 列建立二级索引后的示意图实际上应该是这样子的\n一个页面最少存储2条记录 ​\t我们前边说过一个B+树只需要很少的层级就可以轻松存储数亿条记录，查询速度杠杠的！这是因为B+树本质上就是一个大的多层级目录，每经过一个目录时都会过滤掉许多无效的子目录，直到最后访问到存储真实数据的目录。那如果一个大的目录中只存放一个子目录是个啥效果呢？那就是目录层级非常非常非常多，而且最后的那个存放真实数据的目录中只能存放一条记录。所以InnoDB 的一个数据页至少可以存放两条记录，这也是我们之前唠叨记录行格式的时候说过一个结论（我们当时依据这个结论推导了表中只有一个列时该列在不发生行溢出的情况下最多能存储多少字节，忘了的话回去看看吧）。\nMyISAM中的索引方案简单介绍 ​\t至此，我们介绍的都是InnoDB 存储引擎中的索引方案，我们有必要再简单介绍一下MyISAM 存储引擎中的索引方案。我们知道InnoDB 中索引即数据，也就是聚簇索引的那棵B+ 树的叶子节点中已经把所有完整的用户记录都包含了，而MyISAM 的索引方案虽然也使用B+树，但是却将索引和数据分开存储：\n 将表中的记录按照记录的插入顺序单独存储在一个文件中，称之为数据文件。这个文件并不划分为若干个数据页，有多少记录就往这个文件中塞多少记录就成了。我们可以通过行号而快速访问到一条记录。 MyISAM 记录也需要记录头信息来存储一些额外数据，我们以上边唠叨过的index_demo 表为例，看一下这个表中的记录使用MyISAM 作为存储引擎在存储空间中的表示：  ​\t由于在插入数据的时候并没有刻意按照主键大小排序，所以我们并不能在这些数据上使用二分法进行查找。\n  使用MyISAM 存储引擎的表会把索引信息另外存储到一个称为索引文件的另一个文件中。MyISAM 会单独为表的主键创建一个索引，只不过在索引的叶子节点中存储的不是完整的用户记录，而是主键值 + 行号的组合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！这一点和InnoDB 是完全不相同的，在InnoDB 存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找就能找到对应的记录，而在MyISAM 中却需要进行一次回表操作，意味着MyISAM 中建立的索引相当于全部都是二级索引\n  如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB 中的索引差不多，不过在叶子节点处存储的是相应的列 + 行号。这些索引也全部都是二级索引\n  Myisam 和 InnoDB 区别   InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从MyISAM 变成 InnoDB 的重要原因之一；\n  InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转MYISAM 会失败；\n  InnoDB 是聚簇索引，MyISAM 是非聚簇索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。\n  InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描。而MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；\n  InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；\n  MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)\n  B+树索引的使用  B+ 树索引总结:\n 每个索引都对应一棵B+ 树， B+ 树分为好多层，最下边一层是叶子节点，其余的是内节点。所有用户记录都存储在B+ 树的叶子节点，所有目录项记录都存储在内节点。 InnoDB 存储引擎会自动为主键（如果没有它会自动帮我们添加）建立聚簇索引，聚簇索引的叶子节点包含完整的用户记录。 我们可以为自己感兴趣的列建立二级索引， 二级索引的叶子节点包含的用户记录由索引列 + 主键组成，所以如果想通过二级索引来查找完整的用户记录的话，需要通过回表操作，也就是在通过二级索引找到主键值之后再到聚簇索引中查找完整的用户记录。 B+ 树中每层节点都是按照索引列值从小到大的顺序排序而组成了双向链表，而且每个页内的记录（不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序而形成了一个单链表。如果是联合索引的话，则页面和记录先按照联合索引前边的列排序，如果该列值相同，再按照联合索引后边的列排序。 通过索引查找记录是从B+ 树的根节点开始，一层一层向下搜索。由于每个页面都按照索引列的值建立了Page Directory （页目录），所以在这些页面中的查找非常快。  索引的代价   空间上的代价 这个是显而易见的，每建立一个索引都要为它建立一棵B+ 树，每一棵B+ 树的每一个节点都是一个数据页，一个页默认会占用16KB 的存储空间，一棵很大的B+ 树由许多数据页组成，那可是很大的一片存储空间呢。 时间上的代价 每次对表中的数据进行增、删、改操作时，都需要去修改各个B+ 树索引。而且我们讲过， B+ 树每层节点都是按照索引列的值从小到大的顺序排序而组成了双向链表。不论是叶子节点中的记录，还是内节点中的记录（也就是不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序而形成了一个单向链表。而 增、删、改操作可能会对节点和记录的排序造成破坏，所以存储引擎需要额外的时间进行一些记录移位，页面分裂、页面回收啥的操作来维护好节点和记录的排序。如果我们建了许多索引，每个索引对应的B+ 树都要进行相关的维护操作，这还能不给性能拖后腿么？  所以说，一个表上索引建的越多，就会占用越多的存储空间，在增删改记录的时候性能就越差。\nB+树索引适用的条件  下边我们将唠叨许多种让B+ 树索引发挥最大效能的技巧和注意事项，先创建一个person_info表，这个表是用来存储人的一些基本信息的：\nCREATE TABLE person_info(\rid INT NOT NULL auto_increment,\rname VARCHAR(100) NOT NULL,\rbirthday DATE NOT NULL,\rphone_number CHAR(11) NOT NULL,\rcountry varchar(100) NOT NULL,\rPRIMARY KEY (id),\rKEY idx_name_birthday_phone_number (name, birthday, phone_number)\r);\r 对于这个person_info 表我们需要注意两点：\n 表中的主键是id 列，它存储一个自动递增的整数。所以InnoDB 存储引擎会自动为id 列建立聚簇索引。 我们额外定义了一个二级索引idx_name_birthday_phone_number ，它是由3个列组成的联合索引。所以在这个索引对应的B+ 树的叶子节点处存储的用户记录只保留name 、birthday 、phone_number 这三个列的值以及主键id 的值，并不会保存country 列的值。  person_info 表会为聚簇索引和idx_name_birthday_phone_number 索引建立2棵B+ 树。下边我们画一下索引idx_name_birthday_phone_number 的示意图，不过既然我们已经掌握了InnoDB 的B+ 树索引原理，那我们在画图的时候为了让图更加清晰，所以在省略一些不必要的部分，比如记录的额外信息，各页面的页号等等，其中内节点中目录项记录的页号信息我们用箭头来代替，在记录结构中只保留name 、birthday 、phone_number 、id 这四个列的真实数据值\n从图中可以看出这个idx_name_birthday_phone_number 索引对应的B+ 树中页面和记录的排序方式就是这的：\n 先按照name 列的值进行排序。 如果name 列的值相同，则按照birthday 列的值进行排序。 如果birthday 列的值也相同，则按照phone_number 的值进行排序  全值匹配  如果我们的搜索条件中的列和索引列一致的话，这种情况就称为全值匹配,例如：\nSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27' AND phone_number = '15123983239';\r 这个毫无疑问会走索引，但WHERE 子句中的几个搜索条件的顺序对查询结果有啥影响么？也就是说如果我们调换name 、birthday 、phone_number 这几个搜索列的顺序对查询的执行过程有影响么？例如：\nSELECT * FROM person_info WHERE birthday = '1990-09-27' AND phone_number = '15123983239' AND name = 'Ashburn';\r 答案是：没影响哈。MySQL 有查询优化器，会分析这些搜索条件并且按照可以使用的索引中列的顺序来决定先使用哪个搜索条件，后使用哪个搜索条件。\n匹配左边的列  其实在我们的搜索语句中也可以不用包含全部联合索引中的列，只包含左边的就行，比方说下边的查询语句：\nSELECT * FROM person_info WHERE name = 'Ashburn';\r或者包含多个左边的列也行：\rSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27';\r 那这条查询语句能用到索引吗？\nSELECT * FROM person_info WHERE birthday = '1990-09-27';\r 答案是用不到，因为B+ 树的数据页和记录先是按照name 列的值排序的，在name 列的值相同的情况下才使用birthday 列进行排序，也就是说name 列的值不同的记录中birthday 的值可能是无序的需要特别注意的一点是，如果我们想使用联合索引中尽可能多的列，搜索条件中的各个列必须是联合索引中从最左边开始连续的列。比方说联合索引idx_name_birthday_phone_number 中列的定义顺序是name 、birthday 、phone_number ，如果我们的搜索条件中只有name 和phone_number ，而没有中间的birthday ，比方说这样：\nSELECT * FROM person_info WHERE name = 'Ashburn' AND phone_number = '15123983239';\r 这样只能用到name 列的索引， birthday 和phone_number 的索引就用不上了，因为name 值相同的记录先按照birthday 的值进行排序， birthday 值相同的记录才按照phone_number 值进行排序\n匹配列前缀  为某个列建立索引的意思其实就是在对应的B+ 树的记录中使用该列的值进行排序，比方说person_info 表上建立的联合索引idx_name_birthday_phone_number 会先用name 列的值进行排序，字符串排序使用的当然就是字典序，也就是说这些字符串的前n个字符，也就是前缀都是排好序的，所以对于字符串类型的索引列来说，我们只匹配它的前缀也是可以快速定位记录的，例如：\n走索引 SELECT * FROM person_info WHERE name LIKE 'As%';\r不走索引 SELECT * FROM person_info WHERE name LIKE '%As%';\r 匹配范围值  idx_name_birthday_phone_number 索引的B+ 树示意图，所有记录都是按照索引列的值从小到大的顺序排好序的，所以这极大的方便我们查找索引列的值在某个范围内的记录。比方说下边这个查询语句\nSELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow';\r 由于B+ 树中的数据页和记录是先按name 列排序的，所以我们上边的查询过程其实是这样的：\n 找到name 值为Asa 的记录(查找到范围的下限)。 遍历链表找到name 值为Barlow 的记录（查找到范围的上限）由于所有记录都是由链表连起来的（记录之间用单链表，数据页之间用双链表） 找到这些记录的主键值，再到聚簇索引中回表查找完整的记录。  注意\n如果对多个列同时进行范围查找的话，只有对索引最左边的那个列进行范围查找的时候才能用到B+ 树索引，\nSELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow' AND birthday \u0026gt; '1980-01-01';\r 上边这个查询可以分成两个部分：\n 通过条件name \u0026gt; \u0026lsquo;Asa\u0026rsquo; AND name \u0026lt; \u0026lsquo;Barlow\u0026rsquo; 来对name 进行范围，查找的结果可能有多条name 值不同的 记录 对这些name 值不同的记录继续通过birthday \u0026gt; \u0026lsquo;1980-01-01\u0026rsquo; 条件继续过滤。 这样子对于联合索引idx_name_birthday_phone_number 来说，只能用到name 列的部分，而用不到birthday 列的部分，因为只有name 值相同的情况下才能用birthday 列的值进行排序，而这个查询中通过name 进行范围查找的记录中可能并不是按照birthday 列进行排序的，所以在搜索条件中继续以birthday 列进行查找时是用不到这个B+ 树索引的。  而与上面相反的是，精确匹配某一列并范围匹配另外一列,如\nSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday \u0026gt; '1980-01-01' AND birthday \u0026lt; '2000-12-31' AND phone_number \u0026gt; '15100000000';\r 这个查询的条件可以分为3个部分：\n name = \u0026lsquo;Ashburn\u0026rsquo; ，对name 列进行精确查找，当然可以使用B+ 树索引了。 birthday \u0026gt; \u0026lsquo;1980-01-01\u0026rsquo; AND birthday \u0026lt; \u0026lsquo;2000-12-31\u0026rsquo; ，由于name 列是精确查找，所以通过name =\u0026lsquo;Ashburn\u0026rsquo; 条件查找后得到的结果的name 值都是相同的，它们会再按照birthday 的值进行排序。所以此时对birthday 列进行范围查找是可以用到B+ 树索引的。 phone_number \u0026gt; \u0026lsquo;15100000000\u0026rsquo; ，通过birthday 的范围查找的记录的birthday 的值可能不同，所以这个条件无法再利用B+ 树索引了，只能遍历上一步查询得到的记录。  用于排序  我们在写查询语句的时候经常需要对查询出来的记录通过ORDER BY 子句按照某种规则进行排序。一般情况下，我们只能把记录都加载到内存中，再用一些排序算法，比如快速排序、归并排序等等在内存中对这些记录进行排序，有的时候可能查询的结果集太大以至于不能在内存中进行排序的话，还可能暂时借助磁盘的空间来存放中间结果，排序操作完成后再把排好序的结果集返回到客户端。在MySQL 中，把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名： filesort ），文件排序是很慢的。但是如果ORDER BY 子句里使用到了我们的索引列，就有可能省去在内存或文件中排序的步骤，比如下边这个简单的查询语句\nSELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10;\r 因为有idx_name_birthday_phone_number 索引，所以直接从索引中提取数据，然后进行回表操作取出所有数据\n对于联合索引有个问题需要注意， ORDER BY 的子句后边的列的顺序也必须按照索引列的顺序给出，否则也不能使用索引，同理， ORDER BY name 、ORDER BY name, birthday 这种匹配索引左边的列的形式可以使用部分的B+ 树索引。当联合索引左边列的值为常量，也可以使用后边的列进行排序，比如这样\nSELECT * FROM person_info WHERE name = 'A' ORDER BY birthday, phone_number LIMIT 10;\r 不可以使用索引进行排序的几种情况  ASC、DESC混用 对于使用联合索引进行排序的场景，我们要求各个排序列的排序顺序是一致的，也就是要么各个列都是ASC 规则 排序，要么都是DESC 规则排序。\n如果查询中的各个排序列的排序顺序是一致的，比方说下边这两种情况：\n ORDER BY name, birthday LIMIT 10 这种情况直接从索引的最左边开始往右读10行记录就可以了。 ORDER BY name DESC, birthday DESC LIMIT 10 ， 这种情况直接从索引的最右边开始往左读10行记录就可以了。  但是是先按照name 列进行升序排列，再按照birthday 列进行降序排列的话，比如说这样的查询语句：\nSELECT * FROM person_info ORDER BY name, birthday DESC LIMIT 10;\r 就不能走索引\nWHERE子句中出现非排序使用到的索引列 SELECT * FROM person_info WHERE country = 'China' ORDER BY name LIMIT 10;\r 这个查询只能先把符合搜索条件country = \u0026lsquo;China\u0026rsquo; 的记录提取出来后再进行排序，使用不到索引。\nSELECT * FROM person_info WHERE name = 'A' ORDER BY birthday, phone_number LIMIT 10;\r 虽然这个查询也有搜索条件，但是name = \u0026lsquo;A\u0026rsquo; 可以使用到索引idx_name_birthday_phone_number ，而且过滤剩 下的记录还是按照birthday 、phone_number 列排序的，所以还是可以使用索引进行排序的\n排序列包含非同一个索引的列 有时候用来排序的多个列不是一个索引里的，这种情况也不能使用索引进行排序，比方说：\nSELECT * FROM person_info ORDER BY name, country LIMIT 10;\r name 和country (即使country为索引列也不行)并不属于一个联合索引中的列，所以无法使用索引进行排序\n排序列使用了复杂的表达式 SELECT * FROM person_info ORDER BY UPPER(name) LIMIT 10;\r 使用了UPPER 函数修饰过的列就不是单独的列了，这样就无法使用索引进行排序。\n用于分组 SELECT name, birthday, phone_number, COUNT(*) FROM person_info GROUP BY name, birthday, phone_number\r 和使用B+ 树索引进行排序使用规则相同，分组列的顺序也需要和索引列的顺序一致，也可以只使用索引列中左边的列进行分组等\n回表的代价  SELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow';\r 在使用idx_name_birthday_phone_number 索引进行查询时大致可以分为这两个步骤：\n 从索引idx_name_birthday_phone_number 对应的B+ 树中取出name 值在Asa ～ Barlow 之间的用户记录。 由于索引idx_name_birthday_phone_number 对应的B+ 树用户记录中只包含name 、birthday 、 phone_number 、id 这4个字段，而查询列表是* ，意味着要查询表中所有字段，也就是还要包括country 字段。这时需要把从上一步中获取到的每一条记录的id 字段都到聚簇索引对应的B+ 树中找到完整的用户记录，也就是我们通常所说的回表，然后把完整的用户记录返回给查询用户。  由于索引idx_name_birthday_phone_number 对应的B+ 树中的记录首先会按照name 列的值进行排序，所以值在Asa ～ Barlow 之间的记录在磁盘中的存储是相连的，集中分布在一个或几个数据页中，我们可以很快的把这些连着的记录从磁盘中读出来，这种读取方式我们也可以称为顺序I/O。根据第1步中获取到的记录的id 字段的值可能并不相连，而在聚簇索引中记录是根据id （也就是主键）的顺序排列的，所以根据这些并不连续的id值到聚簇索引中访问完整的用户记录可能分布在不同的数据页中，这样读取完整的用户记录可能要访问更多的数据页，这种读取方式我们也可以称为随机I/O 。一般情况下，顺序I/O比随机I/O的性能高很多。所以这个使用索引idx_name_birthday_phone_number 的查询有这么两个特点：\n 会使用到两个B+ 树索引，一个二级索引，一个聚簇索引。 访问二级索引使用顺序I/O ，访问聚簇索引使用随机I/O 。  需要回表的记录越多，使用二级索引的性能就越低，甚至让某些查询宁愿使用全表扫描也不使用二级索引。比方说name 值在Asa ～ Barlow 之间的用户记录数量占全部记录数量90%以上，那么如果使用idx_name_birthday_phone_number 索引的话，有90%多的id 值需要回表，还不如直接去扫描聚簇索引（也就是全表扫描）。\n那什么时候采用全表扫描的方式，什么时候使用采用二级索引 + 回表的方式去执行查询呢？\n查询优化器会事先对表中的记录计算一些统计数据，然后再利用这些统计数据根据查询的条件来计算一下需要回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用二级索引 + 回表的方式。\n一般情况下，限制查询获取较少的记录数会让优化器更倾向于选择使用二级索引 + 回表的方式进行查询，因为回表的记录越少，性能提升就越高，比方说上边的查询可以改写成这样：\nSELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow' LIMIT 10;\r同样排序也可以：SELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10;\r 覆盖索引 为了彻底告别回表操作带来的性能损耗，最好在查询列表里只包含索引列，比如：\nSELECT name, birthday, phone_number FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlo\rw'\r 因为只查询name , birthday , phone_number 这三个索引列的值，所以通idx_name_birthday_phone_number 索引得到结果后就不必到聚簇索引中再查找记录的剩余列，这样就省去了回表操作带来的性能损耗。我们把这种只需要用到索引的查询方式称为索引覆盖,所以坚决不能用* 号作为查询列表，最好把我们需要查询的列依次标明。\n如何挑选索引  只为用于搜索、排序或分组的列创建索引 只为出现在WHERE 子句中的列、连接子句中的连接列，或者出现在ORDER BY 或GROUP BY 子句中的 列创建索引。而出现在查询列表中的列就没必要建立索引了\n考虑列的基数 列的基数指的是某一列中不重复数据的个数，比方说某个列包含值2, 5, 8, 2, 5, 8, 2, 5, 8 ，虽然有9 条记录，但该列的基数却是3 。也就是说，在记录行数一定的情况下，列的基数越大，该列中的值越分散，列的基数越小，该列中的值越集中。假设某个列的基数为1 ，也就是所有记录在该列中的值都一样，那为该列建立索引是没有用的，因为所有值都一样就无法排序，无法进行快速查找了，而且如果某个建立了二级索引的列的重复值特别多，那么使用这个二级索引查出的记录还可能要做回表操作，这样性能损耗就更大了。\n所以结论就是：最好为那些列的基数大的列建立索引，为基数太小列的建立索引效果可能不好（索引尽量建在数据重复不多的列上，比如XX_id,但XX_time上则不合适）\n索引列的类型尽量小 我们在定义表结构的时候要显式的指定列的类型，以整数类型为例，有TINYINT 、MEDIUMINT 、INT 、BIGINT 这么几种，它们占用的存储空间依次递增，我们这里所说的类型大小指的就是该类型表示的数据范围的大小。 能表示的整数范围当然也是依次递增，如果我们想要对某个整数列建立索引的话，在表示的整数范围允许的情况 下，尽量让索引列使用较小的类型，比如我们能使用INT 就不要使用BIGINT ，能使用MEDIUMINT 就不要使用 INT ～ 这是因为：\n 数据类型越小，在查询时进行的比较操作越快（这是CPU层次的东东） 数据类型越小，索引占用的存储空间就越少，在一个数据页内就可以放下更多的记录，从而减少磁盘I/O 带 来的性能损耗，也就意味着可以把更多的数据页缓存在内存中，从而加快读写效率。  这个建议对于表的主键来说更加适用，因为不仅是聚簇索引中会存储主键值，其他所有的二级索引的节点处都会 存储一份记录的主键值，如果主键适用更小的数据类型，也就意味着节省更多的存储空间和更高效的I/O\n索引字符串值的前缀 只对字符串的前几个字符进行索引也就是说在二级索引的记录中只保留字符串前几个字符。这样在查找记录时虽然不能精确的定位到记录的位置，但是能定位到相应前缀所在的位置，然后根据前缀相同的记录的主键值回表查询完整的字符串值，再对比就好了。这样只在B+ 树中存储字符串的前几个字符的编码，既节约空间，又减少了字符串的比较时间，比方说我们在建表语句中只对name 列的前10个字符进行索引可以这么写：\nCREATE TABLE person_info(\rname VARCHAR(100) NOT NULL,\rbirthday DATE NOT NULL,\rphone_number CHAR(11) NOT NULL,\rcountry varchar(100) NOT NULL,\rKEY idx_name_birthday_phone_number (name(10), birthday, phone_number)\r);\r name(10) 就表示在建立的B+ 树索引中只保留记录的前10 个字符的编码，这种只索引字符串值的前缀的策略是 我们非常鼓励的，尤其是在字符串类型能存储的字符比较多的时候。\nSELECT * FROM person_info ORDER BY name LIMIT 10;\r 因为二级索引中不包含完整的name 列信息，所以无法对前十个字符相同，后边的字符不同的记录进行排序，也 就是使用索引列前缀的方式无法支持使用索引排序，只好乖乖的用文件排序喽。\n让索引列在比较表达式中单独出现 假设表中有一个整数列my_col ，我们为这个列建立了索引。下边的两个WHERE 子句虽然语义是一致的，但是在 效率上却有差别：\n WHERE my_col * 2 \u0026lt; 4 WHERE my_col \u0026lt; 4/2  第1个WHERE 子句中my_col 列并不是以单独列的形式出现的，而是以my_col * 2 这样的表达式的形式出现的， 存储引擎会依次遍历所有的记录，计算这个表达式的值是不是小于4 ，所以这种情况下是使用不到为my_col 列 建立的B+ 树索引的。而第2个WHERE 子句中my_col 列并是以单独列的形式出现的，这样的情况可以直接使用 B+ 树索引。 所以结论就是：如果索引列在比较表达式中不是以单独列的形式出现，而是以某个表达式，或者函数调用形式出 现的话，是用不到索引的。\n主键插入顺序 我们知道，对于一个使用InnoDB 存储引擎的表来说，在我们没有显式的创建索引时，表中的数据实际上都是存储在聚簇索引的叶子节点的。而记录又是存储在数据页中的，数据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果我们插入的记录的主键值是依次增大的话，那我们每插满一个数据页就换到下一个数据页继续插，而如果我们插入的主键值忽大忽小的话，这就比较麻烦了，假设某个数据页存储的记录已经满了，它存储的主键值在1~100 之间：\n如果此时再插入一条主键值为9 的记录，那它插入的位置就如下图：\n可这个数据页已经满了啊，再插进来咋办呢？我们需要把当前页面分裂成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗最好让插入的记录的主键值依次递增\n冗余和重复索引 有时候有的同学有意或者无意的就对同一个列创建了多个索引，比方说这样写建表语句：\nCREATE TABLE person_info(\rid INT UNSIGNED NOT NULL AUTO_INCREMENT,\rname VARCHAR(100) NOT NULL,\rbirthday DATE NOT NULL,\rphone_number CHAR(11) NOT NULL,\rcountry varchar(100) NOT NULL,\rPRIMARY KEY (id),\rKEY idx_name_birthday_phone_number (name(10), birthday, phone_number),\rKEY idx_name (name(10))\r);\r 通过idx_name_birthday_phone_number 索引就可以对name 列进行快速搜索，再创建一个专门针对name 列的索引就算是一个冗余索引，维护这个索引只会增加维护的成本，并不会对搜索有什么好处。这个自己也犯过\n总结   B+ 树索引在空间和时间上都有代价，所以没事儿别瞎建索引。\n  B+ 树索引适用于下边这些情况：\n 全值匹配 匹配左边的列 匹配范围值 精确匹配某一列并范围匹配另外一列 用于排序 用于分组    在使用索引时需要注意下边这些事项：\n  只为用于搜索、排序或分组的列创建索引\n  为列的基数大的列创建索引\n  索引列的类型尽量小\n  可以只对字符串值的前缀建立索引\n  只有索引列在比较表达式中单独出现才可以适用索引\n  为了尽可能少的让聚簇索引发生页面分裂和记录移位的情况，主键一定要依次递增。\n  定位并删除表中的重复和冗余索引\n 尽量使用覆盖索引进行查询，避免回表带来的性能损耗。      mysql中的锁 按锁的使用方式可以分为共享锁和独占锁，按锁的粒度可以分为表锁、行锁、页锁\n共享锁和独占锁  共享锁，英文名： Shared Locks ，简称S锁。在事务要读取一条记录时，需要先获取该记录的S锁。 独占锁，也常称排他锁，英文名： Exclusive Locks ，简称X锁。在事务要改动一条记录时，需要先获取该记录的X锁。  假如事务T1 首先获取了一条记录的S锁之后，事务T2 接着也要访问这条记录：\n 如果事务T2 想要再获取一个记录的S锁，那么事务T2 也会获得该锁，也就意味着事务T1 和T2 在该记录上同时持有S锁。 如果事务T2 想要再获取一个记录的X锁，那么此操作会被阻塞，直到事务T1 提交之后将S锁释放掉。 如果事务T1 首先获取了一条记录的X锁之后，那么不管事务T2 接着想获取该记录的S锁还是X锁都会被阻塞，直到事务T1 提交。  所以我们说S锁和S锁是兼容的， S锁和X锁是不兼容的， X锁和X锁也是不兼容的\n给表加的锁也可以分为共享锁（ S锁）和独占锁（ X锁）\n如果一个事务给表加了S锁，那么：\n 别的事务可以继续获得该表的S锁 别的事务可以继续获得该表中的某些记录的S锁(行锁) 别的事务不可以继续获得该表的X锁 别的事务不可以继续获得该表中的某些记录的X锁(行锁)  给表加X锁：如果一个事务给表加了X锁（意味着该事务要独占这个表），那么：\n 别的事务不可以继续获得该表的S锁 别的事务不可以继续获得该表中的某些记录的S锁(行锁) 别的事务不可以继续获得该表的X锁 别的事务不可以继续获得该表中的某些记录的X锁(行锁)  在对表上表锁时有两个问题：\n 对表整体上S锁，需要判断是否存在行 X锁 对表整体上X锁，需要判断是否存在行 X锁，行S锁  在对表上表锁时，怎么知道表里是否存在行锁呢？于是乎设计InnoDB 的大叔们提出了一种称之为意向锁（英文名： Intention Locks ）：\n 意向共享锁，英文名： Intention Shared Lock ，简称IS锁。当事务准备在某条记录上加S锁时，需要先在表级别加一个IS锁。 意向独占锁，英文名： Intention Exclusive Lock ，简称IX锁。当事务准备在某条记录上加X锁时，需要先在表级别加一个IX锁。  总结一下：IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是兼容的，IX锁和IX锁是兼容的\n表锁 对于MyISAM 、MEMORY 、MERGE 这些存储引擎来说，它们只支持表级锁，而且这些引擎并不支持事务，所以使用这些存储引擎的锁一般都是针对当前会话来说的。比方说在Session 1 中对一个表执行SELECT 操作，就相当于为这个表加了一个表级别的S锁，如果在SELECT 操作未完成时， Session 2 中对这个表执行UPDATE 操作，相当于要获取表的X锁，此操作会被阻塞，直到Session 1 中的SELECT 操作完成，释放掉表级别的S锁后，Session 2 中对这个表执行UPDATE 操作才能继续获取X锁，然后执行具体的更新语句。\nInnoDB存储引擎中的锁 InnoDB 存储引擎既支持表锁，也支持行锁。表锁实现简单，占用资源较少，不过粒度很粗，有时候你仅仅需要锁住几条记录，但使用表锁的话相当于为表中的所有记录都加锁，所以性能比较差。行锁粒度更细，可以实现更精准的并发控制。\n  表级别的S锁、X锁\n在对某个表执行一些诸如ALTER TABLE 、DROP TABLE 这类的DDL 语句时，其他事务对这个表并发执行诸如SELECT 、INSERT 、DELETE 、UPDATE 的语句会发生阻塞，同理，某个事务中对某个表执行SELECT 、INSERT 、DELETE 、UPDATE 语句时，在其他会话中对这个表执行DDL 语句也会发生阻塞。这个过程其实是通过在server层使用一种称之为元数据锁（英文名： Metadata Locks ，简称MDL ）东东来实现的，一般情况下也不会使用InnoDB 存储引擎自己提供的表级别的S锁和X锁。其实这个InnoDB 存储引擎提供的表级S锁或者X锁是相当鸡肋，只会在一些特殊情况下，比方说崩溃恢复过程中用到。不过我们还是可以手动获取一下的，比方说在系统变量autocommit=0，innodb_table_locks =1 时，手动获取InnoDB 存储引擎提供的表t 的S锁或者X锁可以这么写：\nLOCK TABLES t READ ： InnoDB 存储引擎会对表t 加表级别的S锁。\rLOCK TABLES t WRITE ： InnoDB 存储引擎会对表t 加表级别的X锁。\r   表级别的IS锁、IX锁\n当我们在对使用InnoDB 存储引擎的表的某些记录加S锁(行锁)之前，那就需要先在表级别加一个IS锁，当我们在对使用InnoDB 存储引擎的表的某些记录加X锁(行锁)之前，那就需要先在表级别加一个IX锁。IS锁和IX锁的使命只是为了后续在加表级别的S锁和X锁时判断表中是否有已经被加锁的记录，以避免用遍历的方式来查看表中有没有上锁的记录\n  表级别的AUTO-INC锁\n在使用MySQL 过程中，我们可以为表的某个列添加AUTO_INCREMENT 属性，之后在插入记录时，可以不指定该列的值，系统会自动为它赋上递增的值，系统实现这种自动给AUTO_INCREMENT 修饰的列递增赋值的原理主要是两个：\n  采用AUTO-INC 锁，也就是在执行插入语句时就在表级别加一个AUTO-INC 锁，然后为每条待插入记录的AUTO_INCREMENT 修饰的列分配递增的值，在该语句执行结束后，再把AUTO-INC 锁释放掉。这样一个事务在持有AUTO-INC 锁的过程中，其他事务的插入语句都要被阻塞，可以保证一个语句中分配的递增值是连续的。如果我们的插入语句在执行前不可以确定具体要插入多少条记录（无法预计即将插入记录的数量），比方说使用INSERT \u0026hellip; SELECT 、REPLACE \u0026hellip; SELECT 或者LOAD DATA 这种插入语句，一般是使用AUTO-INC 锁为AUTO_INCREMENT 修饰的列生成对应的值。\n  采用一个轻量级的锁，在为插入语句生成AUTO_INCREMENT 修饰的列的值时获取一下这个轻量级锁，然后生成本次插入语句需要用到的AUTO_INCREMENT 列的值之后，就把该轻量级锁释放掉，并不需要等到整个插入语句执行完才释放锁。如果我们的插入语句在执行前就可以确定具体要插入多少条记录，比方说我们上边举的关于表t 的例子 中，在语句执行前就可以确定要插入2条记录，那么一般采用轻量级锁的方式对AUTO_INCREMENT 修饰的列进行赋值。这种方式可以避免锁定表，可以提升插入性能。\n 设计InnoDB的大叔提供了一个称之为innodb_autoinc_lock_mode的系统变量来控制到底使用上述两种方式中的哪种来为AUTO_INCREMENT修饰的列进行赋值，\n当innodb_autoinc_lock_mode值为0时，一律采用AUTO-INC锁；\n当innodb_autoinc_lock_mode值为2时，一律采用轻量级锁；\n当innodb_autoinc_lock_mode值为1时，两种方式混着来（也就是在插入记录数量确定时采用轻量级锁，不确定时使用AUTO-INC锁）。不过当innodb_autoinc_lock_mode值为2时，可能会造成不同事务中的插入语句为AUTO_INCREMENT修饰的列生成的值是交叉的，在有主从复制的场景中是不安全的。\n     行锁 行锁，也称为记录锁，顾名思义就是在记录上加的锁。不过设计InnoDB 的大叔很有才，一个行锁玩出了各种花样，也就是把行锁分成了各种类型。换句话说即使对同一条记录加行锁，如果类型不同，起到的功效也是不同的\n  Record Locks ：\n单个行记录上的锁 Record Lock总是会去锁住索引记录，如果InnoDB存储引擎表建立的时候没有设置任何一个索引，这时InnoDB存储引擎会使用隐式的主键来进行锁定\n  Gap Locks 间隙锁\n我们说MySQL 在REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用MVCC 方案解决，也可以采用加锁方案解决。但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上Record Lock。不过这难不倒设计InnoDB 的 大叔，他们提出了一种称之为Gap Locks 的锁，官方的类型名称为： LOCK_GAP ，我们也可以简称为gap锁。比方说我们把number 值为8 的那条记录加一个gap锁的示意图如下：\n  如图中为number 值为8 的记录加了gap锁，意味着不允许别的事务在number 值为8 的记录前边的间隙插入新记录，其实就是number 列的值(3, 8) 这个区间的新记录是不允许立即插入的。比方说有另外一个事务再想插入一条number 值为4 的新记录，它定位到该条新记录的下一条记录的number 值为8，而这条记录上又有一个gap锁，所以就会阻塞插入操作，直到拥有这个gap锁的事务提交了之后， number 列的值在区间(3, 8) 中的新记录才可以被插入。 这个gap锁的提出仅仅是为了防止插入幻影记录而提出的，虽然有共享gap锁和独占gap锁这样的说法，但是它们起到的作用都是相同的。而且如果你对一条记录加了gap锁（不论是共享gap锁还是独占gap锁），并不会限制其他事务对这条记录加正经记录锁或者继续加gap锁\n  Next-Key Locks\n有时候我们既想锁住某条记录，又想阻止其他事务在该记录前边的间隙插入新记录，所以设计InnoDB 的大叔们就提出了一种称之为Next-Key Locks 的锁，官方的类型名称为： LOCK_ORDINARY ，我们也可以简称为next-key锁。比方说我们把number 值为8 的那条记录加一个next-key锁的示意图如下：\n    Insert Intention Locks\n我们说一个事务在插入一条记录时需要判断一下插入位置是不是被别的事务加了所谓的gap锁（ next-key锁也包含gap锁，后边就不强调了），如果有的话，插入操作需要等待，直到拥有gap锁的那个事务提交。设计InnoDB 的大叔规定事务在等待的时候也需要在内存中生成一个锁结构，表明有事务想在某个间隙中插入新记录，但是现在在等待。把这种类型的锁命名为Insert Intention Locks ，官方的类型名称为： LOCK_INSERT_INTENTION ，我们也可以称为插入意向锁\n比方说现在T1 为number 值为8 的记录加了一个gap锁，然后T2 和T3 分别想向hero 表中插入number 值分别为4 、5 的两条记录，所以现在为number 值为8 的记录加的锁的示意图就如下所示：\n  从图中可以看到，由于T1 持有gap锁，所以T2 和T3 需要生成一个插入意向锁的锁结构并且处于等待状态。当T1 提交后会把它获取到的锁都释放掉，这样T2 和T3 就能获取到对应的插入意向锁了（本质上就是把插入意向锁对应锁结构的is_waiting 属性改为false ）， T2 和T3 之间也并不会相互阻塞，它们可以同时获取到number 值为8的插入意向锁，然后执行插入操作。事实上插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁（ 插入意向锁就是这么鸡肋）。\n  隐式锁\n我们前边说一个事务在执行INSERT 操作时，如果即将插入的间隙已经被其他事务加了gap锁，那么本次INSERT 操作会阻塞，并且当前事务会在该间隙上加一个插入意向锁，否则一般情况下INSERT 操作是不加锁的。那如果一个事务首先插入了一条记录（此时并没有与该记录关联的锁结构），然后另一个事务\n 立即使用SELECT \u0026hellip; LOCK IN SHARE MODE 语句读取这条事务，也就是在要获取这条记录的S锁，或者使用SELECT \u0026hellip; FOR UPDATE 语句读取这条事务或者直接修改这条记录，也就是要获取这条记录的X锁，该咋办？如果允许这种情况的发生，那么可能产生脏读问题。 立即修改这条记录，也就是要获取这条记录的X锁，该咋办？ 如果允许这种情况的发生，那么可能产生脏写问题。 这时候我们前边唠叨了很多遍的事务id 又要起作用了。我们把聚簇索引和二级索引中的记录分开看一下： 情景一：对于聚簇索引记录来说，有一个trx_id 隐藏列，该隐藏列记录着最后改动该记录的事务id 。 那么如果在当前事务中新插入一条聚簇索引记录后，该记录的trx_id 隐藏列代表的的就是当前事务的事务id ，如果其他事务此时想对该记录添加S锁或者X锁时，首先会看一下该记录的trx_id 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个X锁（也就是为当前事务创建一个锁结构， is_waiting 属性是false ），然后自己进入等待状态（也就是为自己也创建一个锁结构，is_waiting 属性是true ）。 情景二：对于二级索引记录来说，本身并没有trx_id 隐藏列，但是在二级索引页面的Page Header 部分有一个PAGE_MAX_TRX_ID 属性，该属性代表对该页面做改动的最大的事务id ，如果PAGE_MAX_TRX_ID 属性值小于当前最小的活跃事务id ，那么说明对该页面做修改的事务都已经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复情景一的做法。    一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），但是由于事务id 这个牛逼的东东的存在，相当于加了一个隐式锁。别的事务在对这条记录加S锁或者X锁时，由于隐式锁的存在，会先帮助当前事务生成一个锁结构，然后自己再生成一个锁结构后进入等待状态。\n极客时间 日志系统 wal，物理日志redolog，逻辑日志binlog\nWAL Wirte-Ahead Logging 先写日志，再写磁盘。\n具体来说，当有一条记录需要更新时，InnoDB先把记录写到redolog里面，并更新内存，这个时候更新就算完成了，同时，InnoDB会在合适的时候，将这个操作记录更新到磁盘\nredolog 重做日志,物理格式的日志，记录的是物理数据页面的修改的信息\n作用：确保事务的持久性。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启 mysql 服务的时候，根据 redo log 进行重做，从而达到事务的持久性这一特性。是InnoDB独有，大小是固定的，从头开始写，写到末尾就又回到开头循环写\nwrite pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。\nwrite pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。\n有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe\n回滚日志 undo log undo log顾名思义，主要就是提供了回滚的作用，但其还有另一个主要作用，就是多个行版本控制(MVCC)，保证事务的原子性。在数据修改的流程中，会记录一条与当前操作相反的逻辑日志到undo log中（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录），如果因为某些原因导致事务异常失败了，可以借助该undo log进行回滚，保证事务的完整性，所以undo log也必不可少。\n归档日志 binlog  作用：用于复制，在主从复制中，从库利用主库上的 binlog 进行重播，实现主从同步。 用于数据库的基于时间点的还原。 内容：逻辑格式的日志，可以简单认为就是执行过的事务中的 sql 语句。但又不完全是 sql 语句这么简单，而是包括了执行的 sql 语句（增删改）反向的信息，也就意味着 delete 对应着 delete 本身和其反向的 insert；update 对应着 update 执行前后的版本的信息；insert 对应着 delete 和 insert 本身的信息。 binlog 有三种模式：Statement（基于 SQL 语句的复制）、Row（基于行的复制） 以及 Mixed（混合模式）  binlog在MySQL的server层产生，不属于任何引擎，主要记录用户对数据库操作的SQL语句（除了查询语句）。之所以将binlog称为归档日志，是因为binlog不会像redo log一样擦掉之前的记录循环写，而是一直记录（超过有效期才会被清理），如果超过单日志的最大值（默认1G，可以通过变量 max_binlog_size 设置），则会新起一个文件继续记录。但由于日志可能是基于事务来记录的(如InnoDB表类型)，而事务是绝对不可能也不应该跨文件记录的，如果正好binlog日志文件达到了最大值但事务还没有提交则不会切换新的文件记录，而是继续增大日志，所以 max_binlog_size 指定的值和实际的binlog日志大小不一定相等。\n更新语句在mysql中执行流程  从内存中找出这条数据记录，对其进行更新； 将对数据页的更改记录到redo log中； 将逻辑操作记录到binlog中； 对于内存中的数据和日志，都是由后台线程，当触发到落盘规则后再异步进行刷盘  以update T set c=c+1 where id=2 为例，执行流程如下\n从图中可看出，事务的提交过程有两个阶段，就是将redo log的写入拆成了两个步骤：prepare和commit，中间再穿插写入binlog，这就叫两阶段提交\n两阶段提交虽然能够保证单事务两个日志的内容一致，但在多事务的情况下，却不能保证两者的提交顺序一致，比如下面这个例子，假设现在有3个事务同时提交：\nT1 (--prepare--binlog---------------------commit)\rT2 (-----prepare-----binlog----commit)\rT3 (--------prepare-------binlog------commit)复制代码\r 由于binlog写入的顺序和redo log提交结束的顺序不一致，导致binlog和redo log所记录的事务提交结束的顺序不一样，最终导致的结果就是主从数据不一致。\n因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。所以在早期的MySQL版本中，通过使用prepare_commit_mutex锁来保证事务提交的顺序，在一个事务获取到锁时才能进入prepare，一直到commit结束才能释放锁，下个事务才可以继续进行prepare操作。通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。除了锁的争用会影响到性能之外，还有一个对性能影响更大的点，就是每个事务提交都会进行两次fsync（写磁盘），一次是redo log落盘，另一次是binlog落盘。大家都知道，写磁盘是昂贵的操作，对于普通磁盘，每秒的QPS大概也就是几百。\n组提交 问题：针对通过在两阶段提交中加锁控制事务提交顺序这种实现方式遇到的性能瓶颈问题，有没有更好的解决方案呢？\n答案自然是有的，在MySQL 5.6 就引入了binlog组提交，即BLGC（Binary Log Group Commit）。binlog组提交的基本思想是，引入队列机制保证InnoDB commit顺序与binlog落盘顺序一致，并将事务分组，组内的binlog刷盘动作交给一个事务进行，实现组提交目的。具体如图：\n第一阶段（prepare阶段）：\n持有prepare_commit_mutex，并且write/fsync redo log到磁盘，设置为prepared状态，完成后就释放prepare_commit_mutex，binlog不作任何操作。\n第二个阶段（commit阶段）：这里拆分成了三步，每一步的任务分配给一个专门的线程处理：\n  Flush Stage（写入binlog缓存）\n① 持有Lock_log mutex [leader持有，follower等待]\n② 获取队列中的一组binlog(队列中的所有事务)\n③ 写入binlog缓存\n  Sync Stage（将binlog落盘）\n①释放Lock_log mutex，持有Lock_sync mutex[leader持有，follower等待]\n②将一组binlog落盘（fsync动作，最耗时，假设sync_binlog为1）。\n  Commit Stage（InnoDB commit，清楚undo信息）\n①释放Lock_sync mutex，持有Lock_commit mutex[leader持有，follower等待]\n② 遍历队列中的事务，逐一进行InnoDB commit\n③ 释放Lock_commit mutex\n  每个Stage都有自己的队列，队列中的第一个事务称为leader，其他事务称为follower，leader控制着follower的行为。每个队列各自有mutex保护，队列之间是顺序的。只有flush完成后，才能进入到sync阶段的队列中；sync完成后，才能进入到commit阶段的队列中。但是这三个阶段的作业是可以同时并发执行的，即当一组事务在进行commit阶段时，其他新事务可以进行flush阶段，实现了真正意义上的组提交，大幅度降低磁盘的IOPS消耗。\n针对组提交为什么比两阶段提交加锁性能更好，简单做个总结：组提交虽然在每个队列中仍然保留了prepare_commit_mutex锁，但是锁的粒度变小了，变成了原来两阶段提交的1/4，所以锁的争用性也会大大降低；另外，组提交是批量刷盘，相比之前的单条记录都要刷盘，能大幅度降低磁盘的IO消耗。\n数据恢复流程 问题：假设事务提交过程中，MySQL进程突然奔溃，重启后是怎么保证数据不丢失的？\n下图就是MySQL重启后，提供服务前会先做的事 \u0026ndash; 恢复数据的流程：\n对上图进行简单描述就是：奔溃重启后会检查redo log中是完整并且处于prepare状态的事务，然后根据XID（事务ID），从binlog中找到对应的事务，如果找不到，则回滚；找到并且事务完整则重新commit redo log，完成事务的提交。\n下面我们根据事务提交流程，在不同的阶段时刻，看看MySQL突然奔溃后，按照上述流程是如何恢复数据的。\n  时刻A（刚在内存中更改完数据页，还没有开始写redo log的时候奔溃）：\n因为内存中的脏页还没刷盘，也没有写redo log和binlog，即这个事务还没有开始提交，所以奔溃恢复跟该事务没有关系；\n  时刻B（正在写redo log或者已经写完redo log并且落盘后，处于prepare状态，还没有开始写binlog的时候奔溃）：\n恢复后会判断redo log的事务是不是完整的，如果不是则根据undo log回滚；如果是完整的并且是prepare状态，则进一步判断对应的事务binlog是不是完整的，如果不完整则一样根据undo log进行回滚；\n  时刻C（正在写binlog或者已经写完binlog并且落盘了，还没有开始commit redo log的时候奔溃）：\n恢复后会跟时刻B一样，先检查redo log中是完整并且处于prepare状态的事务，然后判断对应的事务binlog是不是完整的，如果不完整则一样根据undo log回滚，完整则重新commit redo log；\n  时刻D（正在commit redo log或者事务已经提交完的时候，还没有反馈成功给客户端的时候奔溃）：\n恢复后跟时刻C基本一样，都会对照redo log和binlog的事务完整性，来确认是回滚还是重新提交。\n  选择普通索引还是唯一索引？  对于查询过程来说： a、普通索引，查到满足条件的第一个记录后，继续查找下一个记录，直到第一个不满足条件的记录 b、唯一索引，由于索引唯一性，查到第一个满足条件的记录后，停止检索 但是，两者的性能差距微乎其微。因为InnoDB根据数据页来读写的。 对于更新过程来说： 概念：change buffer 当需要更新一个数据页，如果数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中。下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中的与这个页有关的操作。   change buffer: 是可以持久化的数据。在内存中有拷贝，也会被写入到磁盘上\nmerge: 将change buffer中的操作应用到原数据页上，得到最新结果的过程，称为merge 访问这个数据页会触发merge，系统有后台线程定期merge，在数据库正常关闭的过程中，也会执行merge\n唯一索引的更新不能使用change buffer\nchange buffer用的是buffer pool里的内存，change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。\n将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。 change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显。\n  change buffer使用场景 在一个数据页做merge之前，change buffer记录的变更越多，收益就越大。 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer,但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。  索引的选择和实践： 尽可能使用普通索引。 redo log主要节省的是随机写磁盘的IO消耗(转成顺序写)，而change buffer主要节省的则是随机读磁盘的IO消耗。\nchange buffe数据是否会丢失 1.change buffer有一部分在内存有一部分在ibdata. 做purge操作,应该就会把change buffer里相应的数据持久化到ibdata 2.redo log里记录了数据页的修改以及change buffer新写入的信息 如果掉电,持久化的change buffer数据已经purge,不用恢复。主要分析没有持久化的数据 情况又分为以下几种: (1)change buffer写入,redo log虽然做了fsync但未commit,binlog未fsync到磁盘,这部分数据丢失 (2)change buffer写入,redo log写入但没有commit,binlog以及fsync到磁盘,先从binlog恢复redo log,再从redo log恢复change buffer (3)change buffer写入,redo log和binlog都已经fsync.那么直接从redo log里恢复。\nchange buffer不会丢失，因为change buffer是可以持久化的数据，在磁盘上占据了系统表空间ibdata，对应的内部系统表名为SYS_IBUF_TABLE。因此在异常关机的时候，不会丢失。\nMysql count(*)，count(字段)，count(1)的区别 基于 InnoDB。\n  含义区别\ncount()是一个聚合函数，对于返回的结果集，会逐行判断，若返回的不是 NULL，就会加 1，否则不加。\n因此，count(*)、count(主键 id)和count(1)都表示返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。\n  性能区别\n分析性能，考虑以下几个原则：\n1、server 层要什么就会返回什么；\n2、InnoDB 只返回必要的值；\n3、优化器只优化了count(*)\n 对于count(主键id)，InnoDB 会遍历全表，取每行的主键 id，返回给 server 层，server 层拿到数据后，进行判断累加。 对于count(1)，InnoDB 仍遍历全表，但是不取值，server 层对返回的每一行数据新增一个 1，然后进行判断累加；因此，count(1)要更快些，因为无需取值。从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 对于count(字段)：如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；2、如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 count(* ) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是 null，按行累加。     结论：按照效率排序的话，count(字段)\u0026lt;count(主键 id)\u0026lt;count(1)≈count(* )，所以我建议你，尽量使用count(*)。\n ","id":5,"section":"posts","summary":"[TOC] 这篇文章主要对Mysql是怎样运行的进行提炼总结以及补充 MySQL 的存储引擎 常用的存储引擎有：ARCHIVE、BLACKHOLE、InnoDB、M","tags":["mysql"],"title":"Mysql总结","uri":"https://wzgl998877.github.io/2022/02/mysql%E6%80%BB%E7%BB%93/","year":"2022"},{"content":"这里记录遇到、听到、见到的面试题，可以说是总结复盘的好地方\n[TOC]\n阿里：三个线程交替打印abc。 手写单例模式双重校验锁 分布式事务 接口幂等性 HashMap的底层原理 数组+链表+红黑树\nhttps://tech.meituan.com/2016/06/24/java-hashmap.html\njvm相关 mq怎么保证消息的可靠性 拷贝 在拷贝中分为引用拷贝、浅拷贝、深拷贝\n引用拷贝 引用拷贝会生成一个新的对象引用地址，但是两个最终指向依然是同一个对象。\nclass Son {\rString name;\rint age;\rpublic Son(String name, int age) {\rthis.name = name;\rthis.age = age;\r}\r}\rpublic class test {\rpublic static void main(String[] args) {\rSon s1 = new Son(\u0026quot;son1\u0026quot;, 12);\rSon s2 = s1;\rs1.age = 22;\rSystem.out.println(s1);\rSystem.out.println(s2);\rSystem.out.println(\u0026quot;s1的age:\u0026quot; + s1.age);\rSystem.out.println(\u0026quot;s2的age:\u0026quot; + s2.age);\rSystem.out.println(\u0026quot;s1==s2\u0026quot; + (s1 == s2));//相等\r}\r}\r 浅拷贝 浅拷贝会创建一个新对象，新对象和原对象本身没有任何关系，新对象和原对象不等，但是新对象的属性和可能会和老对象相同。具体可以看如下区别：\n 如果属性是基本类型(int,double,long,boolean等)，拷贝的就是基本类型的值； 如果属性是引用类型，拷贝的就是内存地址（即复制引用但不复制引用的对象） ，因此如果其中一个对象改变了这个地址，就会影响到另一个对象。  如果用一张图来描述一下浅拷贝，它应该是这样的：\n如何实现浅拷贝呢？也很简单，就是在需要拷贝的类上实现Cloneable接口并重写其clone()方法。\n@Data\rpublic class Father {\rprivate String name;\r}\r@Data\rpublic class Son implements Cloneable{\rprivate String age;\rprivate Father father;\r@Override\rprotected Son clone() throws CloneNotSupportedException {\rreturn (Son) super.clone();\r}\rpublic static void main(String[] args) throws CloneNotSupportedException {\rSon son1 = new Son();\rson1.setAge(\u0026quot;1\u0026quot;);\rFather father = new Father();\rfather.setName(\u0026quot;爸爸\u0026quot;);\rson1.setFather(father);\r// 浅拷贝\rSon son2 = son1.clone();\rSon son3 = new Son();\rson3.setFather(son2.getFather());\rson3.setAge(son2.getAge());\rSystem.out.println(son1);\rSystem.out.println(son2);\rSystem.out.println(son3);\rSystem.out.println(son1 == son2);\rSystem.out.println(son1.getFather() == son2.getFather());\r// 改变值，发现所有值都变了\rson2.getFather().setName(\u0026quot;儿砸\u0026quot;);\rson2.setAge(\u0026quot;123\u0026quot;);\rSystem.out.println(son1);\rSystem.out.println(son2);\rSystem.out.println(son3);\r}\r}\r 输出\nSon(age=1, father=Father(name=爸爸))\rSon(age=1, father=Father(name=爸爸))\rSon(age=1, father=Father(name=爸爸))\rfalse\rtrue\rSon(age=1, father=Father(name=儿砸))\rSon(age=123, father=Father(name=儿砸))\rSon(age=1, father=Father(name=儿砸))\r 很明显的看到father这个对象，复制的是引用并不是地址\n深拷贝 对于上述的问题虽然拷贝的两个对象不同，但其内部的一些引用还是相同的，怎么样绝对的拷贝这个对象，使这个对象完全独立于原对象呢？就使用我们的深拷贝了。深拷贝：在对引用数据类型进行拷贝的时候，创建了一个新的对象，并且复制其内的成员变量。\n在具体实现深拷贝上，这里提供两个方式，重写clone()方法和序列法。\n重写clone()方法 如果使用重写clone()方法实现深拷贝，那么要将类中所有自定义引用变量的类也去实现Cloneable接口实现clone()方法。对于字符类可以创建一个新的字符串实现拷贝。\n对于上述代码，Father类实现Cloneable接口并重写clone()方法。son的clone()方法需要对各个引用都拷贝一遍。\n//Father clone()方法\r@Override\rprotected Father clone() throws CloneNotSupportedException {\rreturn (Father) super.clone();\r}\r//Son clone()方法\r@Override\rprotected Son clone() throws CloneNotSupportedException {\rSon son= (Son) super.clone();//待返回克隆的对象\rson.name=new String(name);\rson.father=father.clone();\rreturn son;\r}\r 其他代码不变，执行结果如下：\nSon(age=1, father=Father(name=爸爸))\rSon(age=1, father=Father(name=爸爸))\rSon(age=1, father=Father(name=爸爸))\rfalse\rfalse\rSon(age=1, father=Father(name=爸爸))\rSon(age=123, father=Father(name=儿砸))\rSon(age=1, father=Father(name=儿砸))\r 序列法 通过fastjson完成。。。\nspring相关 ApplicationContext 和BeanFactory 和 FactoryBean的区别以及联系 FactoryBean Spring中有两种类型的Bean，一种是普通Bean，另一种是工厂Bean，即FactoryBean，这两种Bean都被容器管理，以Bean结尾，表示它是一个Bean，不同于普通Bean的是：实现了FactoryBean接口的Bean，通过 getBean() 方法返回的不是FactoryBean 本身，而是 FactoryBean.getObject() 方法所返回的对象，如果想得到FactoryBean必须通过在 '\u0026amp;' + beanName的方式获取，说白了这就是个简单工厂模式,简单例子：\npublic interface Car {\rvoid speed();\r}\rpublic class Moto implements Car{\r@Override\rpublic void speed() {\rSystem.out.println(\u0026quot;摩托可以开100码？\u0026quot;);\r}\r}\r 定义bean 实现FactoryBean接口\n@Component\rpublic class TestFactoryBean implements FactoryBean\u0026lt;Car\u0026gt; {\r@Override\rpublic Car getObject() throws Exception {\rreturn new Moto();\r}\r@Override\rpublic Class\u0026lt;?\u0026gt; getObjectType() {\rreturn null;\r}\r}\r// 测试\r@Test\rpublic void testBean() {\rObject object = SpringContextUtil.getBean(\u0026quot;testFactoryBean\u0026quot;);\rSystem.out.println(object.getClass().getName());\r// 结果为：com.zt.javastudy.spring.Moto\r}\r 这也相当于 FactoryBean.getObject() 代理了getBean() 方法\nObjectFactory 是一个普通的对象工厂接口，spring对ObjectFactory的应用之一就是，将创建对象 的步骤封装到ObjectFactory中 交给自定义的Scope来选择是否需要创建对象来灵活的实现scope，在spring的三级缓存中放的就是objectFactory\nBeanFactory BeanFactory是一个工厂类(接口)，是负责生产和管理bean的一个工厂，是IOC容器的顶层方法\n通过 BeanFactory 启动 IoC 容器时，并不会初始化配置文件当中定义的 Bean，初始化的动作发生在第一次调用 getBean() 的时候。对于单实例的 Bean 来说，BeanFactory 会缓存 Bean 的实例，所以第二次使用 getBean() 方法时，可以直接从 IoC 容器的缓存中来获取 Bean 的实例了**,bean定义的载入和依赖的注入是两个独立的过程**\n使用\nBeanFactory beanFactory = new XmlBeanFactory(new ClassPathResource(\u0026quot;applicationContext.xml\u0026quot;));\r ApplicationContext ApplicationContext由BeanFactory派生而来，提供了更多面向实际应用的功能\n MessageSource, 提供国际化的消息访问 资源访问，如URL和文件 事件传播特性，即支持aop特性 载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层  使用\nApplicationContext applicationContext = new ClassPathXmlApplicationContext(\u0026quot;classpath*:applicationContext.xml\u0026quot;);\r 总结   ApplicationContext继承了BeanFactory，BeanFactory是Spring中比较原始的Factory，它不支持AOP、Web等Spring插件，而ApplicationContext不仅包含了BeanFactory的所有功能，还支持Spring的各种插件，还以一种面向框架的方式工作以及对上下文进行分层和实现继承\n  BeanFactory在初始化容器时，并未实例化 Bean。直到 Bean 被调用时（调用getBean()方法）才会被实例化。对于单实例的 Bean 来说，BeanFactory 会缓存 Bean 的实例，所以第二次使用 getBean() 方法时，可以直接从 IoC 容器的缓存中来获取 Bean 的实例了。\n  ApplicationContext在初始化应用上下文时，会实例化所有单实例的 Bean，所以相对来说，占用空间较大、初始化时间会比 BeanFactory 稍长。\n  refresh方法 Spring容器创建之后，会调用它的refresh方法刷新Spring应用的上下文\npublic void refresh() throws BeansException, IllegalStateException {\rsynchronized (this.startupShutdownMonitor) {\r// Prepare this context for refreshing.\rprepareRefresh();\r// Tell the subclass to refresh the internal bean factory.\rConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\r// Prepare the bean factory for use in this context.\rprepareBeanFactory(beanFactory);\rtry {\r// Allows post-processing of the bean factory in context subclasses.\rpostProcessBeanFactory(beanFactory);\r// Invoke factory processors registered as beans in the context.\rinvokeBeanFactoryPostProcessors(beanFactory);\r// Register bean processors that intercept bean creation.\rregisterBeanPostProcessors(beanFactory);\r// Initialize message source for this context.\rinitMessageSource();\r// Initialize event multicaster for this context.\rinitApplicationEventMulticaster();\r// Initialize other special beans in specific context subclasses.\ronRefresh();\r// Check for listener beans and register them.\rregisterListeners();\r// Instantiate all remaining (non-lazy-init) singletons.\rfinishBeanFactoryInitialization(beanFactory);\r// Last step: publish corresponding event.\rfinishRefresh();\r}\rcatch (BeansException ex) {\rif (logger.isWarnEnabled()) {\rlogger.warn(\u0026quot;Exception encountered during context initialization - \u0026quot; +\r\u0026quot;cancelling refresh attempt: \u0026quot; + ex);\r}\r// Destroy already created singletons to avoid dangling resources.\rdestroyBeans();\r// Reset 'active' flag.\rcancelRefresh(ex);\r// Propagate exception to caller.\rthrow ex;\r}\rfinally {\r// Reset common introspection caches in Spring's core, since we\r// might not ever need metadata for singleton beans anymore...\rresetCommonCaches();\r}\r}\r}\r   prepareRefresh，初始化前的准备工作，例如对系统属性或者环境变量进行准备及验证\n  obtainFreshBeanFactory，初始化BeanFactory，并进行XML文件读取\n  prepareBeanFactory，对BeanFactory进行各种功能填充。@Qualifier与@Autowired应该是大家非常熟悉的注解，那么这两个注解正是在这一步骤中增加的支持\n  postProcessBeanFactory，子类覆盖方法做额外的处理\n 增加对SPEL语言的支持也就是占位符。 增加对属性编辑器的支持。 增加对一些内置类，比如EnvironmentAware、MessageSourceAware的信息注入。 设置了依赖功能可忽略的接口。 注册一些固定依赖的属性。 增加AspectJ的支持（会在第7章中进行详细的讲解）。 将相关环境变量及属性注册以单例模式注册    invokeBeanFactoryPostProcessors，执行bean工厂的扩展方法，从Spring容器中找出BeanDefinitionRegistryPostProcessor和BeanFactoryPostProcessor接口的实现类并按照一定的规则顺序进行执行，这里代码很复杂，主要就是有两个方面一个是执行顺序，一个是执行了什么\n 执行顺序：ConfigurationClassPostProcessor这个postProcessBeanDefinitionRegistry方法优先级最高 postProcessBeanDefinitionRegistry中processConfigBeanDefinitions为核心方法，processConfigBeanDefinitions有parse、this.reader.loadBeanDefinitions需要注意  parse，处理注解，注解的处理顺序为内部类、PropertySources注解、ComponentScans注解、Import注解、ImportResource注解、Bean注解、接口上的默认方法、继续递归到它的父类 loadBeanDefinitions、只处理@import的类      registerBeanPostProcessors，注册拦截bean创建的bean处理器，这里只是注册，真正的调用是在getBean时候， 从Spring容器中找出的BeanPostProcessor接口的bean，并设置到BeanFactory的属性中\n  initMessageSource，为上下文初始化Message源，即对不同语言的消息体进行国际化处理\n  initApplicationEventMulticaster，初始化应用消息广播器，并放入“applicationEventMulticaster”bean中\n  onRefresh，一个模板方法，不同的Spring容器做不同的事情。\n比如web程序的容器ServletWebServerApplicationContext中会调用createWebServer方法去创建内置的Servlet容器，这意味着tomcat等是在这一步启动的。目前SpringBoot只支持3种内置的Servlet容器：\n  Tomcat\n  Jetty\n  Undertow\n    registerListeners，在所有注册的bean中查找listener bean，注册到消息广播器中\n  finishBeanFactoryInitialization，初始化（非惰性的）单例bean，这个方法是核心，调用了getBean方法\n  finishRefresh，完成刷新过程，通知生命周期处理器lifecycleProcessor刷新过程，同时发出Context RefreshEvent通知别人\n  getBean 这个方法就是bean的获取以及创建的方法，首先先看整体流程图\ndoGetBean protected \u0026lt;T\u0026gt; T doGetBean(final String name, @Nullable final Class\u0026lt;T\u0026gt; requiredType,\r@Nullable final Object[] args, boolean typeCheckOnly) throws BeansException {\r// 处理别名BeanName、处理带\u0026amp;符的工厂BeanName\rfinal String beanName = transformedBeanName(name);\rObject bean;\r// 先尝试从缓存中获取Bean实例，这个位置就是三级缓存解决循环依赖的方法\rObject sharedInstance = getSingleton(beanName);\rif (sharedInstance != null \u0026amp;\u0026amp; args == null) {\rif (logger.isDebugEnabled()) {\rif (isSingletonCurrentlyInCreation(beanName)) {\rlogger.debug(\u0026quot;Returning eagerly cached instance of singleton bean '\u0026quot; + beanName +\r\u0026quot;' that is not fully initialized yet - a consequence of a circular reference\u0026quot;);\r}\relse {\rlogger.debug(\u0026quot;Returning cached instance of singleton bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\r}\r// 1. 如果 sharedInstance 是普通的 Bean 实例，则下面的方法会直接返回\r// 2. 如果 sharedInstance 是工厂Bean类型，则需要获取 getObject 方法 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null);\r}\relse {\r// 原型模式循环依赖直接抛出异常.\rif (isPrototypeCurrentlyInCreation(beanName)) {\rthrow new BeanCurrentlyInCreationException(beanName);\r}\r// 当前 bean 不存在于当前bean工厂，则到父工厂查找 bean 实例\rBeanFactory parentBeanFactory = getParentBeanFactory();\rif (parentBeanFactory != null \u0026amp;\u0026amp; !containsBeanDefinition(beanName)) {\r// Not found -\u0026gt; check parent.\rString nameToLookup = originalBeanName(name);\rif (parentBeanFactory instanceof AbstractBeanFactory) {\rreturn ((AbstractBeanFactory) parentBeanFactory).doGetBean(\rnameToLookup, requiredType, args, typeCheckOnly);\r}\relse if (args != null) {\r// Delegation to parent with explicit args.\rreturn (T) parentBeanFactory.getBean(nameToLookup, args);\r}\relse {\r// No args -\u0026gt; delegate to standard getBean method.\rreturn parentBeanFactory.getBean(nameToLookup, requiredType);\r}\r}\rif (!typeCheckOnly) {\rmarkBeanAsCreated(beanName);\r}\rtry {\rfinal RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);\rcheckMergedBeanDefinition(mbd, beanName, args);\r// Guarantee initialization of beans that the current bean depends on.\rString[] dependsOn = mbd.getDependsOn();\rif (dependsOn != null) {\rfor (String dep : dependsOn) {\rif (isDependent(beanName, dep)) {\rthrow new BeanCreationException(mbd.getResourceDescription(), beanName,\r\u0026quot;Circular depends-on relationship between '\u0026quot; + beanName + \u0026quot;' and '\u0026quot; + dep + \u0026quot;'\u0026quot;);\r}\rregisterDependentBean(dep, beanName);\rtry {\rgetBean(dep);\r}\rcatch (NoSuchBeanDefinitionException ex) {\rthrow new BeanCreationException(mbd.getResourceDescription(), beanName,\r\u0026quot;'\u0026quot; + beanName + \u0026quot;' depends on missing bean '\u0026quot; + dep + \u0026quot;'\u0026quot;, ex);\r}\r}\r}\r// 创建单例 bean 实例\rif (mbd.isSingleton()) {\rsharedInstance = getSingleton(beanName, () -\u0026gt; {\rtry {\r// 真正创建 bean的地方\rreturn createBean(beanName, mbd, args);\r}\rcatch (BeansException ex) {\r// Explicitly remove instance from singleton cache: It might have been put there\r// eagerly by the creation process, to allow for circular reference resolution.\r// Also remove any beans that received a temporary reference to the bean.\rdestroySingleton(beanName);\rthrow ex;\r}\r});\rbean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);\r}\relse if (mbd.isPrototype()) {\r// It's a prototype -\u0026gt; create a new instance.\rObject prototypeInstance = null;\rtry {\rbeforePrototypeCreation(beanName);\rprototypeInstance = createBean(beanName, mbd, args);\r}\rfinally {\rafterPrototypeCreation(beanName);\r}\rbean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);\r}\relse {\rString scopeName = mbd.getScope();\rfinal Scope scope = this.scopes.get(scopeName);\rif (scope == null) {\rthrow new IllegalStateException(\u0026quot;No Scope registered for scope name '\u0026quot; + scopeName + \u0026quot;'\u0026quot;);\r}\rtry {\rObject scopedInstance = scope.get(beanName, () -\u0026gt; {\rbeforePrototypeCreation(beanName);\rtry {\rreturn createBean(beanName, mbd, args);\r}\rfinally {\rafterPrototypeCreation(beanName);\r}\r});\rbean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd);\r}\rcatch (IllegalStateException ex) {\rthrow new BeanCreationException(beanName,\r\u0026quot;Scope '\u0026quot; + scopeName + \u0026quot;' is not active for the current thread; consider \u0026quot; +\r\u0026quot;defining a scoped proxy for this bean if you intend to refer to it from a singleton\u0026quot;,\rex);\r}\r}\r}\rcatch (BeansException ex) {\rcleanupAfterBeanCreationFailure(beanName);\rthrow ex;\r}\r}\r// Check if required type matches the type of the actual bean instance.\rif (requiredType != null \u0026amp;\u0026amp; !requiredType.isInstance(bean)) {\rtry {\rT convertedBean = getTypeConverter().convertIfNecessary(bean, requiredType);\rif (convertedBean == null) {\rthrow new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass());\r}\rreturn convertedBean;\r}\rcatch (TypeMismatchException ex) {\rif (logger.isDebugEnabled()) {\rlogger.debug(\u0026quot;Failed to convert bean '\u0026quot; + name + \u0026quot;' to required type '\u0026quot; +\rClassUtils.getQualifiedName(requiredType) + \u0026quot;'\u0026quot;, ex);\r}\rthrow new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass());\r}\r}\rreturn (T) bean;\r}\r 其中getSingleton()方法是解决循环依赖的地方\n// singletonObjects：完成初始化的单例对象的cache（一级缓存）\rprivate final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(64);\r// 完成实例化但是尚未初始化的，提前暴光的单例对象的Cache （二级缓存）\rprivate final Map\u0026lt;String, Object\u0026gt; earlySingletonObjects = new HashMap\u0026lt;String, Object\u0026gt;(16);\r// 进入实例化阶段的单例对象工厂的cache （三级缓存）\rprivate final Map\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt; singletonFactories = new HashMap\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt;(16);\rprotected Object getSingleton(String beanName, boolean allowEarlyReference) {\rObject singletonObject = this.singletonObjects.get(beanName);\rif (singletonObject == null \u0026amp;\u0026amp; isSingletonCurrentlyInCreation(beanName)) {\rsynchronized (this.singletonObjects) {\rsingletonObject = this.earlySingletonObjects.get(beanName);\rif (singletonObject == null \u0026amp;\u0026amp; allowEarlyReference) {\rObjectFactory\u0026lt;?\u0026gt; singletonFactory = this.singletonFactories.get(beanName);\rif (singletonFactory != null) {\rsingletonObject = singletonFactory.getObject();\rthis.earlySingletonObjects.put(beanName, singletonObject);\rthis.singletonFactories.remove(beanName);\r}\r}\r}\r}\rreturn singletonObject;\r}\r 这个方法就是先从一级缓存取，一级取不到，去二级缓存中取，二级缓存取不到，然后去三级缓存取，如果三级缓存中有那么移除三级缓存加入二级缓存，根据流程图，下一个重要的方法还是getSingleton哈哈，不过这个getSingleton中重要的是createBean方法\ngetSingleton 如果缓存中不存在已经加载的单例bean就需要从头开始bean的加载过程了，而Spring中使用getSingleton的重载方法实现bean的加载过程。\npublic Object getSingleton(String beanName, ObjectFactory\u0026lt;?\u0026gt; singletonFactory) {\rAssert.notNull(beanName, \u0026quot;Bean name must not be null\u0026quot;);\rsynchronized (this.singletonObjects) {\r// 一级缓存\rObject singletonObject = this.singletonObjects.get(beanName);\rif (singletonObject == null) {\r// 是否正在创建中\rif (this.singletonsCurrentlyInDestruction) {\rthrow new BeanCreationNotAllowedException(beanName,\r\u0026quot;Singleton bean creation not allowed while singletons of this factory are in destruction \u0026quot; +\r\u0026quot;(Do not request a bean from a BeanFactory in a destroy method implementation!)\u0026quot;);\r}\rif (logger.isDebugEnabled()) {\rlogger.debug(\u0026quot;Creating shared instance of singleton bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\r// 记录加载状态\rbeforeSingletonCreation(beanName);\rboolean newSingleton = false;\rboolean recordSuppressedExceptions = (this.suppressedExceptions == null);\rif (recordSuppressedExceptions) {\rthis.suppressedExceptions = new LinkedHashSet\u0026lt;\u0026gt;();\r}\rtry {\r// 回调createBean\rsingletonObject = singletonFactory.getObject();\rnewSingleton = true;\r}\rcatch (IllegalStateException ex) {\r// Has the singleton object implicitly appeared in the meantime -\u0026gt;\r// if yes, proceed with it since the exception indicates that state.\rsingletonObject = this.singletonObjects.get(beanName);\rif (singletonObject == null) {\rthrow ex;\r}\r}\rcatch (BeanCreationException ex) {\rif (recordSuppressedExceptions) {\rfor (Exception suppressedException : this.suppressedExceptions) {\rex.addRelatedCause(suppressedException);\r}\r}\rthrow ex;\r}\rfinally {\rif (recordSuppressedExceptions) {\rthis.suppressedExceptions = null;\r}\r// 清除加载状态\rafterSingletonCreation(beanName);\r}\rif (newSingleton) {\r// 加入缓存\raddSingleton(beanName, singletonObject);\r}\r}\rreturn singletonObject;\r}\r}\r// this.singletonsCurrentlyInCreation.add(beanName) 将该bean加入正在加载的bean set集合中\rprotected void beforeSingletonCreation(String beanName) {\rif (!this.inCreationCheckExclusions.contains(beanName) \u0026amp;\u0026amp; !this.singletonsCurrentlyInCreation.add(beanName)) {\rthrow new BeanCurrentlyInCreationException(beanName);\r}\r}\r// this.singletonsCurrentlyInCreation.remove(beanName) 将该bean从正在加载bean的set集合中删除\rprotected void afterSingletonCreation(String beanName) {\rif (!this.inCreationCheckExclusions.contains(beanName) \u0026amp;\u0026amp; !this.singletonsCurrentlyInCreation.remove(beanName)) {\rthrow new IllegalStateException(\u0026quot;Singleton '\u0026quot; + beanName + \u0026quot;' isn't currently in creation\u0026quot;);\r}\r}\r// 清除二、三级缓存加入一级缓存\rprotected void addSingleton(String beanName, Object singletonObject) {\rsynchronized (this.singletonObjects) {\r// 加入一级缓存\rthis.singletonObjects.put(beanName, singletonObject);\r// 从三级缓存移除\rthis.singletonFactories.remove(beanName);\r// 从二级缓存移除\rthis.earlySingletonObjects.remove(beanName);\r// 加入单例注册set\rthis.registeredSingletons.add(beanName);\r}\r}\r 在这个方法中singletonFactory.getObject会调用createBean方法去创建bean（一个完整的bean，经过了实例化、属性填充、初始化），创建完bean后会将bean加入到一级缓存\ncreateBean protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)\rthrows BeanCreationException {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Creating instance of bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\rRootBeanDefinition mbdToUse = mbd;\r// Make sure bean class is actually resolved at this point, and\r// clone the bean definition in case of a dynamically resolved Class\r// which cannot be stored in the shared merged bean definition.\r// 解析class\rClass\u0026lt;?\u0026gt; resolvedClass = resolveBeanClass(mbd, beanName);\rif (resolvedClass != null \u0026amp;\u0026amp; !mbd.hasBeanClass() \u0026amp;\u0026amp; mbd.getBeanClassName() != null) {\rmbdToUse = new RootBeanDefinition(mbd);\rmbdToUse.setBeanClass(resolvedClass);\r}\r// Prepare method overrides.\rtry {\r// 验证和准备 覆盖的方法\rmbdToUse.prepareMethodOverrides();\r}\rcatch (BeanDefinitionValidationException ex) {\rthrow new BeanDefinitionStoreException(mbdToUse.getResourceDescription(),\rbeanName, \u0026quot;Validation of method overrides failed\u0026quot;, ex);\r}\rtry {\r// Give BeanPostProcessors a chance to return a proxy instead of the target bean instance.\r// 给BeanPostProcessors一个返回代理而不是目标bean实例的机会\rObject bean = resolveBeforeInstantiation(beanName, mbdToUse);\rif (bean != null) {\rreturn bean;\r}\r}\rcatch (Throwable ex) {\rthrow new BeanCreationException(mbdToUse.getResourceDescription(), beanName,\r\u0026quot;BeanPostProcessor before instantiation of bean failed\u0026quot;, ex);\r}\rtry {\r// 真正创建bean\rObject beanInstance = doCreateBean(beanName, mbdToUse, args);\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Finished creating instance of bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\rreturn beanInstance;\r}\rcatch (BeanCreationException | ImplicitlyAppearedSingletonException ex) {\r// A previously detected exception with proper bean creation context already,\r// or illegal singleton state to be communicated up to DefaultSingletonBeanRegistry.\rthrow ex;\r}\rcatch (Throwable ex) {\rthrow new BeanCreationException(\rmbdToUse.getResourceDescription(), beanName, \u0026quot;Unexpected exception during bean creation\u0026quot;, ex);\r}\r}\r 这段代码可以分为：\n resolveBeanClass，根据设置的class属性或者根据className来解析Class。 mbdToUse.prepareMethodOverrides()，对override属性进行标记及验证。 Object bean = resolveBeforeInstantiation(beanName, mbdToUse)，应用初始化前的后处理器，解析指定bean是否存在初始化前的短路操作。aop功能就是基于这里的 Object beanInstance = doCreateBean(beanName, mbdToUse, args)，真正核心，创建bean。  doCreateBean protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)\rthrows BeanCreationException {\r// Instantiate the bean.\rBeanWrapper instanceWrapper = null;\rif (mbd.isSingleton()) {\rinstanceWrapper = this.factoryBeanInstanceCache.remove(beanName);\r}\rif (instanceWrapper == null) {\r// 实例化，调用构造方法实例化对象\rinstanceWrapper = createBeanInstance(beanName, mbd, args);\r}\rObject bean = instanceWrapper.getWrappedInstance();\rClass\u0026lt;?\u0026gt; beanType = instanceWrapper.getWrappedClass();\rif (beanType != NullBean.class) {\rmbd.resolvedTargetType = beanType;\r}\r// Allow post-processors to modify the merged bean definition.\rsynchronized (mbd.postProcessingLock) {\rif (!mbd.postProcessed) {\rtry {\rapplyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName);\r}\rcatch (Throwable ex) {\rthrow new BeanCreationException(mbd.getResourceDescription(), beanName,\r\u0026quot;Post-processing of merged bean definition failed\u0026quot;, ex);\r}\rmbd.postProcessed = true;\r}\r}\r// Eagerly cache singletons to be able to resolve circular references\r// even when triggered by lifecycle interfaces like BeanFactoryAware.\rboolean earlySingletonExposure = (mbd.isSingleton() \u0026amp;\u0026amp; this.allowCircularReferences \u0026amp;\u0026amp;\risSingletonCurrentlyInCreation(beanName));\rif (earlySingletonExposure) {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Eagerly caching bean '\u0026quot; + beanName +\r\u0026quot;' to allow for resolving potential circular references\u0026quot;);\r}\r// 添加三级缓存\raddSingletonFactory(beanName, () -\u0026gt; getEarlyBeanReference(beanName, mbd, bean));\r}\r// Initialize the bean instance.\rObject exposedObject = bean;\rtry {\r// 填充属性\rpopulateBean(beanName, mbd, instanceWrapper);\r// 调用init方法初始化bean\rexposedObject = initializeBean(beanName, exposedObject, mbd);\r}\rcatch (Throwable ex) {\rif (ex instanceof BeanCreationException \u0026amp;\u0026amp; beanName.equals(((BeanCreationException) ex).getBeanName())) {\rthrow (BeanCreationException) ex;\r}\relse {\rthrow new BeanCreationException(\rmbd.getResourceDescription(), beanName, \u0026quot;Initialization of bean failed\u0026quot;, ex);\r}\r}\r// 循环依赖检查\rif (earlySingletonExposure) {\rObject earlySingletonReference = getSingleton(beanName, false);\rif (earlySingletonReference != null) {\rif (exposedObject == bean) {\rexposedObject = earlySingletonReference;\r}\relse if (!this.allowRawInjectionDespiteWrapping \u0026amp;\u0026amp; hasDependentBean(beanName)) {\rString[] dependentBeans = getDependentBeans(beanName);\rSet\u0026lt;String\u0026gt; actualDependentBeans = new LinkedHashSet\u0026lt;\u0026gt;(dependentBeans.length);\rfor (String dependentBean : dependentBeans) {\rif (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) {\ractualDependentBeans.add(dependentBean);\r}\r}\rif (!actualDependentBeans.isEmpty()) {\rthrow new BeanCurrentlyInCreationException(beanName,\r\u0026quot;Bean with name '\u0026quot; + beanName + \u0026quot;' has been injected into other beans [\u0026quot; +\rStringUtils.collectionToCommaDelimitedString(actualDependentBeans) +\r\u0026quot;] in its raw version as part of a circular reference, but has eventually been \u0026quot; +\r\u0026quot;wrapped. This means that said other beans do not use the final version of the \u0026quot; +\r\u0026quot;bean. This is often the result of over-eager type matching - consider using \u0026quot; +\r\u0026quot;'getBeanNamesForType' with the 'allowEagerInit' flag turned off, for example.\u0026quot;);\r}\r}\r}\r}\r// Register bean as disposable.\rtry {\rregisterDisposableBeanIfNecessary(beanName, bean, mbd);\r}\rcatch (BeanDefinitionValidationException ex) {\rthrow new BeanCreationException(\rmbd.getResourceDescription(), beanName, \u0026quot;Invalid destruction signature\u0026quot;, ex);\r}\rreturn exposedObject;\r}\r 创建bean的实例:createBeanInstance() 该方法作用为实例化 bean\n 如果存在工厂方法则使用工厂方法进行初始化。 一个类有多个构造函数，每个构造函数都有不同的参数，所以需要根据参数锁定构造函数并进行初始化。 如果既不存在工厂方法也不存在带有参数的构造函数，则使用默认的构造函数进行bean的实例化。  解决循环依赖:addSingletonFactory() protected void addSingletonFactory(String beanName, ObjectFactory\u0026lt;?\u0026gt; singletonFactory) {\rAssert.notNull(singletonFactory, \u0026quot;Singleton factory must not be null\u0026quot;);\rsynchronized (this.singletonObjects) {\rif (!this.singletonObjects.containsKey(beanName)) {\r// 加入三级缓存\rthis.singletonFactories.put(beanName, singletonFactory);\r// 删除二级缓存\rthis.earlySingletonObjects.remove(beanName);\rthis.registeredSingletons.add(beanName);\r}\r}\r}\r 创建完bean的实例后，将该单例提早曝光， 将创建该单例的工厂加入三级缓存\n属性注入:populateBean() 主要功能就是属性填充\n主要流程为：\n  根据注入类型（byName/byType），提取依赖的bean，并统一存入PropertyValues中。\nprotected void autowireByName(\rString beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) {\rString[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw);\rfor (String propertyName : propertyNames) {\rif (containsBean(propertyName)) {\r// 递归调用getBean()\rObject bean = getBean(propertyName);\rpvs.add(propertyName, bean);\rregisterDependentBean(propertyName, beanName);\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Added autowiring by name from bean name '\u0026quot; + beanName +\r\u0026quot;' via property '\u0026quot; + propertyName + \u0026quot;' to bean named '\u0026quot; + propertyName + \u0026quot;'\u0026quot;);\r}\r}\relse {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Not autowiring property '\u0026quot; + propertyName + \u0026quot;' of bean '\u0026quot; + beanName +\r\u0026quot;' by name: no matching bean found\u0026quot;);\r}\r}\r}\r}\r 这里面主要流程是在传入的参数pvs中找出已经加载的bean，并递归实例化，进而加入到pvs中\n  将所有PropertyValues中的属性填充至BeanWrapper中\n现在已经完成了对所有注入属性的获取，但是获取的属性是以PropertyValues形式存在的，还并没有应用到已经实例化的bean中，这一工作是在applyPropertyValues中完成。\n  初始化bean:initializeBean() protected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) {\rif (System.getSecurityManager() != null) {\rAccessController.doPrivileged((PrivilegedAction\u0026lt;Object\u0026gt;) () -\u0026gt; {\rinvokeAwareMethods(beanName, bean);\rreturn null;\r}, getAccessControlContext());\r}\relse {\r// 1 执行各种aware方法\rinvokeAwareMethods(beanName, bean);\r}\rObject wrappedBean = bean;\rif (mbd == null || !mbd.isSynthetic()) {\r// 2执行前置处理方法\rwrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName);\r}\rtry {\r// 3执行初始化方法\rinvokeInitMethods(beanName, wrappedBean, mbd);\r}\rcatch (Throwable ex) {\rthrow new BeanCreationException(\r(mbd != null ? mbd.getResourceDescription() : null),\rbeanName, \u0026quot;Invocation of init method failed\u0026quot;, ex);\r}\rif (mbd == null || !mbd.isSynthetic()) {\r// 4执行后置处理方法\rwrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);\r}\rreturn wrappedBean;\r}\r 具体方法为：\n invokeAwareMethods,执行各种aware方法  if (bean instanceof Aware) {\rif (bean instanceof BeanNameAware) {\r((BeanNameAware) bean).setBeanName(beanName);\r}\rif (bean instanceof BeanClassLoaderAware) {\rClassLoader bcl = getBeanClassLoader();\rif (bcl != null) {\r((BeanClassLoaderAware) bean).setBeanClassLoader(bcl);\r}\r}\rif (bean instanceof BeanFactoryAware) {\r((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this);\r}\r}\r 这里面就是执行了各种aware方法\n applyBeanPostProcessorsBeforeInitialization,调用BeanPostProcessor的前置处理中postProcessBeforeInitialization() 方法\npublic Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName)\rthrows BeansException {\rObject result = existingBean;\rfor (BeanPostProcessor processor : getBeanPostProcessors()) {\r// 调用BeanPostProcessor的前置处理中postProcessBeforeInitialization() 方法 Object current = processor.postProcessBeforeInitialization(result, beanName);\rif (current == null) {\rreturn result;\r}\rresult = current;\r}\rreturn result;\r}\r   invokeInitMethods,初始化bean\n  如果实现了initializingBean的接口, 执行afterPropertiesSet()方法\n  如果配置了自定义的init-method，执行指定的方法初始化Bean\n    applyBeanPostProcessorsAfterInitialization，执行BeanPostProcessor中的postProcessAfterInitialization，AOP就是在这实现的！！！\n  其实这也就是所谓的bean的生命周期\n循环依赖 什么是循环依赖？其实就是循环引用，也就是两个或者两个以上的bean互相持有对方，最终形成闭环。比如A依赖于B，B依赖于C，C又依赖于A。\n首先以自己写代码为例，出现循环引用的情况：\npackage com.zt.javastudy.spring;\r/**\r* @author zhengtao\r* @description 测试循环依赖\r* @date 2021/4/9\r*/\rpublic class TestXunHuan{\rpublic static void main(String[] args) {\rSystem.out.println(new StudentC());\r}\r}\r/**\r* StudentC与StudentD存在循环引用\r*/\rclass StudentC {\rpublic StudentC() {\rnew StudentD();\r}\r}\rclass StudentD {\rpublic StudentD() {\rnew StudentC();\r}\r}\r 结果栈溢出：\nException in thread \u0026quot;main\u0026quot; java.lang.StackOverflowError\r spring中循环依赖的三种情况：\n  构造器注入循环依赖   @Service\rpublic class StudentA {\rprivate StudentB b;\rpublic StudentB getB() {\rreturn b;\r}\rpublic void setB(StudentB b) {\rthis.b = b;\r}\r/**\r* 构造函数循环依赖\r* @param b\r*/\rpublic StudentA(StudentB b) {\rthis.b = b;\r}\r}\r@Service\rpublic class StudentB {\rprivate StudentA a;\rpublic StudentA getA() {\rreturn a;\r}\rpublic void setA(StudentA a) {\rthis.a = a;\r}\r/**\r* 构造函数循环依赖\r* @param a\r*/\rpublic StudentB(StudentA a) {\rthis.a = a;\r}\r}\r   field属性注入（setter方法注入）循环依赖 @Service\rpublic class StudentA {\r@Autowired\rprivate StudentB b;\rpublic StudentB getB() {\rreturn b;\r}\rpublic void setB(StudentB b) {\rthis.b = b;\r}\r}\r@Service\rpublic class StudentB {\r@Autowired\rprivate StudentA a;\rpublic StudentA getA() {\rreturn a;\r}\rpublic void setA(StudentA a) {\rthis.a = a;\r}\r}\r   prototype原型模式 field属性注入循环依赖 // 原型模式\r@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\r@Service\rpublic class StudentA {\r@Autowired\rprivate StudentB b;\rpublic StudentB getB() {\rreturn b;\r}\rpublic void setB(StudentB b) {\rthis.b = b;\r}\r}\r// 原型模式\r@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\r@Service\rpublic class StudentB {\r@Autowired\rprivate StudentA a;\rpublic StudentA getA() {\rreturn a;\r}\rpublic void setA(StudentA a) {\rthis.a = a;\r}\r}\r 这种方式运行不会报错，因为原型模式的bean启动的时候不会被初始化，但在使用到这个bean的时候就报错了\nspring 中帮我们解决了第二种field属性注入（setter方法注入）循环依赖， 是使用了三级缓存的方式来解决的：\n 先从一级缓存singletonObjects中去获取。（如果获取到就直接return） 如果获取不到或者对象正在创建中（isSingletonCurrentlyInCreation()），那就再从二级缓存earlySingletonObjects中获取。（如果获取到就直接return） 如果还是获取不到，且允许singletonFactories（allowEarlyReference=true）通过getObject()获取。就从三级缓存singletonFactory.getObject()获取。（如果获取到了就从singletonFactories中移除，并且放进earlySingletonObjects。其实也就是从三级缓存移动（是剪切、不是复制哦~）**到了二级缓存）  加入singletonFactories三级缓存的前提是执行了构造器，所以构造器的循环依赖没法解决\n  这样的意义是什么呢？\n​\tA首先完成了初始化的第一步，并且将自己提前曝光到singletonFactories中也就是加入到三级缓存中，此时进行初始化的第二步，发现自己依赖对象B，此时就尝试去get(B)，发现B还没有被create，所以走create流程，B在初始化第一步的时候发现自己依赖了对象A，于是尝试get(A)，尝试一级缓存singletonObjects(肯定没有，因为A还没初始化完全)，尝试二级缓存earlySingletonObjects（也没有），尝试三级缓存singletonFactories，由于A通过ObjectFactory将自己提前曝光了，所以B能够通过ObjectFactory.getObject拿到A对象(虽然A还没有初始化完全，但是总比没有好呀)，B拿到A对象后顺利完成了初始化阶段1、2、3，完全初始化之后将自己放入到一级缓存singletonObjects中。此时返回A中，A此时能拿到B的对象顺利完成自己的初始化阶段2、3，最终A也完成了初始化，进去了一级缓存singletonObjects中，而且更加幸运的是，由于B拿到了A的对象引用而A现在已经完成了初始化，所以B现在拿到的A对象已经完成了初始化。\n三级缓存失效的情况 使用单例属性注入，但由于生成了代理对象，比如平时使用：@Async注解的场景，会通过AOP自动生成代理对象\n@Service\rpublic class TestService1 {\r@Autowired\rprivate TestService2 testService2;\r@Async\rpublic void test1() {\r}\r}\r@Service\rpublic class TestService2 {\r@Autowired\rprivate TestService1 testService1;\rpublic void test2() {\r}\r}\r 从前面得知程序启动会报错，出现了循环依赖：\norg.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name 'testService1': Bean with name 'testService1' has been injected into other beans [testService2] in its raw version as part of a circular reference, but has eventually been wrapped. This means that said other beans do not use the final version of the bean. This is often the result of over-eager type matching - consider using 'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\r 为什么会循环依赖呢？\n说白了，bean初始化完成之后，后面还有一步去检查：第二级缓存 和 原始对象 是否相等。\nif (earlySingletonExposure) {\r// 从二级缓存获取对象\rObject earlySingletonReference = getSingleton(beanName, false);\rif (earlySingletonReference != null) {\rif (exposedObject == bean) {\r// 替换成代理对象，所以最后加入缓存的代理对象\rexposedObject = earlySingletonReference;\r}\relse if (!this.allowRawInjectionDespiteWrapping \u0026amp;\u0026amp; hasDependentBean(beanName)) {\rString[] dependentBeans = getDependentBeans(beanName);\rSet\u0026lt;String\u0026gt; actualDependentBeans = new LinkedHashSet\u0026lt;\u0026gt;(dependentBeans.length);\rfor (String dependentBean : dependentBeans) {\rif (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) {\ractualDependentBeans.add(dependentBean);\r}\r}\rif (!actualDependentBeans.isEmpty()) {\rthrow new BeanCurrentlyInCreationException(beanName,\r\u0026quot;Bean with name '\u0026quot; + beanName + \u0026quot;' has been injected into other beans [\u0026quot; +\rStringUtils.collectionToCommaDelimitedString(actualDependentBeans) +\r\u0026quot;] in its raw version as part of a circular reference, but has eventually been \u0026quot; +\r\u0026quot;wrapped. This means that said other beans do not use the final version of the \u0026quot; +\r\u0026quot;bean. This is often the result of over-eager type matching - consider using \u0026quot; +\r\u0026quot;'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\u0026quot;);\r}\r}\r}\r}\r 走到这段代码，发现第二级缓存 和 原始对象不相等，所以抛出了循环依赖的异常。\n如果这时候把TestService1改个名字，改成：TestService6，其他的都不变。\n@Service\rpublicclass TestService6 {\r@Autowired\rprivate TestService2 testService2;\r@Async\rpublic void test1() {\r}\r}\r 再重新启动一下程序，神奇般的好了。\nwhat？ 这又是为什么？\n这就要从spring的bean加载顺序说起了，默认情况下，spring是按照文件完整路径递归查找的，按路径+文件名排序，排在前面的先加载。所以TestService1比TestService2先加载，而改了文件名称之后，TestService2比TestService6先加载。\n为什么TestService2比TestService6先加载就没问题呢？\n答案在下面这张图中：\n这种情况testService6中其实第二级缓存是空的，不需要跟原始对象判断，所以不会抛出循环依赖。\nDependsOn循环依赖 还有一种有些特殊的场景，比如我们需要在实例化Bean A之前，先实例化Bean B，这个时候就可以使用@DependsOn注解。\n@DependsOn(value = \u0026quot;testService2\u0026quot;)\r@Service\rpublic class TestService1 {\r@Autowired\rprivate TestService2 testService2;\rpublic void test1() {\r}\r}\r@DependsOn(value = \u0026quot;testService1\u0026quot;)\r@Service\rpublic class TestService2 {\r@Autowired\rprivate TestService1 testService1;\rpublic void test2() {\r}\r}\r 程序启动之后，执行结果：\nCircular depends-on relationship between 'testService2' and 'testService1'\r 这个例子中本来如果TestService1和TestService2都没有加@DependsOn注解是没问题的，反而加了这个注解会出现循环依赖问题。答案在AbstractBeanFactory类的doGetBean方法的这段代码中：它会检查dependsOn的实例有没有循环依赖，如果有循环依赖则抛异常\n为什么要三级缓存解决循环依赖 @Component\rpublic class A {\r// A中注入了B\r@Autowired\rprivate B b;\r}\r@Component\rpublic class B {\r// B中也注入了A\r@Autowired\rprivate A a;\r}\r 不考虑aop 可以将三级缓存去掉，因为不考虑aop时，三级缓存工厂就是实例化的对象\n考虑aop 我们对A进行了AOP代理的话，那么此时getEarlyBeanReference将返回一个代理后的对象，而不是实例化阶段创建的对象，这样就意味着B中注入的A将是一个代理对象而不是A的实例化阶段创建后的对象。\n  明明初始化的时候是A对象，那么Spring是在哪里将代理对象放入到容器中的呢？还是这段代码\nif (earlySingletonExposure) {\r// 从二级缓存获取对象\rObject earlySingletonReference = getSingleton(beanName, false);\rif (earlySingletonReference != null) {\rif (exposedObject == bean) {\r// 替换成代理对象，所以最后加入缓存的代理对象\rexposedObject = earlySingletonReference;\r}\relse if (!this.allowRawInjectionDespiteWrapping \u0026amp;\u0026amp; hasDependentBean(beanName)) {\rString[] dependentBeans = getDependentBeans(beanName);\rSet\u0026lt;String\u0026gt; actualDependentBeans = new LinkedHashSet\u0026lt;\u0026gt;(dependentBeans.length);\rfor (String dependentBean : dependentBeans) {\rif (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) {\ractualDependentBeans.add(dependentBean);\r}\r}\rif (!actualDependentBeans.isEmpty()) {\rthrow new BeanCurrentlyInCreationException(beanName,\r\u0026quot;Bean with name '\u0026quot; + beanName + \u0026quot;' has been injected into other beans [\u0026quot; +\rStringUtils.collectionToCommaDelimitedString(actualDependentBeans) +\r\u0026quot;] in its raw version as part of a circular reference, but has eventually been \u0026quot; +\r\u0026quot;wrapped. This means that said other beans do not use the final version of the \u0026quot; +\r\u0026quot;bean. This is often the result of over-eager type matching - consider using \u0026quot; +\r\u0026quot;'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\u0026quot;);\r}\r}\r}\r}\r protected Object getSingleton(String beanName, boolean allowEarlyReference) {\rObject singletonObject = this.singletonObjects.get(beanName);\rif (singletonObject == null \u0026amp;\u0026amp; isSingletonCurrentlyInCreation(beanName)) {\rsynchronized (this.singletonObjects) {\rsingletonObject = this.earlySingletonObjects.get(beanName);\rif (singletonObject == null \u0026amp;\u0026amp; allowEarlyReference) {\rObjectFactory\u0026lt;?\u0026gt; singletonFactory = this.singletonFactories.get(beanName);\rif (singletonFactory != null) {\r// 获取代理对象\rsingletonObject = singletonFactory.getObject();\rthis.earlySingletonObjects.put(beanName, singletonObject);\rthis.singletonFactories.remove(beanName);\r}\r}\r}\r}\rreturn singletonObject;\r}\r  初始化的时候是对A对象本身进行初始化，而容器中以及注入到B中的都是代理对象，这样不会有问题吗？  不会，这是因为不管是cglib代理还是jdk动态代理生成的代理类，内部都持有一个目标类的引用，当调用代理对象的方法时，实际会去调用目标对象的方法，A完成初始化相当于代理对象自身也完成了初始化\n三级缓存为什么要使用工厂而不是直接使用引用？换而言之，为什么需要这个三级缓存，直接通过二级缓存暴露一个引用不行吗？  这个工厂的目的在于延迟对实例化阶段生成的对象的代理，只有真正发生循环依赖的时候，才去提前生成代理对象，否则只会创建一个工厂并将其放入到三级缓存中，但是不会去通过这个工厂去真正创建对象\n因为spring不知道会不会有代理对象，所以他会把所有bean都放到三级缓存工厂中去，而只有在循环依赖的时候才会去调用 singletonFactory.getObject()获取代理对象，如果不要三级缓存，那么意味着所有的Bean在这一步都要完成AOP代理\n  总结 面试官：”Spring是如何解决的循环依赖？“\n答：Spring通过三级缓存解决了循环依赖，其中一级缓存为单例池（singletonObjects）,二级缓存为早期曝光对象earlySingletonObjects，三级缓存为早期曝光对象工厂（singletonFactories）。当A、B两个类发生循环引用时，在A完成实例化后，就使用实例化后的对象去创建一个对象工厂，并添加到三级缓存中，如果A被AOP代理，那么通过这个工厂获取到的就是A代理后的对象，如果A没有被AOP代理，那么这个工厂获取到的就是A实例化的对象。当A进行属性注入时，会去创建B，同时B又依赖了A，所以创建B的同时又会去调用getBean(a)来获取需要的依赖，此时的getBean(a)会从缓存中获取，第一步，先获取到三级缓存中的工厂；第二步，调用对象工工厂的getObject方法来获取到对应的对象，得到这个对象后将其注入到B中。紧接着B会走完它的生命周期流程，包括初始化、后置处理器等。当B创建完后，会将B再注入到A中，此时A再完成它的整个生命周期。至此，循环依赖结束！\n面试官：”为什么要使用三级缓存呢？二级缓存能解决循环依赖吗？“\n答：如果要使用二级缓存解决循环依赖，意味着所有Bean在实例化后就要完成AOP代理，这样违背了Spring设计的原则，Spring在设计之初就是通过AnnotationAwareAspectJAutoProxyCreator这个后置处理器来在Bean生命周期的最后一步来完成AOP代理，而不是在实例化后就立马进行AOP代理。\nbean的生命周期 Bean 的生命周期概括起来就是 4 个阶段：\n 实例化（Instantiation） 属性赋值（Populate） 初始化（Initialization） 销毁（Destruction）  细说就是这张图\n实例化 解析xml配置或注解的配置的类，得到BeanDefinition 再通过BeanDefinition 反射创建bean对象\n具体步骤为：\n1、Bean 容器找到配置文件中 Spring Bean 的定义，通过资源接口Resource,得到Bean属性和配置 2、解析后得到BeanDefinition 3、根据相应的构造方法，利用 Java Reflection API 创建一个Bean的实例\n属性赋值 设置对象属性，并对对象中的加了@Autowired注解的属性进行属性填充\n初始化 初始化步骤较多，可以分为初始化前、初始化、初始化后\n初始化前   回调Aware方法，比如BeanNameAware，BeanFactoryAware, ApplicationContextAware\n Aware方法是什么？\nSpring的依赖注入的最大亮点是所有的Bean对Spring容器的存在是没有意识的，我们可以将Spring容器换成其他的容器，Spring容器中的Bean的耦合度因此也是极低的。但是我们在实际的开发中，我们却经常要用到Spring容器本身的功能资源，所以Spring容器中的Bean此时就要意识到Spring容器的存在才能调用Spring所提供的资源。我们通过Spring提供的一系列接口Spring Aware来实现具体的功能。Aware接口是回调，监听器和观察者设计模式的混合，它表示bean有资格通过回调方式被Spring容器通知，也就是说：直接或间接实现了这个接口的类，都具有被spring容器通知的能力， 通过让bean 实现 Aware 接口，则能在 bean 中获得相应的 Spring 容器资源。\n  如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入Bean的name。 如果 Bean 实现了 BeanFactoryAware接口，调用setBeanFactory()方法，将bean所在的对象的引用传递过来 或者ApplicationContextAware接口，调用setApplicationContext()方法，将bean所在的对象的引用传递过来 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。  例子\n定义一个bean实现BeanNameAware，那么在setBeanName方法中就可以得到，bean的name\n@Component(\u0026quot;测试beanName\u0026quot;)\rpublic class TestBeanNameAware implements BeanNameAware {\r@Override\rpublic void setBeanName(String s) {\rSystem.out.println(s);\r}\r}\r 创建一个bean，实现ApplicationContextAware接口，那么通过setApplicationContext接口就能得到spring容器\n@Component\rpublic class SpringContextUtil implements ApplicationContextAware {\rprivate static ApplicationContext APPLICATIONCONTEXT;\r@Override\rpublic void setApplicationContext(ApplicationContext context) throws BeansException {\rAPPLICATIONCONTEXT = context;\r}\r/**\r* 根据名字获取bean对象\r*\r* @param name\r* @param \u0026lt;T\u0026gt;\r* @return\r* @throws BeansException\r*/\r@SuppressWarnings(\u0026quot;unchecked\u0026quot;)\rpublic static \u0026lt;T\u0026gt; T getBean(String name) throws BeansException {\rreturn (T) APPLICATIONCONTEXT.getBean(name);\r}\rpublic static \u0026lt;T\u0026gt; T getBean(Class\u0026lt;T\u0026gt; requiredType) {\rreturn APPLICATIONCONTEXT.getBean(requiredType);\r}\r/**\r* 获取所有的实现接口\r*\r* @param requiredType\r* @return\r*/\rpublic static \u0026lt;T\u0026gt; Map\u0026lt;String, T\u0026gt; getBeanOfTypes(Class\u0026lt;T\u0026gt; requiredType) {\rreturn APPLICATIONCONTEXT.getBeansOfType(requiredType);\r}\r}\r 测试\n@Test\rpublic void test() {\r// 在TestBeanName初始化之前就会进行setBeanName方法\rSpringContextUtil.getBean(TestBeanNameAware.class);\r// 结果为打印出来的是测试beanName\r}\r   调用BeanPostProcessor的前置处理中postProcessBeforeInitialization() 方法\n  初始化   如果实现了initializingBean的接口, 执行afterPropertiesSet()方法\n 这里需要注意到经常使用的一个注解，@PostConstruct，该注解执行顺序为，postProcessBeforeInitialization() -\u0026gt; @PostConstruct -\u0026gt; afterPropertiesSet\n   如果配置了自定义的init-method，执行指定的方法初始化Bean\n  初始化后  如果实现了 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法，aop就是在这里进行实现的  销毁   调用由 @PreDestroy 注解的方法\n  当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy()方法\n  当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法\n  AOP aop面向切面编程，主要用于事务、日志等\n  切点(Poincut)：具体定位的连接点：在哪些类，哪些方法上切入（where）\n语法\n@Pointcut(\u0026quot;execution(* com.zt.javastudy.grammar.*.test(..))\u0026quot;)\r*：匹配任何数量字符；\r..：匹配任何数量字符的重复，如在类型模式中匹配任何数量子包；而在方法参数模式中匹配任何数量参数。\r+：匹配指定类型的子类型；仅能作为后缀放在类型模式后边。\r第一个占位符* 是表示返回值类型任意\r(..) 代表方法参数任意\r   连接点(Join point)：被切点筛选出来的地方，Spring只支持方法执行这一种Joinpoint\n  增强/通知(Advice)： 在方法执行的什么时间（**when:**方法前/方法后/方法前后）做什么（**what:**增强的功能）\n Before 前置通知，在方法被调用之前调用 After 后置通知，在方法完成之后调用 After-returning 返回通知，在方法成功执行之后调用 After-throwing 异常通知， 在方法抛出异常之后调用 Around 环绕通知，在被通知的方法调用之前和调用之后调用    切面（Aspect）= 切入点 + 通知，通俗点就是：在什么时机，什么地方，做什么增强！\n  织入(Weaving)：将增强/通知添加到目标类的具体连接点上的过程,通过代理实现，jdk动态代理和cglib\n  @Component\rpublic class AopStudy {\r// 连接点\rpublic void test(){\rSystem.out.println(\u0026quot;真正的方法执行啦\u0026quot;);\r}\r}\r@Component\r// 切面\r@Aspect\rpublic class AopRun {\r// 切点\r@Pointcut(\u0026quot;execution(* com.zt.javastudy.aop.AopStudy.test(..))\u0026quot;)\rpublic void test() {\r}\r@Before(\u0026quot;test()\u0026quot;)\rpublic void beforeTest() {\rSystem.out.println(\u0026quot;beforeTest\u0026quot;);\r}\r@After(\u0026quot;test()\u0026quot;)\rpublic void afterTest() {\rSystem.out.println(\u0026quot;afterTest\u0026quot;);\r}\r@Around(\u0026quot;test()\u0026quot;)\rpublic void arountTest(ProceedingJoinPoint point) {\rSystem.out.println(\u0026quot;around1\u0026quot;);\rtry {\rpoint.proceed();\r} catch (Throwable throwable) {\rthrowable.printStackTrace();\r}\rSystem.out.println(\u0026quot;around2\u0026quot;);\r}\r}\r aop代码实现 说了很多遍了，在BeanPostProcessor 对象中的postProcessAfterInitialization() 方法实现aop\n@Override\rpublic Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName)\rthrows BeansException {\rObject result = existingBean;\rfor (BeanPostProcessor processor : getBeanPostProcessors()) {\rObject current = processor.postProcessAfterInitialization(result, beanName);\rif (current == null) {\rreturn result;\r}\rresult = current;\r}\rreturn result;\r}\r 调用postProcessAfterInitialization方法\n/**\r* Create a proxy with the configured interceptors if the bean is\r* identified as one to proxy by the subclass.\r* @see #getAdvicesAndAdvisorsForBean\r*/\r// 注释就告诉我们这是创建aop代理的地方\r@Override\rpublic Object postProcessAfterInitialization(@Nullable Object bean, String beanName) {\rif (bean != null) {\rObject cacheKey = getCacheKey(bean.getClass(), beanName);\rif (this.earlyProxyReferences.remove(cacheKey) != bean) {\r// 关键方法 如果需要被代理那么就封装指定的bean\rreturn wrapIfNecessary(bean, beanName, cacheKey);\r}\r}\rreturn bean;\r}\rprotected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) {\rif (StringUtils.hasLength(beanName) \u0026amp;\u0026amp; this.targetSourcedBeans.contains(beanName)) {\rreturn bean;\r}\rif (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) {\rreturn bean;\r}\r// 给定的bean是否一个基础设施类如果是就不需要增强\rif (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) {\rthis.advisedBeans.put(cacheKey, Boolean.FALSE);\rreturn bean;\r}\r// Create proxy if we have advice.\r// 获取增加方法或增强器\rObject[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null);\rif (specificInterceptors != DO_NOT_PROXY) {\rthis.advisedBeans.put(cacheKey, Boolean.TRUE);\r// 创建代理\rObject proxy = createProxy(\rbean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean));\rthis.proxyTypes.put(cacheKey, proxy.getClass());\rreturn proxy;\r}\rthis.advisedBeans.put(cacheKey, Boolean.FALSE);\rreturn bean;\r}\r 基础设施类包括：Spring跳过的是适用于当前bean的Advisor的Advice/Aspect对象，说人话就是我们定义的切面注解@AspectJ\n获取增强方法或者增强器 protected Object[] getAdvicesAndAdvisorsForBean(\rClass\u0026lt;?\u0026gt; beanClass, String beanName, @Nullable TargetSource targetSource) {\rList\u0026lt;Advisor\u0026gt; advisors = findEligibleAdvisors(beanClass, beanName);\rif (advisors.isEmpty()) {\rreturn DO_NOT_PROXY;\r}\rreturn advisors.toArray();\r}\rprotected List\u0026lt;Advisor\u0026gt; findEligibleAdvisors(Class\u0026lt;?\u0026gt; beanClass, String beanName) {\r// 获取所有的增强\rList\u0026lt;Advisor\u0026gt; candidateAdvisors = findCandidateAdvisors();\r// 寻找所有增强中适用于bean的增强并应用\rList\u0026lt;Advisor\u0026gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName);\rextendAdvisors(eligibleAdvisors);\rif (!eligibleAdvisors.isEmpty()) {\religibleAdvisors = sortAdvisors(eligibleAdvisors);\r}\rreturn eligibleAdvisors;\r}\r 获取增强器 protected List\u0026lt;Advisor\u0026gt; findCandidateAdvisors() {\r// Add all the Spring advisors found according to superclass rules.\rList\u0026lt;Advisor\u0026gt; advisors = super.findCandidateAdvisors();\r// Build Advisors for all AspectJ aspects in the bean factory.\rif (this.aspectJAdvisorsBuilder != null) {\radvisors.addAll(this.aspectJAdvisorsBuilder.buildAspectJAdvisors());\r}\rreturn advisors;\r}\r 这里的代码过于复杂，主要思路就是\n 获取所有beanName，这一步骤中所有在beanFacotry中注册的Bean都会被提取出来。 遍历所有beanName，并找出声明AspectJ注解的类，进行进一步的处理。 对标记为AspectJ注解的类进行增强器的提取。 将提取结果加入缓存  代码首先完成了对增强器的获取，包括获取注解以及根据注解生成增强的步骤，然后考虑到在配置中可能会将增强配置成延迟初始化，那么需要在首位加入同步实例化增强器以保证增强使用之前的实例化，最后是对DeclareParents注解的获取。\n寻找匹配的增强器 protected List\u0026lt;Advisor\u0026gt; findAdvisorsThatCanApply(\rList\u0026lt;Advisor\u0026gt; candidateAdvisors, Class\u0026lt;?\u0026gt; beanClass, String beanName) {\rProxyCreationContext.setCurrentProxiedBeanName(beanName);\rtry {\rreturn AopUtils.findAdvisorsThatCanApply(candidateAdvisors, beanClass);\r}\rfinally {\rProxyCreationContext.setCurrentProxiedBeanName(null);\r}\r}\r 上个函数中已经完成了所有增强器的解析，但是对于所有增强器来讲，并不一定都适用于当前的Bean，还要挑取出适合的增强器，也就是满足我们配置的通配符的增强器，这个函数就完成了这一工作。\n创建代理 ​\t就是真正创建代理的地方\nprotected Object createProxy(Class\u0026lt;?\u0026gt; beanClass, @Nullable String beanName,\r@Nullable Object[] specificInterceptors, TargetSource targetSource) {\rif (this.beanFactory instanceof ConfigurableListableBeanFactory) {\rAutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass);\r}\rProxyFactory proxyFactory = new ProxyFactory();\rproxyFactory.copyFrom(this);\rif (!proxyFactory.isProxyTargetClass()) {\rif (shouldProxyTargetClass(beanClass, beanName)) {\rproxyFactory.setProxyTargetClass(true);\r}\relse {\revaluateProxyInterfaces(beanClass, proxyFactory);\r}\r}\rAdvisor[] advisors = buildAdvisors(beanName, specificInterceptors);\rproxyFactory.addAdvisors(advisors);\rproxyFactory.setTargetSource(targetSource);\rcustomizeProxyFactory(proxyFactory);\rproxyFactory.setFrozen(this.freezeProxy);\rif (advisorsPreFiltered()) {\rproxyFactory.setPreFiltered(true);\r}\rreturn proxyFactory.getProxy(getProxyClassLoader());\r}\r public Object getProxy(@Nullable ClassLoader classLoader) {\rreturn createAopProxy().getProxy(classLoader);\r}\rprotected final synchronized AopProxy createAopProxy() {\rif (!this.active) {\ractivate();\r}\rreturn getAopProxyFactory().createAopProxy(this);\r}\r// 真正的代码\rpublic AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException {\rif (!IN_NATIVE_IMAGE \u0026amp;\u0026amp;\r(config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config))) {\rClass\u0026lt;?\u0026gt; targetClass = config.getTargetClass();\rif (targetClass == null) {\rthrow new AopConfigException(\u0026quot;TargetSource cannot determine target class: \u0026quot; +\r\u0026quot;Either an interface or a target is required for proxy creation.\u0026quot;);\r}\rif (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) {\rreturn new JdkDynamicAopProxy(config);\r}\rreturn new ObjenesisCglibAopProxy(config);\r}\relse {\rreturn new JdkDynamicAopProxy(config);\r}\r}\r 上面的代码主要是确定使用哪种代理方式进行代理，总的来说就是：\n  如果指定了(proxy-target-classs设为true)使用Cglib，那么就会使用Cglib的方式\nspringboot默认将这个开启，也就是说springboot中默认使用cglib代理，在配置类中AopAutoConfiguration\n@Configuration(proxyBeanMethods = false)\r@ConditionalOnClass(Advice.class)\rstatic class AspectJAutoProxyingConfiguration {\r@Configuration(proxyBeanMethods = false)\r@EnableAspectJAutoProxy(proxyTargetClass = false)\r@ConditionalOnProperty(prefix = \u0026quot;spring.aop\u0026quot;, name = \u0026quot;proxy-target-class\u0026quot;, havingValue = \u0026quot;false\u0026quot;,\rmatchIfMissing = false)\rstatic class JdkDynamicAutoProxyConfiguration {\r}\r@Configuration(proxyBeanMethods = false)\r@EnableAspectJAutoProxy(proxyTargetClass = true)\r@ConditionalOnProperty(prefix = \u0026quot;spring.aop\u0026quot;, name = \u0026quot;proxy-target-class\u0026quot;, havingValue = \u0026quot;true\u0026quot;,\rmatchIfMissing = true)\rstatic class CglibAutoProxyConfiguration {\r}\r}\r   如果没有指定(或为false)\n 被代理类实现了自己的接口，那么就采用JDK动态代理的方式 如果没有实现那么就使用Cglib    jdk动态代理和cglib 3. jdk动态代理和cglib对比    动态代理 cglib jdk     是否提供子类代理 是 否   是否提供接口代理 是 是   区别 必须依赖于CGLib的类库，但是它需要类来实现任何接口代理的是指定的类生成一个子类，覆盖其中的方法 实现InvocationHandler，使用Proxy.newProxyInstance产生代理对象，被代理的对象必须要实现接口     Cglib和jdk动态代理的区别？\n 1、Jdk动态代理：利用拦截器（必须实现InvocationHandler）加上反射机制生成一个代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理 2、 Cglib动态代理：利用ASM框架，对代理对象类生成的class文件加载进来，通过修改其字节码生成子类来处理\n JDK动态代理和cglib字节码生成的区别？\n 1、JDK动态代理只能对实现了接口的类生成代理，而不能针对类 2、Cglib是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法，并覆盖其中方法的增强，但是因为采用的是继承，所以该类或方法最好不要生成final，对于final类或方法，是无法继承的\n Cglib比JDK快？\n 1、cglib底层是ASM字节码生成框架，但是字节码技术生成代理类，在JDL1.6之前比使用java反射的效率要高 2、在jdk6之后逐步对JDK动态代理进行了优化，在调用次数比较少时效率高于cglib代理效率 3、只有在大量调用的时候cglib的效率高，但是在1.8的时候JDK的效率已高于cglib 4、Cglib不能对声明final的方法进行代理，因为cglib是动态生成代理对象，final关键字修饰的类不可变只能被引用不能被修改\n 为什么jdk只能对实现了接口的类生成代理？\n 因为代理对象继承了 JAVA 标准类库 Proxy.java 类，所以只能通过实现目标接口来代理\njdk动态代理 /**\r* @author zhengtao\r* @description jdk 动态代理学习\r* @date 2021/4/29\r*/\rpublic interface IJdkProxyStudy {\r/**\r* 目标方法\r*/\rvoid add();\r}\rpublic class JdkProxyStudyImpl implements IJdkProxyStudy {\r@Override\rpublic void add() {\rSystem.out.println(\u0026quot;add\u0026quot;);\r}\r}\rpublic class MyInvocationHandler implements InvocationHandler {\r// 目标对象\rprivate Object target;\rpublic MyInvocationHandler(Object target) {\rsuper();\rthis.target = target;\r}\r/**\r* 执行目标对象的方法\r* @param proxy\r* @param method\r* @param args\r* @return\r* @throws Throwable\r*/\r@Override\rpublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\rSystem.out.println(\u0026quot;before\u0026quot;);\r// 执行目标对象的方法\rObject result = method.invoke(target, args);\rSystem.out.println(\u0026quot;after\u0026quot;);\rreturn result;\r}\r/**\r* 获得目标对象的代理对象\r* @return 代理对象\r*/\rpublic Object getProxy(){\rreturn Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), target.getClass().getInterfaces(), this);\r}\r}\r 测试jdk动态代理：\n@Test\rpublic void testJdkProxy(){\r// 接口\rIJdkProxyStudy jdkProxyStudy = new JdkProxyStudyImpl();\rMyInvocationHandler invocationHandler = new MyInvocationHandler(jdkProxyStudy);\rIJdkProxyStudy proxy = (IJdkProxyStudy) invocationHandler.getProxy();\rproxy.add();\r// 没有实现接口的类，使用jdk代理报错\rCglibTest cglibTest = new CglibTest();\rMyInvocationHandler invocationHandler1 = new MyInvocationHandler(cglibTest);\rCglibTest proxy1 = (CglibTest) invocationHandler1.getProxy();\rproxy1.test();\r}\r/** 结果\rbefore\radd\rafter\r*/\r 我们再次来回顾一下使用JDK代理的方式，在整个创建过程中，对于InvocationHandler的创建是最为核心的，在自定义的InvocationHandler中需要重写3个函数。\n 构造函数，将代理的对象传入。 invoke方法，此方法中实现了AOP增强的所有逻辑。 getProxy方法，此方法千篇一律，但是必不可少。  Spring中JDK代理实现：\n invoke方法:  /**\r* Implementation of {@code InvocationHandler.invoke}.\r* \u0026lt;p\u0026gt;Callers will see exactly the exception thrown by the target,\r* unless a hook method throws an exception.\r*/\r@Override\r@Nullable\rpublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\rObject oldProxy = null;\rboolean setProxyContext = false;\rTargetSource targetSource = this.advised.targetSource;\rObject target = null;\rtry {\rif (!this.equalsDefined \u0026amp;\u0026amp; AopUtils.isEqualsMethod(method)) {\r// The target does not implement the equals(Object) method itself.\rreturn equals(args[0]);\r}\relse if (!this.hashCodeDefined \u0026amp;\u0026amp; AopUtils.isHashCodeMethod(method)) {\r// The target does not implement the hashCode() method itself.\rreturn hashCode();\r}\relse if (method.getDeclaringClass() == DecoratingProxy.class) {\r// There is only getDecoratedClass() declared -\u0026gt; dispatch to proxy config.\rreturn AopProxyUtils.ultimateTargetClass(this.advised);\r}\relse if (!this.advised.opaque \u0026amp;\u0026amp; method.getDeclaringClass().isInterface() \u0026amp;\u0026amp;\rmethod.getDeclaringClass().isAssignableFrom(Advised.class)) {\r// Service invocations on ProxyConfig with the proxy config...\rreturn AopUtils.invokeJoinpointUsingReflection(this.advised, method, args);\r}\rObject retVal;\rif (this.advised.exposeProxy) {\r// Make invocation available if necessary.\roldProxy = AopContext.setCurrentProxy(proxy);\rsetProxyContext = true;\r}\r// Get as late as possible to minimize the time we \u0026quot;own\u0026quot; the target,\r// in case it comes from a pool.\rtarget = targetSource.getTarget();\rClass\u0026lt;?\u0026gt; targetClass = (target != null ? target.getClass() : null);\r// Get the interception chain for this method.\rList\u0026lt;Object\u0026gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);\r// Check whether we have any advice. If we don't, we can fallback on direct\r// reflective invocation of the target, and avoid creating a MethodInvocation.\rif (chain.isEmpty()) {\r// We can skip creating a MethodInvocation: just invoke the target directly\r// Note that the final invoker must be an InvokerInterceptor so we know it does\r// nothing but a reflective operation on the target, and no hot swapping or fancy proxying.\rObject[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args);\rretVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse);\r}\relse {\r// We need to create a method invocation...\rMethodInvocation invocation =\rnew ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain);\r// Proceed to the joinpoint through the interceptor chain.\rretVal = invocation.proceed();\r}\r// Massage return value if necessary.\rClass\u0026lt;?\u0026gt; returnType = method.getReturnType();\rif (retVal != null \u0026amp;\u0026amp; retVal == target \u0026amp;\u0026amp;\rreturnType != Object.class \u0026amp;\u0026amp; returnType.isInstance(proxy) \u0026amp;\u0026amp;\r!RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) {\r// Special case: it returned \u0026quot;this\u0026quot; and the return type of the method\r// is type-compatible. Note that we can't help if the target sets\r// a reference to itself in another returned object.\rretVal = proxy;\r}\relse if (retVal == null \u0026amp;\u0026amp; returnType != Void.TYPE \u0026amp;\u0026amp; returnType.isPrimitive()) {\rthrow new AopInvocationException(\r\u0026quot;Null return value from advice does not match primitive return type for: \u0026quot; + method);\r}\rreturn retVal;\r}\rfinally {\rif (target != null \u0026amp;\u0026amp; !targetSource.isStatic()) {\r// Must have come from TargetSource.\rtargetSource.releaseTarget(target);\r}\rif (setProxyContext) {\r// Restore old proxy.\rAopContext.setCurrentProxy(oldProxy);\r}\r}\r}\r 上面的函数中最主要的工作就是创建了一个拦截器链，并使用ReflectiveMethodInvocation类进行了链的封装，而在ReflectiveMethodInvocation类的proceed方法中实现了拦截器的逐一调用\ngetProxy方法  public Object getProxy(@Nullable ClassLoader classLoader) {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Creating JDK dynamic proxy: \u0026quot; + this.advised.getTargetSource());\r}\rreturn Proxy.newProxyInstance(classLoader, this.proxiedInterfaces, this);\r}\r Cglib代理学习：\npublic class CglibTest {\rpublic void test(){\rSystem.out.println(\u0026quot;test\u0026quot;);\r}\r}\rpublic class CgLibProxy implements MethodInterceptor {\r// 目标对象\rprivate Object target;\rpublic CgLibProxy(Object target) {\rsuper();\rthis.target = target;\r}\r@Override\rpublic Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {\rSystem.out.println(\u0026quot;before test\u0026quot;);\rObject result = methodProxy.invokeSuper(o, objects);\rSystem.out.println(\u0026quot;after test\u0026quot;);\rreturn result;\r}\rpublic Object getProxy(){\rEnhancer enhancer = new Enhancer();\renhancer.setSuperclass(this.target.getClass());\renhancer.setCallback(this);\rObject proxy = enhancer.create();\rreturn proxy;\r}\r}\r 测试：\n@Test\rpublic void testCglibProxy(){\rCglibTest cglibTest = new CglibTest();\rCgLibProxy cgLibProxy = new CgLibProxy(cglibTest);\rCglibTest proxy = (CglibTest) cgLibProxy.getProxy();\rproxy.test();\r}\r结果\rbefore test\rtest\rafter test\r spring中怎么实现的就不多讲了。\n静态代理  public interface StaticProxy {\rvoid test();\r}\rpublic class StaticProxyImpl implements StaticProxy {\r@Override\rpublic void test() {\rSystem.out.println(\u0026quot;test\u0026quot;);\r}\r}\rpublic class StaticProxyTest implements StaticProxy {\rprivate StaticProxy staticProxy;\rpublic StaticProxyTest(StaticProxy staticProxy) {\rthis.staticProxy = staticProxy;\r}\r@Override\rpublic void test() {\rSystem.out.println(\u0026quot;before test\u0026quot;);\rthis.staticProxy.test();\rSystem.out.println(\u0026quot;after test\u0026quot;);\r}\r}\r 测试\n@Test\rpublic void testStaticProxy(){\rStaticProxy staticProxy = new StaticProxyTest(new StaticProxyImpl());\rstaticProxy.test();\r}\r结果\rbefore test\rtest\rafter test\r 静态代理感觉起来就是每个类都必须有一个代理类来具体实现，所以就效率不高。\nspringmvc Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型(指的就是使用请求-响应模型)的轻量级Web框架，通过把Model(模型)，View(视图)，Controller(控制器)分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。\nspringmvc的组件  前端控制器 DispatcherServlet：接收请求、响应结果，相当于转发器，有了DispatcherServlet 就减少了其它组件之间的耦合度。 处理器映射器 HandlerMapping：根据请求的URL来查找Handler 处理器适配器 HandlerAdapter：负责执行Handler 处理器 Handler：处理器，需要程序员开发 视图解析器 ViewResolver：进行视图的解析，根据视图逻辑名将ModelAndView解析成真正的视图（view） 视图View：View是一个接口， 它的实现类支持不同的视图类型，如jsp，freemarker，pdf等等  SpringMVC的流程   用户发送请求至前端控制器DispatcherServlet；\n  DispatcherServlet收到请求后，调用HandlerMapping处理器映射器，请求获取Handler；\n  处理器映射器根据请求url找到具体的处理器Handler，生成处理器对象及处理器拦截器(如果有则生成)，一并返回给DispatcherServlet；\n  DispatcherServlet 调用 HandlerAdapter处理器适配器，请求执行Handler；\n  HandlerAdapter 经过适配调用 具体处理器进行处理业务逻辑；\n  Handler执行完成返回ModelAndView；\n  HandlerAdapter将Handler执行结果ModelAndView返回给DispatcherServlet；\n  DispatcherServlet将ModelAndView传给ViewResolver视图解析器进行解析；\n  ViewResolver解析后返回具体View；\n  DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中）\n  DispatcherServlet响应用户。\n  Springmvc的优点  与Spring框架集成（如IoC容器、AOP等） 可以支持各种视图技术（jsp，freemarker） 清晰的角色划分：前端控制器（DispatcherServlet）、请求到处理器映射（HandlerMapping）、处理器适配器（HandlerAdapter）、视图解析器（ViewResolver）、处理器或页面控制器（Controller）、验证器（ Validator）、命令对象（Command 请求参数绑定到的对象就叫命令对象）、表单对象（Form Object 提供给表单展示和提交到的对象就叫表单对象） 支持各种请求资源的映射策略。  tomcat怎么启动的 springboot @SpringBootApplication @SpringBootApplication 注解其实是一个组合注解。包含@SpringBootConfiguration、@EnableAutoConfiguration 和 @ComponentScan\n  @SpringBootConfiguration：组合了 @Configuration 注解，实现配置文件的功能。\n  @EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项， 例如：java 如关闭数据源自动配置功能： @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class })。\n  @ComponentScan：Spring组件扫描。\n  自动装配 SpringBoot 定义了一套接口规范，这套规范规定：SpringBoot 在启动时会扫描外部引用 jar 包中的META-INF/spring.factories文件，将文件中配置的类型信息加载到 Spring 容器（此处涉及到 JVM 类加载机制与 Spring 的容器知识），并执行类中定义的各种操作。对于外部 jar 来说，只需要按照 SpringBoot 定义的标准，就能将自己的功能装置进 SpringBoot。\nspringboot中是通过@EnableAutoConfiguration实现自动装配的\n@Target(ElementType.TYPE)\r@Retention(RetentionPolicy.RUNTIME)\r@Documented\r@Inherited\r@AutoConfigurationPackage\r@Import(AutoConfigurationImportSelector.class)\rpublic @interface EnableAutoConfiguration {\rString ENABLED_OVERRIDE_PROPERTY = \u0026quot;spring.boot.enableautoconfiguration\u0026quot;;\r/**\r* Exclude specific auto-configuration classes such that they will never be applied.\r* @return the classes to exclude\r*/\rClass\u0026lt;?\u0026gt;[] exclude() default {};\r/**\r* Exclude specific auto-configuration class names such that they will never be\r* applied.\r* @return the class names to exclude\r* @since 1.3.0\r*/\rString[] excludeName() default {};\r}\r 注解中是导入了AutoConfigurationImportSelector类，这个类实现了 ImportSelector接口，实现了selectImports，该方法主要用于获取所有符合条件的类的全限定类名，这些类需要被加载到 IoC 容器中\n@Override\rpublic String[] selectImports(AnnotationMetadata annotationMetadata) {\rif (!isEnabled(annotationMetadata)) {\rreturn NO_IMPORTS;\r}\rAutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader\r.loadMetadata(this.beanClassLoader);\rAutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(autoConfigurationMetadata,\rannotationMetadata);\rreturn StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());\r}\r 核心方法是getAutoConfigurationEntry\nprotected AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata,\rAnnotationMetadata annotationMetadata) {\r// 判断自动装配开关是否打开\rif (!isEnabled(annotationMetadata)) {\rreturn EMPTY_ENTRY;\r}\r// 获取`EnableAutoConfiguration`注解中的 `exclude` 和 `excludeName`\rAnnotationAttributes attributes = getAttributes(annotationMetadata);\r// 获取需要自动装配的所有配置类\rList\u0026lt;String\u0026gt; configurations = getCandidateConfigurations(annotationMetadata, attributes);\rconfigurations = removeDuplicates(configurations);\rSet\u0026lt;String\u0026gt; exclusions = getExclusions(annotationMetadata, attributes);\rcheckExcludedClasses(configurations, exclusions);\r// 移除exclude中的配置\rconfigurations.removeAll(exclusions);\r// 根据条件注解筛选出符合条件的配置类\rconfigurations = filter(configurations, autoConfigurationMetadata);\rfireAutoConfigurationImportEvents(configurations, exclusions);\rreturn new AutoConfigurationEntry(configurations, exclusions);\r}\r   判断自动装配开关是否打开。默认spring.boot.enableautoconfiguration=true，可在 application.properties 或 application.yml 中设置\n  用于获取EnableAutoConfiguration注解中的 exclude 和 excludeName。\n  getCandidateConfigurations 方法通过SpringFactoriesLoader.loadFactoryNames获取需要自动装配的所有配置类，读取META-INF/spring.factories\n  spring-boot/spring-boot-project/spring-boot-autoconfigure/src/main/resources/META-INF/spring.factories\r 从下图可以看到这个文件的配置内容都被我们读取到了。XXXAutoConfiguration的作用就是按需加载组件。不光是这个依赖下的META-INF/spring.factories被读取到，所有 Spring Boot Starter 下的META-INF/spring.factories都会被读取到。\n 所以如果要想自己的代码被springboot配置，只需要添加META-INF/spring.factories并进行配置，例如druid 数据库连接池的 Spring Boot Starter 就创建了META-INF/spring.factories文件。\n filter 筛选符合条件的配置类  总结 Spring Boot 通过@EnableAutoConfiguration开启自动装配，通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配，自动配置类其实就是通过@Conditional按需加载的配置类\nspring依赖来源 spring事务 Spring 框架中，事务管理相关最重要的 3 个接口如下：\n  TransactionDefinition： 事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则)。\n  PlatformTransactionManager： （平台）事务管理器，Spring 事务策略的核心，约束了事务常用的方法。\n 通过这个接口，Spring 为各个平台如 JDBC(DataSourceTransactionManager)、Hibernate(HibernateTransactionManager)、JPA(JpaTransactionManager)等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。\n public interface PlatformTransactionManager {\r//获得事务\rTransactionStatus getTransaction(@Nullable TransactionDefinition var1) throws TransactionException;\r//提交事务\rvoid commit(TransactionStatus var1) throws TransactionException;\r//回滚事务\rvoid rollback(TransactionStatus var1) throws TransactionException;\r}Copy to clipboardErrorCopied\r   TransactionStatus： 事务运行状态。\npublic interface TransactionStatus{\rboolean isNewTransaction(); // 是否是新的事务\rboolean hasSavepoint(); // 是否有恢复点\rvoid setRollbackOnly(); // 设置为只回滚\rboolean isRollbackOnly(); // 是否为只回滚\rboolean isCompleted; // 是否已完成\r}Copy to clipboardErrorCopied\r     注解@EnableTransactionManagement 实现事务相关的Bean加载（现在自动配置使用AutoConfiguration实现）\n  TransactionInterceptor 主要的实现类，继承TransactionAspectSupport（定义了事务实现的方式）\n  实现原理为使用AOP+ThreadLocal实现。\n  @EnableTransactionManagement @EnableTransactionManagement 是开启注解式事务的注解\n@Target(ElementType.TYPE)\r@Retention(RetentionPolicy.RUNTIME)\r@Documented\r@Import(TransactionManagementConfigurationSelector.class)\rpublic @interface EnableTransactionManagement {\r/**\r* 用来表示默认使用JDK Dynamic Proxy还是CGLIB Proxy\r*/\rboolean proxyTargetClass() default false;\r/**\r* 表示以Proxy-based方式实现AOP还是以Weaving-based方式实现AOP\r*/\rAdviceMode mode() default AdviceMode.PROXY;\r/**\r* 顺序\r*/\rint order() default Ordered.LOWEST_PRECEDENCE;\r}\r @EnableTransactionManagement 注解看起来并没有特别之处，都是一些属性的配置。但它却通过 @Import 引入了另一个配置 TransactionManagentConfigurationSelector 。\nTransactionManangementConfigurationSelector 在Spring中， Selector 通常都是用来选择一些Bean，向容器注册BeanDefinition的(严格意义上Selector仅时选择过程，注册的具体过程是在 ConfigurationClasspathPostProcessor 解析时，调用 ConfigurationClassParser 触发)。 主要的逻辑就是根据代理模式，注册不同的BeanDefinition。 对Proxy的模式而言，注入的有两个：\n AutoProxyRegistrar ProxyTransactionManagementConfiguration  AutoProxyRegistrar Registrar同样也是用来向容器注册Bean的，在Proxy的模式下，它会调用 AopConfigUtils.registerAutoProxyCreatorIfNecessary(registry); 向容器中注册 InfrastructureAdvisorAutoProxyCreator 。而这个类就是我们上文提到的 AbstractAdvisorAutoProxyCreator 的子类。 从而，我们完成了我们的第一个条件——AOP代理。\nProxyTransactionManagementConfiguration @Configuration\rpublic class ProxyTransactionManagementConfiguration extends AbstractTransactionManagementConfiguration {\r@Bean(name = TransactionManagementConfigUtils.TRANSACTION_ADVISOR_BEAN_NAME)\r@Role(BeanDefinition.ROLE_INFRASTRUCTURE)\rpublic BeanFactoryTransactionAttributeSourceAdvisor transactionAdvisor() {\rBeanFactoryTransactionAttributeSourceAdvisor advisor = new BeanFactoryTransactionAttributeSourceAdvisor();\radvisor.setTransactionAttributeSource(transactionAttributeSource());\radvisor.setAdvice(transactionInterceptor());\rif (this.enableTx != null) {\radvisor.setOrder(this.enableTx.\u0026lt;Integer\u0026gt;getNumber(\u0026quot;order\u0026quot;));\r}\rreturn advisor;\r}\r@Bean\r@Role(BeanDefinition.ROLE_INFRASTRUCTURE)\rpublic TransactionAttributeSource transactionAttributeSource() {\rreturn new AnnotationTransactionAttributeSource();\r}\r@Bean\r@Role(BeanDefinition.ROLE_INFRASTRUCTURE)\rpublic TransactionInterceptor transactionInterceptor() {\rTransactionInterceptor interceptor = new TransactionInterceptor();\rinterceptor.setTransactionAttributeSource(transactionAttributeSource());\rif (this.txManager != null) {\r// 注入事务管理器\rinterceptor.setTransactionManager(this.txManager);\r}\rreturn interceptor;\r}\r}\r ProxyTransactionManagementConfiguration 是一个配置类，如果算上其继承的父类，一共是声明了四个类：\n TransactionalEventListenerFactory BeanFactoryTransactionAttributeSourceAdvisor TransactionAttributeSource TransactionInterceptor  后三个类相对比较重要，我们一一分析。\nBeanFactoryTransactionAttributeSourceAdvisor 从名字看就知道这是一个Advisor 切面，那么它身上应该有Pointcut 切点和advice通知，切点是TransactionAttributeSourcePointcut ，主要是一些filter和matches之类的方法，筛选使用了@Transactional 注解的方法或类。如果有这个，则利用所设置的advice（也就是TransactionInterceptor）进行AOP，生成代理对象。\nTransactionAttributeSource TransactionAttributeSource 只是一个接口，扩展了 TransactionDefinition ，增加了 isCandidateClass() 的方法(可以用来帮助Pointcut匹配)。实际创建的是AnnotationTransactionAttributeSource, 用于解析@Transactional注解\nTransactionInterceptor 这里主要做了两件事，一指定事务管理器PlatformTransactionManager，二完成通知处理即增强\n\t@Nullable\rprotected Object invokeWithinTransaction(Method method, @Nullable Class\u0026lt;?\u0026gt; targetClass,\rfinal InvocationCallback invocation) throws Throwable {\rTransactionAttributeSource tas = getTransactionAttributeSource();\rfinal TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null);\rfinal TransactionManager tm = determineTransactionManager(txAttr);\r//省略部分代码\r//获取事物管理器\rPlatformTransactionManager ptm = asPlatformTransactionManager(tm);\rfinal String joinpointIdentification = methodIdentification(method, targetClass, txAttr);\rif (txAttr == null || !(ptm instanceof CallbackPreferringPlatformTransactionManager)) {\r// 打开事务(内部就是getTransactionStatus的过程)\rTransactionInfo txInfo = createTransactionIfNecessary(ptm, txAttr, joinpointIdentification);\rObject retVal;\rtry {\r// 执行业务逻辑 invocation.proceedWithInvocation();\r}\rcatch (Throwable ex) {\r// 异常回滚\rcompleteTransactionAfterThrowing(txInfo, ex);\rthrow ex;\r}\rfinally {\rcleanupTransactionInfo(txInfo);\r}\r//省略部分代码\r//提交事物\rcommitTransactionAfterReturning(txInfo);\rreturn retVal;\r}\r 事务失效场景，\n事务的传播机制 spring 对事务的控制，是使用 aop 切面实现的，我们不用关心事务的开始，提交 ，回滚，只需要在方法上加 @Transactional 注解，这时候就有问题了。\n 场景一： serviceA 方法调用了 serviceB 方法，但两个方法都有事务，这个时候如果 serviceB 方法异常，是让 serviceB 方法提交，还是两个一起回滚。 场景二：serviceA 方法调用了 serviceB 方法，但是只有 serviceA 方法加了事务，是否把 serviceB 也加入 serviceA 的事务，如果 serviceB 异常，是否回滚 serviceA 。 场景三：serviceA 方法调用了 serviceB 方法，两者都有事务，serviceB 已经正常执行完，但 serviceA 异常，是否需要回滚 serviceB 的数据。  所谓spring事务的传播机制，就是定义在存在多个事务同时存在的时候，spring应该如何处理这些事务的行为。这些属性在TransactionDefinition中定义\n   常量名称 常量解释     PROPAGATION_REQUIRED 支持当前事务，如果当前存在事务，则使用该事务。 如果当前没有事务，则创建一个新的事务。是 Spring 默认的事务的传播   PROPAGATION_SUPPORTS 支持当前事务，如果当前存在事务，则使用该事务，如果当前不存在事务，则以非事务的方式运行   PROPAGATION_MANDATORY 支持当前事务，如果当前存在事务，则使用该事务，如果当前事务不存在则抛出异常   PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。新建的事务将和被挂起的事务没有任何关系，是两个独立的事务，外层事务失败回滚之后，不能回滚内层事务执行的结果，内层事务失败抛出异常，外层事务捕获，也可以不处理回滚操作   PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。   PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。   PROPAGATION_NESTED 嵌套事务。如果当前存在事务，它将会成为父级事务的一个子事务，方法结束后并没有提交，只有等父事务结束才提交。如果没有活动事务，则按REQUIRED属性执行    spring缓存 分库分表 shardingjdbc\nspringjdbc，mybatis 重复支付 怎么保证状态的一致性，状态机 springboot启动类的理解 @EnableXXX 在 SpringBoot中 @Enable的实现方式用两种。\n  注解驱动的方式，我们以 @EnableWebMvc为例进行探究；\n  接口编程的方式，我们以 @EnableCaching为例进行探究。\n  注解驱动方式（@EnableWebMvc） 在 Spring Boot项目中，当我们可以使用 @EnableWebMvc注解用来激活我们的 Spring MVC相关的配置，接下来进入源码一探究竟。\n@Retention(RetentionPolicy.RUNTIME)\r@Target(ElementType.TYPE)\r@Documented\r@Import(DelegatingWebMvcConfiguration.class)\rpublic @interface EnableWebMvc {\r}\r 通过观察上面的源码，我们就可以大胆的猜测，其实使用 @EnableWebMvc注解的作用就是导入 DelegatingWebMvcConfiguration.class这个类，接下来我们就进入这个类看看。\n@Configuration\rpublic class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport {\r........\r@Override\rprotected void addInterceptors(InterceptorRegistry registry) {\rthis.configurers.addInterceptors(registry);\r}\r@Override\rprotected void addResourceHandlers(ResourceHandlerRegistry registry) {\rthis.configurers.addResourceHandlers(registry);\r}\r........\r}\r 进入到这个类中我们发现了这个 @Configuration注解，到这儿我们好像明白了写什么。首先这个 DelegatingWebMvcConfiguration类继承了 WebMvcConfigurationSupport类，重写了里面的关于 WebMvc的相关配置，然后作为一个配置类加载到我们的 Spring容器中。至此来实现启动（激活）WebMvc模块。\n接口编程的方式（@EnableCaching） 在 Spring Boot项目中，当我们可以使用 @EnableCaching注解用来激活我们的缓存相关的配置，接着进入源码看看到底做了什么。\n@Target(ElementType.TYPE)\r@Retention(RetentionPolicy.RUNTIME)\r@Documented\r@Import(CachingConfigurationSelector.class)\rpublic @interface EnableCaching {\rboolean proxyTargetClass() default false;\rAdviceMode mode() default AdviceMode.PROXY;\rint order() default Ordered.LOWEST_PRECEDENCE;\r}\r 这里 @EnableCaching同样是使用 @Import导入了一个配置类，而它导入的是 CachingConfigurationSelector，接着我进入这个类看一看。\npublic class CachingConfigurationSelector extends AdviceModeImportSelector\u0026lt;EnableCaching\u0026gt; {\r.....\r@Override\rpublic String[] selectImports(AdviceMode adviceMode) {\rswitch (adviceMode) {\rcase PROXY:\rreturn getProxyImports();\rcase ASPECTJ:\rreturn getAspectJImports();\rdefault:\rreturn null;\r}\r}\r.....\r}\r 发现其实这个类没有被注解标注，但是它继承了 AdviceModeImportSelector\u0026lt;enablecaching\u0026gt;，而这个类又继承了 ImportSelector，并且我们可以看看 ImportSelector的代码：\npublic interface ImportSelector {\r/**\r* Select and return the names of which class(es) should be imported based on\r* the {@link AnnotationMetadata} of the importing @{@link Configuration} class.\r*/\rString[] selectImports(AnnotationMetadata importingClassMetadata);\r}\r 这个类中只用一个方法，那就是 selectImports。也就是说当我们重写了这个方法之后，我们可以在方法中添加自己的逻辑判断，来决定最后导入哪些配置类。这样就可以实现灵活的加载配置。这个方法的返回值 String[]里面存放的是所有复合条件的配置类的全路径信息。\nmysql中事务怎么实现的  ","id":6,"section":"posts","summary":"这里记录遇到、听到、见到的面试题，可以说是总结复盘的好地方 [TOC] 阿里：三个线程交替打印abc。 手写单例模式双重校验锁 分布式事务 接口幂等性 Hash","tags":null,"title":"面试题汇总","uri":"https://wzgl998877.github.io/2022/02/%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/","year":"2022"},{"content":"[TOC]\n算法这东西，可以说玄之又玄，大学就被程序设计这门课统治，工作了，虽然说这东西在实践中用到的少，但是谁让大家都卷呢，你不会就是不行，所以本着打不赢就加入的原则，自己也刷了不少题，今天作下总结。\n算法复杂度 算法的复杂度，其实就是分为时间复杂度和空间复杂度，比如O(1)、O(logn)、O(n)、O(nlogn)、O(n^2) 等等\n时间复杂度 算法的时间复杂度（Time complexity）是一个函数，它定性描述该算法的运行时间。这是一个代表算法输入值的字符串的长度的函数。时间复杂度常用大O符号表述，不包括这个函数的低阶项和首项系数，例如。\nO(2*n^2 + 10*n + 1000) = O(2*n^2 + 10*n) = O(n^2 + n) = O(n^2)\r 各个时间复杂度的例子\n 常见时间复杂度 O(1)常数阶 \u0026lt; O(log n)对数阶 \u0026lt; O(n)线性阶 \u0026lt; O(n^2)平方阶 \u0026lt; O(n^3)立方阶 \u0026lt; O(2^n)指数阶\n package com.zt.javastudy.leetcode;\r/**\r* @author zhengtao\r* 算法复杂度分析\r*/\rpublic class Time {\r/**\r* 0(n) 时间复杂度,一个循环\r*/\rpublic void test1(int[] nums) {\rfor (int i = 0; i \u0026lt; nums.length; i++) {\rSystem.out.println(nums[i]);\r}\r}\r/**\r* O(log n)，对数时间,常见的具有对数时间的算法有二叉树的相关操作和二分搜索。\r* 可以理解为每次都缩小了一半\r*/\rpublic int binary_search(int[] nums, int target) {\rint left = 0, right = nums.length - 1;\rwhile (left \u0026lt;= right) {\rint mid = left + (right - left) / 2;\rif (nums[mid] \u0026lt; target) {\rleft = mid + 1;\r} else if (nums[mid] \u0026gt; target) {\rright = mid - 1;\r} else if (nums[mid] == target) {\r// 直接返回\rreturn mid;\r}\r}\r// 直接返回\rreturn -1;\r}\r/**\r* o(nlogn),内循环是O(log n)\r*/\rpublic void function3(long n) {\rSystem.out.println(\u0026quot;o(nlogn)算法\u0026quot;);\rlong k = 0;\rfor (long i = 0; i \u0026lt; n; i++) {\r// O(log n)\rfor (long j = 1; j \u0026lt; n; j = j * 2) {\rk++;\r}\r}\r}\r/**\r* o(n^2),两层循环\r*\r* @param n\r*/\rpublic static void function2(long n) {\rSystem.out.println(\u0026quot;o(n^2)算法\u0026quot;);\rlong k = 0;\rfor (long i = 0; i \u0026lt; n; i++) {\rfor (long j = 0; j \u0026lt; n; j++) {\rk++;\r}\r} }\r}\r 数组 数组的题目其实算是比较简单的，一是数组是最熟悉的数据结构，二是数组的题目不需要用到一些算法（贪心、回溯、递归等等），数组常见类型的题目有：\n 二分查找 滑动窗口 双指针  下面将从自己刷过题中按这几个题型来总结\n二分查找 二分查找的题目其实很简单，二分法都很熟了，二分查找的核心就是找到具有单调性的正确查找对象，但是涉及到二分查找的问题，主要是注意**「搜索区间」和 while 的终止条件**，目前自己全部是采用两端都闭合的搜索区间，有关二分查找的题目又可以分为以下三种类型\n  直接二分搜索（最简单）\npublic int binary_search(int[] nums, int target) {\r// 左边界为0， 右边界为数组最后一个数\rint left = 0, right = nums.length - 1;\r// 结束条件为 \u0026lt;=\rwhile (left \u0026lt;= right) {\rint mid = left + (right - left) / 2;\rif (nums[mid] \u0026lt; target) {\rleft = mid + 1;\r} else if (nums[mid] \u0026gt; target) {\rright = mid - 1;\r} else if (nums[mid] == target) {\r// 直接返回\rreturn mid;\r}\r}\r// 直接返回\rreturn -1;\r}\r   查找最左边界\n/**\r* 查找最左边界，\r* 因为我们需找到 target 的最左侧索引\r* 所以当 nums[mid] == target 时不要立即返回\r* 而要收紧右侧边界以锁定左侧边界\r*\r* @param nums\r* @param target\r* @return\r*/\rpublic int left_bound(int[] nums, int target) {\rint left = 0, right = nums.length - 1;\rwhile (left \u0026lt;= right) {\rint mid = left + (right - left) / 2;\rif (nums[mid] \u0026gt;= target) {\rright = mid - 1;\r} else {\rleft = mid + 1;\r}\r}\r// 最后要检查 left 越界的情况\rif (left \u0026gt;= nums.length || nums[left] != target)\rreturn -1;\rreturn left;\r}\r   寻找右边界\n/**\r* 寻找最右边界\r* 因为我们需找到 target 的最右侧索引\r* 所以当 nums[mid] == target 时不要立即返回\r* 而要收紧左侧边界以锁定右侧边界\r*\r* @param nums\r* @param target\r* @return\r*/\rpublic int right_bound(int[] nums, int target) {\rint left = 0, right = nums.length - 1;\rwhile (left \u0026lt;= right) {\rint mid = left + (right - left) / 2;\rif (nums[mid] \u0026lt;= target) {\rleft = mid + 1;\r} else {\rright = mid - 1;\r}\r}\r// 最后要检查 right 越界的情况\rif (right \u0026lt; 0 || nums[right] != target)\rreturn -1;\rreturn right;\r}\r   总结完这三种类型的题目后，就是套题目了，其实做题最难的也是这一步，遇到什么题该用什么方法，对我这种不是完全理解各种算法的人来说是很难的，所以只能靠量取胜哈哈\n875. 爱吃香蕉的珂珂 珂珂喜欢吃香蕉。这里有 N 堆香蕉，第 i 堆中有 piles[i] 根香蕉。警卫已经离开了，将在 H 小时后回来。\n珂珂可以决定她吃香蕉的速度 K （单位：根/小时）。每个小时，她将会选择一堆香蕉，从中吃掉 K 根。如果这堆香蕉少于 K 根，她将吃掉这堆的所有香蕉，然后这一小时内不会再吃更多的香蕉。\n珂珂喜欢慢慢吃，但仍然想在警卫回来前吃掉所有的香蕉。\n返回她可以在 H 小时内吃掉所有香蕉的最小速度 K（K 为整数）。\n示例 1：\n输入: piles = [3,6,7,11], H = 8\r输出: 4\r 示例 2：\n输入: piles = [30,11,23,4,20], H = 5\r输出: 30\r 示例 3：\n输入: piles = [30,11,23,4,20], H = 6\r输出: 23\r 提示：\n 1 \u0026lt;= piles.length \u0026lt;= 10^4 piles.length \u0026lt;= H \u0026lt;= 10^9 1 \u0026lt;= piles[i] \u0026lt;= 10^9  这道题可以理解为，每小时最少吃1，最大吃getMax(piles),求的就是从1开始，能吃完的最小速度是多少？再联想这其实就是求最左边界，于是可以用二分法来做\n/**\r暴力做法，直接遍历\r*/\rpublic int minEatingSpeed(int[] piles, int h) {\rint max = getMax(piles);\r// 可以用二分法代替\rfor (int speed = 1; speed \u0026lt;= max; speed++) {\rif (canEat(piles, speed, h)) {\rreturn speed;\r}\r}\rreturn max;\r}\r/**\r二分法求最左边界，标准代码\r*/\rpublic int minEatingSpeed1(int[] piles, int h) {\rint left = 1, right = getMax(piles);\rwhile (left \u0026lt;= right) {\rint middle = (left + right) / 2;\rif (canEat(piles, middle, h)) {\rright = middle - 1;\r} else {\rleft = middle + 1;\r}\r}\rreturn left;\r}\rprivate boolean canEat(int[] piles, int speed, int h) {\rint time = 0;\rfor (int i : piles) {\rif (i % speed == 0) {\rtime += i / speed;\r} else {\rtime += i / speed + 1;\r}\r}\rreturn time \u0026lt;= h;\r}\rprivate int getMax(int[] piles) {\rint max = 0;\rfor (int i : piles) {\rmax = Math.max(max, i);\r}\rreturn max;\r}\r 1011. 在 D 天内送达包裹的能力 传送带上的包裹必须在 days 天内从一个港口运送到另一个港口。\n传送带上的第 i 个包裹的重量为 weights[i]。每一天，我们都会按给出重量（weights）的顺序往传送带上装载包裹。我们装载的重量不会超过船的最大运载重量。\n返回能在 days 天内将传送带上的所有包裹送达的船的最低运载能力。\n示例 1：\n输入：weights = [1,2,3,4,5,6,7,8,9,10], days = 5\r输出：15\r解释：\r船舶最低载重 15 就能够在 5 天内送达所有包裹，如下所示：\r第 1 天：1, 2, 3, 4, 5\r第 2 天：6, 7\r第 3 天：8\r第 4 天：9\r第 5 天：10\r请注意，货物必须按照给定的顺序装运，因此使用载重能力为 14 的船舶并将包装分成 (2, 3, 4, 5), (1, 6, 7), (8), (9), (10) 是不允许的。  示例 2：\n输入：weights = [3,2,2,4,1,4], days = 3\r输出：6\r解释：\r船舶最低载重 6 就能够在 3 天内送达所有包裹，如下所示：\r第 1 天：3, 2\r第 2 天：2, 4\r第 3 天：1, 4\r 示例 3：\n输入：weights = [1,2,3,1,1], D = 4\r输出：3\r解释：\r第 1 天：1\r第 2 天：2\r第 3 天：3\r第 4 天：1, 1\r 提示：\n 1 \u0026lt;= days \u0026lt;= weights.length \u0026lt;= 5 * 104 1 \u0026lt;= weights[i] \u0026lt;= 500  这题很显然，跟上面一题很相似,最低运载能力 x 从1开始到sum(weights)结束，求最小的x，这明显就是二分法求最左边界\npublic int shipWithinDays(int[] weights, int days) {\rint left = 1, right = sum(weights);\rwhile (left \u0026lt;= right) {\rint middle = (left + right) / 2;\rif (canWeight(weights, middle, days)) {\rright = middle - 1;\r} else {\rleft = middle + 1;\r}\r}\rreturn left;\r}\rprivate boolean canWeight(int[] weights, int middle, int days) {\rint sum = 0;\rfor (int i : weights) {\rif (i \u0026gt; middle) {\rreturn false;\r}\rif (sum + i \u0026gt; middle) {\rdays--;\rsum = i;\r} else {\rsum += i;\r}\r}\rreturn days \u0026gt; 0;\r}\rprivate int sum(int[] weights) {\rint sum = 0;\rfor (int i : weights) {\rsum += i;\r}\rreturn sum;\r}\r 410. 分割数组的最大值 给定一个非负整数数组 nums 和一个整数 m ，你需要将这个数组分成 m 个非空的连续子数组。\n设计一个算法使得这 m 个子数组各自和的最大值最小。\n示例 1：\n输入：nums = [7,2,5,10,8], m = 2\r输出：18\r解释：\r一共有四种方法将 nums 分割为 2 个子数组。 其中最好的方式是将其分为 [7,2,5] 和 [10,8] 。\r因为此时这两个子数组各自的和的最大值为18，在所有情况中最小。\r 示例 2：\n输入：nums = [1,2,3,4,5], m = 2\r输出：9\r 示例 3：\n输入：nums = [1,4,4], m = 3\r输出：4\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 1000 0 \u0026lt;= nums[i] \u0026lt;= 106 1 \u0026lt;= m \u0026lt;= min(50, nums.length)  这题目一开始看跟二分法完全不沾边，但是仔细想想跟上面两个有类似之处，「使……最大值尽可能小」，这就和二分法差不多了。\n思路为：现在题目是固定了m的值，让我们确定一个最大子数组和；我们可以反过来，限制一个最大子数组和max，来反推最大子数组和为max时，至少可以将nums分割成几个子数组。因此思路就转为，如果我们找到一个最小max值，满足split(nums, max)和m相等，这个就又是寻找最左边界了，就跟上面两题一样了，前面说过二分法最重要的就是搜索区间，那这个的搜索区间是什么呢？显然，子数组至少包含一个元素，至多包含整个数组，所以「最大」子数组和的取值范围就是闭区间[max(nums), sum(nums)]，也就是最大元素值到整个数组和之间。\npublic int splitArray(int[] nums, int m) {\rint left = Arrays.stream(nums).max().getAsInt(), right = Arrays.stream(nums).sum();\rwhile (left \u0026lt;= right) {\rint middle = (left + right) / 2;\rif (spiltNums(nums, middle) \u0026lt;= m) {\rright = middle - 1;\r} else {\rleft = middle + 1;\r}\r}\rreturn left;\r}\r/**\r* 在每个子数组和不超过 max 的条件下，\r* 计算 nums 至少可以分割成几个子数组\r*\r* @param nums\r* @param max\r* @return\r*/\rpublic int spiltNums(int[] nums, int max) {\rint n = 0;\rint sum = 0;\rfor (int i : nums) {\rif (sum + i \u0026gt; max) {\rsum = i;\rn++;\r} else {\rsum += i;\r}\r}\rreturn n + 1;\r}\r 12. 小张刷题计划 为了提高自己的代码能力，小张制定了 LeetCode 刷题计划，他选中了 LeetCode 题库中的 n 道题，编号从 0 到 n-1，并计划在 m 天内按照题目编号顺序刷完所有的题目（注意，小张不能用多天完成同一题）。\n在小张刷题计划中，小张需要用 time[i] 的时间完成编号 i 的题目。此外，小张还可以使用场外求助功能，通过询问他的好朋友小杨题目的解法，可以省去该题的做题时间。为了防止“小张刷题计划”变成“小杨刷题计划”，小张每天最多使用一次求助。\n我们定义 m 天中做题时间最多的一天耗时为 T（小杨完成的题目不计入做题总时间）。请你帮小张求出最小的 T是多少。\n示例 1：\n 输入：time = [1,2,3,3], m = 2\n输出：3\n解释：第一天小张完成前三题，其中第三题找小杨帮忙；第二天完成第四题，并且找小杨帮忙。这样做题时间最多的一天花费了 3 的时间，并且这个值是最小的。\n 示例 2：\n 输入：time = [999,999,999], m = 4\n输出：0\n解释：在前三天中，小张每天求助小杨一次，这样他可以在三天内完成所有的题目并不花任何时间。\n 限制：\n 1 \u0026lt;= time.length \u0026lt;= 10^5 1 \u0026lt;= time[i] \u0026lt;= 10000 1 \u0026lt;= m \u0026lt;= 1000  这题看起来和上面一题就是一模一样的，从0到sum(time),求最左边界\npublic int minTime(int[] time, int m) {\rint left = 0;\rint right = Arrays.stream(time).sum();\rwhile (left \u0026lt;= right) {\rint middle = (left + right) / 2;\rif (spiltTime(time, middle) \u0026lt;= m) {\rright = middle - 1;\r} else {\rleft = middle + 1;\r}\r}\rreturn left;\r}\r/**\r* 在每个子数组和不超过 max 的条件下，\r* 计算 nums 至少可以分割成几个子数组\r*\r* @param nums\r* @param max\r* @return\r*/\rpublic int spiltTime(int[] nums, int max) {\rint n = 0;\rint sum = 0;\rint time = 0;\rfor (int i : nums) {\rif (time \u0026lt; i) {\rtime = i;\r}\rif (sum + i \u0026gt; max + time) {\rsum = i;\rn++;\rtime = i;\r} else {\rsum += i;\r}\r}\rreturn n + 1;\r}\r 1482. 制作 m 束花所需的最少天数 给你一个整数数组 bloomDay，以及两个整数 m 和 k 。\n现需要制作 m 束花。制作花束时，需要使用花园中 相邻的 k 朵花 。\n花园中有 n 朵花，第 i 朵花会在 bloomDay[i] 时盛开，恰好 可以用于 一束 花中。\n请你返回从花园中摘 m 束花需要等待的最少的天数。如果不能摘到 m 束花则返回 -1 。\n示例 1：\n输入：bloomDay = [1,10,3,10,2], m = 3, k = 1\r输出：3\r解释：让我们一起观察这三天的花开过程，x 表示花开，而 _ 表示花还未开。\r现在需要制作 3 束花，每束只需要 1 朵。\r1 天后：[x, _, _, _, _] // 只能制作 1 束花\r2 天后：[x, _, _, _, x] // 只能制作 2 束花\r3 天后：[x, _, x, _, x] // 可以制作 3 束花，答案为 3\r 示例 2：\n输入：bloomDay = [1,10,3,10,2], m = 3, k = 2\r输出：-1\r解释：要制作 3 束花，每束需要 2 朵花，也就是一共需要 6 朵花。而花园中只有 5 朵花，无法满足制作要求，返回 -1 。\r 示例 3：\n输入：bloomDay = [7,7,7,7,12,7,7], m = 2, k = 3\r输出：12\r解释：要制作 2 束花，每束需要 3 朵。\r花园在 7 天后和 12 天后的情况如下：\r7 天后：[x, x, x, x, _, x, x]\r可以用前 3 朵盛开的花制作第一束花。但不能使用后 3 朵盛开的花，因为它们不相邻。\r12 天后：[x, x, x, x, x, x, x]\r显然，我们可以用不同的方式制作两束花。\r 示例 4：\n输入：bloomDay = [1000000000,1000000000], m = 1, k = 1\r输出：1000000000\r解释：需要等 1000000000 天才能采到花来制作花束\r 示例 5：\n输入：bloomDay = [1,10,2,9,3,8,4,7,5,6], m = 4, k = 2\r输出：9\r 提示：\n bloomDay.length == n 1 \u0026lt;= n \u0026lt;= 10^5 1 \u0026lt;= bloomDay[i] \u0026lt;= 10^9 1 \u0026lt;= m \u0026lt;= 10^6 1 \u0026lt;= k \u0026lt;= n  这题也是一样的，二分法最左边界就完事了\npublic int minDays(int[] bloomDay, int m, int k) {\rint left = Arrays.stream(bloomDay).min().getAsInt(), right = Arrays.stream(bloomDay).max().getAsInt(), max = right;\rif (bloomDay.length \u0026lt; m * k) {\rreturn -1;\r}\rwhile (left \u0026lt;= right) {\rint middle = (left + right) / 2;\rif (bloom(bloomDay, m, k, middle)) {\rright = middle - 1;\r} else {\rleft = middle + 1;\r}\r}\rif (left \u0026gt; max) {\rreturn -1;\r}\rreturn left;\r}\rpublic boolean bloom(int[] bloomDay, int m, int k, int day) {\rint sum = 0;\rfor (int i : bloomDay) {\rif (i \u0026gt; day) {\rsum = 0;\r} else {\rsum++;\rif (sum \u0026gt;= k) {\rsum = 0;\rm--;\r}\r}\r}\rreturn m \u0026lt;= 0;\r}\r 1552. 两球之间的磁力 在代号为 C-137 的地球上，Rick 发现如果他将两个球放在他新发明的篮子里，它们之间会形成特殊形式的磁力。Rick 有 n 个空的篮子，第 i 个篮子的位置在 position[i] ，Morty 想把 m 个球放到这些篮子里，使得任意两球间 最小磁力 最大。\n已知两个球如果分别位于 x 和 y ，那么它们之间的磁力为 |x - y| 。\n给你一个整数数组 position 和一个整数 m ，请你返回最大化的最小磁力。\n示例 1：\n输入：position = [1,2,3,4,7], m = 3\r输出：3\r解释：将 3 个球分别放入位于 1，4 和 7 的三个篮子，两球间的磁力分别为 [3, 3, 6]。最小磁力为 3 。我们没办法让最小磁力大于 3 。\r 示例 2：\n输入：position = [5,4,3,2,1,1000000000], m = 2\r输出：999999999\r解释：我们使用位于 1 和 1000000000 的篮子时最小磁力最大。\r 提示：\n n == position.length 2 \u0026lt;= n \u0026lt;= 10^5 1 \u0026lt;= position[i] \u0026lt;= 10^9 所有 position 中的整数 互不相同 。 2 \u0026lt;= m \u0026lt;= position.length  这题和上面的也都是一样的，最小磁力为x，取值范围为[1，max(position) -min(position)]，但这题是二分搜索最右边界\npublic int maxDistance(int[] position, int m) {\r// 先排序，好求最小距离\rArrays.sort(position);\rint length = position.length;\rint left = 0, right = position[length - 1] - position[0];\rwhile (left \u0026lt;= right) {\rint middle = (left + right) / 2;\r// 求最右边界\rif (distance(position, middle) \u0026gt;= m) {\rleft = middle + 1;\r} else {\rright = middle - 1;\r}\r}\rreturn right;\r}\r/**\r* 最小磁力 为 n时，最多可以分为几段\r*\r* @param position\r* @param n\r* @return\r*/\rpublic int distance(int[] position, int n) {\rint min = position[0];\rint difference = 0;\rint nums = 0;\rfor (int i : position) {\rdifference = i - min;\rif (difference \u0026gt;= n) {\rmin = i;\rnums++;\r}\r}\rreturn nums + 1;\r}\r 29. 两数相除 给定两个整数，被除数 dividend 和除数 divisor。将两数相除，要求不使用乘法、除法和 mod 运算符。\n返回被除数 dividend 除以除数 divisor 得到的商。\n整数除法的结果应当截去（truncate）其小数部分，例如：truncate(8.345) = 8 以及 truncate(-2.7335) = -2\n示例 1:\n输入: dividend = 10, divisor = 3\r输出: 3\r解释: 10/3 = truncate(3.33333..) = truncate(3) = 3\r 示例 2:\n输入: dividend = 7, divisor = -3\r输出: -2\r解释: 7/-3 = truncate(-2.33333..) = -2\r 提示：\n 被除数和除数均为 32 位有符号整数。 除数不为 0。 假设我们的环境只能存储 32 位有符号整数，其数值范围是 [−231, 231 − 1]。本题中，如果除法结果溢出，则返回 231 − 1。  这题其实很明显，不过需要特殊处理，dividend 和 divisor 均有「正数」和「负数」两种可能，当且仅当其中一者为负数时，结果为负，为了方便，我们可以先记录最终结果的正负号，然后将 dividend 和 divisor 都当成正数来处理；dividend 和 divisor 均为 int，可以确定答案的绝对值落在 [0, dividend] 范围内）；\npublic int divide(int dividend, int divisor) {\r// 考虑被除数为最小值的情况\rif (dividend == Integer.MIN_VALUE) {\rif (divisor == 1) {\rreturn Integer.MIN_VALUE;\r}\rif (divisor == -1) {\rreturn Integer.MAX_VALUE;\r}\r}\r// 考虑除数为最小值的情况\rif (divisor == Integer.MIN_VALUE) {\rreturn dividend == Integer.MIN_VALUE ? 1 : 0;\r}\r// 考虑被除数为 0 的情况\rif (dividend == 0) {\rreturn 0;\r}\r// 一般情况，使用二分查找\r// 将所有的正数取相反数，这样就只需要考虑一种情况\rboolean rev = false;\rif (dividend \u0026gt; 0) {\rdividend = -dividend;\rrev = !rev;\r}\rif (divisor \u0026gt; 0) {\rdivisor = -divisor;\rrev = !rev;\r}\rint left = 1, right = Integer.MAX_VALUE, ans = 0;\rwhile (left \u0026lt;= right) {\r// 注意溢出，并且不能使用除法\rint mid = left + ((right - left) \u0026gt;\u0026gt; 1);\rboolean check = quickAdd(divisor, mid, dividend);\rif (check) {\rans = mid;\r// 注意溢出\rif (mid == Integer.MAX_VALUE) {\rbreak;\r}\rleft = mid + 1;\r} else {\rright = mid - 1;\r}\r}\rreturn rev ? -ans : ans;\r}\r// 快速乘 这个没搞懂是什么意思\rpublic boolean quickAdd(int y, int z, int x) {\r// x 和 y 是负数，z 是正数\r// 需要判断 z * y \u0026gt;= x 是否成立\rint result = 0, add = y;\rwhile (z != 0) {\rif ((z \u0026amp; 1) != 0) {\r// 需要保证 result + add \u0026gt;= x\rif (result \u0026lt; x - add) {\rreturn false;\r}\rresult += add;\r}\rif (z != 1) {\r// 需要保证 add + add \u0026gt;= x\rif (add \u0026lt; x - add) {\rreturn false;\r}\radd += add;\r}\r// 不能使用除法\rz \u0026gt;\u0026gt;= 1;\r}\rreturn true;\r}\r 33. 搜索旋转排序数组 整数数组 nums 按升序排列，数组中的值 互不相同 。\n在传递给函数之前，nums 在预先未知的某个下标 k（0 \u0026lt;= k \u0026lt; nums.length）上进行了 旋转，使数组变为 [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,5,6,7] 在下标 3 处经旋转后可能变为 [4,5,6,7,0,1,2] 。\n给你 旋转后 的数组 nums 和一个整数 target ，如果 nums 中存在这个目标值 target ，则返回它的下标，否则返回 -1 。\n示例 1：\n输入：nums = [4,5,6,7,0,1,2], target = 0\r输出：4\r 示例 2：\n输入：nums = [4,5,6,7,0,1,2], target = 3\r输出：-1\r 示例 3：\n输入：nums = [1], target = 0\r输出：-1\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 5000 -10^4 \u0026lt;= nums[i] \u0026lt;= 10^4 nums 中的每个值都 独一无二 题目数据保证 nums 在预先未知的某个下标上进行了旋转 -10^4 \u0026lt;= target \u0026lt;= 10^4  **进阶：**你可以设计一个时间复杂度为 O(log n) 的解决方案吗？\n这题一看就像二分法，数组在旋转前是有序的，旋转后变成 了两部分有序数组，因此对二分法稍作变形即可，我们将数组从中间分开成左右两部分的时候，一定有一部分的数组是有序的。拿示例来看，我们从 6 这个位置分开以后数组变成了 [4, 5, 6] 和 [7, 0, 1, 2] 两个部分，其中左边 [4, 5, 6] 这个部分的数组是有序的，其他也是如此。 这启示我们可以在常规二分查找的时候查看当前 mid 为分割位置分割出来的两个部分 [l, mid] 和 [mid + 1, r] 哪个部分是有序的，并根据有序的那个部分确定我们该如何改变二分查找的上下界，因为我们能够根据有序的那部分判断出 target 在不在这个部分：\n 如果 [l, mid - 1] 是有序数组，且 target 的大小满足 [nums[l],nums[mid])，则我们应该将搜索范围缩小至 [l, mid - 1]，否则在 [mid + 1, r] 中寻找。 如果 [mid, r] 是有序数组，且 target 的大小满足 (nums[mid+1],nums[r]]，则我们应该将搜索范围缩小至 [mid + 1, r]，否则在 [l, mid - 1] 中寻找。  public int search2(int[] nums, int target) {\rint left = 0, right = nums.length - 1;\rwhile (left \u0026lt;= right) {\rint midlle = (left + right) / 2;\rif (nums[midlle] == target) {\rreturn midlle;\r}\r// 左半段为有序数组\rif (nums[0] \u0026lt;= nums[midlle]) {\rif (target \u0026gt;= nums[0] \u0026amp;\u0026amp; target \u0026lt; nums[midlle]) {\rright = midlle - 1;\r} else {\rleft = midlle + 1;\r}\r} else {\rif (target \u0026gt; nums[midlle] \u0026amp;\u0026amp; target \u0026lt;= nums[nums.length - 1]) {\rleft = midlle + 1;\r} else {\rright = midlle - 1;\r}\r}\r}\rreturn -1;\r}\r 81. 搜索旋转排序数组 II 已知存在一个按非降序排列的整数数组 nums ，数组中的值不必互不相同。\n在传递给函数之前，nums 在预先未知的某个下标 k（0 \u0026lt;= k \u0026lt; nums.length）上进行了 旋转 ，使数组变为 [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,4,4,5,6,6,7] 在下标 5 处经旋转后可能变为 [4,5,6,6,7,0,1,2,4,4] 。\n给你 旋转后 的数组 nums 和一个整数 target ，请你编写一个函数来判断给定的目标值是否存在于数组中。如果 nums 中存在这个目标值 target ，则返回 true ，否则返回 false 。\n你必须尽可能减少整个操作步骤。\n示例 1：\n输入：nums = [2,5,6,0,0,1,2], target = 0\r输出：true\r 示例 2：\n输入：nums = [2,5,6,0,0,1,2], target = 3\r输出：false\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 5000 -104 \u0026lt;= nums[i] \u0026lt;= 104 题目数据保证 nums 在预先未知的某个下标上进行了旋转 -104 \u0026lt;= target \u0026lt;= 104  这题和上一题唯一的区别就是，数组元素可以重复，对于数组中有重复元素的情况，二分查找时可能会有a[l]=a[mid]=a[r]，此时无法判断[l, mid] 和 [mid + 1, r]哪个有序\n例如 nums=[3,1,2,3,3,3,3]，target=2，首次二分时无法判断区间 [0,3] 和区间 [4,6] 哪个是有序的。\n对于这种情况，我们只能将当前二分区间的左边界加一，右边界减一，然后在新区间上继续二分查找。\npublic boolean search(int[] nums, int target) {\rint left = 0, right = nums.length - 1;\rwhile (left \u0026lt;= right) {\rint midlle = (left + right) / 2;\rif (nums[midlle] == target) {\rreturn true;\r}\r// 特殊处理，31313这种情形\rif(nums[left] == nums[midlle] \u0026amp;\u0026amp; nums[right] == nums[midlle]) {\rleft++;\rright--;\r}\r// 左半段为有序数组\relse if (nums[left] \u0026lt;= nums[midlle]) {\rif (target \u0026gt;= nums[left] \u0026amp;\u0026amp; target \u0026lt; nums[midlle]) {\rright = midlle - 1;\r} else {\rleft = midlle + 1;\r}\r} else {\rif (target \u0026gt; nums[midlle] \u0026amp;\u0026amp; target \u0026lt;= nums[right]) {\rleft = midlle + 1;\r} else {\rright = midlle - 1;\r}\r}\r}\rreturn false;\r}\r 面试题 10.03. 搜索旋转数组 搜索旋转数组。给定一个排序后的数组，包含n个整数，但这个数组已被旋转过很多次了，次数不详。请编写代码找出数组中的某个元素，假设数组元素原先是按升序排列的。若有多个相同元素，返回索引值最小的一个。\n示例1:\n 输入: arr = [15, 16, 19, 20, 25, 1, 3, 4, 5, 7, 10, 14], target = 5\r输出: 8（元素5在该数组中的索引）\r 示例2:\n 输入：arr = [15, 16, 19, 20, 25, 1, 3, 4, 5, 7, 10, 14], target = 11\r输出：-1 （没有找到）\r 提示:\n arr 长度范围在[1, 1000000]之间  这题和上一题，几乎是一模一样，但是要求返回最左边界\nint n = arr.length;\rint l = 0, r = n - 1;\rwhile (l \u0026lt;= r) {\rint mid = (l + r) / 2;\rif (arr[l] == arr[mid]) {\r// 如果左值不等于目标，说明还没找到，需要逐一清理重复值。\rif (arr[l] != target) { l++;\r} else { return l; }\r} else if (arr[l] \u0026lt; arr[mid]) {\rif (arr[l] \u0026lt;= target \u0026amp;\u0026amp; target \u0026lt;= arr[mid]) {\rr = mid - 1;\r} else {\rl = mid + 1;\r}\r} else {\r// 如果目标在左边，右边界移动到mid,这里判断跟上面不一样是因为，需要求最左边界，所以需要求的是不符合在右边区间的\rif (arr[l] \u0026lt;= target || target \u0026lt;= arr[mid]) { r = mid - 1;\r} else { // 否则目标在右半边，左边界移动到mid+1\rl = mid + 1;\r}\r}\r}\rif (l \u0026gt;= n || arr[l] != target) {\rreturn -1;\r}\rreturn l;\r 34. 在排序数组中查找元素的第一个和最后一个位置 给定一个按照升序排列的整数数组 nums，和一个目标值 target。找出给定目标值在数组中的开始位置和结束位置。\n如果数组中不存在目标值 target，返回 [-1, -1]。\n进阶：\n 你可以设计并实现时间复杂度为 O(log n) 的算法解决此问题吗？  示例 1：\n输入：nums = [5,7,7,8,8,10], target = 8\r输出：[3,4]\r 示例 2：\n输入：nums = [5,7,7,8,8,10], target = 6\r输出：[-1,-1]\r 示例 3：\n输入：nums = [], target = 0\r输出：[-1,-1]\r 提示：\n 0 \u0026lt;= nums.length \u0026lt;= 105 -109 \u0026lt;= nums[i] \u0026lt;= 109 nums 是一个非递减数组 -109 \u0026lt;= target \u0026lt;= 109  这题目看起来就很简单，就是搜索最左边界和最右边界\npublic int[] searchRange(int[] nums, int target) {\rint[] result = {-1, -1};\r// 先找最左边界\rresult[0] = left_bound(nums, target);\r// 最右边界\rresult[1] = right_bound(nums, target);\r// 万万没想到这题还真的就是两遍二分\rreturn result;\r}\r 总结 做了这么多二分法的题，总结出来的一点点小心得就是如果是求线性方程中的解(y=ax+b)，列如nums[1,n] 按顺序排列好，求target是否存在，判断题目是采用二分法后，需要注意的是两点\n 搜索区间 最小值是什么？最大值是什么？ 求最左边界？最右边界？最左边界和最右边界的判断条件需要记得！！！  滑动窗口  滑动窗口属于双指针的一种特例，这个算法技巧的思路非常简单，用i,j表示滑动窗口的左边界和右边界，通过改变i,j来扩展和收缩滑动窗口，可以想象成一个窗口在字符串上游走\n/**\r* 滑动窗口套路 有一个字符串s，需要找到t\r* 思路为：\r* 1、定义一个双指针left,right,作为窗口 window的左右边界\r* 2、右移right（right++），使得window包含t，即找到可行解\r* 3、停止增加right，转而不断增加left指针缩小窗口[left, right)，直到窗口中的字符串不再符合要求（不包含T中的所有字符了）。同时，每次增加left，我们都要更新一轮结果。\r* 4、重复第 2 和第 3 步，直到right到达字符串S的尽头。\r*\r* @param s\r* @param t\r*/\rvoid slidingWindow(String s, String t) {\rMap\u0026lt;Character, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rMap\u0026lt;Character, Integer\u0026gt; need = new HashMap\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; t.length(); i++) {\rneed.put(t.charAt(i), need.getOrDefault(t.charAt(i), 0) + 1);\r}\rint left = 0, right = 0;\rint valid = 0;\rwhile (right \u0026lt; s.length()) {\r// c 是将移入窗口的字符\rchar c = s.charAt(right);\r// 右移窗口\rright++;\r// 进行窗口内数据的一系列更新...，找出可行解\r/*** debug 输出的位置 ***/\rSystem.out.println(left);\rSystem.out.println(right);\r/********************/\r// 判断左侧窗口是否要收缩，找出局部最优解\rwhile (valid == need.size()) {\r// d 是将移出窗口的字符\rchar d = s.charAt(left);\r// 左移窗口\rleft++;\r// 进行窗口内数据的一系列更新...\r}\r}\r}\r 76. 最小覆盖子串 给你一个字符串 s 、一个字符串 t 。返回 s 中涵盖 t 所有字符的最小子串。如果 s 中不存在涵盖 t 所有字符的子串，则返回空字符串 \u0026quot;\u0026quot; 。\n注意：\n 对于 t 中重复字符，我们寻找的子字符串中该字符数量必须不少于 t 中该字符数量。 如果 s 中存在这样的子串，我们保证它是唯一的答案。  示例 1：\n输入：s = \u0026quot;ADOBECODEBANC\u0026quot;, t = \u0026quot;ABC\u0026quot;\r输出：\u0026quot;BANC\u0026quot;\r 示例 2：\n输入：s = \u0026quot;a\u0026quot;, t = \u0026quot;a\u0026quot;\r输出：\u0026quot;a\u0026quot;\r 示例 3:\n输入: s = \u0026quot;a\u0026quot;, t = \u0026quot;aa\u0026quot;\r输出: \u0026quot;\u0026quot;\r解释: t 中两个字符 'a' 均应包含在 s 的子串中，\r因此没有符合条件的子字符串，返回空字符串。\r 这题是一个很典型的滑动窗口,滑动窗口最难的在于怎么找到局部最优解，很显然这题的局部最优解就是，字符串长度最小\npublic String minWindow(String s, String t) {\rMap\u0026lt;Character, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rMap\u0026lt;Character, Integer\u0026gt; need = new HashMap\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; t.length(); i++) {\rneed.put(t.charAt(i), need.getOrDefault(t.charAt(i), 0) + 1);\r}\rint left = 0, right = 0, nums = 0;\rint start = 0, length = Integer.MAX_VALUE;\rwhile (right \u0026lt; s.length()) {\rchar c = s.charAt(right);\rright++;\r// 扩大右边界把window装满t，找到可行解\rif (need.containsKey(c)) {\rwindow.put(c, window.getOrDefault(c, 0) + 1);\rif (window.get(c).equals(need.get(c))) {\rnums++;\r}\r}\r// 收缩左边界，找到局部最优解\rwhile (nums == need.size()) {\rchar d = s.charAt(left);\rif (right - left \u0026lt; length) {\rstart = left;\rlength = right - left;\r}\rleft++;\rif (need.containsKey(d)) {\rif (need.get(d).equals(window.get(d))) {\rnums--;\r}\rwindow.put(d, window.get(d) - 1);\r}\r}\r}\rreturn length == Integer.MAX_VALUE ? \u0026quot;\u0026quot; : s.substring(start, start + length);\r}\r 567. 字符串的排列 给你两个字符串 s1 和 s2 ，写一个函数来判断 s2 是否包含 s1 的排列。如果是，返回 true ；否则，返回 false 。\n换句话说，s1 的排列之一是 s2 的 子串 。\n示例 1：\n输入：s1 = \u0026quot;ab\u0026quot; s2 = \u0026quot;eidbaooo\u0026quot;\r输出：true\r解释：s2 包含 s1 的排列之一 (\u0026quot;ba\u0026quot;).\r 示例 2：\n输入：s1= \u0026quot;ab\u0026quot; s2 = \u0026quot;eidboaoo\u0026quot;\r输出：false\r 这题和上一题几乎是一样的，唯一的不同就是这一题要求的是排列，那么就是在算局部最优解的时候就是判断窗口内的字符串长度是否和s1相等\npublic boolean checkInclusion(String s1, String s2) {\rMap\u0026lt;Character, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rMap\u0026lt;Character, Integer\u0026gt; need = new HashMap\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; s1.length(); i++) {\rneed.put(s1.charAt(i), need.getOrDefault(s1.charAt(i), 0) + 1);\r}\rint valid = 0;\rint left = 0, right = 0;\rwhile(right \u0026lt; s2.length()) {\rchar c = s2.charAt(right);\rright++;\rif(need.containsKey(c)) {\rwindow.put(c, window.getOrDefault(c, 0) + 1);\rif(need.get(c).equals(window.get(c))) {\rvalid++;\r}\r}\rwhile(valid == need.size()) {\r// 局部最优解为窗口内的字符串长度和s1相等\rif(right - left == s1.length()) {\rreturn true;\r}\rchar d = s2.charAt(left);\rleft++;\rif(need.containsKey(d)) {\rif(need.get(d).equals(window.get(d))) {\rvalid--;\r}\rwindow.put(d, window.get(d) - 1);\r}\r}\r}\rreturn false;\r}\r 438. 找到字符串中所有字母异位词 给定两个字符串 s 和 p，找到 s 中所有 p 的 异位词 的子串，返回这些子串的起始索引。不考虑答案输出的顺序。\n异位词 指由相同字母重排列形成的字符串（包括相同的字符串）。\n示例 1:\n输入: s = \u0026quot;cbaebabacd\u0026quot;, p = \u0026quot;abc\u0026quot;\r输出: [0,6]\r解释:\r起始索引等于 0 的子串是 \u0026quot;cba\u0026quot;, 它是 \u0026quot;abc\u0026quot; 的异位词。\r起始索引等于 6 的子串是 \u0026quot;bac\u0026quot;, 它是 \u0026quot;abc\u0026quot; 的异位词。\r 示例 2:\n输入: s = \u0026quot;abab\u0026quot;, p = \u0026quot;ab\u0026quot;\r输出: [0,1,2]\r解释:\r起始索引等于 0 的子串是 \u0026quot;ab\u0026quot;, 它是 \u0026quot;ab\u0026quot; 的异位词。\r起始索引等于 1 的子串是 \u0026quot;ba\u0026quot;, 它是 \u0026quot;ab\u0026quot; 的异位词。\r起始索引等于 2 的子串是 \u0026quot;ab\u0026quot;, 它是 \u0026quot;ab\u0026quot; 的异位词。\r 提示:\n 1 \u0026lt;= s.length, p.length \u0026lt;= 3 * 104 s 和 p 仅包含小写字母  这题和 567 其实就是一样的，567题是找到了就好了，这题是找出所有符合条件的\npublic List\u0026lt;Integer\u0026gt; findAnagrams(String s, String p) {\rMap\u0026lt;Character, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rMap\u0026lt;Character, Integer\u0026gt; need = new HashMap\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; p.length(); i++) {\rneed.put(p.charAt(i), need.getOrDefault(p.charAt(i), 0) + 1);\r}\rint valid = 0;\rint left = 0, right = 0;\rList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rwhile(right \u0026lt; s.length()) {\rchar c = s.charAt(right);\rright++;\rif(need.containsKey(c)) {\rwindow.put(c, window.getOrDefault(c, 0) + 1);\rif(need.get(c).equals(window.get(c))) {\rvalid++;\r}\r}\rwhile(right - left == p.length()) {\rif(valid == need.size()) {\rlist.add(left);\r}\rchar d = s.charAt(left);\rleft++;\rif(need.containsKey(d)) {\rif(need.get(d).equals(window.get(d))) {\rvalid--;\r}\rwindow.put(d, window.get(d) - 1);\r}\r}\r}\rreturn list;\r}\r 当然这类题也可以不用map来做，而是用数组来代替\npublic List\u0026lt;Integer\u0026gt; findAnagrams(String s, String p) {\r// 使用指针在LeetCode上快了好多好多\rint[] need = new int[26];\rint[] window = new int[26];\rfor (int i = 0; i \u0026lt; p.length(); i++) {\rneed[p.charAt(i) - 'a']++;\r}\rint needSize = 0;\rfor (int i : need) {\rif (i \u0026gt; 0) {\rneedSize++;\r}\r}\rint left = 0, right = 0, nums = 0;\rList\u0026lt;Integer\u0026gt; result = new ArrayList\u0026lt;\u0026gt;();\rwhile (right \u0026lt; s.length()) {\rchar c = s.charAt(right);\rright++;\rif (need[c - 'a'] \u0026gt; 0) {\rwindow[c - 'a']++;\rif (need[c - 'a'] == window[c - 'a']) {\rnums++;\r}\r}\rwhile (nums == needSize) {\rif (right - left == p.length()) {\rresult.add(left);\r}\rchar d = s.charAt(left);\rleft++;\rif (need[d - 'a'] \u0026gt; 0) {\rif (need[d - 'a'] == window[d - 'a']) {\rnums--;\r}\rwindow[d - 'a']--;\r}\r}\r}\rreturn result;\r}\r 643. 子数组最大平均数 I 给你一个由 n 个元素组成的整数数组 nums 和一个整数 k 。\n请你找出平均数最大且 长度为 k 的连续子数组，并输出该最大平均数。\n任何误差小于 10-5 的答案都将被视为正确答案。\n示例 1：\n输入：nums = [1,12,-5,-6,50,3], k = 4\r输出：12.75\r解释：最大平均数 (12-5-6+50)/4 = 51/4 = 12.75\r 示例 2：\n输入：nums = [5], k = 1\r输出：5.00000\r 这题就更加简单了，就相当于窗口内永远是k个长度和上面一题有点像\npublic double findMaxAverage1(int[] nums, int k) {\rdouble arg = 0;\rdouble temp;\rfor (int i = 0; i \u0026lt; k; i++) {\rarg += nums[i];\r}\rtemp = arg;\rfor (int i = k; i \u0026lt; nums.length; i++) {\rtemp = temp - nums[i - k] + nums[i];\rarg = Math.max(temp, arg);\r}\rreturn arg / k;\r}\r 1658. 将 x 减到 0 的最小操作数 给你一个整数数组 nums 和一个整数 x 。每一次操作时，你应当移除数组 nums 最左边或最右边的元素，然后从 x 中减去该元素的值。请注意，需要 修改 数组以供接下来的操作使用。\n如果可以将 x 恰好 减到 0 ，返回 最小操作数 ；否则，返回 -1 。\n示例 1：\n输入：nums = [1,1,4,2,3], x = 5\r输出：2\r解释：最佳解决方案是移除后两个元素，将 x 减到 0 。\r 示例 2：\n输入：nums = [5,6,7,8,9], x = 4\r输出：-1\r 示例 3：\n输入：nums = [3,2,20,1,1,3], x = 10\r输出：5\r解释：最佳解决方案是移除后三个元素和前两个元素（总共 5 次操作），将 x 减到 0 。\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 105 1 \u0026lt;= nums[i] \u0026lt;= 104 1 \u0026lt;= x \u0026lt;= 109  这题还是有点意思的，这题目的意思，可以反过来理解，因为删除元素都是在最左边或最右边的元素，因此题目可以转为找sum-x的最长子数组，这样就简单了\npublic int minOperations(int[] nums, int x) {\rint left = 0, right = 0, result = -1;\r// 因为从两边进行剔除，也就可以转为求最大的连续子数组使其和的target\rint target = Arrays.stream(nums).sum() - x;\rif (target \u0026lt; 0) {\rreturn -1;\r}\rint temp = 0;\rwhile (right \u0026lt; nums.length) {\rtemp += nums[right];\rright++;\rwhile (temp \u0026gt; target) {\rtemp -= nums[left];\rleft++;\r}\rif (temp == target) {\rresult = Math.max(result, right - left);\r}\r}\rreturn result == -1 ? -1 : nums.length - result;\r}\r 3. 无重复字符的最长子串 给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。\n示例 1:\n输入: s = \u0026quot;abcabcbb\u0026quot;\r输出: 3 解释: 因为无重复字符的最长子串是 \u0026quot;abc\u0026quot;，所以其长度为 3。\r 示例 2:\n输入: s = \u0026quot;bbbbb\u0026quot;\r输出: 1\r解释: 因为无重复字符的最长子串是 \u0026quot;b\u0026quot;，所以其长度为 1。\r 示例 3:\n输入: s = \u0026quot;pwwkew\u0026quot;\r输出: 3\r解释: 因为无重复字符的最长子串是 \u0026quot;wke\u0026quot;，所以其长度为 3。\r请注意，你的答案必须是 子串 的长度，\u0026quot;pwke\u0026quot; 是一个子序列，不是子串。\r 提示：\n 0 \u0026lt;= s.length \u0026lt;= 5 * 104 s 由英文字母、数字、符号和空格组成  这一题就更加简单了\npublic int lengthOfLongestSubstring(String s) {\rint left = 0, right = 0, result = 0;\rMap\u0026lt;Character, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rwhile (right \u0026lt; s.length()) {\rchar c = s.charAt(right);\rwhile (window.containsKey(c)) {\rchar d = s.charAt(left);\rleft++;\rwindow.remove(d);\r}\rright++;\rwindow.put(c, 1);\rresult = Math.max(result, right - left);\r}\rreturn result;\r}\r 1695 删除子数组的最大得分 给你一个正整数数组 nums ，请你从中删除一个含有 若干不同元素 的子数组**。**删除子数组的 得分 就是子数组各元素之 和 。\n返回 只删除一个 子数组可获得的 最大得分 。\n如果数组 b 是数组 a 的一个连续子序列，即如果它等于 a[l],a[l+1],...,a[r] ，那么它就是 a 的一个子数组。\n示例 1：\n输入：nums = [4,2,4,5,6]\r输出：17\r解释：最优子数组是 [2,4,5,6]\r 示例 2：\n输入：nums = [5,2,1,2,5,2,1,2,5]\r输出：8\r解释：最优子数组是 [5,2,1] 或 [1,2,5]\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 105 1 \u0026lt;= nums[i] \u0026lt;= 104  和上面一题不能说一模一样，只能说完全一致\n395. 至少有 K 个重复字符的最长子串 给你一个字符串 s 和一个整数 k ，请你找出 s 中的最长子串， 要求该子串中的每一字符出现次数都不少于 k 。返回这一子串的长度。\n示例 1：\n输入：s = \u0026quot;aaabb\u0026quot;, k = 3\r输出：3\r解释：最长子串为 \u0026quot;aaa\u0026quot; ，其中 'a' 重复了 3 次。\r 示例 2：\n输入：s = \u0026quot;ababbc\u0026quot;, k = 2\r输出：5\r解释：最长子串为 \u0026quot;ababb\u0026quot; ，其中 'a' 重复了 2 次， 'b' 重复了 3 次。\r 这题和上一题很像，但是这题麻烦的是最少的次数，想了很久没有想明白，看了答案也是一知半解的，\n思路： 我们枚举最长子串中的字符种类数目，它最小为 1，最大为 ∣Σ∣（字符集的大小，本题中为 26）。 对于给定的字符种类数量 t，我们维护滑动窗口的左右边界 l,r、滑动窗口内部每个字符出现的次数 cnt， 以及滑动窗口内的字符种类数目 total。\n当 total \u0026gt; t 时，我们不断地右移左边界 l，并对应地更新 cnt 以及 total，直到 total≤t 为止。 这样，对于任何一个右边界 r，我们都能找到最小的l（记为 lmin），使得 s[lmin\u0026hellip;r] 之间的字符种类数目不多于t对于任何一组s[lmin\u0026hellip;r] 之间存在某个出现次数小于 k （且不为 0，下文不再特殊说明）的字符，我们可以断定：对于任何 l′∈(lmin,r) 而言，s[l'\u0026hellip;r] 依然不可能是满足题意的子串，因为：\n 要么该字符的出现次数降为 0，此时子串内虽然少了一个出现次数小于 k 的字符，但字符种类数目也随之小于t 了； 要么该字符的出现次数降为非 0 整数，此时该字符的出现次数依然小于 k。  根据上面的结论，我们发现：当限定字符种类数目为 t 时，满足题意的最长子串，就一定出自某个s[lmin\u0026hellip;r]。因此，在滑动窗口的维护过程中，就可以直接得到最长子串的大小\npublic int longestSubstring(String s, int k) {\rint ret = 0;\rint n = s.length();\r// 遍历最长子串的字符种类数目\rfor (int t = 1; t \u0026lt;= 26; t++) {\rint l = 0, r = 0;\r// 每个字符出现的次数\rint[] cnt = new int[26];\r// 字符种类数目\rint tot = 0;\r// 当前出现次数小于 k 的字符的数量\rint less = 0;\rwhile (r \u0026lt; n) {\rcnt[s.charAt(r) - 'a']++;\r// 该字符第一次出现\rif (cnt[s.charAt(r) - 'a'] == 1) {\r// 字符种类数目加一\rtot++;\r// 小于 k 的字符的数量加一\rless++;\r}\r// 如果等于k了，less加一\rif (cnt[s.charAt(r) - 'a'] == k) {\rless--;\r}\r// 字符种类超过了t\rwhile (tot \u0026gt; t) {\rcnt[s.charAt(l) - 'a']--;\r// s.charAt(l)出现次数小于k了，less加1\rif (cnt[s.charAt(l) - 'a'] == k - 1) {\rless++;\r}\r// s.charAt(l)不在窗口内了，种类tot减一和less减一\rif (cnt[s.charAt(l) - 'a'] == 0) {\rtot--;\rless--;\r}\rl++;\r}\rif (less == 0) {\rret = Math.max(ret, r - l + 1);\r}\rr++;\r}\r}\rreturn ret;\r}\r 992. K 个不同整数的子数组 给定一个正整数数组 nums和一个整数 \u0008k ，返回 num 中 「好子数组」 的数目。\n如果 nums 的某个子数组中不同整数的个数恰好为 k，则称 nums 的这个连续、不一定不同的子数组为 「好子数组 」。\n 例如，[1,2,3,1,2] 中有 3 个不同的整数：1，2，以及 3。  子数组 是数组的 连续 部分。\n示例 1：\n输入：nums = [1,2,1,2,3], k = 2\r输出：7\r解释：恰好由 2 个不同整数组成的子数组：[1,2], [2,1], [1,2], [2,3], [1,2,1], [2,1,2], [1,2,1,2].\r 示例 2：\n输入：nums = [1,2,1,3,4], k = 3\r输出：3\r解释：恰好由 3 个不同整数组成的子数组：[1,2,1,3], [2,1,3], [1,3,4].\r 这个题目其实是很难想的到的，主要就是将恰好改为最多\n对于一个固定的左边界来说，满足「恰好存在 K 个不同整数的子区间」的右边界 不唯一，且形成区间。\n示例 1：左边界固定的时候，恰好存在 2 个不同整数的子区间为 [1,2],[1,2,1],[1,2,1,2] ，总数为 3。其值为下标 3 - 1 + 1，即区间 [1..3] 的长度。\n把「恰好」改成「最多」会发现 对于每一个确定的左边界，最多包含 K 种不同整数的右边界是唯一确定的，并且在左边界向右移动的过程中，右边界或者在原来的地方，或者在原来地方的右边。\n而「最多存在 个不同整数的子区间的个数」与「恰好存在 K 个不同整数的子区间的个数」的差恰好等于「最多存在 K - 1 个不同整数的子区间的个数」。即 result = Max（k）- max（k-1）理解这点后做题目也就简单了\npublic int subarraysWithKDistinct(int[] nums, int k) {\rreturn maxArraysDistance(nums, k) - maxArraysDistance(nums, k - 1);\r}\r/**\r* 最多有k个不同数字的区间有多少\r* @param nums\r* @param k\r* @return\r*/\rprivate int maxArraysDistance(int[] nums, int k) {\rint left = 0, right = 0, result = 0;\rint valid = 0;\rint[] temp = new int[nums.length + 1];\rwhile(right \u0026lt; nums.length) {\rtemp[nums[right]]++;\rif(temp[nums[right]] == 1) {\rvalid++;\r}\rright++;\rwhile(valid \u0026gt; k) {\rtemp[nums[left]]--;\rif(temp[nums[left]] == 0) {\rvalid--;\r}\rleft++;\r}\r// 为什么会是right - left呢？，看第一幅图就知道了\rresult += right - left;\r}\rreturn result;\r}\r 239. 滑动窗口最大值 给你一个整数数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。\n返回 滑动窗口中的最大值 。\n示例 1：\n输入：nums = [1,3,-1,-3,5,3,6,7], k = 3\r输出：[3,3,5,5,6,7]\r解释：\r滑动窗口的位置 最大值\r--------------- -----\r[1 3 -1] -3 5 3 6 7 3\r1 [3 -1 -3] 5 3 6 7 3\r1 3 [-1 -3 5] 3 6 7 5\r1 3 -1 [-3 5 3] 6 7 5\r1 3 -1 -3 [5 3 6] 7 6\r1 3 -1 -3 5 [3 6 7] 7\r 示例 2：\n输入：nums = [1], k = 1\r输出：[1]\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 105 -104 \u0026lt;= nums[i] \u0026lt;= 104 1 \u0026lt;= k \u0026lt;= nums.length  这一道题其实想起来很简单，第一版暴力，\nclass Solution {\rpublic int[] maxSlidingWindow(int[] nums, int k) {\rint left = 0, right = 0;\rint[] result = new int[nums.length - k + 1];\rint i = 0;\rwhile(right \u0026lt; nums.length) {\rright++;\rif(right - left == k) {\rresult[i++] = max(nums, left, right);\rleft++;\r}\r}\rreturn result;\r}\rprivate int max(int[] nums, int left, int right) {\rint result = nums[left];\rfor(int i = left; i \u0026lt; right; i++) {\rresult = Math.max(result, nums[i]);\r}\rreturn result;\r}\r}\r 很显然，时间花费在了找最大值上，怎么用0（1）的时间复杂度，找出最大值呢？可以用双端队列，维护一个递减的双端队列，为什么维护一个递减的双端队列可以呢？因为每个区间移除的元素一定不是这个区间的最大值，而下个区间的最大值，一定是在上一个区间移除队首元素后的最大值，因此可以成立\npublic int[] maxSlidingWindow(int[] nums, int k) {\r// 维护一个递减的双端队列\rDeque\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;();\rint left = 0, right = 0;\rint[] result = new int[nums.length - k + 1];\rwhile(right \u0026lt; nums.length) {\r// 当前元素大于队尾元素，移除队尾元素\rwhile (!queue.isEmpty() \u0026amp;\u0026amp; nums[right] \u0026gt;= nums[queue.peekLast()]) {\rqueue.removeLast();\r}\r// 添加元素到队尾\rqueue.addLast(right);\rright++;\r// 如果队首元素不在区间内，移除队首元素\rwhile(queue.peekFirst() \u0026lt; left) {\rqueue.removeFirst();\r}\rif(right - left == k) {\r// 在区间内时，队首元素为区间最大元素\rresult[left++] = nums[queue.peekFirst()];\r}\r}\rreturn result;\r}\r 1438. 绝对差不超过限制的最长连续子数组 给你一个整数数组 nums ，和一个表示限制的整数 limit，请你返回最长连续子数组的长度，该子数组中的任意两个元素之间的绝对差必须小于或者等于 limit 。\n如果不存在满足条件的子数组，则返回 0 。\n示例 1：\n输入：nums = [8,2,4,7], limit = 4\r输出：2 解释：所有子数组如下：\r[8] 最大绝对差 |8-8| = 0 \u0026lt;= 4.\r[8,2] 最大绝对差 |8-2| = 6 \u0026gt; 4. [8,2,4] 最大绝对差 |8-2| = 6 \u0026gt; 4.\r[8,2,4,7] 最大绝对差 |8-2| = 6 \u0026gt; 4.\r[2] 最大绝对差 |2-2| = 0 \u0026lt;= 4.\r[2,4] 最大绝对差 |2-4| = 2 \u0026lt;= 4.\r[2,4,7] 最大绝对差 |2-7| = 5 \u0026gt; 4.\r[4] 最大绝对差 |4-4| = 0 \u0026lt;= 4.\r[4,7] 最大绝对差 |4-7| = 3 \u0026lt;= 4.\r[7] 最大绝对差 |7-7| = 0 \u0026lt;= 4. 因此，满足题意的最长子数组的长度为 2 。\r 示例 2：\n输入：nums = [10,1,2,4,7,2], limit = 5\r输出：4 解释：满足题意的最长子数组是 [2,4,7,2]，其最大绝对差 |2-7| = 5 \u0026lt;= 5 。\r 示例 3：\n输入：nums = [4,2,2,2,4,4,2,2], limit = 0\r输出：3\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 10^5 1 \u0026lt;= nums[i] \u0026lt;= 10^9 0 \u0026lt;= limit \u0026lt;= 10^9  这道题和上面一道题，基本一致，不过这道题需要求最大值和最小值，因此维护两个队列,一个递减队列，一个递增队列\npublic int longestSubarray(int[] nums, int limit) {\rint left = 0, right = 0, result = 0;\r// 单调递增队列\rDeque\u0026lt;Integer\u0026gt; minDeque = new LinkedList\u0026lt;\u0026gt;();\r// 单调递减队列\rDeque\u0026lt;Integer\u0026gt; maxDeque = new LinkedList\u0026lt;\u0026gt;();\rwhile (right \u0026lt; nums.length) {\r// 当前元素小于队尾元素，移除队尾元素\rwhile (!minDeque.isEmpty() \u0026amp;\u0026amp; nums[minDeque.peekLast()] \u0026gt;= nums[right]) {\rminDeque.pollLast();\r}\r// 当前元素大于队尾元素，移除队尾元素\rwhile (!maxDeque.isEmpty() \u0026amp;\u0026amp; nums[maxDeque.peekLast()] \u0026lt;= nums[right]) {\rmaxDeque.pollLast();\r}\rminDeque.offerLast(right);\rmaxDeque.offerLast(right);\rwhile (minDeque.peekFirst() \u0026lt; left) {\rminDeque.pollFirst();\r}\rwhile (maxDeque.peekFirst() \u0026lt; left) {\rmaxDeque.pollFirst();\r}\rright++;\rwhile (nums[maxDeque.peekFirst()] - nums[minDeque.peekFirst()] \u0026gt; limit) {\rif (left == maxDeque.peekFirst()) {\rmaxDeque.pollFirst();\r}\rif (left == minDeque.peekFirst()) {\rminDeque.pollFirst();\r}\rleft++;\r}\rresult = Math.max(right - left, result);\r}\rreturn result;\r}\r 424. 替换后的最长重复字符 给你一个字符串 s 和一个整数 k 。你可以选择字符串中的任一字符，并将其更改为任何其他大写英文字符。该操作最多可执行 k 次。\n在执行上述操作后，返回包含相同字母的最长子字符串的长度。\n示例 1：\n输入：s = \u0026quot;ABAB\u0026quot;, k = 2\r输出：4\r解释：用两个'A'替换为两个'B',反之亦然。\r 示例 2：\n输入：s = \u0026quot;AABABBA\u0026quot;, k = 1\r输出：4\r解释：\r将中间的一个'A'替换为'B',字符串变为 \u0026quot;AABBBBA\u0026quot;。\r子串 \u0026quot;BBBB\u0026quot; 有最长重复字母, 答案为 4。\r 这题的思路很清晰，首先维护一个窗口，窗口中左边界右移的时机就是区间内用了k次机会后还不够，这时候需要右 移左边界，那么怎么算区间内用的机会呢，很显然，用区间长度（right - left）减去 出现次数最多的字符的次数(max)，就是需要变化的次数，那么问题来了，怎么维护这个出现次数最多的字符的次数呢？自己想了很久，比如用treeMap实现，但肯定题目不会这么做，看了答案恍然大悟，很简单，最后要求的就是 max + k，k不变，那么结果只跟max有关，因此不需要维护max，只需要记录max的最大值即可！！！。\npublic int characterReplacement(String s, int k) {\rMap\u0026lt;Character, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rint left = 0, right = 0;\r// max 代表区间内出现次数最多的字符\rint max = 0, result = 0;\rwhile (right \u0026lt; s.length()) {\rchar c = s.charAt(right);\rright++;\rwindow.put(c, window.getOrDefault(c, 0) + 1);\r// result = max + k， 因此只需要找到最大的max即可\rmax = Math.max(max, window.get(c));\rif (right - left \u0026gt; max + k) {\rchar d = s.charAt(left);\rleft++;\rwindow.put(d, window.get(d) - 1);\r}\r// 这里为什么要在后面判断呢？，因为循环进入if的条件是，right - left \u0026gt; max + k，而我们答案需要的是max+k，所有需要在left++后执行\rresult = Math.max(right - left, result);\r}\rreturn result;\r}\r 这个题目，需要注意的点有：\n 求max时，只需要求出最大的max而不需要求出每个区间内最大的max 在返回结果时，应该在if条件后执行，而不是if内，因为进入循环的条件就是right - left \u0026gt; max + k，意味着这个区间已经不符合要求了，所以需要left++  1004. 最大连续1的个数 III 给定一个二进制数组 nums 和一个整数 k ，如果可以翻转最多k 个 0 ，则返回 数组中连续 1 的最大个数 。\n示例 1：\n输入：nums = [1,1,1,0,0,0,1,1,1,1,0], K = 2\r输出：6\r解释：[1,1,1,0,0,1,1,1,1,1,1]\r粗体数字从 0 翻转到 1，最长的子数组长度为 6。\r 示例 2：\n输入：nums = [0,0,1,1,0,0,1,1,1,0,1,1,0,0,0,1,1,1,1], K = 3\r输出：10\r解释：[0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1]\r粗体数字从 0 翻转到 1，最长的子数组长度为 10。\r 这题和上面一模一样\npublic int longestOnes(int[] nums, int k) {\rint left = 0, right = 0, result = 0, temp = 0;\rwhile(right \u0026lt; nums.length) {\rif(nums[right] == 0) {\rtemp++;\r}\rright++;\rwhile(temp \u0026gt; k) {\rif(nums[left] == 0) {\rtemp--;\r}\rleft++;\r}\rresult = Math.max(result, right - left);\r}\rreturn result;\r}\r 1208. 尽可能使字符串相等 给你两个长度相同的字符串，s 和 t。\n将 s 中的第 i 个字符变到 t 中的第 i 个字符需要 |s[i] - t[i]| 的开销（开销可能为 0），也就是两个字符的 ASCII 码值的差的绝对值。\n用于变更字符串的最大预算是 maxCost。在转化字符串时，总开销应当小于等于该预算，这也意味着字符串的转化可能是不完全的。\n如果你可以将 s 的子字符串转化为它在 t 中对应的子字符串，则返回可以转化的最大长度。\n如果 s 中没有子字符串可以转化成 t 中对应的子字符串，则返回 0。\n示例 1：\n输入：s = \u0026quot;abcd\u0026quot;, t = \u0026quot;bcdf\u0026quot;, maxCost = 3\r输出：3\r解释：s 中的 \u0026quot;abc\u0026quot; 可以变为 \u0026quot;bcd\u0026quot;。开销为 3，所以最大长度为 3。\r 示例 2：\n输入：s = \u0026quot;abcd\u0026quot;, t = \u0026quot;cdef\u0026quot;, maxCost = 3\r输出：1\r解释：s 中的任一字符要想变成 t 中对应的字符，其开销都是 2。因此，最大长度为 1。\r 示例 3：\n输入：s = \u0026quot;abcd\u0026quot;, t = \u0026quot;acde\u0026quot;, maxCost = 0\r输出：1\r解释：a -\u0026gt; a, cost = 0，字符串未发生变化，所以最大长度为 1。\r 这题又是一模一样的\npublic int equalSubstring(String s, String t, int maxCost) {\rint left = 0, right = 0, result = 0, temp = 0;\rwhile (right \u0026lt; s.length()) {\rtemp += Math.abs(s.charAt(right) - t.charAt(right));\rright++;\rwhile (temp \u0026gt; maxCost) {\rtemp -= Math.abs(s.charAt(left) - t.charAt(left));\rleft++;\r}\rresult = Math.max(right - left, result);\r}\rreturn result;\r}\r 1493. 删掉一个元素以后全为 1 的最长子数组 给你一个二进制数组 nums ，你需要从中删掉一个元素。\n请你在删掉元素的结果数组中，返回最长的且只包含 1 的非空子数组的长度。\n如果不存在这样的子数组，请返回 0 。\n提示 1：\n输入：nums = [1,1,0,1]\r输出：3\r解释：删掉位置 2 的数后，[1,1,1] 包含 3 个 1 。\r 示例 2：\n输入：nums = [0,1,1,1,0,1,1,0,1]\r输出：5\r解释：删掉位置 4 的数字后，[0,1,1,1,1,1,0,1] 的最长全 1 子数组为 [1,1,1,1,1] 。\r 示例 3：\n输入：nums = [1,1,1]\r输出：2\r解释：你必须要删除一个元素。\r 示例 4：\n输入：nums = [1,1,0,0,1,1,1,0,1]\r输出：4\r 示例 5：\n输入：nums = [0,0,0]\r输出：0\r 这题也一点意思都没有，一模一样\npublic int longestSubarray(int[] nums) {\rint left = 0, right = 0, result = 0, temp = 0;\rwhile (right \u0026lt; nums.length) {\rif (nums[right] == 0) {\rtemp++;\r}\rright++;\rwhile (temp \u0026gt; 1) {\rif (nums[left] == 0) {\rtemp--;\r}\rleft++;\r}\rresult = Math.max(result, right - left - 1);\r}\rreturn result;\r}\r 904. 水果成篮 你正在探访一家农场，农场从左到右种植了一排果树。这些树用一个整数数组 fruits 表示，其中 fruits[i] 是第 i 棵树上的水果 种类 。\n你想要尽可能多地收集水果。然而，农场的主人设定了一些严格的规矩，你必须按照要求采摘水果：\n 你只有 两个 篮子，并且每个篮子只能装 单一类型 的水果。每个篮子能够装的水果总量没有限制。 你可以选择任意一棵树开始采摘，你必须从 每棵 树（包括开始采摘的树）上 恰好摘一个水果 。采摘的水果应当符合篮子中的水果类型。每采摘一次，你将会向右移动到下一棵树，并继续采摘。 一旦你走到某棵树前，但水果不符合篮子的水果类型，那么就必须停止采摘。  给你一个整数数组 fruits ，返回你可以收集的水果的 最大 数目。\n示例 1：\n输入：fruits = [1,2,1]\r输出：3\r解释：可以采摘全部 3 棵树。\r 示例 2：\n输入：fruits = [0,1,2,2]\r输出：3\r解释：可以采摘 [1,2,2] 这三棵树。\r如果从第一棵树开始采摘，则只能采摘 [0,1] 这两棵树。\r 示例 3：\n输入：fruits = [1,2,3,2,2]\r输出：4\r解释：可以采摘 [2,3,2,2] 这四棵树。\r如果从第一棵树开始采摘，则只能采摘 [1,2] 这两棵树。\r 示例 4：\n输入：fruits = [3,3,3,1,2,1,1,2,3,3,4]\r输出：5\r解释：可以采摘 [1,2,1,1,2] 这五棵树。\r 一模一样，标准滑动窗口\npublic int totalFruit(int[] fruits) {\rint left = 0, right = 0;\rint result = 0;\rMap\u0026lt;Integer, Integer\u0026gt; need = new HashMap\u0026lt;\u0026gt;();\rwhile (right \u0026lt; fruits.length) {\rneed.put(fruits[right], need.getOrDefault(fruits[right], 0) + 1);\rright++;\rwhile (need.size() \u0026gt; 2) {\rneed.put(fruits[left], need.get(fruits[left]) - 1);\rif (need.get(fruits[left]) == 0) {\rneed.remove(fruits[left]);\r}\rleft++;\r}\rresult = Math.max(right - left, result);\r}\rreturn result;\r}\r 1838. 最高频元素的频数 元素的 频数 是该元素在一个数组中出现的次数。\n给你一个整数数组 nums 和一个整数 k 。在一步操作中，你可以选择 nums 的一个下标，并将该下标对应元素的值增加 1 。\n执行最多 k 次操作后，返回数组中最高频元素的 最大可能频数 。\n示例 1：\n输入：nums = [1,2,4], k = 5\r输出：3\r解释：对第一个元素执行 3 次递增操作，对第二个元素执 2 次递增操作，此时 nums = [4,4,4] 。\r4 是数组中最高频元素，频数是 3 。\r 示例 2：\n输入：nums = [1,4,8,13], k = 5\r输出：2\r解释：存在多种最优解决方案：\r- 对第一个元素执行 3 次递增操作，此时 nums = [4,4,8,13] 。4 是数组中最高频元素，频数是 2 。\r- 对第二个元素执行 4 次递增操作，此时 nums = [1,8,8,13] 。8 是数组中最高频元素，频数是 2 。\r- 对第三个元素执行 5 次递增操作，此时 nums = [1,4,13,13] 。13 是数组中最高频元素，频数是 2 。\r 示例 3：\n输入：nums = [3,9,6], k = 2\r输出：1\r 这题也是很简单的一题，先排序，然后标准的滑动窗口\npublic int maxFrequency(int[] nums, int k) {\rArrays.sort(nums);\rint left = 0, right = 0, temp = 0, count = 0, result = 0;\rwhile(right \u0026lt; nums.length) {\rcount += (nums[right] - temp)*(right - left);\rtemp = nums[right];\rright++;\rwhile(count \u0026gt; k) {\rcount -= temp - nums[left];\rleft++;\r}\rresult = Math.max(result, right - left);\r}\rreturn result;\r}\r 209. 长度最小的子数组 给定一个含有 n 个正整数的数组和一个正整数 target 。\n找出该数组中满足其和 ≥ target 的长度最小的 连续子数组 [numsl, numsl+1, ..., numsr-1, numsr] ，并返回其长度**。**如果不存在符合条件的子数组，返回 0 。\n示例 1：\n输入：target = 7, nums = [2,3,1,2,4,3]\r输出：2\r解释：子数组 [4,3] 是该条件下的长度最小的子数组。\r 示例 2：\n输入：target = 4, nums = [1,4,4]\r输出：1\r 示例 3：\n输入：target = 11, nums = [1,1,1,1,1,1,1,1]\r输出：0\r 提示：\n 1 \u0026lt;= target \u0026lt;= 109 1 \u0026lt;= nums.length \u0026lt;= 105 1 \u0026lt;= nums[i] \u0026lt;= 105  这题很简单标准的滑动窗口\npublic int minSubArrayLen(int target, int[] nums) {\rint left = 0, right = 0, result = Integer.MAX_VALUE, temp = 0;\rwhile (right \u0026lt; nums.length) {\rtemp += nums[right];\rright++;\rwhile (temp \u0026gt;= target) {\rresult = Math.min(right - left, result);\rtemp -= nums[left];\rleft++;\r}\r}\rreturn result == Integer.MAX_VALUE ? 0 : result;\r}\r 862. 和至少为 K 的最短子数组 给你一个整数数组 nums 和一个整数 k ，找出 nums 中和至少为 k 的 最短非空子数组 ，并返回该子数组的长度。如果不存在这样的 子数组 ，返回 -1 。\n子数组 是数组中 连续 的一部分。\n示例 1：\n输入：nums = [1], k = 1\r输出：1\r 示例 2：\n输入：nums = [1,2], k = 4\r输出：-1\r 示例 3：\n输入：nums = [2,-1,2], k = 3\r输出：3\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 105 -105 \u0026lt;= nums[i] \u0026lt;= 105 1 \u0026lt;= k \u0026lt;= 109  这题和上一题唯一的不同就是，nums中可能有负数，所以就不能简单的使用滑动窗口了，而是转而使用前缀和做，即求sum(y) - sum(x) \u0026gt;= k,并保证，y - x最小，思路上有点像1658. 将 x 减到 0 的最小操作数\npublic int shortestSubarray(int[] nums, int k) {\rint n = nums.length, result = n + 1;\rint[] sum = new int[n + 1];\rfor (int i = 0; i \u0026lt; n; i++) {\rsum[i + 1] = sum[i] + nums[i];\r}\rfor (int i = 1; i \u0026lt;= n; i++) {\rfor (int j = 0; j \u0026lt;= i; j++) {\r// 求使sum[i] - sum[j] \u0026gt;= k 成立的最小的i - j\rif (sum[i] - sum[j] \u0026gt;= k) {\rresult = Math.min(i - j, result);\r}\r}\r}\rreturn result == n + 1 ? -1 : result;\r}}\r 这样子时间复杂度为O(n^2)需要优化，怎么优化呢？首先，对于固定的sum[i],有两种情况需要考虑\n 内循环中，j1 \u0026lt; j2,且sum[j1] \u0026gt;= sum[j2] \u0026amp;\u0026amp; sum[i] - sum[j1] \u0026gt;= k,则代表，j1 到 j2中有0或者负数，此时明显i - j2小于i - j1，因此可以看成，我们需要维护一个单调递增的队列 外循环中，当已经找到sum[i] - sum[j] \u0026gt;= k，则此时再递增j也没有用了，因为要维持，i - j最小，意思就为当队尾元素 - 队首元素 \u0026gt;= k 时，移除队首元素  public int shortestSubarray1(int[] nums, int k) {\rint n = nums.length, result = n + 1;\rlong[] sum = new long[n + 1];\rfor (int i = 0; i \u0026lt; n; i++) {\rsum[i + 1] = sum[i] + nums[i];\r}\rDeque\u0026lt;Integer\u0026gt; deque = new LinkedList\u0026lt;\u0026gt;();\rint right = 0;\rwhile (right \u0026lt; n + 1) {\r// 维持单调递增队列\rwhile (!deque.isEmpty() \u0026amp;\u0026amp; sum[right] \u0026lt;= sum[deque.peekLast()]) {\rdeque.pollLast();\r}\r// 当队首元素已满足条件，移除队首元素\rwhile (!deque.isEmpty() \u0026amp;\u0026amp; sum[right] - sum[deque.peekFirst()] \u0026gt;= k) {\rresult = Math.min(right - deque.peekFirst(), result);\rdeque.pollFirst();\r}\rdeque.offerLast(right);\rright++;\r}\rreturn result == n + 1 ? -1 : result;\r}\r 30. 串联所有单词的子串 给定一个字符串 s 和一些 长度相同 的单词 words **。**找出 s 中恰好可以由 words 中所有单词串联形成的子串的起始位置。\n注意子串要与 words 中的单词完全匹配，中间不能有其他字符 ，但不需要考虑 words 中单词串联的顺序。\n示例 1：\n输入：s = \u0026quot;barfoothefoobarman\u0026quot;, words = [\u0026quot;foo\u0026quot;,\u0026quot;bar\u0026quot;]\r输出：[0,9]\r解释：\r从索引 0 和 9 开始的子串分别是 \u0026quot;barfoo\u0026quot; 和 \u0026quot;foobar\u0026quot; 。\r输出的顺序不重要, [9,0] 也是有效答案。\r 示例 2：\n输入：s = \u0026quot;wordgoodgoodgoodbestword\u0026quot;, words = [\u0026quot;word\u0026quot;,\u0026quot;good\u0026quot;,\u0026quot;best\u0026quot;,\u0026quot;word\u0026quot;]\r输出：[]\r 示例 3：\n输入：s = \u0026quot;barfoofoobarthefoobarman\u0026quot;, words = [\u0026quot;bar\u0026quot;,\u0026quot;foo\u0026quot;,\u0026quot;the\u0026quot;]\r输出：[6,9,12]\r 这题和567题是一样，不过不同之处在于，words是个数组 ,但数组中每个单词的长度是相等的，我们维持一个s.length() 的窗口，每次都判断是否与 words 中的单词完全匹配。\npublic List\u0026lt;Integer\u0026gt; findSubstring(String s, String[] words) {\rint left = 0, right = 0;\rint n = words[0].length();\rint length = n * words.length;\rList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rHashMap\u0026lt;String, Integer\u0026gt; need = new HashMap\u0026lt;\u0026gt;();\rfor (String c : words) {\rneed.put(c, need.getOrDefault(c, 0) + 1);\r}\rint vaild = 0;\rwhile (right \u0026lt; s.length()) {\rright++;\r// 维持length的窗口\rif (right - left == length) {\rHashMap\u0026lt;String, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rfor (int i = left; i \u0026lt;= right - n; i += n) {\rString temp = s.substring(i, i + n);\rif (need.containsKey(temp)) {\rwindow.put(temp, window.getOrDefault(temp, 0) + 1);\rif (need.get(temp).equals(window.get(temp))) {\rvaild++;\r}\r}\r}\r// 如果完全匹配\rif (vaild == need.size()) {\rlist.add(left);\r}\rvaild = 0;\rleft++;\r}\r}\rreturn list;\r}\r 但这题显然有个更好的解决方法，我们可以每次移动words[0].length() = n个单位，但这种移动需要移动n次，具体过程如下：\npublic List\u0026lt;Integer\u0026gt; findSubstring1(String s, String[] words) {\rint left = 0, right = 0;\rint n = words[0].length();\rint length = n * words.length;\rList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rHashMap\u0026lt;String, Integer\u0026gt; need = new HashMap\u0026lt;\u0026gt;();\rfor (String c : words) {\rneed.put(c, need.getOrDefault(c, 0) + 1);\r}\rint vaild = 0;\r// 需要这样遍历不然会漏\rfor (int j = 0; j \u0026lt; n; j++) {\rleft = j;\rright = j;\rvaild = 0;\rwhile (right + n \u0026lt;= s.length()) {\r// 每次移动n\rright += n;\rif (right - left == length) {\rHashMap\u0026lt;String, Integer\u0026gt; window = new HashMap\u0026lt;\u0026gt;();\rfor (int i = left; i \u0026lt;= right - n; i += n) {\rString temp = s.substring(i, i + n);\rif (need.containsKey(temp)) {\rwindow.put(temp, window.getOrDefault(temp, 0) + 1);\rif (need.get(temp).equals(window.get(temp))) {\rvaild++;\r}\r}\r}\rif (vaild == need.size()) {\rlist.add(left);\r}\rvaild = 0;\rleft += n;\r}\r}\r}\rreturn list;\r}\r 187. 重复的DNA序列 难度中等334收藏分享切换为英文接收动态反馈\nDNA序列 由一系列核苷酸组成，缩写为 'A', 'C', 'G' 和 'T'.。\n 例如，\u0026quot;ACGAATTCCG\u0026quot; 是一个 DNA序列 。  在研究 DNA 时，识别 DNA 中的重复序列非常有用。\n给定一个表示 DNA序列 的字符串 s ，返回所有在 DNA 分子中出现不止一次的 长度为 10 的序列(子字符串)。你可以按 任意顺序 返回答案。\n示例 1：\n输入：s = \u0026quot;AAAAACCCCCAAAAACCCCCCAAAAAGGGTTT\u0026quot;\r输出：[\u0026quot;AAAAACCCCC\u0026quot;,\u0026quot;CCCCCAAAAA\u0026quot;]\r 示例 2：\n输入：s = \u0026quot;AAAAAAAAAAAAA\u0026quot;\r输出：[\u0026quot;AAAAAAAAAA\u0026quot;]\r 这题就很简单了\npublic List\u0026lt;String\u0026gt; findRepeatedDnaSequences(String s) {\rint left = 0, right = 0;\rSet\u0026lt;String\u0026gt; set = new HashSet\u0026lt;\u0026gt;();\rList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rwhile(right \u0026lt; s.length()) {\rright++;\rif(right - left == 10) {\rString temp = s.substring(left, right);\rif(set.contains(temp)) {\rif(!list.contains(temp)) {\rlist.add(temp);\r}\r} else {\rset.add(temp);\r}\rleft++;\r}\r}\rreturn list;\r}\r 双指针 这个概念其实比较宽泛，可以说二分法和滑动窗口都是双指针的应用\n870. 优势洗牌 给定两个大小相等的数组 A 和 B，A 相对于 B 的优势可以用满足 A[i] \u0026gt; B[i] 的索引 i 的数目来描述。\n返回 A 的任意排列，使其相对于 B 的优势最大化。\n示例 1：\n输入：A = [2,7,11,15], B = [1,10,4,11]\r输出：[2,11,7,15]\r 示例 2：\n输入：A = [12,24,8,32], B = [13,25,32,11]\r输出：[24,32,8,12]\r 提示：\n 1 \u0026lt;= A.length = B.length \u0026lt;= 10000 0 \u0026lt;= A[i] \u0026lt;= 10^9 0 \u0026lt;= B[i] \u0026lt;= 10^9  这个题目其实比较简单，田忌赛马类型，如果我方打不过，就选最弱的应战，如果打的过就选大的里面最小的\npublic int[] advantageCount(int[] nums1, int[] nums2) {\rArrays.sort(nums1);\rboolean[] isUsed = new boolean[nums1.length];\rint[] result = new int[nums1.length];\rint flag = 0;\rint index = 0, j;\rfor (int i : nums2) {\rfor (j = index; j \u0026lt; nums2.length; j++) {\r// 如果发现有比i大的则直接选取\rif (nums1[j] \u0026gt; i \u0026amp;\u0026amp; !isUsed[j]) {\rresult[flag++] = nums1[j];\risUsed[j] = true;\rbreak;\r}\r}\rif (j == nums2.length) {\r// 如果没有比i大的，则选取最小的\rfor (int k = index; k \u0026lt; nums2.length; k++) {\rif (!isUsed[k]) {\rresult[flag++] = nums1[k];\risUsed[k] = true;\rindex = k + 1;\rbreak;\r}\r}\r}\r}\rreturn result;\r}\r 但是这么写时间复杂太高了，复杂度，主要是因为，nums2的位置没有排序，那么想一下，如果nums2排序之后，该怎么比较呢，就很简单了，从后往前遍历，如果nums1[i] \u0026gt; nums2[i] 那么就选当前元素，如果小于，那么就选nums1中最小的，所以我们需要做的就是把nums2也排序，但是需要记录下标\npublic int[] advantageCount1(int[] nums1, int[] nums2) {\rArrays.sort(nums1);\rint length = nums1.length;\rint[][] nums = new int[length][2];\r// 二维数组，记录下nums的下标和值\rfor (int i = 0; i \u0026lt; length; i++) {\rnums[i][0] = i;\rnums[i][1] = nums2[i];\r}\r// 按nums的值进行排序\rArrays.sort(nums, Comparator.comparingInt(a -\u0026gt; a[1]));\rint[] result = new int[length];\rint left = 0, right = length - 1;\rfor (int i = length - 1; i \u0026gt;= 0; i--) {\r// 关键在于，对于nums2中的每一个元素，只有两种情况，一打的过，那么就选当前元素，打不过就选最小的\rint index = nums[i][0], value = nums[i][1];\rif (value \u0026lt; nums1[right]) {\r// 这里就是关键，打的过，那么index上，nums1就出当前元素\rresult[index] = nums1[right];\rright--;\r} else {\r// 打不过，index位置上就选最小的\rresult[index] = nums1[left];\rleft++;\r}\r}\rreturn result;\r}\r 1. 两数之和 给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。\n你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。\n你可以按任意顺序返回答案。\n示例 1：\n输入：nums = [2,7,11,15], target = 9\r输出：[0,1]\r解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。\r 示例 2：\n输入：nums = [3,2,4], target = 6\r输出：[1,2]\r 示例 3：\n输入：nums = [3,3], target = 6\r输出：[0,1]\r 提示：\n 2 \u0026lt;= nums.length \u0026lt;= 104 -109 \u0026lt;= nums[i] \u0026lt;= 109 -109 \u0026lt;= target \u0026lt;= 109 只会存在一个有效答案  这题经典第一题，使用hash表即可\npublic int[] twoSum(int[] nums, int target) {\rMap\u0026lt;Integer, Integer\u0026gt; hashtable = new HashMap\u0026lt;Integer, Integer\u0026gt;();\rfor (int i = 0; i \u0026lt; nums.length; i++) {\rif (hashtable.containsKey(target - nums[i])) {\rreturn new int[]{hashtable.get(target - nums[i]), i};\r}\rhashtable.put(nums[i], i);\r}\rreturn new int[0];\r}\r 15. 三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 *a，b，c ，*使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。\n**注意：**答案中不可以包含重复的三元组。\n示例 1：\n输入：nums = [-1,0,1,2,-1,-4]\r输出：[[-1,-1,2],[-1,0,1]]\r 示例 2：\n输入：nums = []\r输出：[]\r 示例 3：\n输入：nums = [0]\r输出：[]\r 提示：\n 0 \u0026lt;= nums.length \u0026lt;= 3000 -105 \u0026lt;= nums[i] \u0026lt;= 105  这题很简单，其实就是相当于是二数之和的升级版，需要注意的是，不能包含有重复的元素，因此需要去重，\n去重思路为，先排序，然后针对于，出现的重复元素，跳过就好，\n  思路：\n1、排序，保证不会有重复元素,\n「不重复」的本质是什么？我们保持三重循环的大框架不变，只需要保证：\n 第二重循环枚举到的元素不小于当前第一重循环枚举到的元素 第三重循环枚举到的元素不小于当前第二重循环枚举到的元素。  也就是说，我们枚举的三元组 (a,b,c) 满足 a≤b≤c，保证了只有 (a,b,c) 这个顺序会被枚举到，而 (b,a,c)、(c,b,a) 等等这些不会，这样就减少了重复。要实现这一点，我们可以将数组中的元素从小到大进行排序，随后使用普通的三重循环就可以满足上面的要求。\n2、确定好一个元素后，第二个元素，顺着遍历，第三个元素逆着遍历，使用双指针，即n^3变成了n^2\n  public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum(int[] nums) {\rArrays.sort(nums);\rList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; result = new ArrayList\u0026lt;\u0026gt;();\rint n = nums.length;\rfor(int i = 0; i \u0026lt; n; i++) {\rif(i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1]) {\rcontinue;\r}\rint left = i + 1, right = n - 1;\rwhile(left \u0026lt; right) {\rif(left \u0026gt; i + 1 \u0026amp;\u0026amp; nums[left] == nums[left - 1]) {\rleft++;\rcontinue;\r}\rint temp = nums[i] + nums[left] + nums[right];\rif(temp \u0026gt; 0) {\rright--;\r} else if(temp \u0026lt; 0) {\rleft++;\r} else {\rList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rlist.add(nums[i]);\rlist.add(nums[left]);\rlist.add(nums[right]);\rresult.add(list);\rright--;\rleft++;\r}\r}\r}\rreturn result;\r}\r 18. 四数之和 给你一个由 n 个整数组成的数组 nums ，和一个目标值 target 。请你找出并返回满足下述全部条件且不重复的四元组 [nums[a], nums[b], nums[c], nums[d]] （若两个四元组元素一一对应，则认为两个四元组重复）：\n 0 \u0026lt;= a, b, c, d \u0026lt; n a、b、c 和 d 互不相同 nums[a] + nums[b] + nums[c] + nums[d] == target  你可以按 任意顺序 返回答案 。\n示例 1：\n输入：nums = [1,0,-1,0,-2,2], target = 0\r输出：[[-2,-1,1,2],[-2,0,0,2],[-1,0,0,1]]\r 示例 2：\n输入：nums = [2,2,2,2,2], target = 8\r输出：[[2,2,2,2]]\r 提示：\n 1 \u0026lt;= nums.length \u0026lt;= 200 -109 \u0026lt;= nums[i] \u0026lt;= 109 -109 \u0026lt;= target \u0026lt;= 109  这题则是一模一样只是多了层循环而已\npublic List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; fourSum(int[] nums, int target) {\rArrays.sort(nums);\rint length = nums.length;\rList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; result = new ArrayList\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; length; i++) {\r// 去重，选一个就好\rif (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1]) {\rcontinue;\r}\rfor (int j = i + 1; j \u0026lt; length; j++) {\rif (j \u0026gt; i + 1 \u0026amp;\u0026amp; nums[j] == nums[j - 1]) {\rcontinue;\r}\rint left = j + 1, right = length - 1;\rwhile (left \u0026lt; right) {\r// 去重\rif (left \u0026gt; j + 1 \u0026amp;\u0026amp; nums[left] == nums[left - 1]) {\rleft++;\rcontinue;\r}\rint sum = nums[i] + nums[j] + nums[left] + nums[right];\rwhile (sum \u0026gt; target \u0026amp;\u0026amp; left \u0026lt; right) {\rsum -= nums[right];\rright--;\rsum += nums[right];\r}\rif (left == right) {\rbreak;\r}\rif (sum == target) {\rList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rlist.add(nums[i]);\rlist.add(nums[j]);\rlist.add(nums[left]);\rlist.add(nums[right]);\rresult.add(list);\r}\rleft++;\r}\r}\r}\rreturn result;\r}\r 454. 四数相加 II 给你四个整数数组 nums1、nums2、nums3 和 nums4 ，数组长度都是 n ，请你计算有多少个元组 (i, j, k, l) 能满足：\n 0 \u0026lt;= i, j, k, l \u0026lt; n nums1[i] + nums2[j] + nums3[k] + nums4[l] == 0  示例 1：\n输入：nums1 = [1,2], nums2 = [-2,-1], nums3 = [-1,2], nums4 = [0,2]\r输出：2\r解释：\r两个元组如下：\r1. (0, 0, 0, 1) -\u0026gt; nums1[0] + nums2[0] + nums3[0] + nums4[1] = 1 + (-2) + (-1) + 2 = 0\r2. (1, 1, 0, 0) -\u0026gt; nums1[1] + nums2[1] + nums3[0] + nums4[0] = 2 + (-1) + (-1) + 0 = 0\r 示例 2：\n输入：nums1 = [0], nums2 = [0], nums3 = [0], nums4 = [0]\r输出：1\r 提示：\n n == nums1.length n == nums2.length n == nums3.length n == nums4.length 1 \u0026lt;= n \u0026lt;= 200 -228 \u0026lt;= nums1[i], nums2[i], nums3[i], nums4[i] \u0026lt;= 228  这题一看题目就知道和两数之和差不多，但是自己没想到的，放入map时需要加上出现的次数以及可以分为两组进行遍历，自己的想法是，三重循环，加map减少一层循环，其实更加简单分两层遍历，直接减少两层循环\npublic int fourSumCount(int[] nums1, int[] nums2, int[] nums3, int[] nums4) {\rint n = nums1.length;\rMap\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; n; i++) {\rfor (int j = 0; j \u0026lt; n; j++) {\rint temp = nums1[i] + nums2[j];\rmap.put(temp, map.getOrDefault(temp, 0) + 1);\r}\r}\rint result = 0;\rfor (int i = 0; i \u0026lt; n; i++) {\rfor (int j = 0; j \u0026lt; n; j++) {\rint temp = -(nums3[i] + nums4[j]);\rif (map.containsKey(temp)) {\rresult += map.get(temp);\r}\r}\r}\rreturn result;\r}\r 好久没刷题了，立个flag每天搞懂两道题！！！\n2022-06-12\n42. 接雨水 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。\n示例 1：\n输入：height = [0,1,0,2,1,0,1,3,2,1,2,1]\r输出：6\r解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。  示例 2：\n输入：height = [4,2,0,3,2,5]\r输出：9\r 提示：\n n == height.length 1 \u0026lt;= n \u0026lt;= 2 * 104 0 \u0026lt;= height[i] \u0026lt;= 105  这个题目比较经典，解题思路为，位置i能接的水和其左边的最高柱子、右边的最高柱子有关，我们分别称这两个柱子高度为l_max和r_max；位置 i 能接的水就是min(l_max, r_max) - height[i]。\n那么，暴力解法为\npublic int trap(int[] height) {\rint length = height.length;\rint result = 0;\rfor (int i = 1; i \u0026lt; length - 1; i++) {\rint left = 0, right = 0;\r// 求左边最高柱子\rfor (int j = 0; j \u0026lt; i; j++) {\rleft = Math.max(left, height[j]);\r}\r// 求右边最高柱子\rfor (int j = i + 1; j \u0026lt; length; j++) {\rright = Math.max(right, height[j]);\r}\r// 求i位置中能接的水\rif (Math.min(left, right) - height[i] \u0026gt; 0) {\rresult += Math.min(left, right) - height[i];\r}\r}\rreturn result;\r}\r 这个解法是很直接粗暴的，时间复杂度 O(N^2)，空间复杂度 O(1)。可以使用备忘录优化一波\npublic int trap1(int[] height) {\rint length = height.length;\rint result = 0;\rint[] left = new int[length];\rint[] right = new int[length];\r// left[i] 表示位置i左边最高的柱子高度\rfor (int i = 1; i \u0026lt; length; i++) {\rleft[i] = Math.max(left[i - 1], height[i - 1]);\r}\r// right[i] 表示位置i右边最高的柱子高度\rfor (int i = length - 2; i \u0026gt; 0; i--) {\rright[i] = Math.max(right[i + 1], height[i + 1]);\r}\rfor (int i = 1; i \u0026lt; length - 1; i++) {\rif (Math.min(left[i], right[i]) - height[i] \u0026gt; 0) {\rresult += Math.min(left[i], right[i]) - height[i];\r}\r}\rreturn result;\r}\r 可以进一步优化，使用双指针来做\npublic int trap2(int[] height) {\rint left = 0, right = height.length - 1;\rint lMax = 0, rMax = 0;\rint result = 0;\rwhile (left \u0026lt; right) {\r// height[0..left]中最高柱子的高度\rlMax = Math.max(lMax, height[left]);\r// height[right..n-1]的最高柱子的高度\rrMax = Math.max(rMax, height[right]);\r// 两者之间最小的决定接水的多少\rif (lMax \u0026lt; rMax) {\rresult += Math.max(lMax - height[left], 0);\rleft++;\r} else {\rresult += Math.max(rMax - height[right], 0);\rright--;\r}\r}\rreturn result;\r}\r 两者对比\n之前的备忘录解法，l_max[i]和r_max[i]分别代表height[0..i]和height[i..n-1]的最高柱子高度。\nres += min(l_max[i], r_max[i]) - height[i];\r 但是双指针解法中，l_max和r_max代表的是height[0..left]和height[right..n-1]的最高柱子高度。比如这段代码：\nif (l_max \u0026lt; r_max) {\rres += l_max - height[left];\rleft++; }  此时的l_max是left指针左边的最高柱子，但是r_max并不一定是left指针右边最高的柱子，这真的可以得到正确答案吗？\n其实这个问题要这么思考，我们只在乎min(l_max, r_max)。对于上图的情况，我们已经知道l_max \u0026lt; r_max了，至于这个r_max是不是右边最大的，不重要。重要的是height[i]能够装的水只和较低的l_max之差有关：\n11. 盛最多水的容器 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。\n找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。\n返回容器可以储存的最大水量。\n**说明：**你不能倾斜容器。\n示例 1：\n输入：[1,8,6,2,5,4,8,3,7]\r输出：49 解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。\r 示例 2：\n输入：height = [1,1]\r输出：1\r 提示：\n n == height.length 2 \u0026lt;= n \u0026lt;= 105 0 \u0026lt;= height[i] \u0026lt;= 104  这个题目和上一道类似，能接的水由由两板中的短板决定，换成公式说明就是，Math.min(height[left], height[right]) * (right - left)\n 若向内 移动短板 ，水槽的短板 min(h[i], h[j])min(h[i],h[j]) 可能变大，因此下个水槽的面积 可能增大 。 若向内 移动长板 ，水槽的短板 min(h[i], h[j])min(h[i],h[j]) 不变或变小，因此下个水槽的面积 一定变小  public int maxArea(int[] height) {\rint left = 0, right = height.length - 1;\rint result = 0;\rwhile (left \u0026lt; right) {\r// 两板中的短板乘以距离\rint temp = Math.min(height[left], height[right]) * (right - left);\rresult = Math.max(result, temp);\r// 向内移动短板\rif (height[left] \u0026lt; height[right]) {\rleft++;\r} else {\rright--;\r}\r}\rreturn result;\r}\r 总结 双指针的题目感觉没什么好总结的，只可意会✈\n链表 在计算机科学中，链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针(Pointer)。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而顺序表相应的时间复杂度分别是O(logn)和O(1)。\n使用链表结构可以克服数组链表需要预先知道数据大小的缺点，链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理。但是链表失去了数组随机读取的优点，同时链表由于增加了结点的指针域，空间开销比较大。\n单向链表 链表中最简单的一种是单向链表，它包含两个域，一个信息域和一个指针域。这个链接指向列表中的下一个节点，而最后一个节点则指向一个空值。\n 一个单向链表包含两个值: 当前节点的值和一个指向下一个节点的链接\n一个单向链表的节点被分成两个部分。第一个部分保存或者显示关于节点的信息，第二个部分存储下一个节点的地址。单向链表只可向一个方向遍历。\n链表最基本的结构是在每个节点保存数据和到下一个节点的地址，在最后一个节点保存一个特殊的结束标记，另外在一个固定的位置保存指向第一个节点的指针，有的时候也会同时储存指向最后一个节点的指针。一般查找一个节点的时候需要从第一个节点开始每次访问下一个节点，一直访问到需要的位置。但是也可以提前把一个节点的位置另外保存起来，然后直接访问。当然如果只是访问数据就没必要了，不如在链表上储存指向实际数据的指针。这样一般是为了访问链表中的下一个或者前一个（需要储存反向的指针，见下面的双向链表）节点。\n相对于下面的双向链表，这种普通的，每个节点只有一个指针的链表也叫单向链表，或者单链表，通常用在每次都只会按顺序遍历这个链表的时候（例如图的邻接表，通常都是按固定顺序访问的）。\n链表也有很多种不同的变化：\n双向链表 一种更复杂的链表是“双向链表”或“双面链表”。每个节点有两个连接：一个指向前一个节点，（当此“连接”为第一个“连接”时，指向空值或者空列表）；而另一个指向下一个节点，（当此“连接”为最后一个“连接”时，指向空值或者空列表）\n 一个双向链表有三个整数值: 数值, 向后的节点链接, 向前的节点链接\n双向链表也叫双链表。双向链表中不仅有指向后一个节点的指针，还有指向前一个节点的指针。这样可以从任何一个节点访问前一个节点，当然也可以访问后一个节点，以至整个链表。一般是在需要大批量的另外储存数据在链表中的位置的时候用。双向链表也可以配合下面的其他链表的扩展使用。\n由于另外储存了指向链表内容的指针，并且可能会修改相邻的节点，有的时候第一个节点可能会被删除或者在之前添加一个新的节点。这时候就要修改指向首个节点的指针。有一种方便的可以消除这种特殊情况的方法是在最后一个节点之后、第一个节点之前储存一个永远不会被删除或者移动的虚拟节点，形成一个下面说的循环链表。这个虚拟节点之后的节点就是真正的第一个节点。这种情况通常可以用这个虚拟节点直接表示这个链表，对于把链表单独的存在数组里的情况，也可以直接用这个数组表示链表并用第0个或者第-1个（如果编译器支持）节点固定的表示这个虚拟节点。\n循环链表 在一个 循环链表中, 首节点和末节点被连接在一起。这种方式在单向和双向链表中皆可实现。要转换一个循环链表，你开始于任意一个节点然后沿着列表的任一方向直到返回开始的节点。再来看另一种方法，循环链表可以被视为“无头无尾”。这种列表很利于节约数据存储缓存， 假定你在一个列表中有一个对象并且希望所有其他对象迭代在一个非特殊的排列下。\n指向整个列表的指针可以被称作访问指针。\n 用单向链表构建的循环链表\n循环链表中第一个节点之前就是最后一个节点，反之亦然。循环链表的无边界使得在这样的链表上设计算法会比普通链表更加容易。对于新加入的节点应该是在第一个节点之前还是最后一个节点之后可以根据实际要求灵活处理，区别不大(详见下面实例代码)。当然，如果只会在最后插入数据（或者只会在之前），处理也是很容易的。\n另外有一种模拟的循环链表，就是在访问到最后一个节点之后的时候，手工的跳转到第一个节点。访问到第一个节点之前的时候也一样。这样也可以实现循环链表的功能，在直接用循环链表比较麻烦或者可能会出现问题的时候可以用。\n代码定义：\nclass ListNode {\rint val;\rListNode next;\rListNode() {}\rListNode(int val) { this.val = val; }\rListNode(int val, ListNode next) { this.val = val; this.next = next; }\r}\r 搞了这些基础后就进入正题了，开冲！！！\n21. 合并两个有序链表 将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。\n示例 1：\n输入：l1 = [1,2,4], l2 = [1,3,4]\r输出：[1,1,2,3,4,4]\r 示例 2：\n输入：l1 = [], l2 = []\r输出：[]\r 示例 3：\n输入：l1 = [], l2 = [0]\r输出：[0]\r 提示：\n 两个链表的节点数目范围是 [0, 50] -100 \u0026lt;= Node.val \u0026lt;= 100 l1 和 l2 均按 非递减顺序 排列  这个题就很简单，暴力就行，主要就是学习下链表的遍历方法\npublic ListNode mergeTwoLists(ListNode list1, ListNode list2) {\r// 虚拟头节点\rListNode result = new ListNode(-1);\rListNode temp = result;\rListNode l1 = list1, l2 = list2;\rwhile(l1 != null \u0026amp;\u0026amp; l2 != null) {\rif(l1.val \u0026lt; l2.val) {\rtemp.next = l1;\rl1 = l1.next;\r} else {\rtemp.next = l2;\rl2 = l2.next;\r}\rtemp = temp.next;\r}\rtemp.next = l1 == null ? l2 : l1;\rreturn result.next;\r}\r 23. 合并K个升序链表 给你一个链表数组，每个链表都已经按升序排列。\n请你将所有链表合并到一个升序链表中，返回合并后的链表。\n示例 1：\n输入：lists = [[1,4,5],[1,3,4],[2,6]]\r输出：[1,1,2,3,4,4,5,6]\r解释：链表数组如下：\r[\r1-\u0026gt;4-\u0026gt;5,\r1-\u0026gt;3-\u0026gt;4,\r2-\u0026gt;6\r]\r将它们合并到一个有序链表中得到。\r1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;4-\u0026gt;5-\u0026gt;6\r 示例 2：\n输入：lists = []\r输出：[]\r 示例 3：\n输入：lists = [[]]\r输出：[]\r 提示：\n k == lists.length 0 \u0026lt;= k \u0026lt;= 10^4 0 \u0026lt;= lists[i].length \u0026lt;= 500 -10^4 \u0026lt;= lists[i][j] \u0026lt;= 10^4 lists[i] 按 升序 排列 lists[i].length 的总和不超过 10^4  这个题目和上面唯一的不同就是由两个变为了k个,因此自然做法也差不多\n解法一：暴力法 直接遍历进行两两比较\npublic ListNode mergeKLists(ListNode[] lists) {\rListNode node = null;\rfor (int i = 0; i \u0026lt; lists.length; i++) {\rnode = mergeTwoLists(node, lists[i]);\r}\rreturn node;\r}\r 解法二：分治法 分治就是不断缩小其规模，再不断合并扩大的过程\n 一开始数组的规模是6，我们找到中间点，将起一分为二，然后再拆分，直到不能再拆分(规模为1时)时便返回。 之后开始合并，合并的代码借用了合并两个排序链表的代码。当两个规模最小的链表合并完后，其规模就变大了，然后不断重复这个合并过程，直到最终得到一个有序的链表。  /**\r* 使用分治法\r* 分治就是不断缩小其规模，再不断合并扩大的过程\r* @param lists\r* @return\r*/\rpublic ListNode mergeKLists1(ListNode[] lists) {\rreturn merge(lists, 0, lists.length - 1);\r}\r/**\r* @param lists\r* @param l\r* @param r\r* @return\r*/\rpublic ListNode merge(ListNode[] lists, int l, int r) {\rif (l == r) {\rreturn lists[l];\r}\rif (l \u0026gt; r) {\rreturn null;\r}\r// 通过mid将数组一分为二，并不断缩小规模，当规模为1时返回并开始合并\r// 通过合并两个链表，不断增大其规模，整体看就是不断缩小-最后不断扩大的过程\rint mid = (l + r) / 2;\rreturn mergeTwoLists(merge(lists, l, mid), merge(lists, mid + 1, r));\r}\r 解法三：最小堆 合并两个链表我们可以用if-else做判断，但是k个链表，用if-else，这就没法写了。 这时候我们需要一种辅助数据结构-堆，有了堆这个数据结构，难度等级是困难的题目，瞬间变成简单了。 我们把三个链表一股脑的全放到堆里面\n1-\u0026gt;4-\u0026gt;5 1-\u0026gt;3-\u0026gt;4 2-\u0026gt;6 然后由堆根据节点的val自动排好序\n这是一个小根堆，我们只需要每次输出堆顶的元素，直到整个堆为空即可。\npublic ListNode mergeKLists2(ListNode[] lists) {\rif(lists==null || lists.length==0) {\rreturn null;\r}\r//创建一个堆，并设置元素的排序方式\rPriorityQueue\u0026lt;ListNode\u0026gt; queue = new PriorityQueue(Comparator.comparingInt((ListNode o) -\u0026gt; o.val));\r//遍历链表数组，然后将每个链表的每个节点都放入堆中\rfor(int i=0;i\u0026lt;lists.length;i++) {\rwhile(lists[i] != null) {\rqueue.add(lists[i]);\rlists[i] = lists[i].next;\r}\r}\rListNode dummy = new ListNode(-1);\rListNode head = dummy;\r//从堆中不断取出元素，并将取出的元素串联起来\rwhile( !queue.isEmpty() ) {\rdummy.next = queue.poll();\rdummy = dummy.next;\r}\rdummy.next = null;\rreturn head.next;\r}\r 优化，不需要将每个链表的每个节点都放入堆中，只把k个链表的第一个节点放入到堆中，然后之后不断从堆中取出节点，如果这个节点还有下一个节点，就将下个节点也放入堆中\nListNode mergeKLists3(ListNode[] lists) {\rif (lists.length == 0) {\rreturn null;\r}\r// 虚拟头结点\rListNode dummy = new ListNode(-1);\rListNode p = dummy;\r// 优先级队列，最小堆\rPriorityQueue\u0026lt;ListNode\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;(lists.length, Comparator.comparingInt(a -\u0026gt; a.val));\r// 将 k 个链表的头结点加入最小堆\rfor (ListNode head : lists) {\rif (head != null) {\rpq.add(head);\r}\r}\rwhile (!pq.isEmpty()) {\r// 获取最小节点，接到结果链表中\rListNode node = pq.poll();\rp.next = node;\r// 链表不为空加入最小堆\rif (node.next != null) {\rpq.add(node.next);\r}\r// p 指针不断前进\rp = p.next;\r}\rreturn dummy.next;\r}\r 快慢指针 从前往后寻找单链表的第k个节点很简单，一个 for 循环遍历过去就找到了，但是如何寻找从后往前数的第k个节点呢？\n那你可能说，假设链表有n个节点，倒数第k个节点就是正数第n - k个节点，不也是一个 for 循环的事儿吗？\n是的，但是算法题一般只给你一个ListNode头结点代表一条单链表，你不能直接得出这条链表的长度n，而需要先遍历一遍链表算出n的值，然后再遍历链表计算第n - k个节点。\n也就是说，这个解法需要遍历两次链表才能得到出倒数第k个节点。\n那么，我们能不能只遍历一次链表，就算出倒数第k个节点？可以做到的，如果是面试问到这道题，面试官肯定也是希望你给出只需遍历一次链表的解法。\n这个解法就比较巧妙了，假设k = 2，思路如下：\n首先，我们先让一个指针p1指向链表的头节点head，然后走k步：\n现在的p1，只要再走n - k步，就能走到链表末尾的空指针了对吧？\n趁这个时候，再用一个指针p2指向链表头节点head：\n接下来就很显然了，让p1和p2同时向前走，p1走到链表末尾的空指针时走了n - k步，p2也走了n - k步，也就恰好到达了链表的倒数第k个节点：\n这样，只遍历了一次链表，就获得了倒数第k个节点p2。\n上述逻辑的代码如下：\n// 返回链表的倒数第 k 个节点\rListNode findFromEnd(ListNode head, int k) {\rListNode p1 = head;\r// p1 先走 k 步\rfor (int i = 0; i \u0026lt; k; i++) {\rp1 = p1.next;\r}\rListNode p2 = head;\r// p1 和 p2 同时走 n - k 步\rwhile (p1 != null) {\rp2 = p2.next;\rp1 = p1.next;\r}\r// p2 现在指向第 n - k 个节点\rreturn p2;\r}\r 当然，如果用 big O 表示法来计算时间复杂度，无论遍历一次链表和遍历两次链表的时间复杂度都是O(N)，但上述这个算法更有技巧性。下面几道快慢指针的题目做做\n19. 删除链表的倒数第 N 个结点 给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。\n示例 1：\n输入：head = [1,2,3,4,5], n = 2\r输出：[1,2,3,5]\r 示例 2：\n输入：head = [1], n = 1\r输出：[]\r 示例 3：\n输入：head = [1,2], n = 1\r输出：[1]\r 这个题很简单直接快慢指针就好了，但需要考虑使用虚拟头结点，也是为了防止出现空指针的情况，比如说链表总共有 5 个节点，题目就让你删除倒数第 5 个节点，也就是第一个节点，那按照算法逻辑，应该首先找到倒数第 6 个节点。但第一个节点前面已经没有节点了，这就会出错。\npublic static ListNode removeNthFromEnd(ListNode head, int n) {\rListNode fast = head;\r// 使用虚拟头结点，防止空指针\rListNode node = new ListNode(-1, head);\rListNode slow = node;\r// 快指针先走n步\rfor (int i = 0; i \u0026lt; n; i++) {\rfast = fast.next;\r}\r// 快指针走完时，慢指针刚好走到倒数第n个\rwhile (fast != null) {\rslow = slow.next;\rfast = fast.next;\r}\r// 删除第n个节点即可\rslow.next = slow.next.next;\rreturn node.next;\r}\r 876. 链表的中间结点 给定一个头结点为 head 的非空单链表，返回链表的中间结点。\n如果有两个中间结点，则返回第二个中间结点。\n示例 1：\n输入：[1,2,3,4,5]\r输出：此列表中的结点 3 (序列化形式：[3,4,5])\r返回的结点值为 3 。 (测评系统对该结点序列化表述是 [3,4,5])。\r注意，我们返回了一个 ListNode 类型的对象 ans，这样：\rans.val = 3, ans.next.val = 4, ans.next.next.val = 5, 以及 ans.next.next.next = NULL.\r 示例 2：\n输入：[1,2,3,4,5,6]\r输出：此列表中的结点 4 (序列化形式：[4,5,6])\r由于该列表有两个中间结点，值分别为 3 和 4，我们返回第二个结点。\r 这题一样的，每当慢指针slow前进一步，快指针fast就前进两步，这样，当fast走到链表末尾时，slow就指向了链表中点。\npublic ListNode middleNode1(ListNode head) {\rListNode fast = head, low = head;\rwhile (fast != null \u0026amp;\u0026amp; fast.next != null) {\rfast = fast.next.next;\rlow = low.next;\r}\rreturn low;\r}\r 141. 环形链表 给你一个链表的头节点 head ，判断链表中是否有环。\n如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。注意：pos 不作为参数进行传递 。仅仅是为了标识链表的实际情况。\n如果链表中存在环 ，则返回 true 。 否则，返回 false 。\n示例 1：\n输入：head = [3,2,0,-4], pos = 1\r输出：true\r解释：链表中有一个环，其尾部连接到第二个节点。\r 示例 2：\n输入：head = [1,2], pos = 0\r输出：true\r解释：链表中有一个环，其尾部连接到第一个节点。\r 示例 3：\n输入：head = [1], pos = -1\r输出：false\r解释：链表中没有环。\r 提示：\n 链表中节点的数目范围是 [0, 104] -105 \u0026lt;= Node.val \u0026lt;= 105 pos 为 -1 或者链表中的一个 有效索引 。  每当慢指针slow前进一步，快指针fast就前进两步。\n如果fast最终遇到空指针，说明链表中没有环；如果fast最终和slow相遇，那肯定是fast超过了slow一圈，说明链表中含有环。\nboolean hasCycle(ListNode head) {\r// 快慢指针初始化指向 head\rListNode slow = head, fast = head;\r// 快指针走到末尾时停止\rwhile (fast != null \u0026amp;\u0026amp; fast.next != null) {\r// 慢指针走一步，快指针走两步\rslow = slow.next;\rfast = fast.next.next;\r// 快慢指针相遇，说明含有环\rif (slow == fast) {\rreturn true;\r}\r}\r// 不包含环\rreturn false;\r}\r 142. 环形链表 II 给定一个链表的头节点 head ，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。\n如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。如果 pos 是 -1，则在该链表中没有环。注意：pos 不作为参数进行传递，仅仅是为了标识链表的实际情况。\n不允许修改 链表。\n示例 1：\n输入：head = [3,2,0,-4], pos = 1\r输出：返回索引为 1 的链表节点\r解释：链表中有一个环，其尾部连接到第二个节点。\r 示例 2：\n输入：head = [1,2], pos = 0\r输出：返回索引为 0 的链表节点\r解释：链表中有一个环，其尾部连接到第一个节点。\r 示例 3：\n输入：head = [1], pos = -1\r输出：返回 null\r解释：链表中没有环。\r 提示：\n 链表中节点的数目范围在范围 [0, 104] 内 -105 \u0026lt;= Node.val \u0026lt;= 105 pos 的值为 -1 或者链表中的一个有效索引  当快慢指针相遇时，让其中任一个指针指向头节点，然后让它俩以相同速度前进，再次相遇时所在的节点位置就是环开始的位置。\n我们假设快慢指针相遇时，慢指针slow走了k步，那么快指针fast一定走了2k步：\nfast一定比slow多走了k步，这多走的k步其实就是fast指针在环里转圈圈，所以k的值就是环长度的「整数倍」。\n假设相遇点距环的起点的距离为m，那么结合上图的 slow 指针，环的起点距头结点head的距离为k - m，也就是说如果从head前进k - m步就能到达环起点。\n巧的是，如果从相遇点继续前进k - m步，也恰好到达环起点。因为结合上图的 fast 指针，从相遇点开始走k步可以转回到相遇点，那走k - m步肯定就走到环起点了：\n所以，只要我们把快慢指针中的任一个重新指向head，然后两个指针同速前进，k - m步后一定会相遇，相遇之处就是环的起点了\nListNode detectCycle(ListNode head) {\rListNode fast, slow;\rfast = slow = head;\rwhile (fast != null \u0026amp;\u0026amp; fast.next != null) {\rfast = fast.next.next;\rslow = slow.next;\rif (fast == slow) break;\r}\r// 上面的代码类似 hasCycle 函数\rif (fast == null || fast.next == null) {\r// fast 遇到空指针说明没有环\rreturn null;\r}\r// 重新指向头结点\rslow = head;\r// 快慢指针同步前进，相交点就是环起点\rwhile (slow != fast) {\rfast = fast.next;\rslow = slow.next;\r}\rreturn slow;\r}\r 160. 相交链表 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表不存在相交节点，返回 null 。\n图示两个链表在节点 c1 开始相交**：**\n\n题目数据 保证 整个链式结构中不存在环。\n注意，函数返回结果后，链表必须 保持其原始结构 。\n自定义评测：\n评测系统 的输入如下（你设计的程序 不适用 此输入）：\n intersectVal - 相交的起始节点的值。如果不存在相交节点，这一值为 0 listA - 第一个链表 listB - 第二个链表 skipA - 在 listA 中（从头节点开始）跳到交叉节点的节点数 skipB - 在 listB 中（从头节点开始）跳到交叉节点的节点数  评测系统将根据这些输入创建链式数据结构，并将两个头节点 headA 和 headB 传递给你的程序。如果程序能够正确返回相交节点，那么你的解决方案将被 视作正确答案 。\n示例 1：\n\n输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,6,1,8,4,5], skipA = 2, skipB = 3\r输出：Intersected at '8'\r解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。\r从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,6,1,8,4,5]。\r在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。\r 示例 2：\n\n输入：intersectVal = 2, listA = [1,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1\r输出：Intersected at '2'\r解释：相交节点的值为 2 （注意，如果两个链表相交则不能为 0）。\r从各自的表头开始算起，链表 A 为 [1,9,1,2,4]，链表 B 为 [3,2,4]。\r在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。\r 示例 3：\n\n输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2\r输出：null\r解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。\r由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。\r这两个链表不相交，因此返回 null 。\r 这题思路就是一句话，让p1遍历完链表A之后开始遍历链表B，让p2遍历完链表B之后开始遍历链表A\n证明 情况一：两个链表相交\n链表 headA 和 headB 的长度分别是 m 和 n。假设链表 headA 的不相交部分有 a 个节点，链表 headB 的不相交部分有 b 个节点，两个链表相交的部分有 c 个节点，则有 a+c=m，b+c=n。\n 如果 a=b，则两个指针会同时到达两个链表相交的节点，此时返回相交的节点； 如果 a ≠ b，则指针 pA 会遍历完链表headA，指针 pB 会遍历完链表headB，两个指针不会同时到达链表的尾节点，然后指针 pA 移到链表headB 的头节点，指针pB 移到链表headA 的头节点，然后两个指针继续移动，在指针 pA 移动了 a+c+b 次、指针 pB 移动了 b+c+a 次之后，两个指针会同时到达两个链表相交的节点，该节点也是两个指针第一次同时指向的节点，此时返回相交的节点。  情况二：两个链表不相交\n 如果 m=n，则两个指针会同时到达两个链表的尾节点，然后同时变成空值 null，此时返回 null； 如果 m ≠ n 则由于两个链表没有公共节点，两个指针也不会同时到达两个链表的尾节点，因此两个指针都会遍历完两个链表，在指针pA 移动了 m+n次、指针 pB 移动了 n+m 次之后，两个指针会同时变成空值 null，此时返回 null。  就很奇妙哈哈\npublic ListNode getIntersectionNode(ListNode headA, ListNode headB) {\rListNode p1 = headA, p2 = headB;\rwhile(p1 != p2) {\r// p1遍历完链表A之后开始遍历链表B\rp1 = p1 != null ? p1.next : headB;\r// p2遍历完链表B之后开始遍历链表A\rp2 = p2 != null ? p2.next : headA;\r}\rreturn p1;\r}\r 206. 反转链表 给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。\n示例 1：\n输入：head = [1,2,3,4,5]\r输出：[5,4,3,2,1]\r 示例 2：\n输入：head = [1,2]\r输出：[2,1]\r 示例 3：\n输入：head = []\r输出：[]\r 提示：\n 链表中节点的数目范围是 [0, 5000] -5000 \u0026lt;= Node.val \u0026lt;= 5000  双指针-迭代 定义两个指针： p1 和 p2 ；p1 在前 p2 在后。\n 每次让 p1 的 next 指向 p2 ，实现一次局部反转 局部反转完成之后，p1 和 p2 同时往前移动一个位置  public ListNode reverseList(ListNode head) {\rListNode p1 = head, p2 = null;\rwhile(p1 != null) {\rListNode temp = p1.next;\r// p1 的 next 指向 p2 ，实现一次局部反转\rp1.next = p2;\r// p2往前移动一个位置\rp2 = p1;\r// p1往前移动一个位置\rp1 = temp;\r}\rreturn p2;\r}\r 递归 对于递归算法，最重要的就是明确递归函数的定义。具体来说，我们的reverse函数定义是这样的：\n输入一个节点head，将「以head为起点」的链表反转，并返回反转之后的头结点。\n明白了函数的定义，再来看这个问题。比如说我们想反转这个链表：\n那么输入reverse(head)后，会在这里进行递归：\nListNode last = reverse(head.next);\r 根据刚才的函数定义，来弄清楚这段代码会产生什么结果：\n按照定义，这个reverse(head.next)执行完成后，整个链表应该变成了这样：\n并且根据函数定义，reverse函数会返回反转之后的头结点，我们用变量last接收了。\n把节点2的next指向节点1，也就是翻转head和head.next, （head.next）.next = head\nhead.next.next = head;\r 把原来的头结点（现在的尾结点）的next指向null，\nhead.next = null;\r public ListNode reverseList1(ListNode head) {\rif(head == null || head.next == null) {\rreturn head;\r}\rListNode result = reverseList(head.next);\r// 翻转head和head.next, 可以看成（head.next）.next = head\rhead.next.next = head;\r// 尾结点指向null\rhead.next = null;\rreturn result;\r}\r 92. 反转链表 II 给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u0026lt;= right 。请你反转从位置 left 到位置 right 的链表节点，返回 反转后的链表 。\n示例 1：\n输入：head = [1,2,3,4,5], left = 2, right = 4\r输出：[1,4,3,2,5]\r 示例 2：\n输入：head = [5], left = 1, right = 1\r输出：[5]\r 这题和上面一题相比就是加大了难度，也可以用迭代和递归来解决\n迭代-头插法  我们定义两个指针，分别称之为 g(guard 守卫) 和 p(point)。 我们首先根据方法的参数 m 确定 g 和 p 的位置。将 g 移动到第一个要反转的节点的前面，将 p 移动到第一个要反转的节点的位置上。我们以 m=2，n=4为例。将 p 后面的元素删除，然后添加到 g 的后面。也即头插法。 根据 m 和 n 重复步骤（2） 返回 dummyHead.next  这种解法，看懂上面的图就看懂了\npublic ListNode reverseBetween(ListNode head, int left, int right) {\rListNode result = new ListNode(-1, head);\rListNode g = result, p = result.next;\r// 移动到开始位置\rfor (int i = 0; i \u0026lt; left - 1; i++) {\rg = g.next;\rp = p.next;\r}\r// 使用头插法\rfor (int i = left; i \u0026lt; right; i++) {\r// 删除p的后续节点\rListNode temp = p.next;\rp.next = p.next.next;\r// 将删除后的节点插入到g的后面\rtemp.next = g.next;\rg.next = temp;\r}\rreturn result.next;\r}\r 递归 递归的做法有点玄幻，目前还没有参透\n反转链表前 N 个节点\n实现一个这样的函数：\n// 将链表的前 n 个节点反转（n \u0026lt;= 链表长度）\rListNode reverseN(ListNode head, int n)\r 比如说对于下图链表，执行reverseN(head, 3)：\n解决思路和反转整个链表差不多，只要稍加修改即可：\nListNode successor = null; // 后驱节点\r// 反转以 head 为起点的 n 个节点，返回新的头结点\rListNode reverseN(ListNode head, int n) {\rif (n == 1) { // 记录第 n + 1 个节点\rsuccessor = head.next;\rreturn head;\r}\r// 以 head.next 为起点，需要反转前 n - 1 个节点\rListNode last = reverseN(head.next, n - 1);\rhead.next.next = head;\r// 让反转之后的 head 节点和后面的节点连起来\rhead.next = successor;\rreturn last;\r}  具体的区别：\n1、base case 变为n == 1，反转一个元素，就是它本身，同时要记录后驱节点。\n2、刚才我们直接把head.next设置为 null，因为整个链表反转后原来的head变成了整个链表的最后一个节点。但现在head节点在递归反转之后不一定是最后一个节点了，所以要记录后驱successor（第 n + 1 个节点），反转之后将head连接上。\n现在解决反转链表的一部分，给一个索引区间[m,n]（索引从 1 开始），仅仅反转区间中的链表元素。\nListNode reverseBetween(ListNode head, int m, int n)\r 首先，如果m == 1，就相当于反转链表开头的n个元素嘛，也就是我们刚才实现的功能：\nListNode reverseBetween(ListNode head, int m, int n) {\r// base case\rif (m == 1) {\r// 相当于反转前 n 个元素\rreturn reverseN(head, n);\r}\r// ...\r}\r 如果m != 1怎么办？如果我们把head的索引视为 1，那么我们是想从第m个元素开始反转对吧；如果把head.next的索引视为 1 呢？那么相对于head.next，反转的区间应该是从第m - 1个元素开始的；那么对于head.next.next呢……\n区别于迭代思想，这就是递归思想，所以我们可以完成代码：\nListNode reverseBetween(ListNode head, int m, int n) {\r// base case\rif (m == 1) {\rreturn reverseN(head, n);\r}\r// 前进到反转的起点触发 base case\rhead.next = reverseBetween(head.next, m - 1, n - 1);\rreturn head;\r}\r 24. 两两交换链表中的节点 给你一个链表，两两交换其中相邻的节点，并返回交换后链表的头节点。你必须在不修改节点内部的值的情况下完成本题（即，只能进行节点交换）。\n示例 1：\n输入：head = [1,2,3,4]\r输出：[2,1,4,3]\r 示例 2：\n输入：head = []\r输出：[]\r 示例 3：\n输入：head = [1]\r输出：[1]\r 迭代 迭代直接使用头插法，就可以\npublic ListNode swapPairs(ListNode head) {\rListNode result = new ListNode(-1, head);\rListNode p = result.next;\rListNode g = result;\rwhile(p != null \u0026amp;\u0026amp; p.next != null) {\r// 删除p的后续节点\rListNode temp = p.next;\rp.next = p.next.next;\r// 将删除后的节点插入到g的后面\rtemp.next = g.next;\rg.next = temp;\r// 向下移动\rp = p.next;\rg = g.next.next;\r}\rreturn result.next;\r}\r 递归 首先明确定义，swapPairs(head)，表示将节点进行两两交换，并返回新的头节点\npublic ListNode swapPairs1(ListNode head) {\rif (head == null || head.next == null) {\rreturn head;\r}\r// 节点的下一个节点\rListNode newHead = head.next;\r// 节点的下个节点指向newHead后的两两交换过后的节点，即删除newHead节点\rhead.next = swapPairs1(newHead.next);\r// 将newHead节点加到head的前面，完成反转\rnewHead.next = head;\rreturn newHead;\r}\r 25. K 个一组翻转链表 给你链表的头节点 head ，每 k 个节点一组进行翻转，请你返回修改后的链表。\nk 是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。\n你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。\n示例 1：\n输入：head = [1,2,3,4,5], k = 2\r输出：[2,1,4,3,5]\r 示例 2：\n输入：head = [1,2,3,4,5], k = 3\r输出：[3,2,1,4,5]\r 提示：\n 链表中的节点数目为 n 1 \u0026lt;= k \u0026lt;= n \u0026lt;= 5000 0 \u0026lt;= Node.val \u0026lt;= 1000  这题一看就跟上一题很像，所以思路自然也有迭代和递归\n迭代 思路其实很简单，就是使用头插法反转链表，然后将p和g移到下一个需要反转的位置\npublic ListNode reverseKGroup(ListNode head, int k) {\rListNode result = new ListNode(-1, head);\rListNode g = result, p = result.next, flag = head;\rwhile (flag != null) {\r// 不足k个直接返回\rfor (int i = 0; i \u0026lt; k; i++) {\rflag = flag.next;\rif (flag == null \u0026amp;\u0026amp; i != k - 1) {\rreturn result.next;\r}\r}\r// 使用头插法反转链表\rfor (int i = 0; i \u0026lt; k - 1; i++) {\r// 删除p的后续节点\rListNode temp = p.next;\rp.next = p.next.next;\r// 将删除后的节点插入到g的后面\rtemp.next = g.next;\rg.next = temp;\r}\r// p指针后移到下个反转点\rp = p.next;\r// g指针后移到下个反转点\rfor (int i = 0; i \u0026lt; k; i++) {\rg = g.next;\r}\r}\rreturn result.next;\r}\r 递归 递归，看起来没思路，但是先明确递归函数的意思为反转以 head 开头的 k 个元素，流程为\n反转以 head 开头的 k 个元素记为newHead(这一步可以复用reverseN函数)\n将第 k + 1 个元素作为 head 递归调用 reverseKGroup 函数，不够k个元素直接返回head。\n将上一轮翻转后的尾结点指向下一轮翻转后的头节点。这一步是最难得需要多看看\npublic ListNode reverseKGroup1(ListNode head, int k) {\rListNode p = head;\rfor (int i = 0; i \u0026lt; k; i++) {\r// 如果不够反转的，直接返回head\rif (p == null) {\rreturn head;\r}\rp = p.next;\r}\r// 反转前k个元素\rListNode newHead = reverseN(head, k);\r// 递归反转后续链表并连接起来\rhead.next = reverseKGroup1(p, k);\rreturn newHead;\r}\r 203. 移除链表元素 给你一个链表的头节点 head 和一个整数 val ，请你删除链表中所有满足 Node.val == val 的节点，并返回 新的头节点 。\n示例 1：\n输入：head = [1,2,6,3,4,5,6], val = 6\r输出：[1,2,3,4,5]\r 示例 2：\n输入：head = [], val = 1\r输出：[]\r 示例 3：\n输入：head = [7,7,7,7], val = 7\r输出：[]\r 提示：\n 列表中的节点数目在范围 [0, 104] 内 1 \u0026lt;= Node.val \u0026lt;= 50 0 \u0026lt;= val \u0026lt;= 50  这题太简单了，不过要注意递归的做法\n递归 public ListNode removeElements1(ListNode head, int val) {\rif (head == null) {\rreturn null;\r}\rhead.next = removeElements1(head, val);\rreturn head.val == val ? head.next : head;\r}\r 迭代 public ListNode removeElements(ListNode head, int val) {\rListNode node = new ListNode(-1, head);\rListNode temp = node;\rwhile (temp.next != null) {\rif (temp.next.val == val) {\rtemp.next = temp.next.next;\r} else {\rtemp = temp.next;\r}\r}\rreturn node.next;\r}\r 83. 删除排序链表中的重复元素 给定一个已排序的链表的头 head ， 删除所有重复的元素，使每个元素只出现一次 。返回 已排序的链表 。\n示例 1：\n输入：head = [1,1,2]\r输出：[1,2]\r 示例 2：\n输入：head = [1,1,2,3,3]\r输出：[1,2,3]\r 这题很简单，直接遍历就行\npublic ListNode deleteDuplicates(ListNode head) {\rif(head == null) {\rreturn head;\r}\rListNode node = head;\rwhile(node.next != null) {\rif(node.next.val == node.val) {\rnode.next = node.next.next;\r} else {\rnode = node.next;\r}\r}\rreturn head;\r}\r 82. 删除排序链表中的重复元素 II 给定一个已排序的链表的头 head ， 删除原始链表中所有重复数字的节点，只留下不同的数字 。返回 已排序的链表 。\n示例 1：\n输入：head = [1,2,3,3,4,4,5]\r输出：[1,2,5]\r 示例 2：\n输入：head = [1,1,1,2,3]\r输出：[2,3]\r 提示：\n 链表中节点数目在范围 [0, 300] 内 -100 \u0026lt;= Node.val \u0026lt;= 100 题目数据保证链表已经按升序 排列  这题和上一题的不同在于，这题需要把所有重复的节点删掉，所以需要定义一个pre前节点，当把重复出现的元素删除完，或者遍历完的时候，把pre指向cur节点\npublic ListNode deleteDuplicates(ListNode head) {\rListNode node = new ListNode(-1, head);\rListNode pre = node;\rListNode cur = head;\rwhile(cur != null) {\r// 当cur为最后一个元素或者cur不为重复元素时，pre指向cur并且pre向后移动\rif(cur.next == null || cur.val != cur.next.val) {\rpre.next = cur;\rpre = pre.next;\r}\r// 有重复的出现跳过重复的\rwhile(cur.next != null \u0026amp;\u0026amp; cur.val == cur.next.val) {\rcur = cur.next;\r}\r// 将重复的节点本身跳过\rcur = cur.next;\r}\rreturn node.next;\r}\r 143. 重排链表 给定一个单链表 L 的头节点 head ，单链表 L 表示为：\nL0 → L1 → … → Ln - 1 → Ln\r 请将其重新排列后变为：\nL0 → Ln → L1 → Ln - 1 → L2 → Ln - 2 → …\r 不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。\n示例 1：\n输入：head = [1,2,3,4]\r输出：[1,4,2,3]\r 示例 2：\n输入：head = [1,2,3,4,5]\r输出：[1,5,2,4,3]\r 这题看了好久没思路，看答案就觉得好巧妙。。。\n思路为：寻找链表中点 + 链表逆序 + 合并链表，都是之前做过的题目，往前看就可以\n例如：\n1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 5 -\u0026gt; 6\r第一步，将链表平均分成两半\r1 -\u0026gt; 2 -\u0026gt; 3\r4 -\u0026gt; 5 -\u0026gt; 6\r第二步，将第二个链表逆序\r1 -\u0026gt; 2 -\u0026gt; 3\r6 -\u0026gt; 5 -\u0026gt; 4\r第三步，依次连接两个链表\r1 -\u0026gt; 6 -\u0026gt; 2 -\u0026gt; 5 -\u0026gt; 3 -\u0026gt; 4\r public void reorderList(ListNode head) {\r// 找到链表中点\rListNode middle = middleNode(head);\rListNode left = head;\rListNode right = middle.next;\rmiddle.next = null;\r// 反转右边节点\rright = reverseList(right);\r// 合并两个链表\rwhile (left != middle \u0026amp;\u0026amp; right != null) {\rListNode leftTemp = left.next;\rListNode rightTemp = right.next;\rleft.next = right;\rleft = leftTemp;\rright.next = left;\rright = rightTemp;\r}\r}\r 2. 两数相加 给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。\n请你将两个数相加，并以相同形式返回一个表示和的链表。\n你可以假设除了数字 0 之外，这两个数都不会以 0 开头。\n示例 1：\n输入：l1 = [2,4,3], l2 = [5,6,4]\r输出：[7,0,8]\r解释：342 + 465 = 807.\r 示例 2：\n输入：l1 = [0], l2 = [0]\r输出：[0]\r 示例 3：\n输入：l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9]\r输出：[8,9,9,9,0,0,0,1]\r 提示：\n 每个链表中的节点数在范围 [1, 100] 内 0 \u0026lt;= Node.val \u0026lt;= 9 题目数据保证列表表示的数字不含前导零  这个题目看起来难，但是实际上很简单。。。。，因为是逆序，所以直接遍历，然后保存进位就可。。\npublic ListNode addTwoNumbers1(ListNode l1, ListNode l2) {\rListNode result = new ListNode(-1);\rListNode tempNode = result;\r// 进位\rint a = 0;\rwhile (l1 != null || l2 != null) {\rint b = l1 == null ? 0 : l1.val;\rint c = l2 == null ? 0 : l1.val;\rint temp = a + b + c;\ra = temp / 10;\rListNode node = new ListNode(temp % 10);\rtempNode.next = node;\rtempNode = tempNode.next;\rif (l1 != null) {\rl1 = l1.next;\r}\rif (l2 != null) {\rl2 = l2.next;\r}\r}\rreturn result.next;\r}\r 445. 两数相加 II 给你两个 非空 链表来代表两个非负整数。数字最高位位于链表开始位置。它们的每个节点只存储一位数字。将这两数相加会返回一个新的链表。\n你可以假设除了数字 0 之外，这两个数字都不会以零开头。\n示例1：\n输入：l1 = [7,2,4,3], l2 = [5,6,4]\r输出：[7,8,0,7]\r 示例2：\n输入：l1 = [2,4,3], l2 = [5,6,4]\r输出：[8,0,7]\r 示例3：\n输入：l1 = [0], l2 = [0]\r输出：[0]\r 这个题目就是自己以为的第一道题的做法，先反转两个链表，然后相加，最后反转结果链表\npublic ListNode addTwoNumbers(ListNode l1, ListNode l2) {\rl1 = reverseList(l1);\rl2 = reverseList(l2);\rListNode result = new ListNode(-1);\rListNode node = result;\rint a = 0;\rwhile(l1 != null || l2 != null) {\rint b = l1 == null ? 0 : l1.val;\rint c = l2 == null ? 0 : l2.val;\rint temp = a + b + c;\ra = temp / 10;\rnode.next = new ListNode(temp % 10);\rnode = node.next;\rif(l1 != null) {\rl1 = l1.next;\r}\rif(l2 != null) {\rl2 = l2.next;\r}\r}\rif(a != 0) {\rnode.next = new ListNode(a);\r}\rreturn reverseList(result.next);\r}\r 题目说不能反转链表，那么就只能用栈来做了\npublic ListNode addTwoNumbers(ListNode l1, ListNode l2) {\rDeque\u0026lt;Integer\u0026gt; stack1 = new LinkedList\u0026lt;\u0026gt;();\rDeque\u0026lt;Integer\u0026gt; stack2 = new LinkedList\u0026lt;\u0026gt;();\rwhile(l1 != null) {\rstack1.push(l1.val);\rl1 = l1.next;\r}\rwhile(l2 != null) {\rstack2.push(l2.val);\rl2 = l2.next;\r}\rListNode result = null;\rint a = 0;\rwhile(!stack1.isEmpty() || !stack2.isEmpty() || a != 0) {\rint b = stack1.isEmpty() ? 0 : stack1.pop();\rint c = stack2.isEmpty() ? 0 : stack2.pop();\rint temp = a + b + c;\ra = temp / 10;\rListNode node = new ListNode(temp % 10);\r// 标准的反转链表，局部反转，看不懂就反思下\rnode.next = result;\rresult = node;\r}\rreturn result;\r}\r 234. 回文链表 给你一个单链表的头节点 head ，请你判断该链表是否为回文链表。如果是，返回 true ；否则，返回 false 。\n示例 1：\n输入：head = [1,2,2,1]\r输出：true\r 示例 2：\n输入：head = [1,2]\r输出：false\r 提示：\n 链表中节点数目在范围[1, 105] 内 0 \u0026lt;= Node.val \u0026lt;= 9  这题思路为：\n 找到链表中点 反转后半部分 判断是否为回文（从两端往中间靠拢）  public boolean isPalindrome(ListNode head) {\r// 找到中点\rListNode left = middleNode(head);\r// 反转后半部分\rListNode reverseList = reverseList(left);\r// 判断回文\rwhile(reverseList != null) {\rif(head.val != reverseList.val) {\rreturn false;\r}\rhead = head.next;\rreverseList = reverseList.next;\r}\rreturn true;\r}\rpublic ListNode middleNode(ListNode head) {\rListNode fast = head, low = head;\rwhile(fast != null \u0026amp;\u0026amp; fast.next != null) {\rfast = fast.next.next;\rlow = low.next;\r}\r// 特殊处理，当fast不为null，需要往后移low\rif(fast != null) {\rlow = low.next;\r}\rreturn low;\r}\r 不过这里需要注意，寻找中点有不同，需要统一处理为返回后一个节点，即当为奇数时，low再往后移动一位\n哈希表 散列表（Hash table，也叫哈希表），是根据键（Key）而直接访问在内存储存位置的数据结构。也就是说，它通过计算出一个键值的函数，将所需查询的数据映射到表中一个位置来让人访问，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。\n需要掌握的有两个知识点，处理哈希冲突的两种方法，开放定址法和链地址法\n 开放定址法：就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 链地址法：将散列到同一个存储位置的所有元素保存在一个链表中  242. 有效的字母异位词 给定两个字符串 *s* 和 *t* ，编写一个函数来判断 *t* 是否是 *s* 的字母异位词。\n**注意：**若 *s* 和 *t* 中每个字符出现的次数都相同，则称 *s* 和 *t* 互为字母异位词。\n示例 1:\n输入: s = \u0026quot;anagram\u0026quot;, t = \u0026quot;nagaram\u0026quot;\r输出: true\r 示例 2:\n输入: s = \u0026quot;rat\u0026quot;, t = \u0026quot;car\u0026quot;\r输出: false\r 这题太简单了。。。\npublic boolean isAnagram(String s, String t) {\rMap\u0026lt;Character, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;();\rfor(int i = 0; i \u0026lt; s.length(); i++) {\rmap.put(s.charAt(i), map.getOrDefault(s.charAt(i), 0) + 1);\r}\rfor(int i = 0; i \u0026lt; t.length(); i++) {\rmap.put(t.charAt(i), map.getOrDefault(t.charAt(i), 0) - 1);\rif(map.get(t.charAt(i)) == 0) {\rmap.remove(t.charAt(i));\r}\r}\rreturn map.isEmpty();\r}\r 349. 两个数组的交集 给定两个数组 nums1 和 nums2 ，返回 它们的交集 。输出结果中的每个元素一定是 唯一 的。我们可以 不考虑输出结果的顺序 。\n示例 1：\n输入：nums1 = [1,2,2,1], nums2 = [2,2]\r输出：[2]\r 示例 2：\n输入：nums1 = [4,9,5], nums2 = [9,4,9,8,4]\r输出：[9,4]\r解释：[4,9] 也是可通过的\r 这题也很简单\npublic int[] intersection(int[] nums1, int[] nums2) {\rSet\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; nums1.length; i++) {\rset.add(nums1[i]);\r}\rList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; nums2.length; i++) {\rif (set.contains(nums2[i])) {\rlist.add(nums2[i]);\rset.remove(nums2[i]);\r}\r}\rint[] result = new int[list.size()];\rint j = 0;\rfor (Integer i : list) {\rresult[j++] = i;\r}\rreturn result;\r}\r 也可以先排序，再比较\npublic int[] intersection1(int[] nums1, int[] nums2) {\r// 先排序\rArrays.sort(nums1);\rArrays.sort(nums2);\rint length1 = nums1.length, length2 = nums2.length;\rint[] intersection = new int[length1 + length2];\rint index = 0, index1 = 0, index2 = 0;\rwhile (index1 \u0026lt; length1 \u0026amp;\u0026amp; index2 \u0026lt; length2) {\rint num1 = nums1[index1], num2 = nums2[index2];\rif (num1 == num2) {\r// 如果两个数字相等，且该数字不存在\rif (index == 0 || num1 != intersection[index - 1]) {\rintersection[index++] = num1;\r}\rindex1++;\rindex2++;\r} // 如果两个数字不相等，则将指向较小数字的指针右移一位\relse if (num1 \u0026lt; num2) {\rindex1++;\r} else {\rindex2++;\r}\r}\rreturn Arrays.copyOfRange(intersection, 0, index);\r}\r 202. 快乐数 编写一个算法来判断一个数 n 是不是快乐数。\n「快乐数」 定义为：\n 对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和。 然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。 如果这个过程 结果为 1，那么这个数就是快乐数。  如果 n 是 快乐数 就返回 true ；不是，则返回 false 。\n示例 1：\n输入：n = 19\r输出：true\r解释：\r12 + 92 = 82\r82 + 22 = 68\r62 + 82 = 100\r12 + 02 + 02 = 1\r 示例 2：\n输入：n = 2\r输出：false\r 这题就是纯属那种看不懂的题目了，都以为只能纯暴力做了，但是有可能无限循环。。。\n官方思路：\n我们可以先举几个例子。我们从 7 开始。则下一个数字是 49，我们可以不断重复该的过程，直到我们得到 1。因为我们得到了 1，我们知道 7 是一个快乐数，函数应该返回 true。\n再举一个例子，让我们从 116 开始。通过反复通过平方和计算下一个数字，我们最终得到 58，再继续计算之后，我们又回到 58。由于我们回到了一个已经计算过的数字，可以知道有一个循环，因此不可能达到 1。所以对于 116，函数应该返回 false。\n根据我们的探索，我们猜测会有以下三种可能。\n 最终会得到 1。 最终会进入循环。 值会越来越大，最后接近无穷大。  第三个情况比较难以检测和处理。我们怎么知道它会继续变大，而不是最终得到 11 呢？我们可以仔细想一想，每一位数的最大数字的下一位数是多少。\n   Digits Largest Next     1 9 81   2 99 162   3 999 243   4 9999 324   13 9999999999999 1053    对于 3 位数的数字，它不可能大于 243。这意味着它要么被困在 243 以下的循环内，要么跌到 1。4 位或 4 位以上的数字在每一步都会丢失一位，直到降到 3 位为止。所以我们知道，最坏的情况下，算法可能会在 243以下的所有数字上循环，然后回到它已经到过的一个循环或者回到 1。但它不会无限期地进行下去，所以我们排除第三种选择。\n所以这题目就变成了，找是否存在环了。。那就很简单了，经典思路快慢指针、以及可以用set来做\nset public boolean isHappy(int n) {\rSet\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;();\rwhile(n != 1 \u0026amp;\u0026amp; !set.contains(n)) {\rset.add(n);\rn = getNext(n);\r}\rreturn n == 1;\r}\r// 获取下一个数\rpublic int getNext(int n) {\rint result = 0;\rwhile(n != 0) {\rresult += (n % 10)*(n % 10);\rn = n / 10;\r}\rreturn result;\r}\r 快慢指针 public boolean isHappy1(int n) {\rint fast = getNext(getNext(n));\rint slow = getNext(n);\rwhile(fast != 1 \u0026amp;\u0026amp; slow != 1) {\rif(fast == slow) {\rreturn false;\r}\rfast = getNext(getNext(fast));\rslow = getNext(slow);\r}\rreturn true;\r}\r 字符串 344. 反转字符串 编写一个函数，其作用是将输入的字符串反转过来。输入字符串以字符数组 s 的形式给出。\n不要给另外的数组分配额外的空间，你必须**原地修改输入数组**、使用 O(1) 的额外空间解决这一问题。\n示例 1：\n输入：s = [\u0026quot;h\u0026quot;,\u0026quot;e\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;o\u0026quot;]\r输出：[\u0026quot;o\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;l\u0026quot;,\u0026quot;e\u0026quot;,\u0026quot;h\u0026quot;]\r 示例 2：\n输入：s = [\u0026quot;H\u0026quot;,\u0026quot;a\u0026quot;,\u0026quot;n\u0026quot;,\u0026quot;n\u0026quot;,\u0026quot;a\u0026quot;,\u0026quot;h\u0026quot;]\r输出：[\u0026quot;h\u0026quot;,\u0026quot;a\u0026quot;,\u0026quot;n\u0026quot;,\u0026quot;n\u0026quot;,\u0026quot;a\u0026quot;,\u0026quot;H\u0026quot;]\r 这题太简单了。。\npublic void reverseString(char[] s) {\rint length = s.length - 1;\rfor (int i = 0; i \u0026lt;= length / 2; i++) {\rchar temp = s[i];\rs[i] = s[length - i];\rs[length - i] = temp;\r}\r}\r 541. 反转字符串 II 给定一个字符串 s 和一个整数 k，从字符串开头算起，每计数至 2k 个字符，就反转这 2k 字符中的前 k 个字符。\n 如果剩余字符少于 k 个，则将剩余字符全部反转。 如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。  示例 1：\n输入：s = \u0026quot;abcdefg\u0026quot;, k = 2\r输出：\u0026quot;bacdfeg\u0026quot;\r 示例 2：\n输入：s = \u0026quot;abcd\u0026quot;, k = 2\r输出：\u0026quot;bacd\u0026quot;\r 这题也太简单了。。\npublic String reverseStr(String s, int k) {\rchar[] c = s.toCharArray();\rint left = 0, index = 0, length = s.length();\rwhile (length - index \u0026gt;= 2 * k || (length - index \u0026gt;= k \u0026amp;\u0026amp; length - index \u0026lt; 2 * k)) {\rindex += 2 * k;\rint right = left + k - 1;\rreverse(c, left, right);\rleft = index;\r}\rif (left != length) {\rint right = length - 1;\rreverse(c, left, right);\r}\rreturn new String(c);\r}\rpublic void reverse(char[] c, int left, int right) {\rwhile (left \u0026lt;= right) {\rchar temp = c[left];\rc[left] = c[right];\rc[right] = temp;\rleft++;\rright--;\r}\r}\r 151. 颠倒字符串中的单词 给你一个字符串 s ，颠倒字符串中 单词 的顺序。\n单词 是由非空格字符组成的字符串。s 中使用至少一个空格将字符串中的 单词 分隔开。\n返回 单词 顺序颠倒且 单词 之间用单个空格连接的结果字符串。\n**注意：**输入字符串 s中可能会存在前导空格、尾随空格或者单词间的多个空格。返回的结果字符串中，单词间应当仅用单个空格分隔，且不包含任何额外的空格。\n示例 1：\n输入：s = \u0026quot;the sky is blue\u0026quot;\r输出：\u0026quot;blue is sky the\u0026quot;\r 示例 2：\n输入：s = \u0026quot; hello world \u0026quot;\r输出：\u0026quot;world hello\u0026quot;\r解释：颠倒后的字符串中不能存在前导空格和尾随空格。\r 示例 3：\n输入：s = \u0026quot;a good example\u0026quot;\r输出：\u0026quot;example good a\u0026quot;\r解释：如果两个单词间有多余的空格，颠倒后的字符串需要将单词间的空格减少到仅有一个。\r 提示：\n 1 \u0026lt;= s.length \u0026lt;= 104 s 包含英文大小写字母、数字和空格 ' ' s 中 至少存在一个 单词  这题大意了，本来以为肯定是在一个for循环里写完的，所以没有用先移除前后多余的空格然后再进行反转的操作，但是发现写了半天也写不出来。。。\npublic String reverseWords2(String s) {\rint i = 0, right = s.length();\r// 去除前面空格\rfor (int j = 0; j \u0026lt; s.length(); j++) {\rif (s.charAt(i) != ' ') {\rbreak;\r}\ri++;\r}\r// 去除后面空格\rfor (int j = s.length() - 1; j \u0026gt;= 0; j--) {\rif (s.charAt(i) != ' ') {\rbreak;\r}\rright--;\r}\rStringBuilder stringBuilder = new StringBuilder();\rint left = right - 1;\rwhile (left \u0026gt;= i) {\rif (s.charAt(left) == ' ') {\rif (left + 1 != right) {\rstringBuilder.append(s, left + 1, right);\rstringBuilder.append(\u0026quot; \u0026quot;);\r}\rright = left;\r}\rleft--;\r}\rstringBuilder.append(s, left + 1, right);\rreturn stringBuilder.toString();\r}\r 自己之前的思路，想要不额外处理空格，结果写了半天还是要处理的。\npublic String reverseWords(String s) {\rStringBuilder stringBuilder = new StringBuilder();\rint left = s.length() - 1, right = left + 1;\rwhile (left \u0026gt;= 0) {\rif (s.charAt(left) == ' ') {\rif (left + 1 != right) {\rstringBuilder.append(s, left + 1, right);\rif (stringBuilder.length() != 0) {\rstringBuilder.append(\u0026quot; \u0026quot;);\r}\r}\rright = left;\r}\rleft--;\r}\rstringBuilder.append(s, left + 1, right);\r// 去除后面空格\rright = stringBuilder.length();\rfor (int i = stringBuilder.length() - 1; i \u0026gt;= 0; i--) {\rif (stringBuilder.charAt(i) != ' ') {\rbreak;\r}\rright--;\r}\rreturn stringBuilder.substring(0, right);\r}\r 别人的代码，不额外处理的\npublic String reverseWords2(String s) {\rint i = 0, right = s.length();\r// 去除前面空格\rfor (int j = 0; j \u0026lt; s.length(); j++) {\rif (s.charAt(i) != ' ') {\rbreak;\r}\ri++;\r}\r// 去除后面空格\rfor (int j = s.length() - 1; j \u0026gt;= 0; j--) {\rif (s.charAt(i) != ' ') {\rbreak;\r}\rright--;\r}\rStringBuilder stringBuilder = new StringBuilder();\rint left = right - 1;\rwhile (left \u0026gt;= i) {\rif (s.charAt(left) == ' ') {\rif (left + 1 != right) {\rstringBuilder.append(s, left + 1, right);\rstringBuilder.append(\u0026quot; \u0026quot;);\r}\rright = left;\r}\rleft--;\r}\rstringBuilder.append(s, left + 1, right);\rreturn stringBuilder.toString();\r}\r 8. 字符串转换整数 (atoi) 请你来实现一个 myAtoi(string s) 函数，使其能将字符串转换成一个 32 位有符号整数（类似 C/C++ 中的 atoi 函数）。\n函数 myAtoi(string s) 的算法如下：\n 读入字符串并丢弃无用的前导空格 检查下一个字符（假设还未到字符末尾）为正还是负号，读取该字符（如果有）。 确定最终结果是负数还是正数。 如果两者都不存在，则假定结果为正。 读入下一个字符，直到到达下一个非数字字符或到达输入的结尾。字符串的其余部分将被忽略。 将前面步骤读入的这些数字转换为整数（即，\u0026ldquo;123\u0026rdquo; -\u0026gt; 123， \u0026ldquo;0032\u0026rdquo; -\u0026gt; 32）。如果没有读入数字，则整数为 0 。必要时更改符号（从步骤 2 开始）。 如果整数数超过 32 位有符号整数范围 [−231, 231 − 1] ，需要截断这个整数，使其保持在这个范围内。具体来说，小于 −231 的整数应该被固定为 −231 ，大于 231 − 1 的整数应该被固定为 231 − 1 。 返回整数作为最终结果。  注意：\n 本题中的空白字符只包括空格字符 ' ' 。 除前导空格或数字后的其余字符串外，请勿忽略 任何其他字符。  示例 1：\n输入：s = \u0026quot;42\u0026quot;\r输出：42\r解释：加粗的字符串为已经读入的字符，插入符号是当前读取的字符。\r第 1 步：\u0026quot;42\u0026quot;（当前没有读入字符，因为没有前导空格）\r^\r第 2 步：\u0026quot;42\u0026quot;（当前没有读入字符，因为这里不存在 '-' 或者 '+'）\r^\r第 3 步：\u0026quot;42\u0026quot;（读入 \u0026quot;42\u0026quot;）\r^\r解析得到整数 42 。\r由于 \u0026quot;42\u0026quot; 在范围 [-231, 231 - 1] 内，最终结果为 42 。\r 示例 2：\n输入：s = \u0026quot; -42\u0026quot;\r输出：-42\r解释：\r第 1 步：\u0026quot; -42\u0026quot;（读入前导空格，但忽视掉）\r^\r第 2 步：\u0026quot; -42\u0026quot;（读入 '-' 字符，所以结果应该是负数）\r^\r第 3 步：\u0026quot; -42\u0026quot;（读入 \u0026quot;42\u0026quot;）\r^\r解析得到整数 -42 。\r由于 \u0026quot;-42\u0026quot; 在范围 [-231, 231 - 1] 内，最终结果为 -42 。\r 示例 3：\n输入：s = \u0026quot;4193 with words\u0026quot;\r输出：4193\r解释：\r第 1 步：\u0026quot;4193 with words\u0026quot;（当前没有读入字符，因为没有前导空格）\r^\r第 2 步：\u0026quot;4193 with words\u0026quot;（当前没有读入字符，因为这里不存在 '-' 或者 '+'）\r^\r第 3 步：\u0026quot;4193 with words\u0026quot;（读入 \u0026quot;4193\u0026quot;；由于下一个字符不是一个数字，所以读入停止）\r^\r解析得到整数 4193 。\r由于 \u0026quot;4193\u0026quot; 在范围 [-231, 231 - 1] 内，最终结果为 4193 。\r 这题思路很简单，1、去除空格，2、判断正负号，3、算结果，这一步自己搞了半天用倒序算，没有想到一个很简单的正序算，result = result * 10 + num\n自己用倒序算的，导致处理了很多异常情况。。。\npublic int myAtoi(String s) {\rif (s.length() == 0) {\rreturn 0;\r}\rint left = 0, right = s.length() - 1;\rint flag = 0;\rfor (int i = 0; i \u0026lt;= right; i++) {\rif ((s.charAt(i) == ' ' \u0026amp;\u0026amp; flag == 0) || s.charAt(i) == '0') {\rleft++;\rif (s.charAt(i) == '0') {\rflag = 1;\r}\r} else {\rbreak;\r}\r}\rif (left \u0026lt;= right \u0026amp;\u0026amp; (s.charAt(left) == '-' || s.charAt(left) == '+') \u0026amp;\u0026amp; flag == 0) {\rif (s.charAt(left) == '-') {\rflag = 2;\r}\rleft++;\r}\rif (flag != 0) {\rfor (int i = left; i \u0026lt;= right; i++) {\rif (s.charAt(i) == '0') {\rleft++;\r} else {\rbreak;\r}\r}\r}\rint result = 0, nums = 0;\rboolean temp = false;\rfor (int i = right; i \u0026gt;= left; i--) {\rif (s.charAt(i) \u0026gt;= '0' \u0026amp;\u0026amp; s.charAt(i) \u0026lt;= '9') {\rif (nums \u0026gt; 10 || (Math.pow(10, nums) * (s.charAt(i) - '0') + result \u0026gt; Integer.MAX_VALUE)) {\rresult = Integer.MAX_VALUE;\rtemp = true;\r} else {\rresult += nums == 0 ? (s.charAt(i) - '0') : Math.pow(10, nums) * (s.charAt(i) - '0');\r}\rnums++;\r} else {\rresult = 0;\rnums = 0;\r}\r}\rif (flag == 2) {\rresult = result == Integer.MAX_VALUE \u0026amp;\u0026amp; temp ? Integer.MIN_VALUE : -result;\r}\rreturn result;\r}\r 看了答案发现可以用正序算。。就很简单了。。。。\npublic int myAtoi1(String s) {\rif(s.length() == 0) {\rreturn 0;\r}\rint left = 0, right = s.length() - 1;\rboolean flag = false;\r// 去除前面的空格\rfor (int i = 0; i \u0026lt;= right; i++) {\rif (s.charAt(i) != ' ') {\rbreak;\r}\rleft++;\r}\r// 去除正负号\rif (left \u0026lt;= right \u0026amp;\u0026amp; (s.charAt(left) == '-' || s.charAt(left) == '+')) {\rif (s.charAt(left) == '-') {\rflag = true;\r}\rleft++;\r}\rint result = 0;\rfor (int i = left; i \u0026lt;= right; i++) {\rif (s.charAt(i) \u0026gt;= '0' \u0026amp;\u0026amp; s.charAt(i) \u0026lt;= '9') {\rint num = s.charAt(i) - '0';\rif (result \u0026gt; (Integer.MAX_VALUE - num) / 10) {\rreturn flag ? Integer.MIN_VALUE : Integer.MAX_VALUE;\r}\rresult = result * 10 + num;\r} else {\rbreak;\r}\r}\rreturn flag ? -result : result;\r}\r 165. 比较版本号 给你两个版本号 version1 和 version2 ，请你比较它们。\n版本号由一个或多个修订号组成，各修订号由一个 '.' 连接。每个修订号由 多位数字 组成，可能包含 前导零 。每个版本号至少包含一个字符。修订号从左到右编号，下标从 0 开始，最左边的修订号下标为 0 ，下一个修订号下标为 1 ，以此类推。例如，2.5.33 和 0.1 都是有效的版本号。\n比较版本号时，请按从左到右的顺序依次比较它们的修订号。比较修订号时，只需比较 忽略任何前导零后的整数值 。也就是说，修订号 1 和修订号 001 相等 。如果版本号没有指定某个下标处的修订号，则该修订号视为 0 。例如，版本 1.0 小于版本 1.1 ，因为它们下标为 0 的修订号相同，而下标为 1 的修订号分别为 0 和 1 ，0 \u0026lt; 1 。\n返回规则如下：\n 如果 *version1* \u0026gt; *version2* 返回 1， 如果 *version1* \u0026lt; *version2* 返回 -1， 除此之外返回 0。  示例 1：\n输入：version1 = \u0026quot;1.01\u0026quot;, version2 = \u0026quot;1.001\u0026quot;\r输出：0\r解释：忽略前导零，\u0026quot;01\u0026quot; 和 \u0026quot;001\u0026quot; 都表示相同的整数 \u0026quot;1\u0026quot;\r 示例 2：\n输入：version1 = \u0026quot;1.0\u0026quot;, version2 = \u0026quot;1.0.0\u0026quot;\r输出：0\r解释：version1 没有指定下标为 2 的修订号，即视为 \u0026quot;0\u0026quot;\r 示例 3：\n输入：version1 = \u0026quot;0.1\u0026quot;, version2 = \u0026quot;1.1\u0026quot;\r输出：-1\r解释：version1 中下标为 0 的修订号是 \u0026quot;0\u0026quot;，version2 中下标为 0 的修订号是 \u0026quot;1\u0026quot; 。0 \u0026lt; 1，所以 version1 \u0026lt; version2\r 这题思路一看就很简单，先按'.' 分组，然后每组进行比较\npublic int compareVersion1(String version1, String version2) {\rList\u0026lt;String\u0026gt; v1List = Arrays.asList(version1.split(\u0026quot;\\\\.\u0026quot;));\rList\u0026lt;String\u0026gt; v2List = Arrays.asList(version2.split(\u0026quot;\\\\.\u0026quot;));\rint flag = 0;\rfor (int i = 0, j = 0; flag == 0 \u0026amp;\u0026amp; (i \u0026lt; v1List.size() || j \u0026lt; v2List.size()); i++, j++) {\rString v1 = i \u0026gt;= v1List.size() ? \u0026quot;0\u0026quot; : v1List.get(i);\rString v2 = j \u0026gt;= v2List.size() ? \u0026quot;0\u0026quot; : v2List.get(j);\rint l1 = 0, r1 = v1.length(), l2 = 0, r2 = v2.length();\r// 去除前导0\rfor (int k = 0; k \u0026lt; r1; k++) {\rif (v1.charAt(k) != '0') {\rbreak;\r}\rl1++;\r}\rfor (int k = 0; k \u0026lt; r2; k++) {\rif (v2.charAt(k) != '0') {\rbreak;\r}\rl2++;\r}\r// 比较\rif (r1 - l1 == r2 - l2) {\rfor (int k = l1, l = l2; k \u0026lt; r1; k++, l++) {\rif (v1.charAt(k) \u0026gt; v2.charAt(l)) {\rflag = 1;\rbreak;\r} else if (v1.charAt(i) \u0026lt; v2.charAt(j)) {\rflag = -1;\rbreak;\r}\r}\r} else {\rflag = (r1 - l1) \u0026gt; (r2 - l2) ? 1 : -1;\r}\r}\rreturn flag;\r}\r 他妈的比较了半天，看了答案，答案还是牛啊，都忘记了，Integer.parseInt(v1)，可以直接去掉前导0的。。\npublic int compareVersion2 (String version1, String version2) {\rString[] v1 = version1.split(\u0026quot;\\\\.\u0026quot;);\rString[] v2 = version2.split(\u0026quot;\\\\.\u0026quot;);\rfor (int i = 0; i \u0026lt; v1.length || i \u0026lt; v2.length; ++i) {\rint x = 0, y = 0;\rif (i \u0026lt; v1.length) {\rx = Integer.parseInt(v1[i]);\r}\rif (i \u0026lt; v2.length) {\ry = Integer.parseInt(v2[i]);\r}\rif (x \u0026gt; y) {\rreturn 1;\r}\rif (x \u0026lt; y) {\rreturn -1;\r}\r}\rreturn 0;\r}\r 双指针解法 public int compareVersion3(String version1, String version2) {\rint l1 = 0, r1 = version1.length(), l2 = 0, r2 = version2.length();\rwhile (l1 \u0026lt; r1 || l2 \u0026lt; r2) {\rint num1 = 0, num2 = 0;\rfor (; l1 \u0026lt; r1 \u0026amp;\u0026amp; '.' != version1.charAt(l1); l1++) {\rnum1 += num1 * 10 + (version1.charAt(l1) - '0');\r}\rfor (; l2 \u0026lt; r2 \u0026amp;\u0026amp; '.' != version2.charAt(l2); l2++) {\rnum2 += num2 * 10 + (version2.charAt(l2) - '0');\r}\rif (num1 != num2) {\rreturn num1 \u0026gt; num2 ? 1 : -1;\r}\rl1++;\rl2++;\r}\rreturn 0;\r}\r 415. 字符串相加 给定两个字符串形式的非负整数 num1 和num2 ，计算它们的和并同样以字符串形式返回。\n你不能使用任何內建的用于处理大整数的库（比如 BigInteger）， 也不能直接将输入的字符串转换为整数形式。\n示例 1：\n输入：num1 = \u0026quot;11\u0026quot;, num2 = \u0026quot;123\u0026quot;\r输出：\u0026quot;134\u0026quot;\r 示例 2：\n输入：num1 = \u0026quot;456\u0026quot;, num2 = \u0026quot;77\u0026quot;\r输出：\u0026quot;533\u0026quot;\r 示例 3：\n输入：num1 = \u0026quot;0\u0026quot;, num2 = \u0026quot;0\u0026quot;\r输出：\u0026quot;0\u0026quot;\r 一道很简单的题，直接模拟加法就行\npublic String addStrings(String num1, String num2) {\rint l1 = num1.length() - 1, l2 = num2.length() - 1;\rint temp = 0;\rStringBuilder s = new StringBuilder();\rfor (int i = l1, j = l2; i \u0026gt;= 0 || j \u0026gt;= 0 || temp != 0; i--, j--) {\rint a = i \u0026gt;= 0 ? (num1.charAt(i) - '0') : 0;\rint b = j \u0026gt;= 0 ? (num2.charAt(j) - '0') : 0;\rint c = (temp + a + b) % 10;\rtemp = (temp + a + b) / 10;\rs = new StringBuilder().append((char) (c + '0')).append(s);\r}\rreturn s.toString();\r}\r 43. 字符串相乘 给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。\n**注意：**不能使用任何内置的 BigInteger 库或直接将输入转换为整数。\n示例 1:\n输入: num1 = \u0026quot;2\u0026quot;, num2 = \u0026quot;3\u0026quot;\r输出: \u0026quot;6\u0026quot;\r 示例 2:\n输入: num1 = \u0026quot;123\u0026quot;, num2 = \u0026quot;456\u0026quot;\r输出: \u0026quot;56088\u0026quot;\r 这题没什么意思，模拟乘法，然后相加\npublic String multiply(String num1, String num2) {\rString result = \u0026quot;\u0026quot;;\rif (\u0026quot;0\u0026quot;.equals(num1) || \u0026quot;0\u0026quot;.equals(num2)) {\rreturn \u0026quot;0\u0026quot;;\r}\rfor (int i = num1.length() - 1; i \u0026gt;= 0; i--) {\rint a = num1.charAt(i) - '0';\rStringBuilder s = new StringBuilder();\rint temp = 0;\rfor (int j = num2.length() - 1; j \u0026gt;= 0 || temp != 0; j--) {\rint b = j \u0026gt;= 0 ? num2.charAt(j) - '0' : 0;\rint c = (a * b + temp) % 10;\rtemp = (a * b + temp) / 10;\rs.append(c);\r}\rs.reverse();\rfor (int k = num1.length() - 1; k \u0026gt; i; k--) {\rs.append(0);\r}\rresult = addStrings(result, s.toString());\r}\rreturn result;\r}\r 这种做法比较高级，有两点注意的\n 确认长度：num1的长度为m，num2的长度为n，则相乘之后最大长度为m+n 思路：用数组存储计算的结果，模拟进位  public String multiply1(String num1, String num2) {\rif (\u0026quot;0\u0026quot;.equals(num1) || \u0026quot;0\u0026quot;.equals(num2)) {\rreturn \u0026quot;0\u0026quot;;\r}\rStringBuilder result = new StringBuilder();\rint l1 = num1.length(), l2 = num2.length(), m = l1 + l2;\rint[] nums = new int[m];\rfor (int i = l1 - 1; i \u0026gt;= 0; i--) {\rint a = num1.charAt(i) - '0';\rint temp = 0;\rint k = m - (l1 - i);\rfor (int j = l2 - 1; j \u0026gt;= 0 || temp != 0; j--) {\rint b = j \u0026gt;= 0 ? num2.charAt(j) - '0' : 0;\rint c = (a * b + temp + nums[k]) % 10;\rtemp = (a * b + temp + nums[k]) / 10;\rnums[k] = c;\rk--;\r}\r}\rint i;\rfor (i = 0; i \u0026lt; m; i++) {\rif (nums[i] != 0) {\rbreak;\r}\r}\rfor (int j = i; j \u0026lt; m; j++) {\rresult.append(nums[j]);\r}\rreturn result.toString();\r}\r 栈 栈作为一种数据结构，是一种只能在一端进行插入和删除操作的特殊线性表。它按照先进后出的原则存储数据，先进入的数据被压入栈底，最后的数据在栈顶，需要读数据的时候从栈顶开始弹出数据（最后一个数据被第一个读出来）。栈具有记忆作用，对栈的插入与删除操作中，不需要改变栈底指针。\n栈在平常工作中用的比较少，需要多练习下\n227. 基本计算器 II 给你一个字符串表达式 s ，请你实现一个基本计算器来计算并返回它的值。\n整数除法仅保留整数部分。\n你可以假设给定的表达式总是有效的。所有中间结果将在 [-231, 231 - 1] 的范围内。\n**注意：**不允许使用任何将字符串作为数学表达式计算的内置函数，比如 eval() 。\n示例 1：\n输入：s = \u0026quot;3+2*2\u0026quot;\r输出：7\r 示例 2：\n输入：s = \u0026quot; 3/2 \u0026quot;\r输出：1\r 示例 3：\n输入：s = \u0026quot; 3+5 / 2 \u0026quot;\r输出：5\r 这题算是一个很经典的栈的题目，思路为用两个栈来实现。\n 其中一个保存操作数的栈，另一个是保存运算符的栈。 我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈； 当遇到运算符，就与运算符栈的栈顶元素进行比较。自己在理解这句话的时候有偏差，以为是只要比较一次，没有想到是每次都需要比较！！！！  public int calculate(String s) {\rDeque\u0026lt;Integer\u0026gt; num = new LinkedList\u0026lt;\u0026gt;();\rDeque\u0026lt;Character\u0026gt; s1 = new LinkedList\u0026lt;\u0026gt;();\rint i = 0;\rfor (; i \u0026lt; s.length(); i++) {\rchar c = s.charAt(i);\rif (c \u0026gt;= '0' \u0026amp;\u0026amp; c \u0026lt;= '9') {\rint temp = c - '0';\rwhile ((i \u0026lt; s.length() - 1) \u0026amp;\u0026amp; (s.charAt(i + 1) \u0026gt;= '0' \u0026amp;\u0026amp; s.charAt(i + 1) \u0026lt;= '9')) {\ri++;\rtemp = temp * 10 + (s.charAt(i) - '0');\r}\rnum.push(temp);\r} else if (c == '+' || c == '-' || c == '*' || c == '/') {\r// 栈顶优先级比当前元素优先级高，每次都要和栈顶元素进行比较！！！\rwhile (!s1.isEmpty() \u0026amp;\u0026amp; ((s1.peek() == '*' || s1.peek() == '/') || (c == '-' || c == '+'))) {\r// 自己的错误理解\r// while (!s1.isEmpty()) {\r// char c1 = s1.peek();\r// if((s1.peek() == '*' || s1.peek() == '/') || (c == '-' || c == '+')) {\r//\r// }\r// }\rint a = num.pop();\rint b = num.pop();\rint temp;\rchar c2 = s1.pop();\rif (c2 == '+') {\rtemp = a + b;\r} else if (c2 == '-') {\rtemp = b - a;\r} else if (c2 == '*') {\rtemp = a * b;\r} else {\rif (a == 0) {\rtemp = 0;\r} else {\rtemp = b / a;\r}\r}\rnum.push(temp);\r}\rs1.push(c);\r}\r}\rwhile (!s1.isEmpty()) {\rint a = num.pop();\rint b = num.pop();\rint temp;\rchar c1 = s1.pop();\rif (c1 == '+') {\rtemp = a + b;\r} else if (c1 == '-') {\rtemp = b - a;\r} else if (c1 == '*') {\rtemp = a * b;\r} else {\rif (a == 0) {\rtemp = 0;\r} else {\rtemp = b / a;\r}\r}\rnum.push(temp);\r}\rreturn num.pop();\r}\r 224. 基本计算器 给你一个字符串表达式 s ，请你实现一个基本计算器来计算并返回它的值。\n注意:不允许使用任何将字符串作为数学表达式计算的内置函数，比如 eval() 。\n示例 1：\n输入：s = \u0026quot;1 + 1\u0026quot;\r输出：2\r 示例 2：\n输入：s = \u0026quot; 2-1 + 2 \u0026quot;\r输出：3\r 示例 3：\n输入：s = \u0026quot;(1+(4+5+2)-3)+(6+8)\u0026quot;\r输出：23\r 提示：\n 1 \u0026lt;= s.length \u0026lt;= 3 * 105 s 由数字、'+'、'-'、'('、')'、和 ' ' 组成 s 表示一个有效的表达式 \u0026lsquo;+\u0026rsquo; 不能用作一元运算(例如， \u0026ldquo;+1\u0026rdquo; 和 \u0026quot;+(2 + 3)\u0026quot; 无效) \u0026lsquo;-\u0026rsquo; 可以用作一元运算(即 \u0026ldquo;-1\u0026rdquo; 和 \u0026quot;-(2 + 3)\u0026quot; 是有效的) 输入中不存在两个连续的操作符 每个数字和运行的计算将适合于一个有符号的 32位 整数  这题实际上是一样的，但是有个问题是对于**（-2）这种情况怎么处理**，看了答案才发现，直接用原字符串做处理就好了\npublic int calculate2(String s) {\rDeque\u0026lt;Integer\u0026gt; num = new LinkedList\u0026lt;\u0026gt;();\rDeque\u0026lt;Character\u0026gt; s1 = new LinkedList\u0026lt;\u0026gt;();\rint i = 0;\rfor (; i \u0026lt; s.length(); i++) {\rchar c = s.charAt(i);\rif (c \u0026gt;= '0' \u0026amp;\u0026amp; c \u0026lt;= '9') {\rint temp = c - '0';\rwhile ((i \u0026lt; s.length() - 1) \u0026amp;\u0026amp; (s.charAt(i + 1) \u0026gt;= '0' \u0026amp;\u0026amp; s.charAt(i + 1) \u0026lt;= '9')) {\ri++;\rtemp = temp * 10 + (s.charAt(i) - '0');\r}\rnum.push(temp);\r} else if (c == '(') {\rs1.push(c);\r} else if (c == ')') {\r// 遇到右括号则计算到第一个左括号为止\rwhile (!s1.isEmpty() \u0026amp;\u0026amp; s1.peek() != '(') {\reval(num, s1);\r}\rs1.pop();\r} else if (c == '+' || c == '-' || c == '*' || c == '/') {\r// 相邻的元素都是符号，则放入一个0避免错误，解决了(-2)\rif (i \u0026gt; 0 \u0026amp;\u0026amp; (s.charAt(i - 1) == '(' || s.charAt(i - 1) == '+' || s.charAt(i - 1) == '-')) {\rnum.push(0);\r}\r// 栈顶优先级比当前元素优先级高，每次都要和栈顶元素进行比较！！！\rwhile (!s1.isEmpty() \u0026amp;\u0026amp; s1.peek() != '(' \u0026amp;\u0026amp; ((s1.peek() == '*' || s1.peek() == '/') || (c == '-' || c == '+'))) {\reval(num, s1);\r}\rs1.push(c);\r}\r}\rwhile (!s1.isEmpty()) {\reval(num, s1);\r}\rreturn num.pop();\r}\rpublic void eval(Deque\u0026lt;Integer\u0026gt; num, Deque\u0026lt;Character\u0026gt; s1) {\rint a = num.pop();\rint b = num.isEmpty() ? 0 : num.pop();\rint temp;\rchar c1 = s1.pop();\rif (c1 == '+') {\rtemp = a + b;\r} else if (c1 == '-') {\rtemp = b - a;\r} else if (c1 == '*') {\rtemp = a * b;\r} else {\rif (a == 0) {\rtemp = 0;\r} else {\rtemp = b / a;\r}\r}\rnum.push(temp);\r}\r 20. 有效的括号 给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串 s ，判断字符串是否有效。\n有效字符串需满足：\n 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。  示例 1：\n输入：s = \u0026quot;()\u0026quot;\r输出：true\r 示例 2：\n输入：s = \u0026quot;()[]{}\u0026quot;\r输出：true\r 示例 3：\n输入：s = \u0026quot;(]\u0026quot;\r输出：false\r 示例 4：\n输入：s = \u0026quot;([)]\u0026quot;\r输出：false\r 示例 5：\n输入：s = \u0026quot;{[]}\u0026quot;\r输出：true\r 这题很简单，直接栈就解决了\npublic boolean isValid(String s) {\rDeque\u0026lt;Character\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; s.length(); i++) {\rchar c = s.charAt(i);\rif (c == '(' || c == '[' || c == '{') {\rstack.push(c);\r} else if (c == '}') {\rif (stack.isEmpty() || stack.peek() != '{') {\rreturn false;\r} else {\rstack.pop();\r}\r} else if (c == ']') {\rif (stack.isEmpty() || stack.peek() != '[') {\rreturn false;\r} else {\rstack.pop();\r}\r} else if (c == ')') {\rif (stack.isEmpty() || stack.peek() != '(') {\rreturn false;\r} else {\rstack.pop();\r}\r}\r}\rreturn stack.isEmpty();\r}\r 225. 用队列实现栈 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。\n实现 MyStack 类：\n void push(int x) 将元素 x 压入栈顶。 int pop() 移除并返回栈顶元素。 int top() 返回栈顶元素。 boolean empty() 如果栈是空的，返回 true ；否则，返回 false 。  注意：\n 你只能使用队列的基本操作 —— 也就是 push to back、peek/pop from front、size 和 is empty 这些操作。 你所使用的语言也许不支持队列。 你可以使用 list （列表）或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。  这个题目就是按照意思来即可，太简单了\nclass MyStack {\rprivate Queue\u0026lt;Integer\u0026gt; stack;\rpublic MyStack() {\rstack = new LinkedList\u0026lt;\u0026gt;();\r}\rpublic void push(int x) {\rif (stack.isEmpty()) {\rstack.add(x);\r} else {\rQueue\u0026lt;Integer\u0026gt; temp = new LinkedList\u0026lt;\u0026gt;();\rwhile (!stack.isEmpty()) {\rtemp.add(stack.poll());\r}\rstack.add(x);\rwhile (!temp.isEmpty()) {\rstack.add(temp.poll());\r}\r}\r}\rpublic int pop() {\rreturn stack.poll();\r}\rpublic int top() {\rreturn stack.peek();\r}\rpublic boolean empty() {\rreturn stack.isEmpty();\r}\r}\r 大意了，被题目误导了，实际上一个队列就能完成\npublic void push1(int x) {\rint n = stack.size();\rstack.offer(x);\rfor(int i = 0; i \u0026lt; n; i++) {\rstack.offer(stack.poll());\r}\r}\r 232. 用栈实现队列 请你仅使用两个栈实现先入先出队列。队列应当支持一般队列支持的所有操作（push、pop、peek、empty）：\n实现 MyQueue 类：\n void push(int x) 将元素 x 推到队列的末尾 int pop() 从队列的开头移除并返回元素 int peek() 返回队列开头的元素 boolean empty() 如果队列为空，返回 true ；否则，返回 false  说明：\n 你 只能 使用标准的栈操作 —— 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。  这题一样的，直接做就好了\nstatic class MyQueue {\rprivate Deque\u0026lt;Integer\u0026gt; queue;\rpublic MyQueue() {\rqueue = new LinkedList\u0026lt;\u0026gt;();\r}\rpublic void push(int x) {\rDeque\u0026lt;Integer\u0026gt; temp = new LinkedList\u0026lt;\u0026gt;();\rwhile(!queue.isEmpty()) {\rtemp.push(queue.pop());\r}\rtemp.push(x);\rwhile(!temp.isEmpty()) {\rqueue.push(temp.pop());\r}\r}\rpublic int pop() {\rreturn queue.pop();\r}\rpublic int peek() {\rreturn queue.peek();\r}\rpublic boolean empty() {\rreturn queue.isEmpty();\r}\r}\r 155. 最小栈 设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。\n实现 MinStack 类:\n MinStack() 初始化堆栈对象。 void push(int val) 将元素val推入堆栈。 void pop() 删除堆栈顶部的元素。 int top() 获取堆栈顶部的元素。 int getMin() 获取堆栈中的最小元素。  对于栈来说，如果一个元素 a 在入栈时，栈里有其它的元素 b, c, d，那么无论这个栈在之后经历了什么操作，只要 a 在栈中，b, c, d 就一定在栈中，因为在 a 被弹出之前，b, c, d 不会被弹出。\n因此，在操作过程中的任意一个时刻，只要栈顶的元素是 a，那么我们就可以确定栈里面现在的元素一定是 a, b, c, d。\n那么，我们可以在每个元素 a 入栈时把当前栈的最小值 m 存储起来。在这之后无论何时，如果栈顶元素是 a，我们就可以直接返回存储的最小值 m。意思就是在入栈时用一个栈保存当前栈的最小值\nclass MinStack {\rprivate Deque\u0026lt;Integer\u0026gt; stack;\rprivate Deque\u0026lt;Integer\u0026gt; minStack;\rpublic MinStack() {\rstack = new LinkedList\u0026lt;\u0026gt;();\rminStack = new LinkedList\u0026lt;\u0026gt;();\rminStack.push(Integer.MAX_VALUE);\r}\rpublic void push(int val) {\rstack.push(val);\rminStack.push(Math.min(val, minStack.peek()));\r}\rpublic void pop() {\rstack.pop();\rminStack.pop();\r}\rpublic int top() {\rreturn stack.peek();\r}\rpublic int getMin() {\rreturn minStack.peek();\r}\r}\r 这题也可以用一个栈做，每次入栈2个元素，一个是入栈的元素本身，一个是当前栈元素的最小值\nclass MinStack1 {\rprivate Deque\u0026lt;Integer\u0026gt; stack;\rpublic MinStack1() {\rstack = new LinkedList\u0026lt;\u0026gt;();\r}\rpublic void push(int val) {\rif(stack.isEmpty()) {\rstack.push(val);\rstack.push(val);\r} else {\rint temp = stack.peek();\rtemp = Math.min(temp, val);\rstack.push(val);\rstack.push(temp);\r}\r}\rpublic void pop() {\rstack.pop();\rstack.pop();\r}\rpublic int top() {\rint temp = stack.pop();\rint result = stack.peek();\rstack.push(temp);\rreturn result;\r}\rpublic int getMin() {\rreturn stack.peek();\r}\r}\r 394. 字符串解码 给定一个经过编码的字符串，返回它解码后的字符串。\n编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。\n你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。\n此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。\n示例 1：\n输入：s = \u0026quot;3[a]2[bc]\u0026quot;\r输出：\u0026quot;aaabcbc\u0026quot;\r 示例 2：\n输入：s = \u0026quot;3[a2[c]]\u0026quot;\r输出：\u0026quot;accaccacc\u0026quot;\r 示例 3：\n输入：s = \u0026quot;2[abc]3[cd]ef\u0026quot;\r输出：\u0026quot;abcabccdcdcdef\u0026quot;\r 示例 4：\n输入：s = \u0026quot;abc3[cd]xyz\u0026quot;\r输出：\u0026quot;abccdcdcdxyz\u0026quot;\r 这题思路很简单，遇到**]弹出元素，并根据规则拼接好放入栈中**。\n例如：2[a2[bc]]，先转化成 2[abcbc]，再转化成 abcbcabcbc，但代码层面有很多细节需要注意\npublic String decodeString(String s) {\rStringBuilder result = new StringBuilder();\rDeque\u0026lt;String\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; s.length(); i++) {\rchar c = s.charAt(i);\rif (c == ']') {\rStringBuilder temp = new StringBuilder();\rLinkedList\u0026lt;String\u0026gt; sub = new LinkedList\u0026lt;\u0026gt;();\rint k;\r// 在拼接元素时，需要根据压入栈中的元素倒序，而不是所有元素倒序，例如，出栈元素为a,bc,de,拼接好后应为，debca，而不是edcba\rwhile (!stack.isEmpty() \u0026amp;\u0026amp; !Objects.equals(stack.peek(), \u0026quot;[\u0026quot;)) {\rsub.add(stack.pop());\r}\rCollections.reverse(sub);\rfor(String s1 : sub) {\rtemp.append(s1);\r}\rstack.pop();\rk = Integer.parseInt(stack.pop());\rStringBuilder s1 = new StringBuilder();\rwhile (k \u0026gt; 0) {\rs1.append(temp);\rk--;\r}\rstack.push(s1.toString());\r} else if (c \u0026gt;= '0' \u0026amp;\u0026amp; c \u0026lt;= '9') {\r// 为数字时要计算出数字，比如99，放入栈中需要为99 而不是9\rint temp = c - '0';\rwhile (i \u0026lt; s.length() - 1 \u0026amp;\u0026amp; s.charAt(i + 1) \u0026gt;= '0' \u0026amp;\u0026amp; s.charAt(i + 1) \u0026lt;= '9') {\ri++;\rtemp = temp * 10 + (s.charAt(i) - '0');\r}\rstack.push(String.valueOf(temp));\r} else {\rstack.push(String.valueOf(c));\r}\r}\r// 出栈时应该倒过来出栈\rwhile (!stack.isEmpty()) {\rresult.append(stack.pollLast());\r}\rreturn result.toString();\r}\r 402. 移掉 K 位数字 给你一个以字符串表示的非负整数 num 和一个整数 k ，移除这个数中的 k 位数字，使得剩下的数字最小。请你以字符串形式返回这个最小的数字。\n示例 1 ：\n输入：num = \u0026quot;1432219\u0026quot;, k = 3\r输出：\u0026quot;1219\u0026quot;\r解释：移除掉三个数字 4, 3, 和 2 形成一个新的最小的数字 1219 。\r 示例 2 ：\n输入：num = \u0026quot;10200\u0026quot;, k = 1\r输出：\u0026quot;200\u0026quot;\r解释：移掉首位的 1 剩下的数字为 200. 注意输出不能有任何前导零。\r 示例 3 ：\n输入：num = \u0026quot;10\u0026quot;, k = 2\r输出：\u0026quot;0\u0026quot;\r解释：从原数字移除所有的数字，剩余为空就是 0 。\r 提示：\n 1 \u0026lt;= k \u0026lt;= num.length \u0026lt;= 105 num 仅由若干位数字（0 - 9）组成 除了 0 本身之外，num 不含任何前导零  这道题，做了好多次了但是再还是做不对，主要还是思路没有打开，思路很简单：\n遍历num，维护一个单调递增的栈\npublic static String removeKdigits(String num, int k) {\rDeque\u0026lt;Character\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; num.length(); i++) {\r// 比较当前数与栈尾数字大小，如果当前数比栈尾数小则移除栈尾\rwhile (!stack.isEmpty() \u0026amp;\u0026amp; k \u0026gt; 0 \u0026amp;\u0026amp; stack.peekLast() \u0026gt; num.charAt(i)) {\rk--;\rstack.pollLast();\r}\r// 否则放入栈尾\rstack.offerLast(num.charAt(i));\r}\r// 将剩余的k位移掉\rfor (int i = 0; i \u0026lt; k; i++) {\rstack.pollLast();\r}\r// 去掉前导0\rStringBuilder stringBuilder = new StringBuilder();\rboolean flag = true;\rwhile (!stack.isEmpty()) {\rif (flag \u0026amp;\u0026amp; stack.peekFirst() == '0') {\rstack.pollFirst();\rcontinue;\r}\rflag = false;\rstringBuilder.append(stack.pollFirst());\r}\rreturn stringBuilder.length() == 0 ? \u0026quot;0\u0026quot; : stringBuilder.toString();\r}\r 739. 每日温度 给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。\n示例 1:\n输入: temperatures = [73,74,75,71,69,72,76,73]\r输出: [1,1,4,2,1,1,0,0]\r 示例 2:\n输入: temperatures = [30,40,50,60]\r输出: [1,1,1,0]\r 示例 3:\n输入: temperatures = [30,60,90]\r输出: [1,1,0]\r 这题和上面一题很类似，这题只需要维护一个单调递减的单调栈就好了\npublic int[] dailyTemperatures(int[] temperatures) {\rDeque\u0026lt;Integer\u0026gt; stack = new LinkedList\u0026lt;\u0026gt;();\rint[] result = new int[temperatures.length];\rfor(int i = 0; i \u0026lt; temperatures.length; i++) {\rint c = temperatures[i];\r// 如果当前元素比栈尾元素大，则移除栈尾元素\rwhile(!stack.isEmpty() \u0026amp;\u0026amp; temperatures[stack.peekLast()] \u0026lt; c) {\rint temp = stack.pollLast();\rresult[temp] = i - temp;\r}\r// 否则放入栈尾\rstack.offerLast(i);\r}\rreturn result;\r}\r ","id":7,"section":"posts","summary":"[TOC] 算法这东西，可以说玄之又玄，大学就被程序设计这门课统治，工作了，虽然说这东西在实践中用到的少，但是谁让大家都卷呢，你不会就是不行，所以本着","tags":["LeetCode"],"title":"LeetCode刷题总结","uri":"https://wzgl998877.github.io/2022/02/leetcode%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93/","year":"2022"},{"content":"[TOC]\n本篇文章是(Mysql是怎样运行的)阅读笔记，这本书网上很多人的评价都很高，看了下书也不是很厚，所以读读。\nMySQL的架构 mysql 工作的整体流程为，客户端进程向服务器进程发送一段文本（MySQL语句），服务器进程处理后再向客户端进程发送一段文本（处理结果）。如图：\n连接管理 ​\t客户端进程可以采用我们上边介绍的TCP/IP 、命名管道或共享内存、Unix域套接字这几种方式之一来与服务 器进程建立连接，每当有一个客户端进程连接到服务器进程时，服务器进程都会创建一个线程来专门处理与这个 客户端的交互，当该客户端退出时会与服务器断开连接，服务器并不会立即把与该客户端交互的线程销毁掉，而 是把它缓存起来，在另一个新的客户端再进行连接时，把这个缓存的线程分配给该新客户端。这样就起到了不频 繁创建和销毁线程的效果，从而节省开销。 ​\t当连接建立后，与该客户端关联的服务器线程会一直等待客户端发送过来的请求， MySQL 服务器接收到的请求只是一个文本消息，该文本消息还要经过各种处理，才能转化为mysql能识别的语句。\n解析与优化 ​\tMySQL 服务器获得了文本形式的请求后，接着 还要经过九九八十一难的处理，其中的几个比较重要的部分分别是查询缓存、语法解析和查询优化。\n查询缓存 ​\tMySQL 服务器程序处理查询请求时，会把刚刚处理过的查询请求和结果缓存起来，如果下一次有一模一样的请求过来，直接从缓存中查找结果就好了，就不用再傻呵呵的去底层的表中查找了。这个查询缓存可以在不同客户端之间共享，也就是说如果客户端A刚刚查询了一个语句，而客户端B之后发送了同样的查询请求，那么客户端B的这次查询就可以直接使用查询缓存中的数据。 ​\t当然， MySQL 服务器并没有人聪明，如果两个查询请求在任何字符上的不同（例如：空格、注释、大小写），都会导致缓存不会命中。另外，如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema、 performance_schema 数据库中的表，那这个请求就不会被缓存。以某些系统函数举例，可能同样的函数的两次调用会产生不一样的结果，比如函数NOW ，每次调用都会产生最新的当前时间，如果在一个查询请求中调用了这个函数，那即使查询请求的文本信息都一样，那不同时间的两次查询也应该得到不同的结果，如果在第一次查询时就缓存了，那第二次查询的时候直接使用第一次查询的结果就是错误的！ 不过既然是缓存，那就有它缓存失效的时候。\n​\tMySQL的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了INSERT 、 UPDATE 、DELETE 、TRUNCATE TABLE 、ALTER TABLE 、DROP TABLE 或 DROP DATABASE 语句，那使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除！\n 虽然查询缓存有时可以提升系统性能，但也不得不因维护这块缓存而造成一些开销，比如每次都要去查 询缓存中检索，查询请求处理完需要更新查询缓存，维护该查询缓存对应的内存区域。从MySQL 5.7.20 开始，不推荐使用查询缓存，并在MySQL 8.0中删除。\n 语法解析 ​\t如果查询缓存没有命中，接下来就需要进入正式的查询阶段了。因为客户端程序发送过来的请求只是一段文本而已，所以MySQL 服务器程序首先要对这段文本做分析，判断请求的语法是否正确，然后从文本中将要查询的表、各种查询条件都提取出来放到MySQL 服务器内部使用的一些数据结构上来。\n查询优化 ​\t语法解析之后，服务器程序获得到了需要的信息，比如要查询的列是哪些，表是哪个，搜索条件是什么等等，但光有这些是不够的，因为我们写的MySQL 语句执行起来效率可能并不是很高， MySQL 的优化程序会对我们的语句做一些优化，如外连接转换为内连接、表达式简化、子查询转为连接等。优化的结果就是生成一个执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的。我们可以使用EXPLAIN 语句来查看某个语句的执行计划\n存储引擎 ​\t截止到服务器程序完成了查询优化为止，还没有真正的去访问真实的数据表， MySQL 服务器把数据的存储和提取操作都封装到了一个叫存储引擎的模块里。我们知道表是由一行一行的记录组成的，但这只是一个逻辑上的概 念，物理上如何表示记录，怎么从表中读取数据，怎么把数据写入具体的物理存储器上，这都是存储引擎负责 的事情。为了实现不同的功能， MySQL 提供了各式各样的存储引擎，不同存储引擎管理的表具体的存储结构 可能不同，采用的存取算法也可能不同。\n​\t为了管理方便，人们把连接管理、查询缓存、语法解析、查询优化这些并不涉及真实数据存储的功能划分 为MySQL server 的功能，把真实存取数据的功能划分为存储引擎的功能。各种不同的存储引擎向上边的MySQL server 层提供统一的调用接口（也就是存储引擎API），包含了几十个底层函数，像\u0026quot;读取索引第一条内容\u0026quot;、\u0026ldquo;读 取索引下一条内容\u0026rdquo;、\u0026ldquo;插入记录\u0026quot;等等。\n​\t常用的存储引擎有：ARCHIVE、BLACKHOLE、InnoDB、MyISAM等，最常用的就是InnoDB 和MyISAM，其中InnoDB 是MySQL 默认的存储引擎。\n深入研究InnoDB ​\tInnoDB 是一个将表中的数据存储到磁盘上的存储引擎，所以即使关机后重启我们的数据还是存在的。而真正处 理数据的过程是发生在内存中的，所以需要把磁盘中的数据加载到内存中，如果是处理写入或修改请求的话，还 需要把内存中的内容刷新到磁盘上。而我们知道读写磁盘的速度非常慢，和内存读写差了几个数量级，所以当我 们想从表中获取某些记录时， InnoDB 存储引擎需要一条一条的把记录从磁盘上读出来么？不，那样会慢死， InnoDB 采取的方式是：将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小 一般为 16 KB。也就是在一般情况下，一次最少从磁盘中读取16KB的内容到内存中，一次最少把内存中的16KB 内容刷新到磁盘中\nInnoDB行格式 ​\t我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式也被称为行格式或者记录格式。 设计InnoDB 存储引擎的大叔们到现在为止设计了4种不同类型的行格式，分别是Compact 、Redundant 、 Dynamic 和Compressed 行格式。\nCompact ​\t从图中可以看出来，一条完整的记录其实可以被分为记录的额外信息和记录的真实数据两大部分，下边我 们详细看一下这两部分的组成。\n记录的额外信息 ​\t这部分信息是服务器为了描述这条记录而不得不额外添加的一些信息，这些额外信息分为3类，分别是变长字段 长度列表、NULL值列表和记录头信息，我们分别看一下。\n变长字段长度列表\n​\tMySQL 支持一些变长的数据类型，比如VARCHAR(M) 、VARBINARY(M) 、各种TEXT 类型，各种BLOB 类型，我们也可以把拥有这些数据类型的列称为变长字段，变长字段中存储多少字节的数据是不固定的，所以我 们在存储真实数据的时候需要顺便把这些数据占用的字节数也存起来，这样才不至于把MySQL 服务器搞懵，所以 这些变长字段占用的存储空间分为两部分：\n 真正的数据内容 占用的字节数  在Compact 行格式中，把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长 字段长度列表，各变长字段数据占用的字节数按照列的顺序逆序存放，我们再次强调一遍，是逆序存放！\n我们拿record_format_demo 表中的第一条记录来举个例子。因为record_format_demo 表的c1 、c2 、c4 列 都是VARCHAR(10) 类型的，也就是变长的数据类型，所以这三个列的值的长度都需要保存在记录开头处，因为 record_format_demo 表中的各个列都使用的是ascii 字符集，所以每个字符只需要1个字节来进行编码，来看 一下第一条记录各变长字段内容的长度：\n   存储内容 内容长度（十进制表示） 列名 内容长度（十六进制表示）     \u0026lsquo;aaaa\u0026rsquo; 4 c1 0x04   \u0026lsquo;bbb\u0026rsquo; 3 c2 0x03   \u0026rsquo;d' 1 c4 0x01    又因为这些长度值需要按照列的逆序存放，所以最后变长字段长度列表的字节串用十六进制表示的效果就是 （各个字节之间实际上没有空格，用空格隔开只是方便理解）： 01 03 04 把这个字节串组成的变长字段长度列表填入上边的示意图中的效果就是：\n由于第一行记录中c1 、c2 、c4 列中的字符串都比较短，也就是说内容占用的字节数比较小，用1个字节就可 以表示，但是如果变长列的内容占用的字节数比较多，可能就需要用2个字节来表示。具体用1个还是2个字节来 表示真实数据占用的字节数， InnoDB 有它的一套规则，我们首先声明一下W 、M 和L 的意思：\n 假设某个字符集中表示一个字符最多需要使用的字节数为W ，也就是使用SHOW CHARSET 语句的结果中的 Maxlen 列，比方说utf8 字符集中的W 就是3 ， gbk 字符集中的W 就是2 ， ascii 字符集中的W 就是1 。 对于变长类型VARCHAR(M) 来说，这种类型表示能存储最多M 个字符（注意是字符不是字节），所以这个类 型能表示的字符串最多占用的字节数就是M×W 。 假设它实际存储的字符串占用的字节数是L 。   所以确定使用1个字节还是2个字节表示真正字符串占用的字节数的规则就是这样： 如果M×W \u0026lt;= 255 ，那么使用1个字节来表示真正字符串占用的字节数，为什么呢？很简单因为一个字节由8位二进制构成而8位二进制最多也只能表示256这个数，（但还有一位是标志位所以就是255）。\n​\t如果M×W \u0026gt; 255 ，则分为两种情况： ​\t如果L \u0026lt;= 127 ，则用1个字节来表示真正字符串占用的字节数。 ​\t如果L \u0026gt; 127 ，则用2个字节来表示真正字符串占用的字节数。 InnoDB在读记录的变长字段长度列表时先查看表结构，如果某个变长字段允许存储的最大字节 数大于255时，该怎么区分它正在读的某个字节是一个单独的字段长度还是半个字段长度呢？ 设计InnoDB的大叔使用该字节的第一个二进制位作为标志位：如果该字节的第一个位为0，那 **该字节就是一个单独的字段长度（**使用一个字节表示不大于127的二进制的第一个位都为0）， 如果该字节的第一个位为1，那该字节就是半个字段长度。 对于一些占用字节数非常多的字段，比方说某个字段长度大于了16KB，那么如果该记录在单个 页面中无法存储时，InnoDB会把一部分数据存放到所谓的溢出页中（我们后边会唠叨），在变 长字段长度列表处只存储留在本页面中的长度，所以使用两个字节也可以存放下来。\n 总结一下就是说：如果该可变字段允许存储的最大字节数（ M×W ）超过255字节并且真实存储的字节数（ L ） 超过127字节，则使用2个字节，否则使用1个字节。\nNULL值列表\n​\t我们知道表中的某些列可能存储NULL 值，如果把这些NULL 值都放到记录的真实数据中存储会很占地方，所 以Compact 行格式把这些值为NULL 的列统一管理起来，存储到NULL 值列表中，它的处理过程是这样的\n 首先统计表中允许存储NULL 的列有哪些。 我们前边说过，主键列、被NOT NULL 修饰的列都是不可以存储NULL 值的，所以在统计的时候不会把这些列 算进去。比方说表record_format_demo 的3个列c1 、c3 、c4 都是允许存储NULL 值的，而c2 列是被 NOT NULL 修饰，不允许存储NULL 值。 如果表中没有允许存储 NULL 的列，则 NULL值列表 也不存在了，否则将每个允许存储NULL 的列对应一个 二进制位，二进制位按照列的顺序逆序排列，二进制位表示的意义如下： 二进制位的值为1 时，代表该列的值为NULL 。 二进制位的值为0 时，代表该列的值不为NULL 。 再一次强调，二进制位按照列的顺序逆序排列，所以第一个列c1 和最后一个二进制位对应。：  MySQL 规定NULL值列表必须用整数个字节的位表示，如果使用的二进制位个数不是整数个字节，则在字节 的高位补0 。表record_format_demo 只有3个值允许为NULL 的列，对应3个二进制位，不足一个字节，所以在字节的高位补0 ，效果就是这样：  知道了规则后，再看看null值列表应该怎么存储，因为只有c1 、c3 、c4 都是允许存储NULL 值的所以只需要一个字节。\n对于第一条记录来说， c1 、c3 、c4 这3个列的值都不为NULL ，所以它们对应的二进制位都是0 ，用十六进制表示就是0x00 对于第二条记录来说， c1 、c3 、c4 这3个列中c3 和c4 的值都为NULL ，用十六进制表示就是： 0x06 。所以添加后为\n记录头信息\n​\t除了变长字段长度列表、NULL值列表之外，还有一个用于描述记录的记录头信息，它是由固定的5 个字节组 成。5 个字节也就是40 个二进制位，不同的位代表不同的意思\n记录的真实数据 ​\t对于record_format_demo 表来说， 记录的真实数据除了c1 、c2 、c3 、c4 这几个我们自己定义的列的数据 以外， MySQL 会为每个记录默认的添加一些列（也称为隐藏列），具体的列如下： 实际上这几个列的真正名称其实是：DB_ROW_ID （行ID唯一标识一条记录）、DB_TRX_ID（事务ID）、DB_ROLL_PTR（回滚指针）。 这里需要提一下InnoDB 表对主键的生成策略：优先使用用户自定义主键作为主键，如果用户没有定义主键，则 选取一个Unique 键作为主键，如果表中连Unique 键都没有定义的话，则InnoDB 会为表默认添加一个名为 row_id 的隐藏列作为主键。所以我们从上表中可以看出：InnoDB存储引擎会为每条记录都添加 transaction_id 和 roll_pointer 这两个列，但是 row_id 是可选的（在没有自定义主键以及Unique键的情况下才会添加该列）。 这些隐藏列的值不用我们操心， InnoDB 存储引擎会自己帮我们生成的。加上记录的真实数据的两个记录为：\n看这个图的时候我们需要注意几点：\n 表record_format_demo 使用的是ascii 字符集，所以0x61616161 就表示字符串\u0026rsquo;aaaa' ， 0x626262 就表 示字符串\u0026rsquo;bbb' ，以此类推。 注意第1条记录中c3 列的值，它是CHAR(10) 类型的，它实际存储的字符串是： \u0026lsquo;cc\u0026rsquo; ，而ascii 字符集中 的字节表示是'0x6363' ，虽然表示这个字符串只占用了2个字节，但整个c3 列仍然占用了10个字节的空 间，除真实数据以外的8个字节的统统都用空格字符填充，空格字符在ascii 字符集的表示就是0x20 。 注意第2条记录中c3 和c4 列的值都为NULL ，它们被存储在了前边的NULL值列表处，在记录的真实数据处 就不再冗余存储，从而节省存储空间。  对于 CHAR(M) 类型的列来说，当列采用的是定长字符集时，该列占用的字节数不会被加到变长字段长度列表，而如果采用变长字符集时，该列占用的字节数也会被加到变长字段长度列表。\n​\t另外有一点还需要注意，变长字符集的CHAR(M) 类型的列要求至少占用M 个字节，而VARCHAR(M) 却没有这个要求。比方说对于使用utf8 字符集的CHAR(10) 的列来说，该列存储的数据字节长度的范围是10～30个字节。即 使我们向该列中存储一个空字符串也会占用10 个字节，这是怕将来更新该列的值的字节长度大于原有值的字节 长度而小于10个字节时，可以在该记录处直接更新，而不是在存储空间中重新分配一个新的记录空间，导致原有 的记录空间成为所谓的碎片。\n行溢出数据 ​\t我们知道对于VARCHAR(M) 类型的列最多可以占用65535 个字节。其中的M 代表该类型最多存储的字符数量，如果我们使用ascii 字符集的话，一个字符就代表一个字节。为什么最多是65535呢？很简单，因为前面讲过的\n如果该可变字段允许存储的最大字节数（ M×W ）超过255字节并且真实存储的字节数（ L ） 超过127字节，则使用2个字节，否则使用1个字节。也就是说一个可变字段允许存储的最大字节数的长度最多只能用2个字节存储，而两个字节能表示的最大长度就是256*256=65536然后需要减去一个标志位。\n​\tMySQL 对一条记录占用的最大存储空间是有限制的，除了BLOB 或者TEXT 类型的列之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535 个字节。所以MySQL 服务器建议我们把存储类型改为TEXT 或者BLOB 的类型。这个65535 个字节除了列本身的数据之外，还包括一些其他的数据（ storage overhead ），比如说我们为了存储一个VARCHAR(M) 类型的列，其实需要占用3部分存储空间：\n 真实数据 真实数据占用字节的长度(变长字段长度列表最多为两个字节) NULL 值标识，如果该列有NOT NULL 属性则可以没有这部分存储空间（NULL值列表，一个字节）   因此，如果该VARCHAR(M)类型的列没有NOT NULL 属性，那最多只能存储65532 个字节的数据（65535-2-1），而根据字符集的不同，最大能存储的字符数也不同，比如gbk 字符集表示一个字符最多需要2 个字 节，那在该字符集下， M 的最大取值就是32766 （也就是：65532/2），也就是说最多能存储32766 个字符；utf8 字符集表示一个字符最多需要3 个字节，那在该字符集下， M 的最大取值就是21844 ，就是说最多能存储21844 （也就是：65532/3）个字符。\n 记录中的数据太多产生的溢出 ​\tMySQL 中磁盘和内存交互的基本单位是页，也就是说MySQL 是以页为基本单位来管理存储空间的，我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB ，也就是16384 字节，而一个VARCHAR(M) 类 型的列就最多可以存储65532 个字节，这样就可能造成一个页存放不了一条记录的尴尬情况\n​\t对于Compact 和Reduntant 行格式来说，如果某一列中的数据非常多的话，在本记录的真实数据处只会存储该列的前768 个字节的数据和一个指向其他页的地址，然后把剩下的数据存放到其他页中，这个过程也叫做行溢出，存储超出768 字节的那些页面也被称为溢出页\n 不只是 VARCHAR(M) 类型的列，其他的 TEXT、BLOB 类型的列在存储数据非常多的时候也会发生行溢出。\n 总结：对于Compact 来说，所占用的额外信息最多为27 字节：\n  2个字节用于存储真实数据的长度（最多）\n  1个字节用于存储列是否是NULL值（如果该列有NOT NULL 属性则可以没有这部分存储空间）\n  5个字节大小的头信息\n  6个字节的row_id 列（在没有自定义主键以及Unique键的情况下）\n  6个字节的transaction_id 列\n  7个字节的roll_pointer 列\n  Dynamic和Compressed行格式 ​\tDynamic 和Compressed 行格式 ，这俩行格式和Compact 行格式挺像，只不过在处理行溢出数据时有点儿分歧，它们不会在记录的真实数据处存储字段真实数据的前768 个字节，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址，就像这样：\n其中mysql5.6 默认使用 Compact，mysql 5.7 默认使用Dynamic\nInnoDB数据页结构 ​\t页是InnoDB 管理存储空间的基本单位，一个页的大小一般是16KB 。InnoDB 为了不同的目的而设计了许多种不同类型的页，比如存放表空间头部信息的页，存放Insert Buffer信息的页，存放INODE 信息的页，存放undo 日志信息的页等。我们聚焦的是那些存放我们表中记录的那种类型的页，官方称这种存放记录的页为索引（ INDEX ）页。数据页代表的这块16KB 大小的存储空间可以被划分为多个部分，不同部分有不同的功能，各个部分如图所示：\n记录在页中的存储 ​\t在页的7个组成部分中，我们自己存储的记录会按照我们指定的行格式存储到User Records 部分。但是在一开 始生成页的时候，其实并没有User Records 这个部分，每当我们插入一条记录，都会从Free Space 部分，也就 是尚未使用的存储空间中申请一个记录大小的空间划分到User Records 部分，当Free Space 部分的空间全部 被User Records 部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新 的页了，这个过程的图示如下：\n为了更好的管理在User Records 中的这些记录， InnoDB 可费了一番力气呢，在哪费力气了呢？不就是把记录按 照指定的行格式一条一条摆在User Records 部分么？其实这话还得从记录行格式的记录头信息中说起。\n记录头信息的秘密 由上图可以看出记录头信息中共有5个字节的数据，记录头信息中各个属性的大体意思为（基于Compact 行格式）：\n   名称 大小（单位：bit 位） 描述     预留位1 1 没有使用   预留位2 1 没有使用   delete_mask 1 标记该记录是否被删除   min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记   n_owned 4 表示当前记录拥有的记录数   heap_no 13 表示当前记录在记录堆的位置信息   record_type 3 表示当前记录的类型， 0 表示普通记录， 1 表示B+树非叶节点记录， 2 表示最小记录， 3表示最大记录   next_record 16 表示下一条记录的相对位置    下面，根据几个实例一起分析下\n  delete_mask 这个属性标记着当前记录是否被删除，占用1个二进制位，值为0 的时候代表记录并没有被删除，为1 的时 候代表记录被删除掉了。这些被删除的记录之所以不立即从磁盘上移除，是因为移除它们之后把其他的记录在磁盘上重新排列需要性能消耗，所以只是打一个删除标记而已，所有被删除掉的记录都会组成一个所谓的垃圾链表，在这个链表中的记录占用的空间称之为所谓的可重用空间，之后如果有新记录插入到表中的话，可能把这些被删除的记录占用的存储空间覆盖掉。\n  heap_no\n这个属性表示当前记录在本页中的位置，从图中可以看出来，我们插入的4条记录在本页中的位置分别 是： 2 、3 、4 、5 。是不是少了点啥？是的，怎么不见heap_no 值为0 和1 的记录呢？这其实是设计InnoDB 的大叔们玩的一个小把戏，他们自动给每个页里边儿加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为伪记录或者虚拟记录。这两个伪记录一个代表最小记录，一个代表最大记录，等一下哈~，记录可以比大小么？是的，记录也可以比大小，对于一条完整的记录来说，比较记录的大小就是比较主键的大小。比方说我们插入的4行记录的主键值分别是： 1 、2 、3 、4 ，这也就意味着这4条记录的大小从小到大依次递增。但是不管我们向页中插入了多少自己的记录，设计InnoDB 的大叔们都规定他们定义的两条伪记录分别为最小记录与最大记录。这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的，如图所示\n  ​\t由于这两条记录不是我们自己定义的记录，所以它们并不存放在页的User Records 部分，他们被单独放在 ​\t一个称为Infimum + Supremum 的部分。\n  next_record\n它表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量。比方说第一条记录的next_record 值为32 ，意味着从第一条记录的真实数据的地址处向后找32 个字节便是下一条记录的真实数据。如果你熟悉数据结构的话，就立即明白了，这其实是个链表，可以通过一条记录找到它的下一条记录。但是需要注意注意再注意的一点是， 下一条记录指得并不是按照我们插入顺序的下一条记录，而是按照主键值由小到大的顺序的下一条记录。而且规定 Infimum记录（也就是最小记录） 的下一条记录就是本页中主键值最小的用户记录，而本页中主键值最大的用户记录的下一条记录就是 Supremum记录（也就是最大记录） ，为了更形象的表示一下这个next_record 起到的作用，我们用箭头来替代一下next_record 中的地址偏移量\n  从图中可以看出来，我们的记录按照主键从小到大的顺序形成了一个单链表。最大记录的next_record 的 值为0 ，这也就是说最大记录是没有下一条记录了，它是这个单链表中的最后一个节点。如果从中删除掉 一条记录，这个链表也是会跟着变化的，比如我们把第2条记录删掉：\n从图中可以看出来，删除第2条记录前后主要发生了这些变化：\n  第2条记录并没有从存储空间中移除，而是把该条记录的delete_mask 值设置为1\n  第2条记录的next_record 值变为了0，意味着该记录没有下一条记录了。\n  第1条记录的next_record 指向了第3条记录。\n  还有一点你可能忽略了，就是最大记录的n_owned 值从5 变成了4 ，关于这一点的变化我们稍后会详细说明的。所以，不论我们怎么对页中的记录做增删改操作，InnoDB始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。\n  Page Directory（页目录） 现在我们了解了记录在页中按照主键值由小到大顺序串联成一个单链表，那如果我们想根据主键值查找页中的某 条记录该咋办呢？比如说这样的查询语句： SELECT * FROM page_demo WHERE c1 = 3;\n我们平常想从一本书中查找某个内容的时候，一般会先看目录，找到需要查找的内容对应的书的页码，然后到对 应的页码查看内容。设计InnoDB 的大叔们为我们的记录也制作了一个类似的目录，他们的制作过程是这样的：\n 将所有正常的记录（包括最大和最小记录，不包括标记为已删除的记录）划分为几个组。 每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的n_owned 属性表示该记录拥有多少条记 录，也就是该组内共有几条记录。 将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页的尾部的地方，这个地方就是所 谓的Page Directory ，也就是页目录（此时应该返回头看看页面各个部分的图）。页面目录中的这些地址 偏移量被称为槽（英文名： Slot ），所以这个页面目录就是由槽组成的。  比方说现在的page_demo 表中正常的记录共有6条， InnoDB 会把它们分成两组，第一组中只有一个最小记录，第二组中是剩余的5条记录，看下边的示意图：\n从这个图中我们需要注意这么几点：\n 现在页目录部分中有两个槽，也就意味着我们的记录被分成了两个组， 槽1 中的值是112 ，代表最大记录 的地址偏移量（就是从页面的0字节开始数，数112个字节）； 槽0 中的值是99 ，代表最小记录的地址偏移 量。 注意最小和最大记录的头信息中的n_owned 属性  最小记录的n_owned 值为1 ，这就代表着以最小记录结尾的这个分组中只有1 条记录，也就是最小记录 本身。 最大记录的n_owned 值为5 ，这就代表着以最大记录结尾的这个分组中只有5 条记录，包括最大记录本 身还有我们自己插入的4 条记录。    用图表示就是：\n为什么最小记录的n_owned 值为1，而最大记录的n_owned 值为5 呢，这里头有什么猫腻么？ 是的，设计InnoDB 的大叔们对每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录， 最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。 所以分组是按照下边的步骤进行的：\n 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对 应的记录的n_owned 值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一 个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。  了解了页目录的组成后，那么怎么就加快查找速度的过程呢？看下这个例子\n比方说我们想找主键值为6 的记录，过程是这样的：\n 计算中间槽的位置： (0+4)/2=2 ，所以查看槽2 对应记录的主键值为8 ，又因为8 \u0026gt; 6 ，所以设置 high=2 ， low 保持不变。 重新计算中间槽的位置： (0+2)/2=1 ，所以查看槽1 对应的主键值为4 ，又因为4 \u0026lt; 6 ，所以设置 low=1 ， high 保持不变。 因为high - low 的值为1，所以确定主键值为5 的记录在槽2 对应的组中。此刻我们需要找到槽2 中主键 值最小的那条记录，然后沿着单向链表遍历槽2 中的记录。但是我们前边又说过，每个槽对应的记录都是该 组中主键值最大的记录，这里槽2 对应的记录是主键值为8 的记录，怎么定位一个组中最小的记录呢？别忘 了各个槽都是挨着的，我们可以很轻易的拿到槽1 对应的记录（主键值为4 ），该条记录的下一条记录就 是槽2 中主键值最小的记录，该记录的主键值为5 。所以我们可以从这条主键值为5 的记录出发，遍历槽 2 中的各条记录，直到找到主键值为6 的那条记录即可。由于一个组中包含的记录条数只能是1~8条，所以 遍历一个组中的记录的代价是很小的。  这就是典型的二分法哈哈，总结：\n 通过二分法确定该记录所在的槽，并找到该槽中主键值最小的那条记录。 通过记录的next_record 属性遍历该槽所在的组中的各个记录。  这个就是索引为什么这么快的理由吗？？？\nPage Header（页面头部） 设计InnoDB 的大叔们为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第 一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header 的部分，它是 页结构的第二部分，这个部分占用固定的56 个字节，专门存储各种状态信息，具体各个字节都是干嘛的看下\n   名称 占用空间大小 描述     PAGE_N_DIR_SLOTS 2 字节 在页目录中的槽数量   PAGE_HEAP_TOP 2 字节 还未使用的空间最小地址，也就是说从该地址之后就是Free Space   PAGE_N_HEAP 2 字节 本页中的记录的数量（包括最小和最大记录以及标记为删除的记录）   PAGE_FREE 2 字节 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record 也会组成一个单链表，这个单链表中的记录可以被重新利用）   PAGE_GARBAGE 2 字节 已删除记录占用的字节数   PAGE_LAST_INSERT 2 字节 最后插入记录的位置   PAGE_DIRECTION 2 字节 记录插入的方向   PAGE_N_DIRECTION 2 字节 一个方向连续插入的记录数量   PAGE_N_RECS 2 字节 该页中记录的数量（不包括最小和最大记录以及被标记为删除的记录）   PAGE_MAX_TRX_ID 8 字节 修改当前页的最大事务ID，该值仅在二级索引中定义   PAGE_LEVEL 2 字节 当前页在B+树中所处的层级   PAGE_INDEX_ID 8 字节 索引ID，表示当前页属于哪个索引   PAGE_BTR_SEG_LEAF 10 字节 B+树叶子段的头部信息，仅在B+树的Root页定义   PAGE_BTR_SEG_TOP 10 字节 B+树非叶子段的头部信息，仅在B+树的Root页定义     PAGE_DIRECTION 假如新插入的一条记录的主键值比上一条记录的主键值大，我们说这条记录的插入方向是右边，反之则是左 边。用来表示最后一条记录插入方向的状态就是PAGE_DIRECTION 。 PAGE_N_DIRECTION 假设连续几次插入新记录的方向都是一致的， InnoDB 会把沿着同一个方向插入记录的条数记下来，这个条 数就用PAGE_N_DIRECTION 这个状态表示。当然，如果最后一条记录的插入方向改变了的话，这个状态的值 会被清零重新统计。  File Header（文件头部） ​\tPage Header 是专门针对数据页记录的各种状态信息，比方说页里头有多少个记录了呀，有多少个槽了呀。我们现在描述的File Header 针对各种类型的页都通用，也就是说不同类型的页都会以File Header 作为第一个组成部分，它描述了一些针对各种页都通用的一些信息，比方说这个页的编号是多少，它的上一个页、下一个页 这个部分占用固定的38 个字节，是由下边这些内容组成的\n   名称 占用空间大小 描述     FIL_PAGE_SPACE_OR_CHKSUM 4 字节 页的校验和（checksum值）   FIL_PAGE_OFFSET 4 字节 页号，InnoDB 通过页号来可以唯一定位一个页   FIL_PAGE_PREV 4 字节 上一个页的页号   FIL_PAGE_NEXT 4 字节 下一个页的页号   FIL_PAGE_LSN 8 字节 页面被最后修改时对应的日志序列位置（英文名是：Log SequenceNumber）   FIL_PAGE_TYPE 2 字节 该页的类型   FIL_PAGE_FILE_FLUSH_LSN 8 字节 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值   FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4 字节 页属于哪个表空间    详解：\n  FIL_PAGE_SPACE_OR_CHKSUM\n这个代表当前页面的校验和（checksum）。啥是个校验和？就是对于一个很长很长的字节串来说，我们会 通过某种算法来计算一个比较短的值来代表这个很长的字节串，这个比较短的值就称为校验和。这样在比 较两个很长的字节串之前先比较这两个长字节串的校验和，如果校验和都不一样两个长字节串肯定是不同 的，所以省去了直接比较两个比较长的字节串的时间损耗。\n  FIL_PAGE_TYPE\n这个代表当前页的类型，我们前边说过， InnoDB 为了不同的目的而把页分为不同的类型，我们上边介绍的 其实都是存储记录的数据页，其实还有很多别的类型的页，我们存放记录的数据页的类型其实是FIL_PAGE_INDEX ，也就是所谓的索引页。\n  FIL_PAGE_PREV 和FIL_PAGE_NEXT\n我们前边强调过， InnoDB 都是以页为单位存放数据的，有时候我们存放某种类型的数据占用的空间非常大 （比方说一张表中可以有成千上万条记录）， InnoDB 可能不可以一次性为这么多数据分配一个非常大的存 储空间，如果分散到多个不连续的页中存储的话需要把这些页关联起来， FIL_PAGE_PREV 和FIL_PAGE_NEXT 就分别代表本页的上一个和下一个页的页号。这样通过建立一个双向链表把许许多多的页就都串联起来了， 而无需这些页在物理上真正连着\n  File Trailer 我们知道InnoDB 存储引擎会把数据存储到磁盘上，但是磁盘速度太慢，需要以页为单位把数据加载到内存中处 理，如果该页中的数据在内存中被修改了，那么在修改后的某个时间需要把数据同步到磁盘中。但是在同步了一 半的时候中断电了咋办，这不是莫名尴尬么？为了检测一个页是否完整（也就是在同步的时候有没有发生只同步 一半的尴尬情况），设计InnoDB 的大叔们在每个页的尾部都加了一个File Trailer 部分，这个部分由8 个字 节组成，可以分成2个小部分：\n 前4个字节代表页的校验和 这个部分是和File Header 中的校验和相对应的。每当一个页面在内存中修改了，在同步之前就要把它的校 验和算出来，因为File Header 在页面的前边，所以校验和会被首先同步到磁盘，当完全写完时，校验和也 会被写到页的尾部，如果完全同步成功，则页的首部和尾部的校验和应该是一致的。如果写了一半儿断电 了，那么在File Header 中的校验和就代表着已经修改过的页，而在File Trialer 中的校验和代表着原先 的页，二者不同则意味着同步中间出了错。 后4个字节代表页面被最后修改时对应的日志序列位置（LSN） 这个部分也是为了校验页的完整性的  这个File Trailer 与File Header 类似，都是所有类型的页通用的。\n总结  InnoDB为了不同的目的而设计了不同类型的页，我们把用于存放记录的页叫做数据页。 一个数据页可以被大致划分为7个部分，分别是   File Header ，表示页的一些通用信息，占固定的38字节。 Page Header ，表示数据页专有的一些信息，占固定的56个字节。 Infimum + Supremum ，两个虚拟的伪记录，分别表示页中的最小和最大记录，占固定的26 个字节。 User Records ：真实存储我们插入的记录的部分，大小不固定。 Free Space ：页中尚未使用的部分，大小不确定。 Page Directory ：页中的某些记录相对位置，也就是各个槽在页面中的地址偏移量，大小不固定，插 入的记录越多，这个部分占用的空间越多。 File Trailer ：用于检验页是否完整的部分，占用固定的8个字节。  每个记录的头信息中都有一个next_record 属性，从而使页中的所有记录串联成一个单链表。 InnoDB 会为把页中的记录划分为若干个组，每个组的最后一个记录的地址偏移量作为一个槽，存放在 Page Directory 中，所以在一个页中根据主键查找记录是非常快的，分为两步：   通过二分法确定该记录所在的槽。 通过记录的next_record属性遍历该槽所在的组中的各个记录。  每个数据页的File Header 部分都有上一个和下一个页的编号，所以所有的数据页会组成一个双链表。 为保证从内存中同步到磁盘的页的完整性，在页的首部和尾部都会存储页中数据的校验和和页面最后修改时 对应的LSN 值，如果首部和尾部的校验和和LSN 值校验不成功的话，就说明同步过程出现了问题。  深入研究B+树索引 前边我们详细唠叨了InnoDB 数据页的7个组成部分，知道了各个数据页可以组成一个双向链表，而每个数据页中的记录会按照主键值从小到大的顺序组成一个单向链表，每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。页和记录的关系示意图如下：\n没有索引的查找 我们先了解一下没有索引的时候是怎么查找记录的。为了方便大家理解，我们下边先只唠叨搜索条件为对某个列精确匹配的情况，所谓精确匹配，就是搜索条件中用等于= 连接起的表达式，比如这样：\nSELECT [列名列表] FROM 表名 WHERE 列名 = xxx;\r 在一个页中的查找 假设目前表中的记录比较少，所有的记录都可以被存放到一个页中，在查找记录的时候可以根据搜索条件的不同 分为两种情况：\n 以主键为搜索条件 这个查找过程我们已经很熟悉了，可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应 分组中的记录即可快速找到指定的记录。 以其他列作为搜索条件 对非主键列的查找的过程可就不这么幸运了，因为在数据页中并没有对非主键列建立所谓的页目录，所以 我们无法通过二分法快速定位相应的槽。这种情况下只能从最小记录开始依次遍历单链表中的每条记录， 然后对比每条记录是不是符合搜索条件。很显然，这种查找的效率是非常低的。  在很多页中查找 大部分情况下我们表中存放的记录都是非常多的，需要好多的数据页来存储这些记录。在很多页中查找记录的话 可以分为两个步骤：\n 定位到记录所在的页。 从所在的页内中查找相应的记录。  在没有索引的情况下，不论是根据主键列或者其他列的值进行查找，由于我们并不能快速的定位到记录所在的 页，所以只能从第一个页沿着双向链表一直往下找，在每一个页中根据我们刚刚唠叨过的查找方式去查找指定的 记录。因为要遍历所有的数据页，所以这种方式显然是超级耗时的。\n索引 新建一个index_demo 表，该表有2个INT 类型的列，1个CHAR(1) 类型的列，而且我们规定了c1 列为主键，这个表使用Compact 行格式来实际存储记录的。为了我们理解上的方便，我们简化了一下index_demo 表的行格式示 意图：\n把一些记录放到页里边的示意图就是：\n一个简单的索引方案 ​\t我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以不得不依次遍历所有的数据页\n​\t所以如果我们想快速的定位到需要查找的记录在哪些数据页中该咋办？还记得我们为根据主键值快速定位一条记录在页中的位置而设立的页目录么？我们也可以想办法为快速定位记录所在的数据页而建立一个别的目录，建这个目录必须完成下边这些事儿：\n  下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。\n我们这里需要做一个假设：假设我们的每个数据页最多能存放3条记录（实际上一个 数据页非常大，可以存放下好多记录）。有了这个假设之后我们向index_demo 表插入3条记录：\nmysql\u0026gt; INSERT INTO index_demo VALUES(1, 4, 'u'), (3, 9, 'd'), (5, 3, 'y');\r 那么这些记录已经按照主键值的大小串联成一个单向链表了，如图所示：\n  此时我们再来插入一条记录:\nINSERT INTO index_demo VALUES(4, 4, 'a');\r 因为页10 最多只能放3条记录，所以我们不得不再分配一个新页：\n新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着。它们只是通过维护着上一个页和下一个页的编号而建立了链表关系\n页10 中用户记录最大的主键值是5 ，而页28 中有一条记录的主键值是4 ，因为5\u0026gt;4 ，所以这就不符合下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值的要求，所以在插入主键值为4 的记录的时候需要伴随着一次记录移动，也就是把主键值为5 的记录移动到页28 中，然后再把主键值为4 的记录插入到页10 中，这个过程的示意图如下:\n这个过程表明了在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保 证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程 我们也可以称为页分裂。\n  给所有的页建立一个目录项\n由于数据页的编号可能并不是连续的，所以在向index_demo 表中插入许多条记录后，可能是这样的效果：\n  ​\t因为这些16KB 的页在物理存储上可能并不挨着，所以如果想从这么多页中根据主键值快速定位某些记录所 在的页，我们需要给它们做个目录，每个页对应一个目录项，每个目录项包括下边两个部分：\n 页的用户记录中最小的主键值，我们用key 来表示。 页号，我们用page_no 表示。  所以我们为上边几个页做好的目录就像这样子：\n以页28 为例，它对应目录项2 ，这个目录项中包含着该页的页号28 以及该页中用户记录的最小主键值5 。我们只需要把几个目录项在物理存储器上连续存储，比如把他们放到一个数组里，就可以实现根据主键值快速查找某条记录的功能了。比方说我们想找主键值为20 的记录，具体查找过程分两步：\n 先从目录项中根据二分法快速确定出主键值为20 的记录在目录项3 中（因为 12 \u0026lt; 20 \u0026lt; 209 ），它对应的页是页9 。 再根据前边说的在页中查找记录的方式去页9 中定位具体的记录。  至此，针对数据页做的简易目录就搞定了。不过忘了说了，这个目录有一个别名，称为索引\nInnoDB中的索引方案 上边之所以称为一个简易的索引方案，是因为我们为了在根据主键值进行查找时使用二分法快速定位具体的目录 项而假设所有目录项都可以在物理存储器上连续存储，但是这样做有几个问题：\n InnoDB 是使用页来作为管理存储空间的基本单位，也就是最多能保证16KB 的连续存储空间，而随着表中记录数量的增多，需要非常大的连续的存储空间才能把所有的目录项都放下，这对记录数量非常多的表是不现实的。 我们时常会对记录进行增删，假设我们把页28 中的记录都删除了， 页28 也就没有存在的必要了，那意味着目录项2 也就没有存在的必要了，这就需要把目录项2 后的目录项都向前移动一下，这种牵一发而动全身的设计不是什么好主意～  所以，设计InnoDB 的大叔们需要一种可以灵活管理所有目录项的方式。他们灵光乍现，忽然发现这些目录项其实长得跟我们的用户记录差不多，只不过目录项中的两个列是主键和页号而已，所以他们复用了之前存储用户记录的数据页来存储目录项，为了和用户记录做一下区分，我们把这些用来表示目录项的记录称为目录项记录。那InnoDB 怎么区分一条记录是普通的用户记录还是目录项记录呢？别忘了记录头信息里的 record_type 属性，它的各个取值代表的意思如下：\n 0 ：普通的用户记录 1 ：目录项记录 2 ：最小记录 3 ：最大记录 哈哈，原来这个值为1 的record_type 是这个意思呀，我们把前边使用到的目录项放到数据页中的样子就是这 样：  从图中可以看出来，我们新分配了一个编号为30 的页来专门存储目录项记录。这里再次强调一遍目录项记录 和普通的用户记录的不同点：\n 目录项记录的record_type 值是1，而普通用户记录的record_type 值是0。 目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，可能包含很多列，另外还有InnoDB 自己添加的隐藏列。 还记得我们之前在唠叨记录头信息的时候说过一个叫min_rec_mask 的属性么，只有在存储目录项记录的页中的主键值最小的目录项记录的min_rec_mask 值为1 ，其他别的记录的min_rec_mask 值都是0 。  除了上述几点外，这两者就没啥差别了，它们用的是一样的数据页，页的组成结构也是一样一样的（就是我们前边介绍过的7个部分），都会为主键值生成Page Directory （页目录），从而在按照主键值进行查找时可以使用二分法来加快查询速度。现在以查找主键为20 的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步:\n 先到存储目录项记录的页，也就是页30 中通过二分法快速定位到对应目录项，因为12 \u0026lt; 20 \u0026lt; 209 ，所以定位到对应的记录所在的页就是页9 。 再到存储用户记录的页9 中根据二分法快速定位到主键值为20 的用户记录  虽然说目录项记录中只存储主键值和对应的页号，比用户记录需要的存储空间小多了，但是不论怎么说一个页只有16KB 大小，能存放的目录项记录也是有限的，那如果表中的数据太多，以至于一个数据页不足以存放所有的目录项记录，该咋办呢？\n当然是再多整一个存储目录项记录的页喽～ 为了大家更好的理解新分配一个目录项记录页的过程，我们假设一个存储目录项记录的页最多只能存放4条目录项记录，所以如果此时我们再向上图中插入一条主键值为320 的用户记录的话，那就需要分配一个新的存储目录项记录的页喽：\n从图中可以看出，我们插入了一条主键值为320 的用户记录之后需要两个新的数据页：\n 为存储该用户记录而新生成了页31 。 因为原先存储目录项记录的页30 的容量已满（我们前边假设只能存储4条目录项记录），所以不得不需要一个新的页32 来存放页31 对应的目录项。  现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤，以查 找主键值为20 的记录为例：\n 确定目录项记录页 我们现在的存储目录项记录的页有两个，即页30 和页32 ，又因为页30 表示的目录项的主键值的范围是[1, 320) ， 页32 表示的目录项的主键值不小于320 ，所以主键值为20 的记录对应的目录项记录在页30中。 通过目录项记录页确定用户记录真实所在的页。 在真实存储用户记录的页中定位到具体的记录。  那么问题来了，在这个查询步骤的第1步中我们需要定位存储目录项记录的页，但是这些页在存储空间中也可能不挨着，如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个 存储目录项记录的页呢？其实也简单，为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子\n如图，我们生成了一个存储更高级目录项的页33 ，这个页中的两条记录分别代表页30 和页32 ，如果用户记录的主键值在[1, 320) 之间，则到页30 中查找更详细的目录项记录，如果主键值不小于320 的话，就到页32中查找更详细的目录项记录。随着表中记录的增加，这个目录的层级会继续增加，如果简化一下，那么我们可以用下边这个图来描述它:\n这他妈的就是B+树了！！！\n更多b+树问题参考博文:\n什么是B树\n什么是B+树\nB树和B+树的插入、删除图文详解\n不论是存放用户记录的数据页，还是存放目录项记录的数据页，我们都把它们存放到B+ 树这个数据结构中了，所以我们也称这些数据页为节点。从图中可以看出来，我们的实际用户记录其实都存放在B+树的最底层的节点上，这些节点也被称为叶子节点或叶节点，其余用来存放目录项的节点称为非叶子节点或者内节点，其中B+ 树最上边的那个节点也称为根节点。\n从图中可以看出来，一个B+ 树的节点其实可以分成好多层，设计InnoDB 的大叔们为了讨论方便，规定最下边的那层，也就是存放我们用户记录的那层为第0 层，之后依次往上加。之前的讨论我们做了一个非常极端的假设： 存放用户记录的页最多存放3条记录，存放目录项记录的页最多存放4条记录。其实真实环境中一个页存放的记录 数量是非常大的，假设，假设，假设所有存放用户记录的叶子节点代表的数据页可以存放100条用户记录，所有 存放目录项记录的内节点代表的数据页可以存放1000条目录项记录，那么：\n 如果B+ 树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放100 条记录。 如果B+ 树有2层，最多能存放1000×100=100000 条记录。 如果B+ 树有3层，最多能存放1000×1000×100=100000000 条记录。 如果B+ 树有4层，最多能存放1000×1000×1000×100=100000000000 条记录。  你的表里能存放100000000000 条记录么？所以一般情况下，我们用到的B+ 树都不会超过4层，那我们通过主键值去查找某条记录最多只需要做4个页面内的查找（查找3个目录项页和一个用户记录页），又因为在每个页面内有所谓的Page Directory （页目录），所以在页面内也可以通过二分法实现快速定位记录。\n聚簇索引 我们上边介绍的B+ 树本身就是一个目录，或者说本身就是一个索引。它有两个特点：\n 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义：   页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成 一个双向链表。  B+ 树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。  我们把具有这两种特性的B+ 树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL 语句中显式的使用INDEX 语句去创建（后边会介绍索引相关的语句），InnoDB 存储引擎会自动的为我们创建聚簇索引。另外有趣的一点是，在InnoDB 存储引擎中， 聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。\n二级索引 ​\t大家有木有发现，上边介绍的聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+ 树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件该咋办呢？难道只能从头到尾沿着链表依次遍历记录么？ 不，我们可以多建几棵B+ 树，不同的B+ 树中的数据采用不同的排序规则。比方说我们用c2 列的大小作为数据 页、页中记录的排序规则，再建一棵B+ 树，效果如下图所示：\n这个B+ 树与上边介绍的聚簇索引有几处不同：\n 使用记录c2 列的大小进行记录和页的排序，这包括三个方面的含义：  页内的记录是按照c2 列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2 列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2 列大小顺序排成一个双向链表。   B+ 树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。  所以如果我们现在想通过c2 列的值查找某些记录的话就可以使用我们刚刚建好的这个B+ 树了。以查找c2 列的值为4 的记录为例，查找过程如下：\n 确定目录项记录页根据根页面，也就是页44 ，可以快速定位到目录项记录所在的页为页42 （因为2 \u0026lt; 4 \u0026lt; 9 ）。 通过目录项记录页确定用户记录真实所在的页。 在页42 中可以快速定位到实际存储用户记录的页，但是由于c2 列并没有唯一性约束，所以c2 列值为4 的记录可能分布在多个数据页中，又因为2 \u0026lt; 4 ≤ 4 ，所以确定实际存储用户记录的页在页34 和页35 中。 在真实存储用户记录的页中定位到具体的记录。 到页34 和页35 中定位到具体的记录。 但是这个B+ 树的叶子节点中的记录只存储了c2 和c1 （也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。  我们根据这个以c2 列大小排序的B+ 树只能确定我们要查找记录的主键值，所以如果我们想根据c2 列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程也被称为回表。也就是根据c2 列的值查询一条完整的用户记录需要使用到2 棵B+ 树！！！\n因为这种按照非主键列建立的B+ 树需要一次回表操作才可以定位到完整的用户记录，所以这种B+ 树也被称为二级索引（英文名secondary index ），或者辅助索引。由于我们使用的是c2 列的大小作为B+ 树的排序规则，所以我们也称这个B+ 树为为c2列建立的索引。\n联合索引 我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+ 树按照c2 和c3 列的大小进行排序，这个包含两层含义：\n 先把各个记录和页按照c2 列进行排序。 在记录的c2 列相同的情况下，采用c3 列进行排序  如图所示，我们需要注意一下几点：\n 每条目录项记录都由c2 、c3 、页号这三个部分组成，各条记录先按照c2 列的值进行排序，如果记录的c2 列相同，则按照c3 列的值进行排序。 B+ 树叶子节点处的用户记录由c2 、c3 和主键c1 列组成。  千万要注意一点，以c2和c3列的大小为排序规则建立的B+树称为联合索引，本质上也是一个二级索引。它的意思与分别为c2和c3列分别建立索引的表述是不同的，不同点如下：\n 建立联合索引只会建立如上图一样的1棵B+ 树。 为c2和c3列分别建立索引会分别以c2 和c3 列的大小为排序规则建立2棵B+ 树。  B+树索引的注意事项 根页面万年不动窝 我们前边介绍B+ 树索引的时候，为了大家理解上的方便，先把存储用户记录的叶子节点都画出来，然后接着画 存储目录项记录的内节点，实际上B+ 树的形成过程是这样的：\n 每当为某个表创建一个B+ 树索引（聚簇索引不是人为创建的，默认就有）的时候，都会为这个索引创建一个根节点页面。最开始表中没有数据的时候，每个B+ 树索引对应的根节点中既没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点中。 当根节点中的可用空间用完时继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页，比如页a 中，然后对这个新页进行页分裂的操作，得到另一个新页，比如页b 。这时新插入的记录根据键值（也就是聚簇索引中的主键值，二级索引中对应的索引列的值）的大小就会被分配到页a 或者页b 中，而根节点便升级为存储目录项记录的页。  这个过程需要大家特别注意的是：一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建立一个索引，那么它的根节点的页号便会被记录到某个地方，然后凡是InnoDB 存储引擎需要用到这个索引的时候，都会从那个固定的地方取出根节点的页号，从而来访问这个索引。\n内节点中目录项记录的唯一性 我们知道B+ 树索引的内节点中目录项记录的内容是索引列 + 页号的搭配，但是这个搭配对于二级索引来说有点儿不严谨。还拿index_demo 表为例，假设这个表中的数据是这样的\n   c1 c2 c3     1 1 \u0026lsquo;u\u0026rsquo;   3 1 \u0026rsquo;d'   5 1 \u0026lsquo;y\u0026rsquo;   7 1 \u0026lsquo;a\u0026rsquo;    如果二级索引中目录项记录的内容只是索引列 + 页号的搭配的话，那么为c2 列建立索引后的B+ 树应该长这 样：\n如果我们想新插入一行记录，其中c1 、c2 、c3 的值分别是： 9 、1 、\u0026lsquo;c\u0026rsquo; ，那么在修改这个为c2 列建立的二级索引对应的B+ 树时便碰到了个大问题：由于页3 中存储的目录项记录是由c2列 + 页号的值构成的， 页3 中的两条目录项记录对应的c2 列的值都是1 ，而我们新插入的这条记录的c2 列的值也是1 ，那我们这条新插入的记录到底应该放到页4 中，还是应该放到页5 中啊？\n为了让新插入记录能找到自己在哪个页里，我们需要保证在B+树的同一层内节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的：\n 索引列的值 主键值 页号  也就是我们把主键值也添加到二级索引内节点中的目录项记录了，这样就能保证B+ 树每一层节点中各条目录项记录除页号这个字段外是唯一的，所以我们为c2 列建立二级索引后的示意图实际上应该是这样子的\n一个页面最少存储2条记录 ​\t我们前边说过一个B+树只需要很少的层级就可以轻松存储数亿条记录，查询速度杠杠的！这是因为B+树本质上就是一个大的多层级目录，每经过一个目录时都会过滤掉许多无效的子目录，直到最后访问到存储真实数据的目录。那如果一个大的目录中只存放一个子目录是个啥效果呢？那就是目录层级非常非常非常多，而且最后的那个存放真实数据的目录中只能存放一条记录。所以InnoDB 的一个数据页至少可以存放两条记录，这也是我们之前唠叨记录行格式的时候说过一个结论（我们当时依据这个结论推导了表中只有一个列时该列在不发生行溢出的情况下最多能存储多少字节，忘了的话回去看看吧）。\nMyISAM中的索引方案简单介绍 ​\t至此，我们介绍的都是InnoDB 存储引擎中的索引方案，我们有必要再简单介绍一下MyISAM 存储引擎中的索引方案。我们知道InnoDB 中索引即数据，也就是聚簇索引的那棵B+ 树的叶子节点中已经把所有完整的用户记录都包含了，而MyISAM 的索引方案虽然也使用树形结构，但是却将索引和数据分开存储：\n 将表中的记录按照记录的插入顺序单独存储在一个文件中，称之为数据文件。这个文件并不划分为若干个 数据页，有多少记录就往这个文件中塞多少记录就成了。我们可以通过行号而快速访问到一条记录。 MyISAM 记录也需要记录头信息来存储一些额外数据，我们以上边唠叨过的index_demo 表为例，看一下这个 表中的记录使用MyISAM 作为存储引擎在存储空间中的表示：  ​\t由于在插入数据的时候并没有刻意按照主键大小排序，所以我们并不能在这些数据上使用二分法进行查找。\n  使用MyISAM 存储引擎的表会把索引信息另外存储到一个称为索引文件的另一个文件中。MyISAM 会单独为 表的主键创建一个索引，只不过在索引的叶子节点中存储的不是完整的用户记录，而是主键值 + 行号的组 合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！ 这一点和InnoDB 是完全不相同的，在InnoDB 存储引擎中，我们只需要根据主键值对聚簇索引进行一次查 找就能找到对应的记录，而在MyISAM 中却需要进行一次回表操作，意味着MyISAM 中建立的索引相当于全 部都是二级索引\n  如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB 中的索引差不 多，不过在叶子节点处存储的是相应的列 + 行号。这些索引也全部都是二级索引\n  MySQL中创建和删除索引的语句 建表时\nCREATE TALBE 表名 (\r各种列的信息 ··· ,\r[KEY|INDEX] 索引名 (需要被索引的单个列或多个列)\r)\r 其中的KEY 和INDEX 是同义词，任意选用一个就可以\n添加索引\nALTER table tableName ADD INDEX indexName(columnName)\r 删除索引\nALTER TABLE 表名 DROP [INDEX|KEY] 索引名\r 索引名建议：以idx_ 为前缀，后边跟着需要建立索引的列名，多个列名之间用下划线_ 分隔开。\nB+树索引的使用  B+ 树索引总结:\n 每个索引都对应一棵B+ 树， B+ 树分为好多层，最下边一层是叶子节点，其余的是内节点。所有用户记录都存储在B+ 树的叶子节点，所有目录项记录都存储在内节点。 InnoDB 存储引擎会自动为主键（如果没有它会自动帮我们添加）建立聚簇索引，聚簇索引的叶子节点包含完整的用户记录。 我们可以为自己感兴趣的列建立二级索引， 二级索引的叶子节点包含的用户记录由索引列 + 主键组成，所以如果想通过二级索引来查找完整的用户记录的话，需要通过回表操作，也就是在通过二级索引找到主键值之后再到聚簇索引中查找完整的用户记录。 B+ 树中每层节点都是按照索引列值从小到大的顺序排序而组成了双向链表，而且每个页内的记录（不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序而形成了一个单链表。如果是联合索引的话，则页面和记录先按照联合索引前边的列排序，如果该列值相同，再按照联合索引后边的列排序。 通过索引查找记录是从B+ 树的根节点开始，一层一层向下搜索。由于每个页面都按照索引列的值建立了Page Directory （页目录），所以在这些页面中的查找非常快。  索引的代价   空间上的代价 这个是显而易见的，每建立一个索引都要为它建立一棵B+ 树，每一棵B+ 树的每一个节点都是一个数据页，一个页默认会占用16KB 的存储空间，一棵很大的B+ 树由许多数据页组成，那可是很大的一片存储空间呢。 时间上的代价 每次对表中的数据进行增、删、改操作时，都需要去修改各个B+ 树索引。而且我们讲过， B+ 树每层节点都是按照索引列的值从小到大的顺序排序而组成了双向链表。不论是叶子节点中的记录，还是内节点中的记录（也就是不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序而形成了一个单向链表。而 增、删、改操作可能会对节点和记录的排序造成破坏，所以存储引擎需要额外的时间进行一些记录移位，页面分裂、页面回收啥的操作来维护好节点和记录的排序。如果我们建了许多索引，每个索引对应的B+ 树都要进行相关的维护操作，这还能不给性能拖后腿么？  所以说，一个表上索引建的越多，就会占用越多的存储空间，在增删改记录的时候性能就越差。\nB+树索引适用的条件  下边我们将唠叨许多种让B+ 树索引发挥最大效能的技巧和注意事项，先创建一个person_info表，这个表是用来存储人的一些基本信息的：\nCREATE TABLE person_info(\rid INT NOT NULL auto_increment,\rname VARCHAR(100) NOT NULL,\rbirthday DATE NOT NULL,\rphone_number CHAR(11) NOT NULL,\rcountry varchar(100) NOT NULL,\rPRIMARY KEY (id),\rKEY idx_name_birthday_phone_number (name, birthday, phone_number)\r);\r 对于这个person_info 表我们需要注意两点：\n 表中的主键是id 列，它存储一个自动递增的整数。所以InnoDB 存储引擎会自动为id 列建立聚簇索引。 我们额外定义了一个二级索引idx_name_birthday_phone_number ，它是由3个列组成的联合索引。所以在这个索引对应的B+ 树的叶子节点处存储的用户记录只保留name 、birthday 、phone_number 这三个列的值以及主键id 的值，并不会保存country 列的值。  person_info 表会为聚簇索引和idx_name_birthday_phone_number 索引建立2棵B+ 树。下边我们画一下索引idx_name_birthday_phone_number 的示意图，不过既然我们已经掌握了InnoDB 的B+ 树索引原理，那我们在画图的时候为了让图更加清晰，所以在省略一些不必要的部分，比如记录的额外信息，各页面的页号等等，其中内节点中目录项记录的页号信息我们用箭头来代替，在记录结构中只保留name 、birthday 、phone_number 、id 这四个列的真实数据值\n从图中可以看出这个idx_name_birthday_phone_number 索引对应的B+ 树中页面和记录的排序方式就是这的：\n 先按照name 列的值进行排序。 如果name 列的值相同，则按照birthday 列的值进行排序。 如果birthday 列的值也相同，则按照phone_number 的值进行排序  全值匹配  如果我们的搜索条件中的列和索引列一致的话，这种情况就称为全值匹配,例如：\nSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27' AND phone_number = '15123983239';\r 这个毫无疑问会走索引，但WHERE 子句中的几个搜索条件的顺序对查询结果有啥影响么？也就是说如果我们调换name 、birthday 、phone_number 这几个搜索列的顺序对查询的执行过程有影响么？例如：\nSELECT * FROM person_info WHERE birthday = '1990-09-27' AND phone_number = '15123983239' AND name = 'Ashburn';\r 答案是：没影响哈。MySQL 有查询优化器，会分析这些搜索条件并且按照可以使用的索引中列的顺序来决定先使用哪个搜索条件，后使用哪个搜索条件。\n匹配左边的列  其实在我们的搜索语句中也可以不用包含全部联合索引中的列，只包含左边的就行，比方说下边的查询语句：\nSELECT * FROM person_info WHERE name = 'Ashburn';\r或者包含多个左边的列也行：\rSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday = '1990-09-27';\r 那这条查询语句能用到索引吗？\nSELECT * FROM person_info WHERE birthday = '1990-09-27';\r 答案是用不到，因为B+ 树的数据页和记录先是按照name 列的值排序的，在name 列的值相同的情况下才使用birthday 列进行排序，也就是说name 列的值不同的记录中birthday 的值可能是无序的需要特别注意的一点是，如果我们想使用联合索引中尽可能多的列，搜索条件中的各个列必须是联合索引中从最左边开始连续的列。比方说联合索引idx_name_birthday_phone_number 中列的定义顺序是name 、birthday 、phone_number ，如果我们的搜索条件中只有name 和phone_number ，而没有中间的birthday ，比方说这样：\nSELECT * FROM person_info WHERE name = 'Ashburn' AND phone_number = '15123983239';\r 这样只能用到name 列的索引， birthday 和phone_number 的索引就用不上了，因为name 值相同的记录先按照birthday 的值进行排序， birthday 值相同的记录才按照phone_number 值进行排序\n匹配列前缀  为某个列建立索引的意思其实就是在对应的B+ 树的记录中使用该列的值进行排序，比方说person_info 表上建立的联合索引idx_name_birthday_phone_number 会先用name 列的值进行排序，字符串排序使用的当然就是字典序，也就是说这些字符串的前n个字符，也就是前缀都是排好序的，所以对于字符串类型的索引列来说，我们只匹配它的前缀也是可以快速定位记录的，例如：\n走索引 SELECT * FROM person_info WHERE name LIKE 'As%';\r不走索引 SELECT * FROM person_info WHERE name LIKE '%As%';\r 匹配范围值  idx_name_birthday_phone_number 索引的B+ 树示意图，所有记录都是按照索引列的值从小到大的顺序排好序的，所以这极大的方便我们查找索引列的值在某个范围内的记录。比方说下边这个查询语句\nSELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow';\r 由于B+ 树中的数据页和记录是先按name 列排序的，所以我们上边的查询过程其实是这样的：\n 找到name 值为Asa 的记录(查找到范围的下限)。 遍历链表找到name 值为Barlow 的记录（查找到范围的上限）由于所有记录都是由链表连起来的（记录之间用单链表，数据页之间用双链表） 找到这些记录的主键值，再到聚簇索引中回表查找完整的记录。  注意\n如果对多个列同时进行范围查找的话，只有对索引最左边的那个列进行范围查找的时候才能用到B+ 树索引，\nSELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow' AND birthday \u0026gt; '1980-01-01';\r 上边这个查询可以分成两个部分：\n 通过条件name \u0026gt; \u0026lsquo;Asa\u0026rsquo; AND name \u0026lt; \u0026lsquo;Barlow\u0026rsquo; 来对name 进行范围，查找的结果可能有多条name 值不同的 记录 对这些name 值不同的记录继续通过birthday \u0026gt; \u0026lsquo;1980-01-01\u0026rsquo; 条件继续过滤。 这样子对于联合索引idx_name_birthday_phone_number 来说，只能用到name 列的部分，而用不到birthday 列的部分，因为只有name 值相同的情况下才能用birthday 列的值进行排序，而这个查询中通过name 进行范围查找的记录中可能并不是按照birthday 列进行排序的，所以在搜索条件中继续以birthday 列进行查找时是用不到这个B+ 树索引的。  而与上面相反的是，精确匹配某一列并范围匹配另外一列,如\nSELECT * FROM person_info WHERE name = 'Ashburn' AND birthday \u0026gt; '1980-01-01' AND birthday \u0026lt; '2000-12-31' AND phone_number \u0026gt; '15100000000';\r 这个查询的条件可以分为3个部分：\n name = \u0026lsquo;Ashburn\u0026rsquo; ，对name 列进行精确查找，当然可以使用B+ 树索引了。 birthday \u0026gt; \u0026lsquo;1980-01-01\u0026rsquo; AND birthday \u0026lt; \u0026lsquo;2000-12-31\u0026rsquo; ，由于name 列是精确查找，所以通过name =\u0026lsquo;Ashburn\u0026rsquo; 条件查找后得到的结果的name 值都是相同的，它们会再按照birthday 的值进行排序。所以此时对birthday 列进行范围查找是可以用到B+ 树索引的。 phone_number \u0026gt; \u0026lsquo;15100000000\u0026rsquo; ，通过birthday 的范围查找的记录的birthday 的值可能不同，所以这个条件无法再利用B+ 树索引了，只能遍历上一步查询得到的记录。  用于排序  我们在写查询语句的时候经常需要对查询出来的记录通过ORDER BY 子句按照某种规则进行排序。一般情况下，我们只能把记录都加载到内存中，再用一些排序算法，比如快速排序、归并排序等等在内存中对这些记录进行排序，有的时候可能查询的结果集太大以至于不能在内存中进行排序的话，还可能暂时借助磁盘的空间来存放中间结果，排序操作完成后再把排好序的结果集返回到客户端。在MySQL 中，把这种在内存中或者磁盘上进行排序的方式统称为文件排序（英文名： filesort ），文件排序是很慢的。但是如果ORDER BY 子句里使用到了我们的索引列，就有可能省去在内存或文件中排序的步骤，比如下边这个简单的查询语句\nSELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10;\r 因为有idx_name_birthday_phone_number 索引，所以直接从索引中提取数据，然后进行回表操作取出所有数据\n对于联合索引有个问题需要注意， ORDER BY 的子句后边的列的顺序也必须按照索引列的顺序给出，否则也不能使用索引，同理， ORDER BY name 、ORDER BY name, birthday 这种匹配索引左边的列的形式可以使用部分的B+ 树索引。当联合索引左边列的值为常量，也可以使用后边的列进行排序，比如这样\nSELECT * FROM person_info WHERE name = 'A' ORDER BY birthday, phone_number LIMIT 10;\r 不可以使用索引进行排序的几种情况  ASC、DESC混用 对于使用联合索引进行排序的场景，我们要求各个排序列的排序顺序是一致的，也就是要么各个列都是ASC 规则 排序，要么都是DESC 规则排序。\n如果查询中的各个排序列的排序顺序是一致的，比方说下边这两种情况：\n ORDER BY name, birthday LIMIT 10 这种情况直接从索引的最左边开始往右读10行记录就可以了。 ORDER BY name DESC, birthday DESC LIMIT 10 ， 这种情况直接从索引的最右边开始往左读10行记录就可以了。  但是是先按照name 列进行升序排列，再按照birthday 列进行降序排列的话，比如说这样的查询语句：\nSELECT * FROM person_info ORDER BY name, birthday DESC LIMIT 10;\r 就不能走索引\nWHERE子句中出现非排序使用到的索引列 SELECT * FROM person_info WHERE country = 'China' ORDER BY name LIMIT 10;\r 这个查询只能先把符合搜索条件country = \u0026lsquo;China\u0026rsquo; 的记录提取出来后再进行排序，使用不到索引。\nSELECT * FROM person_info WHERE name = 'A' ORDER BY birthday, phone_number LIMIT 10;\r 虽然这个查询也有搜索条件，但是name = \u0026lsquo;A\u0026rsquo; 可以使用到索引idx_name_birthday_phone_number ，而且过滤剩 下的记录还是按照birthday 、phone_number 列排序的，所以还是可以使用索引进行排序的\n排序列包含非同一个索引的列 有时候用来排序的多个列不是一个索引里的，这种情况也不能使用索引进行排序，比方说：\nSELECT * FROM person_info ORDER BY name, country LIMIT 10;\r name 和country (即使country为索引列也不行)并不属于一个联合索引中的列，所以无法使用索引进行排序\n排序列使用了复杂的表达式 SELECT * FROM person_info ORDER BY UPPER(name) LIMIT 10;\r 使用了UPPER 函数修饰过的列就不是单独的列了，这样就无法使用索引进行排序。\n用于分组 SELECT name, birthday, phone_number, COUNT(*) FROM person_info GROUP BY name, birthday, phone_number\r 和使用B+ 树索引进行排序使用规则相同，分组列的顺序也需要和索引列的顺序一致，也可以只使用索引列中左边的列进行分组等\n回表的代价  SELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow';\r 在使用idx_name_birthday_phone_number 索引进行查询时大致可以分为这两个步骤：\n 从索引idx_name_birthday_phone_number 对应的B+ 树中取出name 值在Asa ～ Barlow 之间的用户记录。 由于索引idx_name_birthday_phone_number 对应的B+ 树用户记录中只包含name 、birthday 、 phone_number 、id 这4个字段，而查询列表是* ，意味着要查询表中所有字段，也就是还要包括country 字段。这时需要把从上一步中获取到的每一条记录的id 字段都到聚簇索引对应的B+ 树中找到完整的用户记录，也就是我们通常所说的回表，然后把完整的用户记录返回给查询用户。  由于索引idx_name_birthday_phone_number 对应的B+ 树中的记录首先会按照name 列的值进行排序，所以值在Asa ～ Barlow 之间的记录在磁盘中的存储是相连的，集中分布在一个或几个数据页中，我们可以很快的把这些连着的记录从磁盘中读出来，这种读取方式我们也可以称为顺序I/O。根据第1步中获取到的记录的id 字段的值可能并不相连，而在聚簇索引中记录是根据id （也就是主键）的顺序排列的，所以根据这些并不连续的id值到聚簇索引中访问完整的用户记录可能分布在不同的数据页中，这样读取完整的用户记录可能要访问更多的数据页，这种读取方式我们也可以称为随机I/O 。一般情况下，顺序I/O比随机I/O的性能高很多。所以这个使用索引idx_name_birthday_phone_number 的查询有这么两个特点：\n 会使用到两个B+ 树索引，一个二级索引，一个聚簇索引。 访问二级索引使用顺序I/O ，访问聚簇索引使用随机I/O 。  需要回表的记录越多，使用二级索引的性能就越低，甚至让某些查询宁愿使用全表扫描也不使用二级索引。比方说name 值在Asa ～ Barlow 之间的用户记录数量占全部记录数量90%以上，那么如果使用idx_name_birthday_phone_number 索引的话，有90%多的id 值需要回表，还不如直接去扫描聚簇索引（也就是全表扫描）。\n那什么时候采用全表扫描的方式，什么时候使用采用二级索引 + 回表的方式去执行查询呢？\n查询优化器会事先对表中的记录计算一些统计数据，然后再利用这些统计数据根据查询的条件来计算一下需要回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用二级索引 + 回表的方式。\n一般情况下，限制查询获取较少的记录数会让优化器更倾向于选择使用二级索引 + 回表的方式进行查询，因为回表的记录越少，性能提升就越高，比方说上边的查询可以改写成这样：\nSELECT * FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlow' LIMIT 10;\r同样排序也可以：SELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10;\r 覆盖索引 为了彻底告别回表操作带来的性能损耗，最好在查询列表里只包含索引列，比如：\nSELECT name, birthday, phone_number FROM person_info WHERE name \u0026gt; 'Asa' AND name \u0026lt; 'Barlo\rw'\r 因为只查询name , birthday , phone_number 这三个索引列的值，所以通idx_name_birthday_phone_number 索引得到结果后就不必到聚簇索引中再查找记录的剩余列，这样就省去了回表操作带来的性能损耗。我们把这种只需要用到索引的查询方式称为索引覆盖,所以坚决不能用* 号作为查询列表，最好把我们需要查询的列依次标明。\n如何挑选索引  只为用于搜索、排序或分组的列创建索引 只为出现在WHERE 子句中的列、连接子句中的连接列，或者出现在ORDER BY 或GROUP BY 子句中的 列创建索引。而出现在查询列表中的列就没必要建立索引了\n考虑列的基数 列的基数指的是某一列中不重复数据的个数，比方说某个列包含值2, 5, 8, 2, 5, 8, 2, 5, 8 ，虽然有9 条记录，但该列的基数却是3 。也就是说，在记录行数一定的情况下，列的基数越大，该列中的值越分散，列的基数越小，该列中的值越集中。假设某个列的基数为1 ，也就是所有记录在该列中的值都一样，那为该列建立索引是没有用的，因为所有值都一样就无法排序，无法进行快速查找了，而且如果某个建立了二级索引的列的重复值特别多，那么使用这个二级索引查出的记录还可能要做回表操作，这样性能损耗就更大了。\n所以结论就是：最好为那些列的基数大的列建立索引，为基数太小列的建立索引效果可能不好（索引尽量建在数据重复不多的列上，比如XX_id,但XX_time上则不合适）\n索引列的类型尽量小 我们在定义表结构的时候要显式的指定列的类型，以整数类型为例，有TINYINT 、MEDIUMINT 、INT 、BIGINT 这么几种，它们占用的存储空间依次递增，我们这里所说的类型大小指的就是该类型表示的数据范围的大小。 能表示的整数范围当然也是依次递增，如果我们想要对某个整数列建立索引的话，在表示的整数范围允许的情况 下，尽量让索引列使用较小的类型，比如我们能使用INT 就不要使用BIGINT ，能使用MEDIUMINT 就不要使用 INT ～ 这是因为：\n 数据类型越小，在查询时进行的比较操作越快（这是CPU层次的东东） 数据类型越小，索引占用的存储空间就越少，在一个数据页内就可以放下更多的记录，从而减少磁盘I/O 带 来的性能损耗，也就意味着可以把更多的数据页缓存在内存中，从而加快读写效率。  这个建议对于表的主键来说更加适用，因为不仅是聚簇索引中会存储主键值，其他所有的二级索引的节点处都会 存储一份记录的主键值，如果主键适用更小的数据类型，也就意味着节省更多的存储空间和更高效的I/O\n索引字符串值的前缀 只对字符串的前几个字符进行索引也就是说在二级索引的记录中只保留字符串前几个字符。这样在查找记录时虽然不能精确的定位到记录的位置，但是能定位到相应前缀所在的位置，然后根据前缀相同的记录的主键值回表查询完整的字符串值，再对比就好了。这样只在B+ 树中存储字符串的前几个字符的编码，既节约空间，又减少了字符串的比较时间，比方说我们在建表语句中只对name 列的前10个字符进行索引可以这么写：\nCREATE TABLE person_info(\rname VARCHAR(100) NOT NULL,\rbirthday DATE NOT NULL,\rphone_number CHAR(11) NOT NULL,\rcountry varchar(100) NOT NULL,\rKEY idx_name_birthday_phone_number (name(10), birthday, phone_number)\r);\r name(10) 就表示在建立的B+ 树索引中只保留记录的前10 个字符的编码，这种只索引字符串值的前缀的策略是 我们非常鼓励的，尤其是在字符串类型能存储的字符比较多的时候。\nSELECT * FROM person_info ORDER BY name LIMIT 10;\r 因为二级索引中不包含完整的name 列信息，所以无法对前十个字符相同，后边的字符不同的记录进行排序，也 就是使用索引列前缀的方式无法支持使用索引排序，只好乖乖的用文件排序喽。\n让索引列在比较表达式中单独出现 假设表中有一个整数列my_col ，我们为这个列建立了索引。下边的两个WHERE 子句虽然语义是一致的，但是在 效率上却有差别：\n WHERE my_col * 2 \u0026lt; 4 WHERE my_col \u0026lt; 4/2  第1个WHERE 子句中my_col 列并不是以单独列的形式出现的，而是以my_col * 2 这样的表达式的形式出现的， 存储引擎会依次遍历所有的记录，计算这个表达式的值是不是小于4 ，所以这种情况下是使用不到为my_col 列 建立的B+ 树索引的。而第2个WHERE 子句中my_col 列并是以单独列的形式出现的，这样的情况可以直接使用 B+ 树索引。 所以结论就是：如果索引列在比较表达式中不是以单独列的形式出现，而是以某个表达式，或者函数调用形式出 现的话，是用不到索引的。\n主键插入顺序 我们知道，对于一个使用InnoDB 存储引擎的表来说，在我们没有显式的创建索引时，表中的数据实际上都是存储在聚簇索引的叶子节点的。而记录又是存储在数据页中的，数据页和记录又是按照记录主键值从小到大的顺序进行排序，所以如果我们插入的记录的主键值是依次增大的话，那我们每插满一个数据页就换到下一个数据页继续插，而如果我们插入的主键值忽大忽小的话，这就比较麻烦了，假设某个数据页存储的记录已经满了，它存储的主键值在1~100 之间：\n如果此时再插入一条主键值为9 的记录，那它插入的位置就如下图：\n可这个数据页已经满了啊，再插进来咋办呢？我们需要把当前页面分裂成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗最好让插入的记录的主键值依次递增\n冗余和重复索引 有时候有的同学有意或者无意的就对同一个列创建了多个索引，比方说这样写建表语句：\nCREATE TABLE person_info(\rid INT UNSIGNED NOT NULL AUTO_INCREMENT,\rname VARCHAR(100) NOT NULL,\rbirthday DATE NOT NULL,\rphone_number CHAR(11) NOT NULL,\rcountry varchar(100) NOT NULL,\rPRIMARY KEY (id),\rKEY idx_name_birthday_phone_number (name(10), birthday, phone_number),\rKEY idx_name (name(10))\r);\r 通过idx_name_birthday_phone_number 索引就可以对name 列进行快速搜索，再创建一个专门针对name 列的索引就算是一个冗余索引，维护这个索引只会增加维护的成本，并不会对搜索有什么好处。这个自己也犯过\n总结   B+ 树索引在空间和时间上都有代价，所以没事儿别瞎建索引。\n  B+ 树索引适用于下边这些情况：\n 全值匹配 匹配左边的列 匹配范围值 精确匹配某一列并范围匹配另外一列 用于排序 用于分组    在使用索引时需要注意下边这些事项：\n 只为用于搜索、排序或分组的列创建索引 为列的基数大的列创建索引 索引列的类型尽量小 可以只对字符串值的前缀建立索引 只有索引列在比较表达式中单独出现才可以适用索引 为了尽可能少的让聚簇索引发生页面分裂和记录移位的情况，主键一定要依次递增。 定位并删除表中的重复和冗余索引 尽量使用覆盖索引进行查询，避免回表带来的性能损耗。    InnoDB的表空间 表空间是一个抽象的概念，对于系统表空间来说，对应着文件系统中一个或多个实际文件；对于每个独立表空间来说，对应着文件系统中一个名为表名.ibd 的实际文件。大家可以把表空间想象成被切分为许许多多个页的池子，当我们想为某个表插入一条记录的时候，就从池子中捞出一个对应的页来把数据写进去。\n独立表空间结构  InnoDB 支持许多种类型的表空间，重点关注独立表空间和系统表空间的结构。它们的结构比较相似。\n区（extent）的概念  表空间中的页实在是太多了，为了更好的管理这些页面，提出了区（英文名： extent ）的概念。对于16KB的页来说，连续的64个页就是一个区，也就是说一个区默认占用1MB空间大小。不论是系统表空间还是独立表空间，都可以看成是由若干个区组成的，每256个区被划分成一组\n这些组的头几个页面的类型都是类似的，就像这样\n从上图中我们能得到如下信息：\n 第一个组最开始的3个页面的类型是固定的，也就是说extent 0 这个区最开始的3个页面的类型是固定的， 分别是：  FSP_HDR 类型：这个类型的页面是用来登记整个表空间的一些整体属性以及本组所有的区，也就是extent 0 ~ extent 255 这256个区的属性，稍后详细唠叨。需要注意的一点是，整个表空间只有一个FSP_HDR 类型的页面。 IBUF_BITMAP 类型：这个类型的页面是存储本组所有的区的所有页面关于INSERT BUFFER 的信息。 INODE 类型：这个类型的页面存储了许多称为INODE 的数据结构。   其余各组最开始的2个页面的类型是固定的，也就是说extent 256 、extent 512 这些区最开始的2个页面 的类型是固定的，分别是：  XDES 类型：全称是extent descriptor ，用来登记本组256个区的属性，也就是说对于在extent 256 区中的该类型页面存储的就是extent 256 ~ extent 511 这些区的属性，对于在extent 512 区中的该 类型页面存储的就是extent 512 ~ extent 767 这些区的属性。上边介绍的FSP_HDR 类型的页面其实 和XDES 类型的页面的作用类似，只不过FSP_HDR 类型的页面还会额外存储一些表空间的属性。 IBUF_BITMAP 类型：上边介绍过了。    总之，表空间被划分为许多连续的区，每个区默认由64个页组成，每256个区划分为一组，每个组的最开始的几个页面类型是固定的\n段（segment）的概念  为啥好端端的提出一个区（ extent ）的概念呢？我们以前分析问题的套路都是这样的：表中的记录存储到页里 边儿，然后页作为节点组成B+ 树，这个B+ 树就是索引，但我们来考虑一下下边这个场景：\n 我们每向表中插入一条记录，本质上就是向该表的聚簇索引以及所有二级索引代表的B+ 树的节点中插入数 据。而B+ 树的每一层中的页都会形成一个双向链表，如果是以页为单位来分配存储空间的话，双向链表相 邻的两个页之间的物理位置可能离得非常远。我们介绍B+ 树索引的适用场景的时候特别提到范围查询只需 要定位到最左边的记录和最右边的记录，然后沿着双向链表一直扫描就可以了，而如果链表中相邻的两个页 物理位置离得非常远，就是所谓的随机I/O 。再一次强调，磁盘的速度和内存的速度差了好几个数量级， 随 机I/O 是非常慢的，所以我们应该尽量让链表中相邻的页的物理位置也相邻，这样进行范围查询的时候才可 以使用所谓的顺序I/O  所以才引入了区（ extent ）的概念，一个区就是在物理位置上连续的64个页。。在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区为单位分配，甚至在表中的数据十分非常特别多的时候，可以一次性分配多个连续的区。\n我们提到的范围查询，其实是对B+ 树叶子节点中的记录进行顺序扫描，而如果不区分叶子节点和非叶子节点，统统把节点代表的页面放到申请到的区中的话，进行范围扫描的效果就大打折扣了。所以设计InnoDB 的大叔们对B+ 树的叶子节点和非叶子节点进行了区别对待，也就是说叶子节点有自己独有的区，非叶子节点也有自己独有的区。存放叶子节点的区的集合就算是一个段（ segment ），存放非叶子节点的区的集合也算是一个段。也就是说一个索引会生成2个段，一个叶子节点段，一个非叶子节点段。\n默认情况下一个使用InnoDB 存储引擎的表只有一个聚簇索引，一个索引会生成2个段，而段是以区为单位申请存 储空间的，一个区默认占用1M存储空间，所以默认情况下一个只存了几条记录的小表也需要2M的存储空间么？ 以后每次添加一个索引都要多申请2M的存储空间么？这对于存储记录比较少的表简直是天大的浪费。设计InnoDB 的大叔们提出了一个碎片（fragment）区的概念，也就是在一个碎片区中，并不是所有的页都是为了存储同一个段的数据而存在的，而是碎片区中的页可以用于不同的目的，比如有些页用于段A，有些页用于段B，有些页甚至哪个段都不属于。碎片区直属于表空间，并不属于任何一个段。所以此后为某个段分配存储空间的策略是这样的：\n 在刚开始向表中插入数据的时候，段是从某个碎片区以单个页面为单位来分配存储空间的。 当某个段已经占用了32个碎片区页面之后，就会以完整的区为单位来分配存储空间。  所以现在段不能仅定义为是某些区的集合，更精确的应该是某些零散的页面以及一些完整的区的集合\n区的分类  表空间是由若干个区组成的，这些区大体上可以分为4种类型：\n 空闲的区（Free）：现在还没有用到这个区中的任何页面。 有剩余空间的碎片区(FREE_FRAG)：表示碎片区中还有可用的页面。 没有剩余空间的碎片区(FULL_FRAG)：表示碎片区中的所有页面都被使用，没有空闲页面。 附属于某个段的区(FSEG)。每一个索引都可以分为叶子节点段和非叶子节点段，除此之外InnoDB还会另外定义一些特殊作用的段，在这些段中的数据量很大时将使用区来作为基本的分配单位。  这4种类型的区也可以被称为区的4种状态（ Free、FREE_FRAG、FULL_FRAG、FSEG）,处于FREE 、FREE_FRAG 以及FULL_FRAG 这三种状态的区都是独立的，算是直属于表空间；而处于FSEG 状态的区是附属于某个段的\n 如果把表空间比作是一个集团军，段就相当于师，区就相当于团。一般的团都是隶属于某个师的，就像 是处于FSEG的区全都隶属于某个段，而处于FREE、FREE_FRAG以及FULL_FRAG这三种状态的区却 直接隶属于表空间，就像独立团直接听命于军部一样。\n 为了方便管理这些区，设计InnoDB 的大叔设计了一个称为XDES Entry 的结构（全称就是Extent Descriptor Entry），每一个区都对应着一个XDES Entry 结构，这个结构记录了对应的区的一些属性。\nXDES Entry 是一个40个字节的结构，大致分为4个部分，各个部分的释义如下：\n Segment ID （8字节） 每一个段都有一个唯一的编号，用ID表示，此处的Segment ID 字段表示就是该区所在的段。当然前提是该 区已经被分配给某个段了，不然的话该字段的值没啥意义。 List Node （12字节） 这个部分可以将若干个XDES Entry 结构串联成一个链表，如果我们想定位表空间内的某一个位置的话，只需指定页号以及该位置在指定页号中的页内偏移量即可。  Pre Node Page Number 和Pre Node Offset 的组合就是指向前一个XDES Entry 的指针 Next Node Page Number 和Next Node Offset 的组合就是指向后一个XDES Entry 的指针。    也就是说把一些XDES Entry 结构连成了一个链表\n  State （4字节）\n这个字段表明区的状态。可选的值就是我们前边说过的那4个，分别是： FREE 、FREE_FRAG 、FULL_FRAG 和FSEG\n  Page State Bitmap （16字节） 这个部分共占用16个字节，也就是128个比特位。我们说一个区默认有64个页，这128个比特位被划分为64 个部分，每个部分2个比特位，对应区中的一个页。比如Page State Bitmap 部分的第1和第2个比特位对应 着区中的第1个页面，第3和第4个比特位对应着区中的第2个页面，依此类推， Page State Bitmap 部分的第 127和128个比特位对应着区中的第64个页面。这两个比特位的第一个位表示对应的页是否是空闲的，第二个 比特位还没有用\n  XDES Entry链表 我们已经提出了五花八门的概念，什么区、段、碎片区、附属于段的区、XDES Entry 结构，走远了千万别忘了自己为什么出发，我们把事情搞这么麻烦的初心仅仅是想提高向表插入数据的效率又不至于数据量少的表浪费空间。我们知道向表中插入数据本质上就是向表中各个索引的叶子节点段、非叶子节点段插入数据，也知道了不同的区有不同的状态，再回到最初的起点，捋一捋向某个段中插入数据的过程\n  当段中数据较少的时候，首先会查看表空间中是否有状态为FREE_FRAG 的区，也就是找还有空闲空间的碎片区，如果找到了，那么从该区中取一些零碎的页把数据插进去；否则到表空间下申请一个状态为FREE 的区，也就是空闲的区，把该区的状态变为FREE_FRAG ，然后从该新申请的区中取一些零碎的页把数据插进去。之后不同的段使用零碎页的时候都会从该区中取，直到该区中没有空闲空间，然后该区的状态就变成了FULL_FRAG 。\n现在的问题是你怎么知道表空间里的哪些区是FREE 的，哪些区的状态FREE_FRAG 的，哪些区是FULL_FRAG 的？要知道表空间的大小是可以不断增大的，当增长到GB级别的时候，区的数量也就上千了， 我们总不能每次都遍历这些区对应的XDES Entry 结构吧？这时候就是XDES Entry 中的List Node 部分发挥奇效的时候了，我们可以通过List Node 中的指针，做这么三件事：\n  把状态为FREE 的区对应的XDES Entry 结构通过List Node 来连接成一个链表，这个链表我们就称之为FREE 链表。\n 把状态为FREE_FRAG 的区对应的XDES Entry 结构通过List Node 来连接成一个链表，这个链表我们就称之为FREE_FRAG 链表。 把状态为FULL_FRAG 的区对应的XDES Entry 结构通过List Node 来连接成一个链表，这个链表我们就称之为FULL_FRAG 链表。 这样每当我们想找一个FREE_FRAG 状态的区时，就直接把FREE_FRAG 链表的头节点拿出来，从这个节点中取一些零碎的页来插入数据，当这个节点对应的区用完时，就修改一下这个节点的State 字段的值，然后从FREE_FRAG 链表中移到FULL_FRAG 链表中。同理，如果FREE_FRAG 链表中一个节点都没有，那 么就直接从FREE 链表中取一个节点移动到FREE_FRAG 链表的状态，并修改该节点的STATE 字段值为FREE_FRAG ，然后从这个节点对应的区中获取零碎的页就好了。    当段中数据已经占满了32个零散的页后，就直接申请完整的区来插入数据\n还是那个问题，我们怎么知道哪些区属于哪个段的呢？再遍历各个XDES Entry 结构？遍历是不可能遍历的，这辈子都不可能遍历的，有链表还遍历个毛线啊。所以我们把状态为FSEG 的区对应的XDES Entry 结构都加入到一个链表喽？傻呀，不同的段哪能共用一个区呢？你想把索引a的叶子节点段和索引b的叶子节点段都存储到一个区中么？显然我们想要每个段都有它独立的链表，所以可以根据段号（也就是Segment ID ）来建立链表，有多少个段就建多少个链表？好像也有点问题，因为一个段中可以有好多个区，有的区是完全空闲的，有的区还有一些页面可以用，有的区已经没有空闲页面可以用了，所以我们有必要继续细分，设计InnoDB 的大叔们为每个段中的区对应的XDES Entry 结构建立了三个链表：\n FREE 链表：同一个段中，所有页面都是空闲的区对应的XDES Entry 结构会被加入到这个链表。注意和直属于表空间的FREE 链表区别开了，此处的FREE 链表是附属于某个段的。 NOT_FULL 链表：同一个段中，仍有空闲空间的区对应的XDES Entry 结构会被加入到这个链表。 FULL 链表：同一个段中，已经没有空闲空间的区对应的XDES Entry 结构会被加入到这个链表。    再次强调一遍，每一个索引都对应两个段，每个段都会维护上述的3个链表\nCREATE TABLE t (\rc1 INT NOT NULL AUTO_INCREMENT,\rc2 VARCHAR(100),\rc3 VARCHAR(100),\rPRIMARY KEY (c1),\rKEY idx_c2 (c2)\r)ENGINE=InnoDB;\r 这个表t 共有两个索引，一个聚簇索引，一个二级索引idx_c2 ，所以这个表共有4个段，每个段都会维护上述3个链表，总共是12个链表，加上我们上边说过的直属于表空间的3个链表，整个独立表空间共需要维护15个链表。所以段在数据量比较大时插入数据的话，会先获取NOT_FULL 链表的头节点，直接把数据插入这个头节点对应的区中即可，如果该区的空间已经被用完，就把该节点移到FULL 链表中\n段的结构  我们前边说过，段其实不对应表空间中某一个连续的物理区域，而是一个逻辑上的概念，由若干个零散的页面以及一些完整的区组成。像每个区都有对应的 XDES Entry 来记录这个区中的属性一样，设计 InnoDB 的大叔为每个段都定义了一个 INODE Entry 结构来记录一下段中的属性。大家看一下示意图\n它的各个部分释义如下：\n Segment ID 就是指这个 INODE Entry 结构对应的段的编号（ID）。 NOT_FULL_N_USED 这个字段指的是在 NOT_FULL 链表中已经使用了多少个页面。下次从 NOT_FULL 链表分配空闲页面时可以直接根据这个字段的值定位到。而不用从链表中的第一个页面开始遍历着寻找空闲页面。 3个 List Base Node 分别为段的 FREE 链表、 NOT_FULL 链表、 FULL 链表定义了 List Base Node ，这样我们想查找某个段的某个链表的头节点和尾节点的时候，就可以直接到这个部分找到对应链表的 List Base Node Magic Number ： 这个值是用来标记这个 INODE Entry 是否已经被初始化了（初始化的意思就是把各个字段的值都填进去了） Fragment Array Entry 我们前边强调过无数次段是一些零散页面和一些完整的区的集合，每个 Fragment Array Entry 结构都对应着一个零散的页面，这个结构一共4个字节，表示一个零散页面的页号。  各类型页面详细情况  到现在为止我们已经大概清楚了表空间、段、区、XDES Entry、INODE Entry、各种以 XDES Enty 为节点的链表的基本概念了，但是每个区对应的 XDES Entry 结构到底存储在表空间的什么地方？直属于表空间的 FREE 、 FREE_FRAG 、 FULL_FRAG 链表的基节点到底存储在表空间的什么地方？每个段对应的 INODE Entry 结构到底存在表空间的什么地方？我们前边介绍了每256个连续的区算是一个组，想解决刚才提出来的这些个疑问还得从每个组开头的一些类型相同的页面说起\nFSP_HDR 类型 首先看第一个组的第一个页面，当然也是表空间的第一个页面，页号为 0 。这个页面的类型是 FSP_HDR ，它存储了表空间的一些整体属性以及第一个组内256个区的对应的 XDES Entry 结构，直接看这个类型的页面的示意图\n总结 这部分内容比较偏理论，所以自己看的时候也没有特别仔细看，如果以后能遇到可以返回来仔细看看，最后一张图总结表空间：\n单表访问方法 MySQL Server 有一个称为 查询优化器 的模块，一条查询语句进行语法解析之后就会被交给查询优化器来进行优化，优化的结果就是生成一个所谓的 执行计划 ，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询。先来瞅瞅 MySQL 怎么执行单表查询（就是 FROM 子句后边只有一个表，最简单的那种查询～）。先建表并且建立索引\nCREATE TABLE single_table (\rid INT NOT NULL AUTO_INCREMENT,\rkey1 VARCHAR(100),\rkey2 INT,\rkey3 VARCHAR(100),\rkey_part1 VARCHAR(100),\rkey_part2 VARCHAR(100),\rkey_part3 VARCHAR(100),\rcommon_field VARCHAR(100),\rPRIMARY KEY (id),\rKEY idx_key1 (key1),\rUNIQUE KEY idx_key2 (key2),\rKEY idx_key3 (key3),\rKEY idx_key_part(key_part1, key_part2, key_part3)\r) Engine=InnoDB CHARSET=utf8;\r 我们为这个 single_table 表建立了1个聚簇索引和4个二级索引，分别是：\n 为 id 列建立的聚簇索引。 为 key1 列建立的 idx_key1 二级索引。 为 key2 列建立的 idx_key2 二级索引，而且该索引是唯一二级索引。 为 key3 列建立的 idx_key3 二级索引。 为 key_part1 、 key_part2 、 key_part3 列建立的 idx_key_part 二级索引，这也是一个联合索引。  访问方法（access method）的概念  MySQL 中我们平时所写的那些查询语句本质上只是一种声明式的语法，只是告诉 MySQL 我们要获取的数据符合哪些规则，至于 MySQL 背地里是怎么把查询结果搞出来的那是 MySQL 自己的事儿。对于单个表的查询来说，设计MySQL的大叔把查询的执行方式大致分 为下边两种：\n 使用全表扫描进行查询 这种执行方式很好理解，就是把表的每一行记录都扫一遍嘛，把符合搜索条件的记录加入到结果集就完了。 使用索引进行查询 因为直接使用全表扫描的方式执行查询要遍历好多记录，所以代价可能太大了。如果查询语句中的搜索条件可以使用到某个索引，那直接使用索引来执行查询可能会加快查询执行的时间。使用索引来执行查询的方式五花八门，又可以细分为许多种类：  针对主键或唯一二级索引的等值查询 针对普通二级索引的等值查询 针对索引列的范围查询 直接扫描整个索引    把 MySQL 执行查询语句的方式称之为 访问方法 或者 访问类型，下边细细道来各种 访问方法 的具体内容。\nconst 有的时候我们可以通过主键列来定位一条记录，比方说这个查询：\nSELECT * FROM single_table WHERE id = 1438;\r MySQL 会直接利用主键值在聚簇索引中定位对应的用户记录,B+ 树叶子节点中的记录是按照索引列排序的，对于聚簇索引来说，它对应的 B+ 树叶子节点中的记录就是按照 id 列排序的。 B+ 树本来就是一个矮矮的大胖子，所以这样根据主键值定位一条记录的速度贼快。类似的，我们根据唯一二级索引列来定位一条记录的速度也是贼快的，比如下边这个查询\nSELECT * FROM single_table WHERE key2 = 3841;\r 通过主键或者唯一二级索引列与常数的等值比较来定位一条记录是像坐火箭一样快的，所以把这种通过主键或者唯一二级索引列来定位一条记录的访问方法定义为： const ，意思是常数级别的，代价是可以忽略不计的。不过这种 const 访问方法只能在主键列或者唯一二级索引列和一个常数进行等值比较时才有效，如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较，这个const 访问方法才有效（这是因为只有该索引中全部列都采用等值比较才可以定位唯一的一条记录）\n对于唯一二级索引来说，查询该列为 NULL 值的情况比较特殊，比如这样：\nSELECT * FROM single_table WHERE key2 IS NULL;\r 因为唯一二级索引列并不限制 NULL 值的数量，所以上述语句可能访问到多条记录，也就是说 上边这个语句不可以使用 const 访问方法来执行\nref 有时候我们对某个普通的二级索引列与常数进行等值比较，比如这样：\nSELECT * FROM single_table WHERE key1 = 'abc';\r 对于这个查询，我们当然可以选择全表扫描来逐一对比搜索条件是否满足要求，我们也可以先使用二级索引找到对应记录的 id 值，然后再回表到聚簇索引中查找完整的用户记录。由于普通二级索引并不限制索引列值的唯一性，所以可能找到多条对应的记录，也就是说使用二级索引来执行查询的代价取决于等值匹配到的二级索引记录条数。如果匹配的记录较少，则回表的代价还是比较低的，所以 MySQL 可能选择使用索引而不是全表扫描的方式来执行查询。设计 MySQL 的大叔就把这种搜索条件为二级索引列与常数等值比较，采用二级索引来执行查询的访问方法称为： ref 。我们看一下采用 ref 访问方法执行查询的图示：\n需要注意下边两种情况：\n  二级索引列值为 NULL 的情况 不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含 NULL 值的数量并不限制，所以我们采用key IS NULL 这种形式的搜索条件最多只能使用 ref 的访问方法，而不是 const 的访问方法。\n  对于某个包含多个索引列的二级索引(聚合索引)来说，只要是最左边的连续索引列是与常数的等值比较就可能采用 ref的访问方法，比方说下边这几个查询：\nSELECT * FROM single_table WHERE key_part1 = 'god like';\rSELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 = 'legendary';\rSELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 = 'legendary' AND key_part3 = 'penta kill';\r 但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为 ref 了，比方说这样：\n  SELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 \u0026gt; 'legendary';\r ref_or_null 有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为 NULL 的记录也找出来，就像下边这个查询：\nSELECT * FROM single_demo WHERE key1 = 'abc' OR key1 IS NULL;\r 当使用二级索引而不是全表扫描的方式执行该查询时，这种类型的查询使用的访问方法就称为ref_or_null ，这个 ref_or_null 访问方法的执行过程如下：\nrange 之前介绍的几种访问方法都是在对索引列与某一个常数进行等值比较的时候才可能使用到（ ref_or_null 比较奇特，还计算了值为 NULL 的情况），但是有时候我们面对的搜索条件更复杂，比如下边这个查询：\nSELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 \u0026gt;= 38 AND key2 \u0026lt;= 79);\r 如果采用 二级索引 + 回表 的方式来执行的话，那么此时的搜索条件就不只是要求索引列与常数的等值匹配了，而是索引列需要匹配某个或某些范围的值,把这种利用索引(可以是聚簇索引，也可以是二级索引)进行范围匹配的访问方法称之为： range\nindex SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = 'abc';\r 由于 key_part2 并不是联合索引 idx_key_part 最左索引列，所以我们无法使用 ref 或者 range 访问方法来执行这个语句。但是这个查询符合下边这两个条件：\n 它的查询列表只有3个列： key_part1 , key_part2 , key_part3 ，而索引 idx_key_part 又包含这三个列。 搜索条件中只有 key_part2 列。这个列也包含在索引 idx_key_part 中。  也就是说我们可以直接通过遍历 idx_key_part 索引的叶子节点的记录来比较key_part2 = \u0026lsquo;abc\u0026rsquo; 这个条件是否成立，把匹配成功的二级索引记录的 key_part1 , key_part2 , key_part3 列的值直接加到结果集中就行了。由于二级索引记录比聚簇索记录小的多（聚簇索引记录要存储所有用户定义的列以及所谓的隐藏列，而二级索引记录只需要存放索引列和主键），而且这个过程也不用进行回表操作，所以直接遍历二级索引比直接遍历聚簇索引的成本要小很多，把这种采用遍历二级索引记录的执行方式称之为index\nall 最直接的查询执行方式全表扫描，对于 InnoDB 表来说也就是直接扫描聚簇索引，把这种使用全表扫描执行查询的方式称之为： all 。\n注意事项 二级索引 + 回表  一般情况下只能利用单个二级索引执行查询，比方说下边的这个查询\nSELECT * FROM single_table WHERE key1 = 'abc' AND key2 \u0026gt; 1000;\rkey1 和 key2都建有索引\r 优化器一般会根据 single_table 表的统计数据来判断到底使用哪个条件到对应的二级索引中查询扫描的行数会更少，选择那个扫描行数较少的条件到对应的二级索引中查询。然后将从该二级索引中查询到的结果经过回表得到完整的用户记录后再根据其余的 WHERE 条件过滤记录。 一般来说，等值查找比范围查找需要扫描的行数更少（也就是 ref 的访问方法一般比 range 好，但这也不总是一定的，也可能采用 ref 访问方法的那个索引列的值为特定值的行数特别多），所以这里假设优化器决定使用idx_key1 索引进行查询，那么整个查询过程可以分为两个步骤：\n 步骤1：使用二级索引定位记录的阶段，也就是根据条件 key1 = \u0026lsquo;abc\u0026rsquo; 从 idx_key1 索引代表的 B+ 树中找到对应的二级索引记录。 步骤2：回表阶段，也就是根据上一步骤中找到的记录的主键值进行 回表 操作，也就是到聚簇索引中找到对应的完整的用户记录，再根据条件 key2 \u0026gt; 1000 到完整的用户记录继续过滤。将最终符合过滤条件的记录返回给用户。  为什么呢？因为二级索引的节点中的记录只包含索引列和主键，所以在步骤1中使用 idx_key1 索引进行查询时只会用到与 key1 列有关的搜索条件，其余条件，比如 key2 \u0026gt; 1000 这个条件在步骤1中是用不到的，只有在步骤2完成回表操作后才能继续针对完整的用户记录中继续过滤\n明确range访问方法使用的范围区间 其实对于 B+ 树索引来说，只要索引列和常数使用 = 、 \u0026lt;=\u0026gt; 、 IN 、 NOT IN 、 IS NULL 、 IS NOT NULL 、\u0026gt;、 \u0026lt; 、 \u0026gt;= 、 \u0026lt;= 、 BETWEEN 、 != （不等于也可以写成 \u0026lt;\u0026gt; ）或者 LIKE 操作符连接起来，就可以产生一个所谓的 区间 。\n IN操作符的效果和若干个等值匹配操作符=之间用OR连接起来是一样的，也就是说会产生多个单点区间，比如下边这两个语句的效果是一样的： SELECT * FROM single_table WHERE key2 IN (1438, 6328); SELECT * FROM single_table WHERE key2 = 1438 OR key2 = 6328;\n 当我们想使用 range 访问方法来执行一个查询语句时，重点就是找出该查询可用的索引以及这些索引对应的范围区间。下边分两种情况看一下怎么从由 AND 或 OR 组成的复杂搜索条件中提取出正确的范围区间。\n所有搜索条件都可以使用某个索引的情况 每个搜索条件都可以使用到某个索引,那肯定是可以用索引进行范围查询的，例如\nSELECT * FROM single_table WHERE key2 \u0026gt; 100 AND key2 \u0026gt; 200;\rSELECT * FROM single_table WHERE key2 \u0026gt; 100 OR key2 \u0026gt; 200;\r 有的搜索条件无法使用索引的情况 SELECT * FROM single_table WHERE key2 \u0026gt; 100 AND common_field = 'abc';\r 请注意，这个查询语句中能利用的索引只有 idx_key2 一个，而 idx_key2 这个二级索引的记录中又不包含common_field 这个字段，所以在使用二级索引 idx_key2 定位记录的阶段用不到 common_field = \u0026lsquo;abc\u0026rsquo; 这个条件，这个条件是在回表获取了完整的用户记录后才使用的，而 范围区间 是为了到索引中取记录中提出的概念，所以在确定 范围区间 的时候不需要考虑 common_field = \u0026lsquo;abc\u0026rsquo; 这个条件，我们在为某个索引确定范围区间的时候只需要把用不到相关索引的搜索条件替换为 TRUE 就好了\n我们把上边的查询中用不到 idx_key2 的搜索条件替换后就是这样：\rSELECT * FROM single_table WHERE key2 \u0026gt; 100 AND TRUE;\r化简之后就是这样：\rSELECT * FROM single_table WHERE key2 \u0026gt; 100;\r 也就是说最上边那个查询使用 idx_key2 的范围区间就是： (100, +∞) 。\n再来看一下使用 OR 的情况：\nSELECT * FROM single_table WHERE key2 \u0026gt; 100 OR common_field = 'abc';\r同理，我们把使用不到 idx_key2 索引的搜索条件替换为 TRUE ：\rSELECT * FROM single_table WHERE key2 \u0026gt; 100 OR TRUE;\r接着化简：\rSELECT * FROM single_table WHERE TRUE;\r 这也就说说明如果我们强制使用 idx_key2 执行查询的话，对应的范围区间就是 (-∞, +∞) ，也就是需要将全部二级索引的记录进行回表，这个代价肯定比直接全表扫描都大了。也就是说一个使用到索引的搜索条件和没有使用该索引的搜索条件使用 OR 连接起来后是无法使用该索引的。\n索引合并 MySQL 在一般情况下执行一个查询时最多只会用到单个二级索引，但不是还有特殊情况么，在这些特殊情况下也可能在一个查询中使用到多个二级索引，设计 MySQL 的大叔把这种使用到多个索引来完成一次查询的执行方法称之为： index merge ，具体的索引合并算法有下边三种\nIntersection合并  Intersection 翻译过来的意思是 交集 。这里是说某个查询可以使用多个二级索引，将从多个二级索引中查询到的结果取交集，比方说下边这个查询：\nSELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b';\r 假设这个查询使用 Intersection 合并的方式执行的话，那这个过程就是这样的：\n 从 idx_key1 二级索引对应的 B+ 树中取出 key1 = \u0026lsquo;a\u0026rsquo; 的相关记录。 从 idx_key3 二级索引对应的 B+ 树中取出 key3 = \u0026lsquo;b\u0026rsquo; 的相关记录。 二级索引的记录都是由 索引列 + 主键 构成的，所以我们可以计算出这两个结果集中 id 值的交集。 按照上一步生成的 id 值列表进行回表操作，也就是从聚簇索引中把指定 id 值的完整用户记录取出来，返回给用户。  为啥不直接使用 idx_key1 或者 idx_key3 只根据某个搜索条件去读取一个二级索引，然后回表后再过滤另外一个搜索条件呢？这里要分析一下两种查询执行方式之间需要的成本代价。 只读取一个二级索引的成本：\n 按照某个搜索条件读取一个二级索引 根据从该二级索引得到的主键值进行回表操作，然后再过滤其他的搜索条件  读取多个二级索引之后取交集成本：\n 按照不同的搜索条件分别读取不同的二级索引 将从多个二级索引得到的主键值取交集，然后进行回表操作  虽然读取多个二级索引比读取一个二级索引消耗性能，但是读取二级索引的操作是顺序I/O ，而回表操作是 随机I/O ，所以如果只读取一个二级索引时需要回表的记录数特别多，而读取多个二级索引之后取交集的记录数非常少，当节省的因为 回表 而造成的性能损耗比访问多个二级索引带来的性能损耗更高时，读取多个二级索引后取交集比只读取一个二级索引的成本更低。\nMySQL 在某些特定的情况下才可能会使用到 Intersection 索引合并：\n  情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只出现匹配部分列的情况。 比方说下边这个查询可能用到 idx_key1 和 idx_key_part 这两个二级索引进行 Intersection 索引合并的操作：\nSELECT * FROM single_table WHERE key1 = 'a' AND key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c';\r 而下边这两个查询就不能进行 Intersection 索引合并：\n因为对 key1 进行了范围匹配\rSELECT * FROM single_table WHERE key1 \u0026gt; 'a' AND key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c';\r因为联合索引 idx_key_part 中的 key_part2 列并没有出现在搜索条件中\rSELECT * FROM single_table WHERE key1 = 'a' AND key_part1 = 'a';\r   情况二：主键列可以是范围匹配 比方说下边这个查询可能用到主键和 idx_key1 进行 Intersection 索引合并的操作：\nSELECT * FROM single_table WHERE id \u0026gt; 100 AND key1 = 'a';\r 对于 InnoDB 的二级索引来说，记录先是按照索引列进行排序，如果该二级索引是一个联合索引，那么会按照联合索引中的各个列依次排序。而二级索引的用户记录是由 索引列 + 主键 构成的，二级索引列的值相同的记录可能会有好多条，这些索引列的值相同的记录又是按照主键 的值进行排序的。所以重点来了，之所以在二级索引列都是等值匹配的情况下才可能使用 Intersection 索引合并，是因为只有在这种情况下根据二级索引查询出的结果集是按照主键值排序的。\n  假设某个查询使用Intersection 索引合并的方式从 idx_key1 和 idx_key2 这两个二级索引中获取到的主键值分别是：\n 从 idx_key1 中获取到已经排好序的主键值：1、3、5 从 idx_key2 中获取到已经排好序的主键值：2、3、4  那么求交集的过程就是这样：逐个取出这两个结果集中最小的主键值，如果两个值相等，则加入最后的交集结果中，否则丢弃当前较小的主键值，再取该丢弃的主键值所在结果集的后一个主键值来比较，直到某个结果集中的主键值用完了，过程如下：\n 先取出这两个结果集中较小的主键值做比较，因为 1 \u0026lt; 2 ，所以把 idx_key1 的结果集的主键值 1 丢弃，取出后边的 3 来比较。 因为 3 \u0026gt; 2 ，所以把 idx_key2 的结果集的主键值 2 丢弃，取出后边的 3 来比较。 因为 3 = 3 ，所以把 3 加入到最后的交集结果中，继续两个结果集后边的主键值来比较。 后边的主键值也不相等，所以最后的交集结果中只包含主键值 3 。  这个过程其实可快了，时间复杂度是 O(n) ，但是如果从各个二级索引中查询出的结果集并不是按照主键排序的话，那就要先把结果集中的主键值排序完再来做上边的那个过程，就比较耗时了\n另外，不仅是多个二级索引之间可以采用 Intersection 索引合并，索引合并也可以有聚簇索引参加，也就是我们上边写的 情况二 ：在搜索条件中有主键的范围匹配的情况下也可以使用 Intersection 索引合并索引合并。\n当然，上边说的 情况一 和 情况二 只是发生 Intersection 索引合并的必要条件，不是充分条件。也就是说即使情况一、情况二成立，也不一定发生 Intersection 索引合并，这得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，而通过 Intersection 索引合并后需要回表的记录数大大减少时才会使用 Intersection 索引合并\nUnion合并  Union 是并集的意思，适用于使用不同索引的搜索条件之间使用 OR 连接起来的情况。与 Intersection 索引合并类似，MySQL 在某些特定的情况下才可能会使用到 Union 索引合并\n  情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只出现匹配部分列的情况。 比方说下边这个查询可能用到 idx_key1 和 idx_key_part 这两个二级索引进行 Union 索引合并的操作：\nSELECT * FROM single_table WHERE key1 = 'a' OR ( key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c');\r 而下边这两个查询就不能进行 Union 索引合并：\nSELECT * FROM single_table WHERE key1 \u0026gt; 'a' OR (key_part1 = 'a' AND key_part2 = 'b'\rAND key_part3 = 'c');\rSELECT * FROM single_table WHERE key1 = 'a' OR key_part1 = 'a';\r 第一个查询是因为对 key1 进行了范围匹配，第二个查询是因为联合索引idx_key_part 中的 key_part2 列并没有出现在搜索条件中，所以这两个查询不能进行 Union 索引合并。\n  情况二：主键列可以是范围匹配\n  情况三：使用 Intersection 索引合并的搜索条件 这种情况其实也挺好理解，就是搜索条件的某些部分使用 Intersection 索引合并的方式得到的主键集合和其他方式得到的主键集合取交集，比方说这个查询：\nSELECT * FROM single_table WHERE key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c' OR (key1 = 'a' AND key3 = 'b');\r 优化器可能采用这样的方式来执行这个查询：\n 先按照搜索条件 key1 = \u0026lsquo;a\u0026rsquo; AND key3 = \u0026lsquo;b\u0026rsquo; 从索引 idx_key1 和 idx_key3 中使用 Intersection 索引合并的方式得到一个主键集合。 再按照搜索条件 key_part1 = \u0026lsquo;a\u0026rsquo; AND key_part2 = \u0026lsquo;b\u0026rsquo; AND key_part3 = \u0026lsquo;c\u0026rsquo; 从联合索引idx_key_part 中得到另一个主键集合。 采用 Union 索引合并的方式把上述两个主键集合取并集，然后进行回表操作，将结果返回给用户。    当然，查询条件符合了这些情况也不一定就会采用 Union 索引合并，也得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数比较少，通过 Union 索引合并后进行访问的代价比全表扫描更小时才会使用 Union 索引合并。\nSort-Union合并 Union 索引合并的使用条件太苛刻，必须保证各个二级索引列在进行等值匹配的条件下才可能被用到，比方说下边这个查询就无法使用到 Union 索引合并：\nSELECT * FROM single_table WHERE key1 \u0026lt; 'a' OR key3 \u0026gt; 'z'\r 这是因为根据 key1 \u0026lt; \u0026lsquo;a\u0026rsquo; 从 idx_key1 索引中获取的二级索引记录的主键值不是排好序的，根据 key3 \u0026gt;\u0026lsquo;z\u0026rsquo; 从 idx_key3 索引中获取的二级索引记录的主键值也不是排好序的，但是 key1 \u0026lt; \u0026lsquo;a\u0026rsquo; 和 key3 \u0026gt; \u0026lsquo;z\u0026rsquo; 这两个条件又特别让我们动心，所以我们可以这样：\n 先根据 key1 \u0026lt; \u0026lsquo;a\u0026rsquo; 条件从 idx_key1 二级索引总获取记录，并按照记录的主键值进行排序 再根据 key3 \u0026gt; \u0026lsquo;z\u0026rsquo; 条件从 idx_key3 二级索引总获取记录，并按照记录的主键值进行排序 因为上述的两个二级索引主键值都是排好序的，剩下的操作和 Union 索引合并方式就一样了。  我们把上述这种先按照二级索引记录的主键值进行排序，之后按照 Union 索引合并方式执行的方式称之为 Sort-Union 索引合并，很显然，这种 Sort-Union 索引合并比单纯的 Union 索引合并多了一步对二级索引记录的主键值排序的过程。\n联合索引替代Intersection索引合并 SELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b';\r 这个查询之所以可能使用 Intersection 索引合并的方式执行，还不是因为 idx_key1 和 idx_key3 是两个单独的 B+ 树索引，你要是把这两个列搞一个联合索引，那直接使用这个联合索引就把事情搞定了。因此如果出Intersection索引合并最好采用联合索引优化。\n多表连接 嵌套循环连接（Nested-Loop Join）  对于两表连接来说，驱动表只会被访问一遍，但被驱动表却要被访问到好多遍，具体访问几遍取决于对驱动表执行单表查询后的结果集中的记录条数。对于内连接来说，选取哪个表为驱动表都没关系，而外连接的驱动表是固定的，也就是说左（外）连接的驱动表就是左边的那个表，右（外）连接的驱动表就是右边的那个表\nt1 表和t2 表执行内连接查询的大致过程如下：\n 步骤1：选取驱动表，使用与驱动表相关的过滤条件，选取代价最低的单表访问方法来执行对驱动表的单表 查询。 步骤2：对上一步骤中查询驱动表得到的结果集中每一条记录，都分别到被驱动表中查找匹配的记录。  如果有3个表进行连接的话，那么步骤2 中得到的结果集就像是新的驱动表，然后第三个表就成为了被驱动表， 重复上边过程，也就是步骤2 中得到的结果集中的每一条记录都需要到t3 表中找一找有没有匹配的记录，用伪 代码表示一下这个过程就是这样：\nfor each row in t1 { #此处表示遍历满足对t1单表查询结果集中的每一条记录\rfor each row in t2 { #此处表示对于某条t1表的记录来说，遍历满足对t2单表查询结果集中的\r每一条记录\rfor each row in t3 { #此处表示对于某条t1和t2表的记录组合来说，对t3表进行单表查询\rif row satisfies join conditions, send to client\r}\r}\r}\r 这个过程就像是一个嵌套的循环，所以这种驱动表只访问一次，但被驱动表却可能被多次访问，访问次数取决于 对驱动表执行单表查询后的结果集中的记录条数的连接执行方式称之为嵌套循环连接（ Nested-Loop Join ）， 这是最简单，也是最笨拙的一种连接查询算法\n使用索引加快连接速度  我们知道在嵌套循环连接的步骤2 中可能需要访问多次被驱动表，如果访问被驱动表的方式都是全表扫描的话，妈呀，那得要扫描好多次呀～～～ 回顾一下最开始介绍的t1 表和t2 表进行内连接的例子：\nSELECT * FROM t1, t2 WHERE t1.m1 \u0026gt; 1 AND t1.m1 = t2.m2 AND t2.n2 \u0026lt; 'd';\r 我们使用的其实是嵌套循环连接算法执行的连接查询，再把上边那个查询执行过程表拉下来给大家看一下：\n查询驱动表t1 后的结果集中有两条记录， 嵌套循环连接算法需要对被驱动表查询2次：\n 当t1.m1 = 2 时，去查询一遍t2 表，对t2 表的查询语句相当于： SELECT * FROM t2 WHERE t2.m2 = 2 AND t2.n2 \u0026lt; \u0026rsquo;d'; 当t1.m1 = 3 时，再去查询一遍t2 表，此时对t2 表的查询语句相当于： SELECT * FROM t2 WHERE t2.m2 = 3 AND t2.n2 \u0026lt; \u0026rsquo;d';  可以看到，原来的t1.m1 = t2.m2 这个涉及两个表的过滤条件在针对t2 表做查询时关于t1 表的条件就已经确 定了，所以我们只需要单单优化对t2 表的查询了，上述两个对t2 表的查询语句中利用到的列是m2 和n2 列， 我们可以：\n 在m2 列上建立索引，因为对m2 列的条件是等值查找，比如t2.m2 = 2 、t2.m2 = 3 等，所以可能使用到 ref 的访问方法，假设使用ref 的访问方法去执行对t2 表的查询的话，需要回表之后再判断t2.n2 \u0026lt; d 这 个条件是否成立。这里有一个比较特殊的情况，就是假设m2 列是t2 表的主键或者唯一二级索引列，那么使用t2.m2 = 常数值这样的条件从t2 表中查找记录的过程的代价就是常数级别的。我们知道在单表中使用主键值或者唯一二级索引列的值进行等值查找的方式称之为const ，而设计MySQL 的大叔把在连接查询中对被驱动表使用主键值或者唯一二级索引列的值进行等值查找的查询执行方式称之为： eq_ref 。 在n2 列上建立索引，涉及到的条件是t2.n2 \u0026lt; \u0026rsquo;d' ，可能用到range 的访问方法，假设使用range 的访问 方法对t2 表的查询的话，需要回表之后再判断在m2 列上的条件是否成立。 m2 和n2 列上都存在索引的话，那么就需要从这两个里边儿挑一个代价更低的去执行对t2 表的查询。当 然，建立了索引不一定使用索引，只有在二级索引 + 回表的代价比全表扫描的代价更低时才会使用索引  基于块的嵌套循环连接（Block Nested-Loop Join） 扫描一个表的过程其实是先把这个表从磁盘上加载到内存中，然后从内存中比较匹配条件是否满足。现实生活中 的表可不像t1 、t2 这种只有3条记录，成千上万条记录都是少的，几百万、几千万甚至几亿条记录的表到处都 是。内存里可能并不能完全存放的下表中所有的记录，所以在扫描表前边记录的时候后边的记录可能还在磁盘 上，等扫描到后边记录的时候可能内存不足，所以需要把前边的记录从内存中释放掉。我们前边又说过，采用嵌 套循环连接算法的两表连接过程中，被驱动表可是要被访问好多次的，如果这个被驱动表中的数据特别多而且不 能使用索引进行访问，那就相当于要从磁盘上读好几次这个表，这个I/O 代价就非常大了，所以我们得想办法： 尽量减少访问被驱动表的次数。\n当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录 只会和驱动表结果集的一条记录做匹配，之后就会被从内存中清除掉。然后再从驱动表结果集中拿出另一条记 录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从 磁盘上加载到内存中多少次。\n所以我们可不可以在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载被驱动表的代价了。所以设计MySQL 的大叔提出了一个 join buffer 的概念， join buffer 就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个 join buffer 中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer 中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的I/O 代价\n最好的情况是join buffer 足够大，能容纳驱动表结果集中的所有记录，这样只需要访问一次被驱动表就可以完 成连接操作了。设计MySQL 的大叔把这种加入了join buffer 的嵌套循环连接算法称之为基于块的嵌套连接 （Block Nested-Loop Join）算法。\n这个join buffer 的大小是可以通过启动参数或者系统变量join_buffer_size 进行配置，默认大小为262144字 节（也就是256KB ），最小可以设置为128字节，对于优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer 中，只有查询列表中的列和过滤条件中的列才会被放到join buffer 中，所以再次提醒我们，最好不要把* 作为查询列表，只需要把我们关心的列放到查询列表就好了\nMySQL基于成本的优化 MySQL 执行一个查询可以有不同的执行方案，它会选择其中成本最低那种方案去真正的执行查询。不过我们之前对成本的描述是非常模糊的，其实在MySQL 中一条查询语句的执行成本是由下边这两个方面组成的：\n I/O 成本 我们的表经常使用的MyISAM 、InnoDB 存储引擎都是将数据和索引都存储到磁盘上的，当我们想查询表中的 记录时，需要先把数据或者索引加载到内存中然后再操作。这个从磁盘到内存这个加载的过程损耗的时间称 之为I/O 成本。 CPU 成本 读取以及检测记录是否满足对应的搜索条件、对结果集进行排序等这些操作损耗的时间称之为CPU 成本。  对于InnoDB 存储引擎来说，页是磁盘和内存之间交互的基本单位，设计MySQL 的大叔规定读取一个页面花费的 成本默认是1.0 ，读取以及检测一条记录是否符合搜索条件的成本默认是0.2 。1.0 、0.2 这些数字称之为成 本常数。\n单表查询的成本 基于成本的优化步骤 在一条单表查询语句真正执行之前， MySQL 的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出 成本最低的方案，这个成本最低的方案就是所谓的执行计划，之后才会调用存储引擎提供的接口真正的执行查询，这个过程总结一下就是这样：\n 根据搜索条件，找出所有可能使用的索引 计算全表扫描的代价 计算使用不同索引执行查询的代价 对比各种执行方案的代价，找出成本最低的那一个  我们以这个实例来分析一下这些步骤\nSELECT * FROM single_table WHERE\rkey1 IN ('a', 'b', 'c') AND\rkey2 \u0026gt; 10 AND key2 \u0026lt; 1000 AND\rkey3 \u0026gt; key2 AND\rkey_part1 LIKE '%hello%' AND\rcommon_field = '123';\r 根据搜索条件，找出所有可能使用的索引 我们分析一下上边查询中涉及到的几个搜索条件：\n key1 IN (\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;) ，这个搜索条件可以使用二级索引idx_key1 。 key2 \u0026gt; 10 AND key2 \u0026lt; 1000 ，这个搜索条件可以使用二级索引idx_key2 。 key3 \u0026gt; key2 ，这个搜索条件的索引列由于没有和常数比较，所以并不能使用到索引。 key_part1 LIKE \u0026lsquo;%hello%\u0026rsquo; ， key_part1 通过LIKE 操作符和以通配符开头的字符串做比较，不可以适用 索引。 common_field = \u0026lsquo;123\u0026rsquo; ，由于该列上压根儿没有索引，所以不会用到索引。  综上所述，上边的查询语句可能用到的索引，也就是possible keys 只有idx_key1 和idx_key2 。\n计算全表扫描的代价 对于InnoDB 存储引擎来说，全表扫描的意思就是把聚簇索引中的记录都依次和给定的搜索条件做一下比较，把 符合搜索条件的记录加入到结果集，所以需要将聚簇索引对应的页面加载到内存中，然后再检测记录是否符合搜 索条件。由于查询成本= I/O 成本+ CPU 成本，所以计算全表扫描的代价需要两个信息：\n 聚簇索引占用的页面数 该表中的记录数  这两个信息从哪来呢？设计MySQL 的大叔为每个表维护了一系列的统计信息，使用SHOW TABLE STATUS 语句来查看表的统计信息,例如\nSHOW TABLE STATUS LIKE 't_bls_order_detail';\r 这些属性都基本上可以认识。其中\n  Rows 本选项表示表中的记录条数。对于使用MyISAM 存储引擎的表来说，该值是准确的，对于使用InnoDB 存储引 擎的表来说，该值是一个估计值。\n  Data_length 本选项表示表占用的存储空间字节数。使用MyISAM 存储引擎的表来说，该值就是数据文件的大小，对于使 用InnoDB 存储引擎的表来说，该值就相当于聚簇索引占用的存储空间大小，也就是说可以这样计算该值的 大小：\nData_length = 聚簇索引的页面数量 x 每个页面的大小\n  我们的single_table 使用默认16KB 的页面大小，而上边查询结果显示Data_length 的值是1589248 ，所以我们可以反向来推导出聚簇索引的页面数量： 聚簇索引的页面数量 = 1589248 ÷ 16 ÷ 1024 = 97\n我们现在已经得到了聚簇索引占用的页面数量以及该表记录数的估计值，所以就可以计算全表扫描成本了\n I/O 成本 97 x 1.0 + 1.1 = 98.1 97 指的是聚簇索引占用的页面数， 1.0 指的是加载一个页面的成本常数，后边的1.1 是一个微调值，我们 不用在意。 CPU 成本： 9693 x 0.2 + 1.0 = 1939.6 9693 指的是统计数据中表的记录数，对于InnoDB 存储引擎来说是一个估计值， 0.2 指的是访问一条记录 所需的成本常数，后边的1.0 是一个微调值，我们不用在意。 总成本： 98.1 + 1939.6 = 2037.7  综上所述，对于single_table 的全表扫描所需的总成本就是2037.7 。\n计算使用不同索引执行查询的代价 从第1步分析我们得到，上述查询可能使用到idx_key1 和idx_key2 这两个索引，我们需要分别分析单独使用这 些索引执行查询的成本，最后还要分析是否可能使用到索引合并。这里需要提一点的是， MySQL 查询优化器先分 析使用唯一二级索引的成本，再分析使用普通索引的成本，所以我们也先分析idx_key2 的成本，然后再看使用 idx_key1 的成本。\nidx_key2 对应的搜索条件是： key2 \u0026gt; 10 AND key2 \u0026lt; 1000 ，也就是说对应的范围区间就是： (10, 1000) ，对于使用二级索引 + 回表方式的查询，设计MySQL 的大叔计算这种查询的成本依赖两个方面的数据：\n  范围区间数量 不论某个范围区间的二级索引到底占用了多少页面，查询优化器粗暴的认为读取索引的一个范围区间的I/O 成本和读取一个页面是相同的。本例中使用idx_key2 的范围区间只有一个： (10, 1000) ，所以相当于访问 这个范围区间的二级索引付出的I/O 成本就是： 1 x 1.0 = 1.0\n  需要回表的记录数 优化器需要计算二级索引的某个范围区间到底包含多少条记录，对于本例来说就是要计算idx_key2 在(10,\n1000）这个范围区间中包含多少二级索引记录，计算过程是这样的：\n 步骤1：先根据key2 \u0026gt; 10 这个条件访问一下idx_key2 对应的B+ 树索引，找到满足key2 \u0026gt; 10 这个条 件的第一条记录，我们把这条记录称之为区间最左记录。我们前头说过在B+ 数树中定位一条记录的过 程是贼快的，是常数级别的，所以这个过程的性能消耗是可以忽略不计的。 步骤2：然后再根据key2 \u0026lt; 1000 这个条件继续从idx_key2 对应的B+ 树索引中找出第一条满足这 个条件的记录，我们把这条记录称之为区间最右记录，这个过程的性能消耗也可以忽略不计的。 步骤3：如果区间最左记录和区间最右记录相隔不太远（在MySQL 5.7.21 这个版本里，只要相 隔不大于10个页面即可），那就可以精确统计出满足key2 \u0026gt; 10 AND key2 \u0026lt; 1000 条件的二级索引 记录条数。否则只沿着区间最左记录向右读10个页面，计算平均每个页面中包含多少记录，然后 用这个平均值乘以区间最左记录和区间最右记录之间的页面数量就可以了。那么问题又来了，怎 么估计区间最左记录和区间最右记录之间有多少个页面呢？解决这个问题还得回到B+ 树索引的 结构中来：    如图，我们假设区间最左记录在页b 中， 区间最右记录在页c 中，那么我们想计算区间最左记 录和区间最右记录之间的页面数量就相当于计算页b 和页c 之间有多少页面，而每一条目录项 记录都对应一个数据页，所以计算页b 和页c 之间有多少页面就相当于计算它们父节点（也就是 页a）中对应的目录项记录之间隔着几条记录\n知道了如何统计二级索引某个范围区间的记录数之后，就需要回到现实问题中来，根据上述算法测得 idx_key2 在区间(10, 1000) 之间大约有95 条记录。读取这95 条二级索引记录需要付出的CPU 成本 就是： 95 x 0.2 + 0.01 = 19.01 其中95 是需要读取的二级索引记录条数， 0.2 是读取一条记录成本常数， 0.01 是微调\n在通过二级索引获取到记录之后，还需要干两件事儿：\n  根据这些记录里的主键值到聚簇索引中做回表操作 这里需要大家使劲儿睁大自己滴溜溜的大眼睛仔细瞧，设计MySQL 的大叔评估回表操作的I/O 成本 依旧很豪放，他们认为每次回表操作都相当于访问一个页面，也就是说二级索引范围区间有多少记 录，就需要进行多少次回表操作，也就是需要进行多少次页面I/O 。我们上边统计了使用idx_key2 二级索引执行查询时，预计有95 条二级索引记录需要进行回表操作，所以回表操作带来 的I/O 成本就是： 95 x 1.0 = 95.0 其中95 是预计的二级索引记录数， 1.0 是一个页面的I/O 成本常数。\n  回表操作后得到的完整用户记录，然后再检测其他搜索条件是否成立 回表操作的本质就是通过二级索引记录的主键值到聚簇索引中找到完整的用户记录，然后再检测除 key2 \u0026gt; 10 AND key2 \u0026lt; 1000 这个搜索条件以外的搜索条件是否成立。因为我们通过范围区间获取 到二级索引记录共95 条，也就对应着聚簇索引中95 条完整的用户记录，读取并检测这些完整的用 户记录是否符合其余的搜索条件的CPU 成本如下：\n95 x 0.2 = 19.0\n  所以本例中使用idx_key2 执行查询的成本就如下所示：\n I/O 成本： 1.0 + 95 x 1.0 = 96.0 (范围区间的数量 + 预估的二级索引记录条数) CPU 成本： 95 x 0.2 + 0.01 + 95 x 0.2 = 38.01 （读取二级索引记录的成本 + 读取并检测回表后聚簇索 引记录的成本） 综上所述，使用idx_key2 执行查询的总成本就是： 96.0 + 38.01 = 134.01  使用idx_key1执行的成本就不分析了，其实成本分析我们一般都用不到，只是想了解过程而已。\n基于索引统计数据的成本计算 有时候使用索引执行查询时会有许多单点区间，比如使用IN 语句就很容易产生非常多的单点区间，比如下边这 个查询：\nSELECT * FROM single_table WHERE key1 IN ('aa1', 'aa2', 'aa3', ... , 'zzz');\r 很显然，这个查询可能使用到的索引就是idx_key1 ，由于这个索引并不是唯一二级索引，所以并不能确定一个 单点区间对应的二级索引记录的条数有多少，需要我们去计算。计算方式我们上边已经介绍过了，就是先获取索 引对应的B+ 树的区间最左记录和区间最右记录，然后再计算这两条记录之间有多少记录（记录条数少的时候 可以做到精确计算，多的时候只能估算）。设计MySQL 的大叔把这种通过直接访问索引对应的B+ 树来计算某个 范围区间对应的索引记录条数的方式称之为index dive 。\n有零星几个单点区间的话，使用index dive 的方式去计算这些单点区间对应的记录数也不是什么问题，可是你 架不住有的孩子憋足了劲往IN 语句里塞东西呀，我就见过有的同学写的IN 语句里有20000个参数的🤣🤣，这 就意味着MySQL 的查询优化器为了计算这些单点区间对应的索引记录条数，要进行20000次index dive 操作， 这性能损耗可就大了，搞不好计算这些单点区间对应的索引记录条数的成本比直接全表扫描的成本都大了。设计 MySQL 的大叔们多聪明啊，他们当然考虑到了这种情况，所以提供了一个系统变量\nSHOW VARIABLES LIKE '%dive%';\r  大家需要注意一下，在MySQL 5.7.3以及之前的版本中，eq_range_index_dive_limit的默认值为10，之 后的版本默认值为200。所以如果大家采用的是5.7.3以及之前的版本的话，很容易采用索引统计数据而 不是index dive的方式来计算查询成本。当你的查询中使用到了IN查询，但是却实际没有用到索引，就 应该考虑一下是不是由于 eq_range_index_dive_limit 值太小导致的\n 也就是说如果我们的IN 语句中的参数个数小于这个值的话，将使用index dive 的方式计算各个单点区间对应的 记录条数，如果大于或等于200个的话，可就不能使用index dive 了，要使用所谓的索引统计数据来进行估算\n像会为每个表维护一份统计数据一样， MySQL 也会为表中的每一个索引维护一份统计数据，查看某个表中索引的 统计数据可以使用SHOW INDEX FROM 表名的语法，比如：\nshow index from t_bls_order_detail;\r Non_unique 索引列的值是否是唯一的，聚簇索引和唯一二级索引的该列值为0 ，普通二级索引该列值为1 。 Key_name 索引的名称。 Seq_in_index 索引列在索引中的位置，从1开始计数。比如对于联合索引idx_key_part ，来说， key_part1 、key_part2和key_part3 对应的位置分别是1、2、3。 Column_name 索引列的名称。 Collation 索引列中的值是按照何种排序方式存放的，值为A 时代表升序存放，为NULL 时代表降序存放。 Cardinality 索引列中不重复值的数量。后边我们会重点看这个属性的。 Sub_part 对于存储字符串或者字节串的列来说，有时候我们只想对这些串的前n 个字符或字节建立索引，这个属性表示的就是那个n 值。如果对完整的列建立索引的话，该属性的值就是NULL 。 Packed 索引列如何被压缩， NULL 值表示未被压缩。这个属性我们暂时不了解，可以先忽略掉。 Null 该索引列是否允许存储NULL 值。 Index_type 使用索引的类型，我们最常见的就是BTREE ，其实也就是B+ 树索引。 Comment 索引列注释信息。 Index_comment 索引注释信息。\n我们现在最在意的是Cardinality 属性， Cardinality 直译过来就是基数的意思，表示索引列中不重复值的个数。比如对于一个一万行记录的表来说，某个索引列的Cardinality 属性是10000 ，那意味着该列中没有重复的值，如果Cardinality 属性是1 的话，就意味着该列的值全部是重复的。不过需要注意的是，对于InnoDB存储引擎来说，使用SHOW INDEX语句展示出来的某个索引列的Cardinality属性是一个估计值，并不是精确的。\n当IN 语句中的参数个数大于或等于系统变量eq_range_index_dive_limit 的值的话，就不会使用 index dive 的方式计算各个单点区间对应的索引记录条数，而是使用索引统计数据，这里所指的索引统计数据 指的是这两个值：\n 使用SHOW TABLE STATUS 展示出的Rows 值，也就是一个表中有多少条记录。 使用SHOW INDEX 语句展示出的Cardinality 属性。 结合上一个Rows 统计数据，我们可以针对索引列，计算出平均一个值重复多少次。 一个值的重复次数 ≈ Rows ÷ Cardinality  此时再看上边那条查询语句： SELECT * FROM single_table WHERE key1 IN (\u0026lsquo;aa1\u0026rsquo;, \u0026lsquo;aa2\u0026rsquo;, \u0026lsquo;aa3\u0026rsquo;, \u0026hellip; , \u0026lsquo;zzz\u0026rsquo;); 假设IN 语句中有20000个参数的话，就直接使用统计数据来估算这些参数需要单点区间对应的记录条数了，每 个参数大约对应10 条记录，所以总共需要回表的记录数就是： 20000 x 10 = 200000\n连接查询的成本 Condition filtering介绍 我们前边说过， MySQL 中连接查询采用的是嵌套循环连接算法，驱动表会被访问一次，被驱动表可能会被访问多 次，所以对于两表连接查询来说，它的查询成本由下边两个部分构成：\n 单次查询驱动表的成本 多次查询被驱动表的成本（具体查询多少次取决于对驱动表查询的结果集中有多少条记录）  我们把对驱动表进行查询后得到的记录条数称之为驱动表的扇出（英文名： fanout ）。很显然驱动表的扇出值 越小，对被驱动表的查询次数也就越少，连接查询的总成本也就越低。\n两表连接的成本分析 连接查询的成本计算公式是这样的：\n 连接查询总成本 = 单次访问驱动表的成本 + 驱动表扇出数 x 单次 访问被驱动表的成本\n 对于左（外）连接和右（外）连接查询来说，它们的驱动表是固定的，所以想要得到最优的查询方案只需要： 分别为驱动表和被驱动表选择成本最低的访问方法。 可是对于内连接来说，驱动表和被驱动表的位置是可以互换的，所以需要考虑两个方面的问题：\n 不同的表作为驱动表最终的查询成本可能是不同的，也就是需要考虑最优的表连接顺序。 然后分别为驱动表和被驱动表选择成本最低的访问方法。  比如对于下边这个查询来说：\nSELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2\rON s1.key1 = s2.common_field\rWHERE s1.key2 \u0026gt; 10 AND s1.key2 \u0026lt; 1000 AND\rs2.key2 \u0026gt; 1000 AND s2.key2 \u0026lt; 2000;\r 可以选择的连接顺序有两种：\n s1 连接s2 ，也就是s1 作为驱动表， s2 作为被驱动表。 s2 连接s1 ，也就是s2 作为驱动表， s1 作为被驱动表。  查询优化器需要分别考虑这两种情况下的最优查询成本，然后选取那个成本更低的连接顺序以及该连接顺序下各 个表的最优访问方法作为最终的查询计划。我们分别来看一下）：\n  使用s1 作为驱动表的情况\n 分析对于驱动表的成本最低的执行方案 首先看一下涉及s1 表单表的搜索条件有哪些： s1.key2 \u0026gt; 10 AND s1.key2 \u0026lt; 1000 所以这个查询可能使用到idx_key2 索引，从全表扫描和使用idx_key2 这两个方案中选出成本最低 的那个，这个过程我们上边都唠叨过了，很显然使用idx_key2 执行查询的成本更低些。 然后分析对于被驱动表的成本最低的执行方案 此时涉及被驱动表idx_key2 的搜索条件就是： s2.common_field = 常数（这是因为对驱动表s1 结果集中的每一条记录，都需要进行一次被驱动 表s2 的访问，此时那些涉及两表的条件现在相当于只涉及被驱动表s2 了。） s2.key2 \u0026gt; 1000 AND s2.key2 \u0026lt; 2000 很显然，第一个条件由于common_field 没有用到索引，所以并没有什么卵用，此时访问 single_table2 表时可用的方案也是全表扫描和使用idx_key2 两种，很显然使用idx_key2 的成 本更小。 所以此时使用single_table 作为驱动表时的总成本就是（暂时不考虑使用join buffer 对成本的影 响）： 使用idx_key2访问s1的成本 + s1的扇出 × 使用idx_key2访问s2的成本    使用s2 作为驱动表的情况 分析对于驱动表的成本最低的执行方案 首先看一下涉及s2 表单表的搜索条件有哪些： s2.key2 \u0026gt; 10 AND s2.key2 \u0026lt; 1000 所以这个查询可能使用到idx_key2 索引，从全表扫描和使用idx_key2 这两个方案中选出成本最低的那个，这个过程我们上边都唠叨过了，很显然使用idx_key2 执行查询的成本更低些。然后分析对于被驱动表的成本最低的执行方案此时涉及被驱动表idx_key2 的搜索条件就是： s1.key1 = 常数 s1.key2 \u0026gt; 1000 AND s1.key2 \u0026lt; 2000 这时就很有趣了，使用idx_key1 可以进行ref 方式的访问，使用idx_key2 可以使用range 方式的访问。这是优化器需要从全表扫描、使用idx_key1 、使用idx_key2 这几个方案里选出一个成本最低的方案。这里有个问题啊，因为idx_key2 的范围区间是确定的： (10, 1000) ，怎么计算使用idx_key2 的成本我们上边已经说过了，可是在没有真正执行查询前， s1.key1 = 常数中的常数值我们是不知道的，怎么衡量使用idx_key1 执行查询的成本呢？其实很简单，直接使用索引统计数据就好了（就是索引列平均一个值重复多少次）。一般情况下， ref 的访问方式要比range 成本最低，这里假设使用idx_key1 进行对s2 的访问。所以此时使用single_table 作为驱动表时的总成本就是： 使用idx_key2访问s2的成本 + s2的扇出 × 使用idx_key1访问s1的成本\n  最后优化器会比较这两种方式的最优访问成本，选取那个成本更低的连接顺序去真正的执行查询。从上边的计算 过程也可以看出来，连接查询成本占大头的其实是驱动表扇出数 x 单次访问被驱动表的成本，所以我们的优化重点其实是下边这两个部分：\n 尽量减少驱动表的扇出 对被驱动表的访问成本尽量低  这一点对于我们实际书写连接查询语句时十分有用，我们需要尽量在被驱动表的连接列上建立索引，这样就 可以使用ref 访问方法来降低访问被驱动表的成本了。如果可以，被驱动表的连接列最好是该表的主键或者 唯一二级索引列，这样就可以把访问被驱动表的成本降到更低了。\n如果某个索引列中NULL 值特别多的话，这种统计方式会让优化器认为某个列中平均一个值重复次数特别多，所以倾向于不使用索引进行访问,最好不在索引列中存放NULL值\n生产问题分析  这是一个统计的需求，其中t_bls_order_detail有一百多万条数据，t_bls_order_flow这个表次之，其他两个表数据量较少，当时写完这个sql，一上线就慢sql了\nselect\rs.bar_name as barName,\rsum(o.number) as saleNum,\rsum(o.trans_amt) as saleAmt,\rsum(IFNULL(o.refunded_num, 0) * (o.PRICE + o.PARTS_PRICE)) as refundAmt\rfrom t_bls_order_detail o\rinner join t_bls_order_flow f on o.order_id = f.order_id and\r(f.status = '1' or (f.status = '3' and f.refund_total_amt = f.trans_amt))\rinner join t_bls_product_bar p on o.shop_id = p.shop_id and o.product_id = p.product_id\rinner join t_bls_sales_bar s on p.shop_id = s.shop_id and p.bar_id = s.bar_id\rWHERE o.status = '1'\rand o.SHOP_ID = '1333584333856333855'\rand o.create_time \u0026gt;= '2020-12-30 00:00:00'\rand o.create_time \u0026lt;= '2021-12-30 23:59:59'\rgroup by s.bar_name\rHAVING saleNum \u0026gt; 0\rorder by saleAmt desc;\r MySQL基于规则的优化 条件化简   常量传递（constant_propagation）\n有时候某个表达式是某个列和某个常量做等值匹配，比如这样： a = 5 当这个表达式和其他涉及列a 的表达式使用AND 连接起来时，可以将其他表达式中的a 的值替换为5 比如样： a = 5 AND b \u0026gt; a 就可以被转换为： a = 5 AND b \u0026gt; 5\n  等值传递（equality_propagation）\n有时候多个列之间存在等值匹配的关系，比如这样： a = b and b = c and c = 5 这个表达式可以被简化为： a = 5 and b = 5 and c = 5\n  表达式计算\n在查询开始执行之前，如果表达式中只包含常量的话，它的值会被先计算出来，比如这个： a = 5 + 1 因为5 + 1 这个表达式只包含常量，所以就会被化简成： a = 6 但是这里需要注意的是，如果某个列并不是以单独的形式作为表达式的操作数时，比如出现在函数中出现在某 个更复杂表达式中，就像这样： ABS(a) \u0026gt; 5 或者：-a \u0026lt; -8 优化器是不会尝试对这些表达式进行化简的。因为只有搜索条件中索引列和常数使用某些运算符连接起来才可能使用到索引，所以如果可以的话，最好让索引列以单独的形式出现在表达式中。\n  外连接消除  内连接的驱动表和被驱动表的位置可以相互转换，而左（外）连接和右（外）连接的驱动表和被驱动表是固定的。这就导致内连接可能通过优化表的连接顺序来降低整体的查询成本，而外连接却无法优化表的连接顺序\n外连接和内连接的本质区别就是：对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用NULL值填充；而内连接的驱动表的记录如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录会被舍弃\n那么只要在搜索条件中指定关于被驱动表相关列的值不为NULL ，那么外连接中在被驱动表中找不到符合ON 子句条件的驱动表记录也就被排除出最后的结果集了，也就是说：在这种情况下：外连接和内连接也就没有什么区别了！\n这两个sql本质上是一样的，WHERE 子句中指定了被驱动表t2 的m2 列等于2 ，也就相当于间接的指定了m2 列不为\rNULL 值，\rSELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2 WHERE t2.m2 = 2;\rSELECT * FROM t1 INNER JOIN t2 ON t1.m1 = t2.m2 WHERE t2.m2 = 2;\r 把这种在外连接查询中，指定的WHERE 子句中包含被驱动表中的列不为NULL 值的条件称之为空值拒绝（英文名： reject-NULL ）。在被驱动表的WHERE子句符合空值拒绝的条件后，外连接和内连接可以相互转换。这种转换带来的好处就是查询优化器可以通过评估表的不同连接顺序的成本，选出成本最低的那种连接顺序来执行查询。\n子查询优化 由子查询结果集组成的表称之为派生表\n按返回的结果集区分子查询 因为子查询本身也算是一个查询，所以可以按照它们返回的不同结果集类型而把这些子查询分为不同的类型\n  标量子查询 那些只返回一个单一值的子查询称之为标量子查询，比如这样：\nSELECT (SELECT m1 FROM t1 LIMIT 1);\rSELECT * FROM t1 WHERE m1 = (SELECT MIN(m2) FROM t2);\r 这两个查询语句中的子查询都返回一个单一的值，也就是一个标量。这些标量子查询可以作为一个单一值 或者表达式的一部分出现在查询语句的各个地方。\n  行子查询 顾名思义，就是返回一条记录的子查询，不过这条记录需要包含多个列（只包含一个列就成了标量子查询了）。比如这样：\nSELECT * FROM t1 WHERE (m1, n1) = (SELECT m2, n2 FROM t2 LIMIT 1);\r其中的(SELECT m2, n2 FROM t2 LIMIT 1) 就是一个行子查询\r   列子查询 列子查询自然就是查询出一个列的数据喽，不过这个列的数据需要包含多条记录（只包含一条记录就成了标 量子查询了）。比如这样：\nSELECT * FROM t1 WHERE m1 IN (SELECT m2 FROM t2);\r其中的(SELECT m2 FROM t2) 就是一个列子查询，\r   表子查询 顾名思义，就是子查询的结果既包含很多条记录，又包含很多个列，比如这样：\nSELECT * FROM t1 WHERE (m1, n1) IN (SELECT m2, n2 FROM t2);\r其中的(SELECT m2, n2 FROM t2) 就是一个表子查询\r   按与外层查询关系来区分子查询  不相关子查询 如果子查询可以单独运行出结果，而不依赖于外层查询的值，我们就可以把这个子查询称之为不相关子查 询。我们前边介绍的那些子查询全部都可以看作不相关子查询。 相关子查询 如果子查询的执行需要依赖于外层查询的值，我们就可以把这个子查询称之为相关子查询。比如： SELECT * FROM t1 WHERE m1 IN (SELECT m2 FROM t2 WHERE n1 = n2); 例子中的子查询是(SELECT m2 FROM t2 WHERE n1 = n2) ，可是这个查询中有一个搜索条件是n1 = n2 也就是说子查询的执行需要依赖于外层查询的值，所以这个子查询就是一个相关子查询。  子查询在MySQL中是怎么执行的 标量子查询、行子查询的执行方式 我们经常在下边两个场景中使用到标量子查询或者行子查询：\n SELECT 子句中，我们前边说过的在查询列表中的子查询必须是标量子查询。 子查询使用= 、\u0026gt; 、\u0026lt; 、\u0026gt;= 、\u0026lt;= 、\u0026lt;\u0026gt; 、!= 、\u0026lt;=\u0026gt; 等操作符和某个操作数组成一个布尔表达式，这样的子查询必须是标量子查询或者行子查询。  对于上述两种场景中的不相关标量子查询或者行子查询来说，它们的执行方式是简单的，比方说下边这个查询语句：\nSELECT * FROM s1 WHERE key1 = (SELECT common_field FROM s2 WHERE key3 = 'a' LIMIT 1);\r 它的执行方式：\n 先单独执行(SELECT common_field FROM s2 WHERE key3 = \u0026lsquo;a\u0026rsquo; LIMIT 1) 这个子查询。 然后在将上一步子查询得到的结果当作外层查询的参数再执行外层查询SELECT * FROM s1 WHERE key1 =  也就是说，对于包含不相关的标量子查询或者行子查询的查询语句来说，MySQL会分别独立的执行外层查询和子 查询，就当作两个单表查询就好了。\nSELECT * FROM s1 WHERE key1 = (SELECT common_field FROM s2 WHERE s1.key3 = s2.key3 LIMIT 1);\r 它的执行方式就是这样的：\n 先从外层查询中获取一条记录，本例中也就是先从s1 表中获取一条记录。 然后从上一步骤中获取的那条记录中找出子查询中涉及到的值，本例中就是从s1 表中获取的那条记录中找 出s1.key3 列的值，然后执行子查询。 最后根据子查询的查询结果来检测外层查询WHERE 子句的条件是否成立，如果成立，就把外层查询的那条记 录加入到结果集，否则就丢弃。 再次执行第一步，获取第二条外层查询中的记录，依次类推～  IN子查询优化 物化表 SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a');\r 我们最开始的感觉就是这种不相关的IN 子查询和不相关的标量子查询或者行子查询是一样一样的，都是把外层查询和子查询当作两个独立的单表查询来对待,但是如果单独执行子查询后的结果集太多的话，就会导致这 些问题：\n 结果集太多，可能内存中都放不下～ 对于外层查询来说，如果子查询的结果集太多，那就意味着IN 子句中的参数特别多，这就导致：  无法有效的使用索引，只能对外层查询进行全表扫描。 在对外层查询执行全表扫描时，由于IN 子句中的参数太多，这会导致检测一条记录是否符合和IN 子句 中的参数匹配花费的时间太长。 比如说IN 子句中的参数只有两个： SELECT * FROM tbl_name WHERE column IN (a, b); 这样相当于需要对tbl_name 表中的每条记录判断一下它的column 列是否符合column = a OR column = b 。在IN 子句中的参数比较少时这并不是什么问题，如果IN 子句中的参数比较多时，比如这样： SELECT * FROM tbl_name WHERE column IN (a, b, c \u0026hellip;, \u0026hellip;); 那么这样每条记录需要判断一下它的column 列是否符合column = a OR column = b OR column = c OR \u0026hellip; ，这样性能耗费可就多了。    于是乎设计MySQL 的大叔想了一个招：不直接将不相关子查询的结果集当作外层查询的参数，而是将该结果集写 入一个临时表里。写入临时表的过程是这样的：\n 该临时表的列就是子查询结果集中的列。 写入临时表的记录会被去重。 我们说IN 语句是判断某个操作数在不在某个集合中，集合中的值重不重复对整个IN 语句的结果并没有啥子 关系，所以我们在将结果集写入临时表时对记录进行去重可以让临时表变得更小，更省地方～   小贴士： 临时表如何对记录进行去重？这不是小意思嘛，临时表也是个表，只要为表中记录的所有列建立 主键或者唯一索引就好了嘛～\n  一般情况下子查询结果集不会大的离谱，所以会为它建立基于内存的使用Memory 存储引擎的临时表，而且 会为该表建立哈希索引。   IN语句的本质就是判断某个操作数在不在某个集合里，如果集合中的数据建立了哈希索引，那么这个匹配的过程就是超级快的。所谓的哈希索引其实就是哈希表，key为hash值即为通过特定算法由指定列数据计算出来，value为存储地址即为所在数据行存储在硬盘上的地址（也有可能是其他存储地址，其实MEMORY会将hash表导入内存）\n具体参考https://www.jianshu.com/p/732a16af1f8d\n如果子查询的结果集非常大，超过了系统变量tmp_table_size 或者max_heap_table_size ，临时表会转而 使用基于磁盘的存储引擎来保存结果集中的记录，索引类型也对应转变为B+ 树索引。\n 把这个将子查询结果集中的记录保存到临时表的过程称之为物化（英文名：Materialize ）。为了方便起见，我们就把那个存储子查询结果集的临时表称之为物化表。正因为物化表中的记录都建立了索引（基于内存的物化表有哈希索引，基于磁盘的有B+树索引），通过索引执行IN 语句判断某个操作数在不在子查询结果集中变得非常快，从而提升了子查询语句的性能。\n物化表转连接 SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a');\r 当我们把子查询进行物化之后，假设子查询物化表的名称为materialized_table ，该物化表存储的子查询结果 集的列为m_val,那么这个查询其实可以从下边两种角度来看待：\n 从表s1 的角度来看待，整个查询的意思其实是：对于s1 表中的每条记录来说，如果该记录的key1 列的值 在子查询对应的物化表中，则该记录会被加入最终的结果集。画个图表示一下就是这样：   从子查询物化表的角度来看待，整个查询的意思其实是：对于子查询物化表的每个值来说，如果能在s1 表 中找到对应的key1 列的值与该值相等的记录，那么就把这些记录加入到最终的结果集。画个图表示一下就 是这样：  其实上边的查询就相当于表s1 和子查询物化表materialized_table 进行内连接：\nSELECT s1.* FROM s1 INNER JOIN materialized_table ON key1 = m_val;\r 转化成内连接之后就有意思了，查询优化器可以评估不同连接顺序需要的成本是多少，选取成本最低的那种查询 方式执行查询\n 如果使用s1 表作为驱动表的话，总查询成本由下边几个部分组成：  物化子查询时需要的成本 扫描s1 表时的成本 s1表中的记录数量 × 通过m_val = xxx 对materialized_table 表进行单表访问的成本（我们前边说过物化表中的记录是不重复的，并且为物化表中的列建立了索引，所以这个步骤显然是非常快的）。   如果使用materialized_table 表作为驱动表的话，总查询成本由下边几个部分组成： 物化子查询时需要的成本 扫描物化表时的成本 物化表中的记录数量 × 通过key1 = xxx 对s1 表进行单表访问的成本。  将子查询转换为semi-join SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a');\r 对于s1 表的某条记录来说，由于我们只关心s2 表中是否存在记录满足s1.key1 = s2.common_field 这个条件，而不关心具体有多少条记录与之匹配，我们上边所说的IN 子查询和两表连接之间并不完全等价。但是将子查询转换为连接又真的可以充分发挥优化器的作用，所以设计MySQL 的大叔在这里提出了一个新概念 \u0026mdash; 半连接（英文名： semi-join ）。将s1 表和s2 表进行半连接的意思就是：对于s1 表的某条记录来说，我们只关心在s2 表中是否存在与之匹配的记录是否存在，而不关心具体有多少条记录与之匹配，最终的结果集中只保留s1 表的记录。为了让大家有更直观的感受，我们假设MySQL内部是这么改写上边的子查询的：\nSELECT s1.* FROM s1 SEMI JOIN s2 ON s1.key1 = s2.common_field WHERE key3 = 'a';\r 对于某些使用IN 语句的相关子查询可以使用半连接来执行查询，由于相关子查询并不是一个独立的查询，所以不能转换为物化表来执行查询\nsemi-join的适用条件 当然，并不是所有包含IN 子查询的查询语句都可以转换为semi-join ，只有形如这样的查询才可以被转换为\nSELECT ... FROM outer_tables\rWHERE expr IN (SELECT ... FROM inner_tables ...) AND ...\r或者这样的形式也可以：\rSELECT ... FROM outer_tables\rWHERE (oe1, oe2, ...) IN (SELECT ie1, ie2, ... FROM inner_tables ...) AND ...\r 用文字总结一下，只有符合下边这些条件的子查询才可以被转换为semi-join ：\n 该子查询必须是和IN 语句组成的布尔表达式，并且在外层查询的WHERE 或者ON 子句中出现。 外层查询也可以有其他的搜索条件，只不过和IN 子查询的搜索条件必须使用AND 连接起来。 该子查询必须是一个单一的查询，不能是由若干查询由UNION 连接起来的形式。 该子查询不能包含GROUP BY 或者HAVING 语句或者聚集函数。  不适用于semi-join的情况   外层查询的WHERE条件中有其他搜索条件与IN子查询组成的布尔表达式使用OR 连接起来\nSELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a') OR key2 \u0026gt; 100;\r   使用NOT IN 而不是IN 的情况\nSELECT * FROM s1 WHERE key1 NOT IN (SELECT common_field FROM s2 WHERE key3 = 'a')\r   在SELECT 子句中的IN子查询的情况\nSELECT key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a') FROM s1 ;\r   子查询中包含GROUP BY 、HAVING 或者聚集函数的情况\nSELECT * FROM s1 WHERE key2 IN (SELECT COUNT(*) FROM s2 GROUP BY key1);\r   子查询中包含UNION 的情况\nSELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a'UNION\rSELECT common_field FROM s2 WHERE key3 = 'b');\r   IN 子查询尝试专为EXISTS 子查询\nSELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2 where s1.common_field = s2.common_field) OR key2 \u0026gt; 1000;\rSELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 where s1.common_field =s2.common_field AND s2.key3 = s1.key1) OR key2 \u0026gt; 1000;\r 转为EXISTS 子查询时便可以使用到s2 表的idx_key3 索引了\n  小结一下  如果IN 子查询符合转换为semi-join 的条件，查询优化器会优先把该子查询为semi-join ，然后再考虑下边5种执行半连接的策略中哪个成本最低：  Table pullout DuplicateWeedout LooseScan Materialization FirstMatch   选择成本最低的那种执行策略来执行子查询。 如果IN 子查询不符合转换为semi-join 的条件，那么查询优化器会从下边两种策略中找出一种成本更低的方式执行子查询：  先将子查询物化之后再执行查询 执行IN to EXISTS 转换。    [NOT] EXISTS子查询的执行 如果[NOT] EXISTS 子查询是不相关子查询，可以先执行子查询，得出该[NOT] EXISTS 子查询的结果是TRUE 还 是FALSE ，并重写原先的查询语句，比如对这个查询来说：\nSELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE key1 = 'a') OR key2 \u0026gt; 100;\r假设该EXISTS子查询的结果为TRUE\rSELECT * FROM s1 WHERE TRUE OR key2 \u0026gt; 100;\rSELECT * FROM s1 WHERE TRUE;\r 对于相关的[NOT] EXISTS 子查询来说，比如这个查询：\nSELECT * FROM s1 WHERE EXISTS (SELECT 1 FROM s2 WHERE s1.common_field = s2.common_field);\r 很不幸，这个查询只能按照我们年少时的那种执行相关子查询的方式来执行。\n 先从外层查询中获取一条记录，本例中也就是先从s1 表中获取一条记录。 然后从上一步骤中获取的那条记录中找出子查询中涉及到的值，本例中就是从s1 表中获取的那条记录中找出s1.key2 列的值，然后执行子查询。 最后根据子查询的查询结果来检测外层查询WHERE 子句的条件是否成立，如果成立，就把外层查询的那 条记录加入到结果集，否则就丢弃。 再次执行第一步，获取第二条外层查询中的记录，依次类推～  派生表的优化 过把子查询放在外层查询的FROM 子句后，那么这个子查询的结果相当于一个派生表\nSELECT * FROM (\r-- 这个子查询就是派生表\rSELECT id AS d_id, key3 AS d_key3 FROM s2 WHERE key1 = 'a'\r) AS derived_s1 WHERE d_key3 = 'a';\r 对于含有派生表的查询， MySQL 提供了两种执行策略：\n  最容易想到的就是把派生表物化。 我们可以将派生表的结果集写到一个内部的临时表中，然后就把这个物化表当作普通表一样参与查询。当 然，在对派生表进行物化时，设计MySQL 的大叔使用了一种称为延迟物化的策略，也就是在查询中真正使 用到派生表时才回去尝试物化派生表，而不是还没开始执行查询呢就把派生表物化掉。比方说对于下边这个 含有派生表的查询来说：\nSELECT * FROM (\rSELECT * FROM s1 WHERE key1 = 'a'\r) AS derived_s1 INNER JOIN s2 ON derived_s1.key1 = s2.key1 WHERE s2.key2 = 1;\r 如果采用物化派生表的方式来执行这个查询的话，那么执行时首先会到s1 表中找出满足s1.key2 = 1 的记录，如果压根儿找不到，说明参与连接的s1 表记录就是空的，所以整个查询的结果集就是空的，所以也就没有必要去物化查询中的派生表了\n  将派生表和外层的表合并，也就是将查询重写为没有派生表的形式\n我们来看这个贼简单的包含派生表的查询：\nSELECT * FROM (SELECT * FROM s1 WHERE key1 = 'a') AS derived_s1;\r等价于\rSELECT * FROM s1 WHERE key1 = 'a';\r 对于一些稍微复杂的包含派生表的语句，比如我们上边提到的那个：\nSELECT * FROM (SELECT * FROM s1 WHERE key1 = 'a') AS derived_s1 INNER JOIN s2 ON derived_s1.key1 = s2.key1 WHERE s2.key2 = 1;\r我们可以将派生表与外层查询的表合并，然后将派生表中的搜索条件放到外层查询的搜索条件中：\rSELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.key1 = 'a' AND s2.key2 = 1;\r   这样通过将外层查询和派生表合并的方式成功的消除了派生表，也就意味着我们没必要再付出创建和访问临时表的成本了。可是并不是所有带有派生表的查询都能被成功的和外层查询合并，当派生表中有这些语句就不可以和外层查询合并：\n 聚集函数，比如MAX()、MIN()、SUM()啥的 DISTINCT GROUP BY HAVING LIMIT UNION 或者 UNION ALL 派生表对应的子查询的SELECT 子句中含有另一个子查询  这一章读下来并没有感觉学到很多东西\nExplain详解 一条查询语句在经过MySQL 查询优化器的各种基于成本和规则的优化会后生成一个所谓的执行计划，这个执行 计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访问方法来具体执行 查询等等。使用EXPLAIN 语句来帮助我们查看某个查询语句的具体执行计划\n   列名 描述     id 在一个大的查询语句中每个SELECT 关键字都对应一个唯一的id   select_type SELECT 关键字对应的那个查询的类型   table 表名   partitions 匹配的分区信息   type 针对单表的访问方法   possible_keys 可能用到的索引   key 实际上使用的索引   key_len 实际使用到的索引长度   ref 当使用索引列等值查询时，与索引列进行等值匹配的对象信息   rows 预估的需要读取的记录条数   filtered 某个表经过搜索条件过滤后剩余记录条数的百分比   Extra 一些额外的信息    id  查询语句中每出现一个SELECT 关键字，设计MySQL 的大叔就会为它分配一个唯一的id 值。\n对于连接查询来说，一个SELECT 关键字后边的FROM 子句中可以跟随多个表，所以在连接查询的执行计划中，每 个表都会对应一条记录，但是这些记录的id值都是相同的，sql为生产问题分析中的sql\n在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前边的表表示驱动表，出现在后边的表表示被驱动表\n 对于包含子查询的查询语句来说，就可能涉及多个SELECT 关键字，所以在包含子查询的查询语句的执行计划中，每个SELECT 关键字都会对应一个唯一的id 值，但是查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询,。所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了\n 总结：\n表示一个查询中各个子查询的执行顺序;\n id相同执行顺序由上至下。 id不同，id值越大优先级越高，越先被执行。 id为 null 时表示一个结果集，不需要使用它查询，常出现在包含 union等查询语句中  select_type  每一个SELECT 关键字代表的小查询都定义了一个称之为select_type 的属性，意思是我们只要知道了某个小查询的select_type 属性，就知道了这个小查询在整个大查询中扮演了一个什么角色,具体详情如下：\n  SIMPLE\n查询语句中不包含UNION 或者子查询的查询都算作是SIMPLE 类型,例如\nSELECT * FROM s1;\rSELECT * FROM s1 INNER JOIN s2;\r   PRIMARY\n对于包含UNION 、UNION ALL 或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type 值就是PRIMARY\n  UNION\n对于包含UNION 或者UNION ALL 的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type 值就是UNION\n  UNION RESULT\nMySQL 选择使用临时表来完成UNION 查询的去重工作，针对该临时表的查询的select_type 就是UNION RESULT\n  SUBQUERY\n如果包含子查询的查询语句不能够转为对应的semi-join 的形式，并且该子查询是不相关子查询，并且查询 优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT 关键字代表的那个查 询的select_type 就是SUBQUERY\n  DEPENDENT SUBQUERY\n如果包含子查询的查询语句不能够转为对应的semi-join 的形式，并且该子查询是相关子查询，则该子查询 的第一个SELECT 关键字代表的那个查询的select_type 就是DEPENDENT SUBQUERY\n  DEPENDENT UNION\n在包含UNION 或者UNION ALL 的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小 查询之外，其余的小查询的select_type 的值就是DEPENDENT UNION\n  DERIVED\n对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type 就是DERIVED\n  MATERIALIZED\n当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type 属性就是MATERIALIZED\n  type  我们前边说过执行计划的一条记录就代表着MySQL 对某个表的执行查询时的访问方法，其中的type 列就表明了 这个访问方法是个啥，完整的访问方法如下： system ， const ，eq_ref ， ref ， fulltext ， ref_or_null ， index_merge ， unique_subquery ， index_subquery ，range ， index ， ALL。\n  system\n当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的 访问方法就是system 。比方说我们新建一个MyISAM 表，并为其插入一条记录\n  const\n就是当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const\n  eq_ref\n在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref\n  ref\n通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref\n  ref_or_null\n当对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL 值时，那么对该表的访问方法就可能是 ref_or_null\n  index_merge\n一般情况下对于某个表的查询只能使用到一个索引，但我们唠叨单表访问方法时特意强调了在某些场景下可 以使用Intersection 、Union 、Sort-Union 这三种索引合并的方式来执行查询\n  unique_subquery\n类似于两表连接中被驱动表的eq_ref 访问方法， unique_subquery 是针对在一些包含IN 子查询的查询语 句中，如果查询优化器决定将IN 子查询转换为EXISTS 子查询，而且子查询可以使用到主键进行等值匹配的 话，那么该子查询执行计划的type 列的值就是unique_subquery\nSELECT * FROM s1 WHERE key2 IN (SELECT id FROM s2 where s1.key1 = s2.key1) OR key3 = 'a';\r   index_subquery\nindex_subquery 与unique_subquery 类似，只不过访问子查询中的表时使用的是普通的索引\n ```\rSELECT * FROM s1 WHERE common_field IN (SELECT key3 FROM s2 where s1.key1 = s2.key1) OR key3 = 'a';\r```\r   range\n使用索引获取某些范围区间的记录，那么就可能使用到range 访问方法\n  index\n当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是index\nSELECT key_part2 FROM s1 WHERE key_part3 = 'a';\r上述查询中的搜索列表中只有key_part2 一个列，而且搜索条件中也只有key_part3 一个列，这两个列又恰\r好包含在idx_key_part 这个索引中，可是搜索条件key_part3 不能直接使用该索引进行ref 或者range 方\r式的访问，只能扫描整个idx_key_part 索引的记录，所以查询计划的type 列的值就是index\r   All\n全表扫描\n  一般来说，这些访问方法按照我们介绍它们的顺序性能依次变差。其中除了All 这个访问方法外，其余的访问方 法都能用到索引，除了index_merge 访问方法外，其余的访问方法都最多只能用到一个索引。\npossible_keys和key possible_keys 列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些， key 列表示实际用到的索引有哪些,possible_keys列中的值并不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，所以如果可以的话，尽量删除那些用不到的索引。\nkey_len key_len 列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的：\n 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的 变长类型的索引列来说，比如某个索引列的类型是VARCHAR(100) ，使用的字符集是utf8 ，那么该列实际占 用的最大存储空间就是100 × 3 = 300 个字节。 如果该索引列可以存储NULL 值，则key_len 比不可以存储NULL 值时多1个字节。 对于变长字段来说，都会有2个字节的空间来存储该变长列的实际长度。  ref 当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是const 、eq_ref 、ref 、ref_or_null 、 unique_subquery 、index_subquery 其中之一时， ref 列展示的就是与索引列作等值匹配的东东是个啥，比如 只是一个常数(const)或者是某个列。还是这个图\nrows 执行计划的rows 列就代表预计扫描的索引记录行数\nExtra Extra 列是用来说明一些额外信息的，我们可以通过这些额外信息来更准确的理解MySQL 到底将如何执行给定的查询语句\n  Using index 当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra 列将会提示该额外信息\n  Using index condition\n有些搜索条件中虽然出现了索引列，但却不能使用到索引\n  SELECT * FROM s1 WHERE key1 \u0026gt; 'z' AND key1 LIKE '%a';\r 其中的key1 \u0026gt; \u0026lsquo;z\u0026rsquo; 可以使用到索引，但是key1 LIKE \u0026lsquo;%a\u0026rsquo; 却无法使用到索引，在以前版本的MySQL 中，是按照下边步骤来执行这个查询的：\n  先根据key1 \u0026gt; \u0026lsquo;z\u0026rsquo; 这个条件，从二级索引idx_key1 中获取到对应的二级索引记录。\n  根据上一步骤得到的二级索引记录中的主键值进行回表，找到完整的用户记录再检测该记录是否符合key1 LIKE \u0026lsquo;%a\u0026rsquo; 这个条件，将符合条件的记录加入到最后的结果集。\n但是虽然key1 LIKE \u0026lsquo;%a\u0026rsquo; 不能组成范围区间参与range 访问方法的执行，但这个条件毕竟只涉及到了 key1 列，所以设计MySQL 的大叔把上边的步骤改进了一下：\n  先根据key1 \u0026gt; \u0026lsquo;z\u0026rsquo; 这个条件，定位到二级索引idx_key1 中对应的二级索引记录\n  对于指定的二级索引记录，先不着急回表，而是先检测一下该记录是否满足key1 LIKE \u0026lsquo;%a\u0026rsquo; 这个条件，如果这个条件不满足，则该二级索引记录压根儿就没必要回表。\n  对于满足key1 LIKE \u0026lsquo;%a\u0026rsquo; 这个条件的二级索引记录执行回表操作。\n  我们说回表操作其实是一个随机IO ，比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表操作的成本。设计MySQL 的大叔们把他们的这个改进称之为索引条件下推\n如果在查询语句的执行过程中将要使用索引条件下推这个特性在Extra 列中将会显示Using index condition\n  Using where\n  当使用全表扫描来执行对某个表的查询，并且该语句的WHERE 子句中有针对该表的搜索条件时，在 Extra 列中会提示上述额外信息。\n  当使用索引访问来执行对某个表的查询，并且该语句的WHERE 子句中有除了该索引包含的列之外的其他搜索条件时，在Extra 列中也会提示上述额外信息。\nSELECT * FROM s1 WHERE common_field = 'a';\rkey1是索引，但是common_field不是\rSELECT * FROM s1 WHERE key1 = 'a' AND common_field = 'a';\r     Using join buffer (Block Nested Loop)\n在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度， MySQL 一般会为其分配一块名叫 join buffer 的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法\n  Not exists\n当我们使用左（外）连接时，如果WHERE 子句中包含要求被驱动表的某个列等于NULL 值的搜索条件，而且 那个列又是不允许存储NULL 值的\nSELECT * FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.id IS NULL;\r s1 表是驱动表， s2 表是被驱动表， s2.id 列是不允许存储NULL 值的，而WHERE 子句中又包含s2.id IS NULL 的搜索条件，这意味着必定是驱动表的记录在被驱动表中找不到匹配ON 子句条件的记录才会把该驱动表的记录加入到最终的结果集，所以对于某条驱动表中的记录来说，如果能在被驱动表中找到1条符合ON 子句条件的记录，那么该驱动表的记录就不会被加入到最终的结果集，也就是说**我们没有必要到被驱动表中找到全部符合ON子句条件的记录，**其实就是说不需要查s2表了吗？\n  Using intersect(\u0026hellip;) 、Using union(\u0026hellip;) 和Using sort_union(\u0026hellip;)\n如果执行计划的Extra 列出现了Using intersect(\u0026hellip;) 提示，说明准备使用Intersect 索引合并的方式执行查询，括号中的\u0026hellip; 表示需要进行索引合并的索引名称；如果出现了Using union(\u0026hellip;) 提示，说明准备使用Union 索引合并的方式执行查询；出现了Using sort_union(\u0026hellip;) 提示，说明准备使用Sort-Union 索引合并的方式执行查询。\n  Using filesort\n排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）进行排序，设计MySQL 的大叔把这种在内存中或者磁盘上进行排序的方式统称为文件排序\n  Using temporary\n在许多查询的执行过程中， MySQL 可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在 执行许多包含DISTINCT 、GROUP BY 、UNION 等子句的查询过程中，如果不能有效利用索引来完成查询， MySQL 很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行计划 的Extra 列将会显示Using temporary，MySQL 会在包含GROUP BY子句的查询中默认添加上ORDER BY 子句\n这两条语句作用相等\rselect trans_amt from t_bls_order_detail group by trans_amt;\rselect trans_amt from t_bls_order_detail group by trans_amt order by trans_amt;\r 执行计划中出现Using temporary 并不是一个好的征兆，因为建立与维护临时表要付出很大成本的，所以我们最好能使用索引来替代掉使用临时表\n  Start temporary, End temporary\n我们前边唠叨子查询的时候说过，查询优化器会优先尝试将IN 子查询转换成semi-join ，而semi-join 又 有好多种执行策略，当执行策略为DuplicateWeedout 时，也就是通过建立临时表来实现为外层查询中的记 录进行去重操作时，驱动表查询执行计划的Extra 列将显示Start temporary 提示，被驱动表查询执行计划 的Extra 列将显示End temporary 提示，\n  LooseScan\n在将In 子查询转为semi-join 时，如果采用的是LooseScan 执行策略，则在驱动表执行计划的Extra 列就是显示LooseScan\n  FirstMatch(tbl_name)\n在将In 子查询转为semi-join 时，如果采用的是FirstMatch 执行策略，则在被驱动表执行计划的Extra列就是显示FirstMatch(tbl_name)\n  Json格式的执行计划 我们上边介绍的EXPLAIN 语句输出中缺少了一个衡量执行计划好坏的重要属性 —— 成本。不过设计MySQL 的大 叔贴心的为我们提供了一种查看某个执行计划花费的成本的方式：\n在EXPLAIN 单词和真正的查询语句中间加上FORMAT=JSON 。\n{\r\u0026quot;query_block\u0026quot;: {\r# 整个查询语句只有1个SELECT关键字，该关键字对应的id号为1\r\u0026quot;select_id\u0026quot;: 1,\r\u0026quot;ordering_operation\u0026quot;: {\r\u0026quot;using_filesort\u0026quot;: true,\r\u0026quot;grouping_operation\u0026quot;: {\r\u0026quot;using_temporary_table\u0026quot;: true,\r\u0026quot;using_filesort\u0026quot;: false,\r# 几个表之间采用嵌套循环连接算法执行\r\u0026quot;nested_loop\u0026quot;: [\r# 以下是参与嵌套循环连接算法的各个表的信息\r{\r\u0026quot;table\u0026quot;: {\r# 第一个表是驱动表\r\u0026quot;table_name\u0026quot;: \u0026quot;o\u0026quot;,\r\u0026quot;access_type\u0026quot;: \u0026quot;range\u0026quot;,\r\u0026quot;possible_keys\u0026quot;: [\r\u0026quot;t_bls_order_detail_order_id_index\u0026quot;,\r\u0026quot;ID_TIME\u0026quot;\r],\r\u0026quot;key\u0026quot;: \u0026quot;ID_TIME\u0026quot;,\r\u0026quot;used_key_parts\u0026quot;: [\r\u0026quot;shop_id\u0026quot;,\r\u0026quot;create_time\u0026quot;\r],\r\u0026quot;key_length\u0026quot;: \u0026quot;105\u0026quot;,\r\u0026quot;rows\u0026quot;: 21,\r\u0026quot;filtered\u0026quot;: 76.19,\r\u0026quot;index_condition\u0026quot;: \u0026quot;((`dingjia_yszx`.`o`.`shop_id` = '1333584333856333855') and (`dingjia_yszx`.`o`.`create_time` \u0026gt;= '2021-12-30 00:00:00') and (`dingjia_yszx`.`o`.`create_time` \u0026lt;= '2021-12-30 23:59:59'))\u0026quot;,\r\u0026quot;attached_condition\u0026quot;: \u0026quot;((`dingjia_yszx`.`o`.`status` = '1') and (`dingjia_yszx`.`o`.`order_id` is not null))\u0026quot;\r}\r},\r{\r\u0026quot;table\u0026quot;: {\r\u0026quot;table_name\u0026quot;: \u0026quot;f\u0026quot;,\r\u0026quot;access_type\u0026quot;: \u0026quot;eq_ref\u0026quot;,\r\u0026quot;possible_keys\u0026quot;: [\r\u0026quot;PRIMARY\u0026quot;\r],\r\u0026quot;key\u0026quot;: \u0026quot;PRIMARY\u0026quot;,\r\u0026quot;used_key_parts\u0026quot;: [\r\u0026quot;order_id\u0026quot;\r],\r\u0026quot;key_length\u0026quot;: \u0026quot;98\u0026quot;,\r\u0026quot;ref\u0026quot;: [\r\u0026quot;dingjia_yszx.o.order_id\u0026quot;\r],\r\u0026quot;rows\u0026quot;: 1,\r\u0026quot;filtered\u0026quot;: 100,\r\u0026quot;attached_condition\u0026quot;: \u0026quot;((`dingjia_yszx`.`f`.`status` = '1') or ((`dingjia_yszx`.`f`.`trans_amt` = `dingjia_yszx`.`f`.`refund_total_amt`) and (`dingjia_yszx`.`f`.`status` = '3')))\u0026quot;\r}\r},\r{\r\u0026quot;table\u0026quot;: {\r\u0026quot;table_name\u0026quot;: \u0026quot;p\u0026quot;,\r\u0026quot;access_type\u0026quot;: \u0026quot;ALL\u0026quot;,\r\u0026quot;possible_keys\u0026quot;: [\r\u0026quot;idx_product_bar_barid\u0026quot;,\r\u0026quot;idx_product_bar_shopid\u0026quot;\r],\r\u0026quot;rows\u0026quot;: 784,\r\u0026quot;filtered\u0026quot;: 50.383,\r\u0026quot;using_join_buffer\u0026quot;: \u0026quot;Block Nested Loop\u0026quot;,\r\u0026quot;attached_condition\u0026quot;: \u0026quot;(((`dingjia_yszx`.`p`.`product_id` = `dingjia_yszx`.`o`.`product_id`) and (`dingjia_yszx`.`p`.`shop_id` = `dingjia_yszx`.`o`.`shop_id`)) and (`dingjia_yszx`.`p`.`bar_id` is not null))\u0026quot;\r}\r},\r{\r\u0026quot;table\u0026quot;: {\r\u0026quot;table_name\u0026quot;: \u0026quot;s\u0026quot;,\r\u0026quot;access_type\u0026quot;: \u0026quot;eq_ref\u0026quot;,\r\u0026quot;possible_keys\u0026quot;: [\r\u0026quot;PRIMARY\u0026quot;\r],\r\u0026quot;key\u0026quot;: \u0026quot;PRIMARY\u0026quot;,\r\u0026quot;used_key_parts\u0026quot;: [\r\u0026quot;bar_id\u0026quot;\r],\r\u0026quot;key_length\u0026quot;: \u0026quot;98\u0026quot;,\r\u0026quot;ref\u0026quot;: [\r\u0026quot;dingjia_yszx.p.bar_id\u0026quot;\r],\r\u0026quot;rows\u0026quot;: 1,\r\u0026quot;filtered\u0026quot;: 100,\r\u0026quot;attached_condition\u0026quot;: \u0026quot;(`dingjia_yszx`.`s`.`shop_id` = `dingjia_yszx`.`o`.`shop_id`)\u0026quot;\r}\r}\r]\r}\r}\r}\r}\r Extented EXPLAIN 最后，设计MySQL 的大叔还为我们留了个彩蛋，在我们使用EXPLAIN 语句查看了某个查询的执行计划后，紧接着 还可以使用SHOW WARNINGS 语句查看与这个查询的执行计划有关的一些扩展信息\nSHOW WARNINGS 展示出来的信息有三个字段，分别是Level 、Code 、Message 。我们最常见的就是Code 为1003 的信息，当Code 值为1003 时， Message 字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。\n事务的隔离级别与MVCC 复习一些基础概念\n 事务：是由一系列对系统中数据进行访问与更新的操作所组成的一个程序执行逻辑单元，\n 事务的四个特征(ACID)\n  原子性（Atomicity）：事务包含的 各项操作在一次执行中，要么执行成功，要么全部不执行\n  一致性（Consistency）:事务执行的结果必须是使数据库从一个一致性状态到另一个一致性状态\n  隔离性（Isolation）：并发的事务是相互隔离的，一个事务的执行不能被其他事务干扰。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）\n  持久性（Durability）：事务一旦提交，它对数据库中对应数据的状态变更就应该是永久性的。\n  事务并发执行遇到的问题   脏写（ Dirty Write ）: 如果一个事务修改了另一个未提交事务修改过的数据，那就意味着发生了脏写 脏读（ Dirty Read ）：一个事务读到了另一个未提交事务修改过的数据，A事务读取B事务尚未提交的数据，此时如果B事务发生错误并执行回滚操作，那么A事务读取到的数据就是脏数据。 不可重复读（Non-Repeatable Read）：前后多次读取，数据内容不一致。事务A在执行读取操作，由整个事务A比较大，前后读取同一条数据需要经历很长的时间 。而在事务A第一次读取数据，比如此时读取了小明的年龄为20岁，事务B执行更改操作，将小明的年龄更改为30岁，此时事务A第二次读取到小明的年龄时，发现其年龄是30岁，和之前的数据不一样了，也就是数据不重复了。 幻读（Phantom）：一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来  SQL标准中的四种隔离级别 这些问题按照严重性来排一下序： 脏写 \u0026gt; 脏读 \u0026gt; 不可重复读 \u0026gt; 幻读 设立一些隔离级别，隔离级别越低，越严重的问题就越可能发生。有一帮人（并不是设计MySQL 的大叔们）制定了一个所谓的SQL标准，在标准中设立了4个隔离级别：\n READ UNCOMMITTED ：未提交读。可能发生脏读、不可重复读和幻读问题。 READ COMMITTED ：已提交读。可能发生不可重复读和幻读问题，但是不可以发生脏读问题。 REPEATABLE READ ：可重复读。可能发生幻读问题，但是不可以发生脏读和不可重复读的问题。 SERIALIZABLE ：可串行化。各种问题都不可以发生。  MVCC  多版本并发控制技术的英文全称是 Multiversion Concurrency Control，简称 MVCC。\n多版本并发控制（MVCC） 是通过保存数据在某个时间点的快照来实现并发控制的。也就是说，不管事务执行多长时间，事务内部看到的数据是不受其它事务影响的，根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。\n简单来说，多版本并发控制 的思想就是保存数据的历史版本，通过对数据行的多个版本管理来实现数据库的并发控制。这样我们就可以通过比较版本号决定数据是否显示出来，读取数据的时候不需要加锁也可以保证事务的隔离效果。\n可以认为 多版本并发控制（MVCC） 是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行\n版本链 对于使用InnoDB 存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列：\n trx_id ：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的事务id 赋值给trx_id 隐藏列。 roll_pointer ：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到undo日志中，然后这个隐藏 列就相当于一个指针，可以通过它来找到该记录修改前的信息。  在hero表中插入一条记录，假设插入该记录的事务id 为80 ，那么此刻该条记录的示意图如下所示：\n 实际上insert undo只在事务回滚时起作用，当事务提交后，该类型的undo日志就没用了，它占用的Undo Log Segment也会被系统回收（也就是该undo日志占用的Undo页面链表要么被重用，要么被释放）。 虽然真正的insert undo日志占用的存储空间被释放了，但是roll_pointer的值并不会被清除，roll_pointer属性占用7个字节，第一个比特位就标记着它指向的undo日志的类型，如果该比特位的值为1时，就代表着它指向的undo日志类型为insert undo。所以我们之后在画图时都会把insert undo给去掉，大家留意一下就好了。\n 假设之后两个事务id 分别为100 、200 的事务对这条记录进行UPDATE 操作，操作流程如下：\n 能不能在两个事务中交叉更新同一条记录呢？哈哈，这不就是一个事务修改了另一个未提交事务修改过的数据，沦为了脏写了么？InnoDB使用锁来保证不会有脏写情况的发生，也就是在第一个事务更新了某条记录后，就会给这条记录加锁，另一个事务再次更新时就需要等待第一个事务提交了，把锁释放之后才可以继续更新。关于锁的更多细节我们后续的文章中再唠叨哈～\n 每次对记录进行改动，都会记录一条undo日志，每条undo日志也都有一个roll_pointer 属性（ INSERT 操作对应的undo日志没有该属性，因为该记录并没有更早的版本），可以将这些undo日志都连起来，串成一个链表，所以现在的情况就像下图一样：\n对该记录每次更新后，都会将旧值放到一条undo日志中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被roll_pointer 属性连接成一个链表，我们把这个链表称之为版本链，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的事务id\nReadView 对于使用READ UNCOMMITTED 隔离级别的事务来说，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了；对于使用SERIALIZABLE 隔离级别的事务来说，设计InnoDB 的大叔规定使用加锁的方式来访问记录;对于使用READ COMMITTED 和REPEATABLE READ 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是：需要判断一下版本链中的哪个版本是当前事务可见的。为此，设计InnoDB 的大叔提出了一个ReadView 的概念，这个ReadView 中主要包含4个比较重要的内容:\n  m_ids ：表示在生成ReadView 时当前系统中活跃的读写事务的事务id 列表。\n  min_trx_id ：表示在生成ReadView 时当前系统中活跃的读写事务中最小的事务id ，也就是m_ids 中的最小值。\n  max_trx_id ：表示生成ReadView 时系统中应该分配给下一个事务的id 值。\n 注意max_trx_id并不是m_ids中的最大值，事务id是递增分配的。比方说现在有id为1，2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，m_ids就包括1和2，min_trx_id的值就是1，max_trx_id的值就是4。\n   creator_trx_id ：表示生成该ReadView 的事务的事务id 。 我们前边说过，只有在对表中的记录做改动时（执行INSERT、DELETE、UPDATE这些语句时）才会为事务分配事务id，否则在一个只读事务中的事务id值都默认为0。\n  在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见：\n 如果被访问版本的trx_id 属性值与ReadView 中的creator_trx_id 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。 如果被访问版本的trx_id 属性值小于ReadView 中的min_trx_id 值，表明生成该版本的事务在当前事务生成ReadView 前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的trx_id 属性值大于ReadView 中的max_trx_id 值，表明生成该版本的事务在当前事务生成ReadView 后才开启，所以该版本不可以被当前事务访问。 如果被访问版本的trx_id 属性值在ReadView 的min_trx_id 和max_trx_id 之间，那就需要判断一下trx_id 属性值是不是在m_ids 列表中，如果在，说明创建ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建ReadView 时生成该版本的事务已经被提交，该版本可以被访问。  如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性，依此类推，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该条记录对该事务完全不可见，查询结果就不包含该记录。\n在MySQL 中， READ COMMITTED 和REPEATABLE READ 隔离级别的的一个非常大的区别就是它们生成ReadView的时机不同。假设现在表hero 中只有一条由事务id 为80 的事务插入的一条记录\nREAD COMMITTED —— 每次读取数据前都生成一个ReadView 比方说现在系统里有两个事务id 分别为100 、200 的事务在执行：\nransaction 100\rBEGIN;\rUPDATE hero SET name = '关羽' WHERE number = 1;\rUPDATE hero SET name = '张飞' WHERE number = 1;\rTransaction 200\rBEGIN;\r更新了一些别的表的记录\r...\r 再次强调一遍，**事务执行过程中，只有在第一次真正修改记录时（比如使用INSERT、DELETE、UPDATE语句），才会被分配一个单独的事务id，这个事务id是递增的。**所以我们才在Transaction 200中更新一些别的表的记录，目的是让它分配事务id。\n此刻，表hero 中number 为1 的记录得到的版本链表如下所示：\n假设现在有一个使用READ COMMITTED 隔离级别的事务开始执行：\nBEGIN;\rSELECT1：Transaction 100、200未提交\rSELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备'\r 这个SELECT1 的执行过程如下：\n 在执行SELECT 语句时会先生成一个ReadView ， ReadView 的m_ids 列表的内容就是[100, 200] ， min_trx_id 为100 ， max_trx_id 为201 ， creator_trx_id 为0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列name 的内容是\u0026rsquo;张飞' ，该版本的 trx_id 值为100 ，在m_ids 列表内，所以不符合可见性要求，根据roll_pointer 跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;关羽' ，该版本的trx_id 值也为100 ，也在m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;刘备' ，该版本的trx_id 值为80 ，小于ReadView 中的min_trx_id 值100 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列name 为\u0026rsquo;刘备' 的记录。  之后，我们把事务id 为100 的事务提交一下，就像这样：\nTransaction 100\rBEGIN;\rUPDATE hero SET name = '关羽' WHERE number = 1;\rUPDATE hero SET name = '张飞' WHERE number = 1;\rCOMMIT;\r然后再到事务id 为200 的事务中更新一下表hero 中number 为1 的记录：\rTransaction 200\rBEGIN;\r更新了一些别的表的记录\r...\rUPDATE hero SET name = '赵云' WHERE number = 1;\rUPDATE hero SET name = '诸葛亮' WHERE number = 1;\r 此刻，表hero 中number 为1 的记录的版本链就长这样：\n然后再到刚才使用READ COMMITTED 隔离级别的事务中继续查找这个number 为1 的记录，如下：\nBEGIN;\rSELECT1：Transaction 100、200均未提交\rSELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备'\rSELECT2：Transaction 100提交，Transaction 200未提交\rSELECT * FROM hero WHERE number = 1; # 得到的列name的值为'张飞'\r 这个SELECT2 的执行过程如下：\n 在执行SELECT 语句时会又会单独生成一个ReadView ，该ReadView 的m_ids 列表的内容就是[200] （ 事务id 为100 的那个事务已经提交了，所以再次生成快照时就没有它了）， min_trx_id 为200 ，max_trx_id 为201 ， creator_trx_id 为0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列name 的内容是\u0026rsquo;诸葛亮' ，该版本的trx_id 值为200 ，在m_ids 列表内，所以不符合可见性要求，根据roll_pointer 跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;赵云' ，该版本的trx_id 值为200 ，也在m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;张飞' ，该版本的trx_id 值为100 ，小于ReadView 中的min_trx_id 值200 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列name 为\u0026rsquo;张飞' 的记录。  以此类推，如果之后事务id 为200 的记录也提交了，再此在使用READ COMMITTED 隔离级别的事务中查询表hero 中number 值为1 的记录时，得到的结果就是\u0026rsquo;诸葛亮' 了，具体流程我们就不分析了。总结一下就是：使用READ COMMITTED隔离级别的事务在每次查询开始时都会生成一个独立的ReadView。\nREPEATABLE READ —— 在第一次读取数据时生成一个ReadView 对于使用REPEATABLE READ 隔离级别的事务来说，只会在第一次执行查询语句时生成一个ReadView ，之后的查询就不会重复生成了。\n比方说现在系统里有两个事务id 分别为100 、200 的事务在执行：\nTransaction 100\rBEGIN;\rUPDATE hero SET name = '关羽' WHERE number = 1;\rUPDATE hero SET name = '张飞' WHERE number = 1;\rTransaction 200\rBEGIN;\r更新了一些别的表的记录\r 此刻，表hero 中number 为1 的记录得到的版本链表如下所示：\n假设现在有一个使用REPEATABLE READ 隔离级别的事务开始执行：\n# 使用REPEATABLE READ隔离级别的事务\rBEGIN;\r# SELECT1：Transaction 100、200未提交\rSELECT * FROM hero WHERE number = 1; # 得到的列name的值为'刘备'\r 这个SELECT1 的执行过程如下：\n 在执行SELECT 语句时会先生成一个ReadView ， ReadView 的m_ids 列表的内容就是[100, 200] ，min_trx_id 为100 ， max_trx_id 为201 ， creator_trx_id 为0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列name 的内容是\u0026rsquo;张飞' ，该版本的trx_id 值为100 ，在m_ids 列表内，所以不符合可见性要求，根据roll_pointer 跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;关羽' ，该版本的trx_id 值也为100 ，也在m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;刘备' ，该版本的trx_id 值为80 ，小于ReadView 中的min_trx_id 值100 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列name 为\u0026rsquo;刘备' 的记录。  之后，我们把事务id 为100 的事务提交一下,然后再到事务id 为200 的事务中更新一下表hero 中number 为1 的记录：\n# Transaction 200\rBEGIN;\r# 更新了一些别的表的记录\r...\rUPDATE hero SET name = '赵云' WHERE number = 1;\rUPDATE hero SET name = '诸葛亮' WHERE number = 1;\r 此刻，表hero 中number 为1 的记录的版本链就长这样：\n之后，我们把事务id 为100 的事务提交一下,执行过程如下：\n 因为当前事务的隔离级别为REPEATABLE READ ，而之前在执行SELECT1 时已经生成过ReadView 了，所以此时直接复用之前的ReadView ，之前的ReadView 的m_ids 列表的内容就是[100, 200] ， min_trx_id 为100 ， max_trx_id 为201 ， creator_trx_id 为0 。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本的列name 的内容是\u0026rsquo;诸葛亮' ，该版本的trx_id 值为200 ，在m_ids 列表内，所以不符合可见性要求，根据roll_pointer 跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;赵云' ，该版本的trx_id 值为200 ，也在m_ids 列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;张飞' ，该版本的trx_id 值为100 ，而m_ids 列表中是包含值为100 的事务id 的，所以该版本也不符合要求，同理下一个列name 的内容是\u0026rsquo;关羽' 的版本也不符合要求。继续跳 到下一个版本。 下一个版本的列name 的内容是\u0026rsquo;刘备' ，该版本的trx_id 值为80 ，小于ReadView 中的min_trx_id 值100 ，所以这个版本是符合要求的，最后返回给用户的版本就是这条列c 为\u0026rsquo;刘备' 的记录。  也就是说两次SELECT 查询得到的结果是重复的，记录的列c 值都是\u0026rsquo;刘备' ，这就是可重复读的含义。如果我们之后再把事务id 为200 的记录提交了，然后再到刚才使用REPEATABLE READ 隔离级别的事务中继续查找这个number 为1 的记录，得到的结果还是\u0026rsquo;刘备'。\nMVCC小结 所谓的MVCC （Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用READ COMMITTD 、REPEATABLE READ 这两种隔离级别的事务在执行普通的SEELCT 操作时访问记录的版本链的过程，这样子可以使不同事务的读-写、写-读操作并发执行，从而提升系统性能。\nREAD COMMITTD 、REPEATABLE READ 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了。\n 我们之前说执行DELETE语句或者更新主键的UPDATE语句并不会立即把对应的记录完全从页面中删除，而是执行一个所谓的delete mark操作，相当于只是对记录打上了一个删除标志位，这主要就是为MVCC服务的，大家可以对比上边举的例子自己试想一下怎么使用。 另外，所谓的MVCC只是在我们进行普通的SEELCT查询时才生效，截止到目前我们所见的所有SELECT语句都算是普通的查询，至于啥是个不普通的查询，我们稍后再说哈\n mysql中的锁 mysql中的锁，说起来简单，但一直没有深入研究过，现在仔细看看\n解决并发事务带来问题的两种基本方式 并发事务访问相同记录的情况大致可以划分为3种：\n  读-读情况：即并发事务相继读取相同的记录。\n  写-写情况：即并发事务相继对相同的记录做出改动。\n在这种情况下会发生脏写的问题，任何一种隔离级别都不允许这种问题的发生。所以在多个未提交事务相继对一条记录做改动时，需要让它们排队执行，这个排队的过程其实是通过锁来实现的。这个所谓的锁其实是一个内存中的结构，在事务执行前本来是没有锁的，也就是说一开始是没有锁结构和记录进行关联的，当一个事务想对这条记录做改动时，首先会看看内存中有没有与这条记录关联的锁结构，当没有的时候就会在内存中生成一个锁结构与之关联。比方说事务T1 要对这条记录做改动，就需要生成一个锁结构与之关联：\n  ​\t其实在锁结构里有很多信息，不过为了简化理解，我们现在只把两个比较重要的属性拿了出来：\n trx信息：代表这个锁结构是哪个事务生成的。 is_waiting ：代表当前事务是否在等待。  如图所示，当事务T1 改动了这条记录后，就生成了一个锁结构与该记录关联，因为之前没有别的事务为这条记录加锁，所以is_waiting 属性就是false ，我们把这个场景就称之为获取锁成功，或者加锁成功，然后就可以继续执行操作了。 在事务T1 提交之前，另一个事务T2 也想对该记录做改动，那么先去看看有没有锁结构与这条记录关联，发现有一个锁结构与之关联后，然后也生成了一个锁结构与这条记录关联，不过锁结构的is_waiting 属性值为true ，表示当前事务需要等待，我们把这个场景就称之为获取锁失败，或者加锁失败，或者没有成功的获取到锁，画个图表示就是这样：\n 读-写或写-读情况：也就是一个事务进行读取操作，另一个进行改动操作。\n这种情况下可能发生脏读、不可重复读、幻读的问题。mysql中有两种方式解决\n 读操作利用多版本并发控制（ MVCC ），写操作进行加锁。读-写操作彼此并不冲突，性能更高 读、写操作都采用加锁的方式。     脏读的产生是因为当前事务读取了另一个未提交事务写的一条记录，如果另一个事务在写记录的时候就给这条记录加锁，那么当前事务就无法继续读取该记录了，所以也就不会有脏读问题的产生了。\n不可重复读的产生是因为当前事务先读取一条记录，另外一个事务对该记录做了改动之后并提交之后，当前事务再次读取时会获得不同的值，如果在当前事务读取记录时就给该记录加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读了。\n幻读问题的产生是因为当前事务读取了一个范围的记录，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围的记录时发现了新插入的新记录，我们把新插入的那些记录称之为幻影记录。采用加锁的方式解决幻读问题就有那么一丢丢麻烦了，因为当前事务在第一次读取记录时那些幻影记录并不存在，所以读取的时候加锁就有点尴尬 —— 因为你并不知道给谁加锁，没关系，这难不倒设计InnoDB的大叔的，我们稍后揭晓答案，稍安勿躁。\n 一致性读（Consistent Reads） 事务利用MVCC 进行的读取操作称之为一致性读，或者一致性无锁读，有的地方也称之为快照读。所有普通的SELECT 语句（ plain SELECT ）在READ COMMITTED 、REPEATABLE READ 隔离级别下都算是一致性读，比方说： SELECT * FROM t; SELECT * FROM t1 INNER JOIN t2 ON t1.col1 = t2.col2 一致性读并不会对表中的任何记录做加锁操作，其他事务可以自由的对表中的记录做改动。\n锁定读（Locking Reads）  共享锁和独占锁  共享锁，英文名： Shared Locks ，简称S锁。在事务要读取一条记录时，需要先获取该记录的S锁。 独占锁，也常称排他锁，英文名： Exclusive Locks ，简称X锁。在事务要改动一条记录时，需要先获取该记录的X锁。  假如事务T1 首先获取了一条记录的S锁之后，事务T2 接着也要访问这条记录：\n 如果事务T2 想要再获取一个记录的S锁，那么事务T2 也会获得该锁，也就意味着事务T1 和T2 在该记录上同时持有S锁。 如果事务T2 想要再获取一个记录的X锁，那么此操作会被阻塞，直到事务T1 提交之后将S锁释放掉。 如果事务T1 首先获取了一条记录的X锁之后，那么不管事务T2 接着想获取该记录的S锁还是X锁都会被阻塞，直到事务T1 提交。  所以我们说S锁和S锁是兼容的， S锁和X锁是不兼容的， X锁和X锁也是不兼容的\n我们前边说在采用加锁方式解决脏读、不可重复读、幻读这些问题时，读取一条记录时需要获取一下该记录的S锁，其实这是不严谨的，有时候想在读取记录时就获取记录的X锁，来禁止别的事务读写该记录，为此设计MySQL 的大叔提出了两种比较特殊的SELECT 语句格式：\n-- 对读取的记录加S锁：\rSELECT ... LOCK IN SHARE MODE;\r也就是在普通的SELECT 语句后边加LOCK IN SHARE MODE ，如果当前事务执行了该语句，那么它会为读取到的记录加S锁，这样允许别的事务继续获取这些记录的S锁（比方说别的事务也使用SELECT ... LOCK IN\rSHARE MODE 语句来读取这些记录），但是不能获取这些记录的X锁（比方说使用SELECT ... FOR UPDATE\r语句来读取这些记录，或者直接修改这些记录）。如果别的事务想要获取这些记录的X锁，那么它们会阻\r塞，直到当前事务提交之后将这些记录上的S锁释放掉。\r-- 对读取的记录加X锁：\rSELECT ... FOR UPDATE;\r也就是在普通的SELECT 语句后边加FOR UPDATE ，如果当前事务执行了该语句，那么它会为读取到的记录\r加X锁，这样既不允许别的事务获取这些记录的S锁（比方说别的事务使用SELECT ... LOCK IN SHARE\rMODE 语句来读取这些记录），也不允许获取这些记录的X锁（比方也说使用SELECT ... FOR UPDATE 语句\r来读取这些记录，或者直接修改这些记录）。如果别的事务想要获取这些记录的S锁或者X锁，那么它们会\r阻塞，直到当前事务提交之后将这些记录上的X锁释放掉。\r 写操作  平常所用到的写操作无非是DELETE 、UPDATE 、INSERT 这三种：\n DELETE ： 对一条记录做DELETE 操作的过程其实是先在B+ 树中定位到这条记录的位置，然后获取一下这条记录的X锁，然后再执行delete mark 操作。我们也可以把这个定位待删除记录在B+ 树中位置的过程看成是一个获取X锁的锁定读。 UPDATE ： 在对一条记录做UPDATE 操作时分为三种情况：  如果未修改该记录的键值并且被更新的列占用的存储空间在修改前后未发生变化，则先在B+ 树中定位到这条记录的位置，然后再获取一下记录的X锁，最后在原记录的位置进行修改操作。其实我们也可以把这个定位待修改记录在B+ 树中位置的过程看成是一个获取X锁的锁定读。 如果未修改该记录的键值并且至少有一个被更新的列占用的存储空间在修改前后发生变化，则先在B+ 树中定位到这条记录的位置，然后获取一下记录的X锁，将该记录彻底删除掉（就是把记录彻底移入垃圾链表），最后再插入一条新记录。这个定位待修改记录在B+ 树中位置的过程看成是一个获取X锁的锁定读，新插入的记录由INSERT 操作提供的隐式锁进行保护。 如果修改了该记录的键值，则相当于在原记录上做DELETE 操作之后再来一次INSERT 操作，加锁操作就需要按照DELETE 和INSERT 的规则进行了。   INSERT ： 一般情况下，新插入一条记录的操作并不加锁，设计InnoDB 的大叔通过一种称之为隐式锁的东东来保护这条新插入的记录在本事务提交前不被别的事务访问  多粒度锁 我们前边提到的锁都是针对记录的，也可以被称之为行级锁或者行锁，对一条记录加锁影响的也只是这条记录而已，我们就说这个锁的粒度比较细；其实一个事务也可以在表级别进行加锁，自然就被称之为表级锁或者表锁，对一个表加锁影响整个表中的记录，我们就说这个锁的粒度比较粗。给表加的锁也可以分为共享锁（ S锁）和独占锁（ X锁）\n如果一个事务给表加了S锁，那么：\n 别的事务可以继续获得该表的S锁 别的事务可以继续获得该表中的某些记录的S锁(行锁) 别的事务不可以继续获得该表的X锁 别的事务不可以继续获得该表中的某些记录的X锁(行锁)  给表加X锁：如果一个事务给表加了X锁（意味着该事务要独占这个表），那么：\n 别的事务不可以继续获得该表的S锁 别的事务不可以继续获得该表中的某些记录的S锁(行锁) 别的事务不可以继续获得该表的X锁 别的事务不可以继续获得该表中的某些记录的X锁(行锁)  在对表上表锁时有两个问题：\n 对表整体上S锁，需要判断是否存在行 X锁 对表整体上X锁，需要判断是否存在行 X锁，行S锁  在对表上表锁时，怎么知道表里是否存在行锁呢？于是乎设计InnoDB 的大叔们提出了一种称之为意向锁（英文名： Intention Locks ）：\n 意向共享锁，英文名： Intention Shared Lock ，简称IS锁。当事务准备在某条记录上加S锁时，需要先在表级别加一个IS锁。 意向独占锁，英文名： Intention Exclusive Lock ，简称IX锁。当事务准备在某条记录上加X锁时，需要先在表级别加一个IX锁。  总结一下：IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是兼容的，IX锁和IX锁是兼容的\n行锁和表锁  MySQL 支持多种存储引擎，不同存储引擎对锁的支持也是不一样的\n其他存储引擎中的锁 对于MyISAM 、MEMORY 、MERGE 这些存储引擎来说，它们只支持表级锁，而且这些引擎并不支持事务，所以使用这些存储引擎的锁一般都是针对当前会话来说的。比方说在Session 1 中对一个表执行SELECT 操作，就相当于为这个表加了一个表级别的S锁，如果在SELECT 操作未完成时， Session 2 中对这个表执行UPDATE 操作，相当于要获取表的X锁，此操作会被阻塞，直到Session 1 中的SELECT 操作完成，释放掉表级别的S锁后，Session 2 中对这个表执行UPDATE 操作才能继续获取X锁，然后执行具体的更新语句。\nInnoDB存储引擎中的锁 InnoDB 存储引擎既支持表锁，也支持行锁。表锁实现简单，占用资源较少，不过粒度很粗，有时候你仅仅需要锁住几条记录，但使用表锁的话相当于为表中的所有记录都加锁，所以性能比较差。行锁粒度更细，可以实现更精准的并发控制。\nInnoDB中的表级锁   表级别的S锁、X锁\n在对某个表执行一些诸如ALTER TABLE 、DROP TABLE 这类的DDL 语句时，其他事务对这个表并发执行诸如SELECT 、INSERT 、DELETE 、UPDATE 的语句会发生阻塞，同理，某个事务中对某个表执行SELECT 、INSERT 、DELETE 、UPDATE 语句时，在其他会话中对这个表执行DDL 语句也会发生阻塞。这个过程其实是通过在server层使用一种称之为元数据锁（英文名： Metadata Locks ，简称MDL ）东东来实现的，一般情况下也不会使用InnoDB 存储引擎自己提供的表级别的S锁和X锁。\n其实这个InnoDB 存储引擎提供的表级S锁或者X锁是相当鸡肋，只会在一些特殊情况下，比方说崩溃恢复过程中用到。不过我们还是可以手动获取一下的，比方说在系统变量autocommit=0，innodb_table_locks =1 时，手动获取InnoDB 存储引擎提供的表t 的S锁或者X锁可以这么写：\n   LOCK TABLES t READ ： InnoDB 存储引擎会对表t 加表级别的S锁。 LOCK TABLES t WRITE ： InnoDB 存储引擎会对表t 加表级别的X锁。\n\r+ 表级别的IS锁、IX锁\r当我们在对使用InnoDB 存储引擎的表的某些记录加S锁(行锁)之前，那就需要先在表级别加一个IS锁，当我们在对使用InnoDB 存储引擎的表的某些记录加X锁(行锁)之前，那就需要先在表级别加一个IX锁。IS锁和IX锁的使命只是为了后续在加表级别的S锁和X锁时判断表中是否有已经被加锁的记录，以避免用遍历的方式来查看表中有没有上锁的记录\r+ 表级别的AUTO-INC锁\r在使用MySQL 过程中，我们可以为表的某个列添加AUTO_INCREMENT 属性，之后在插入记录时，可以不指定该列的值，系统会自动为它赋上递增的值，系统实现这种自动给AUTO_INCREMENT 修饰的列递增赋值的原理主要是两个：\r+ 采用AUTO-INC 锁，也就是在执行插入语句时就在表级别加一个AUTO-INC 锁，然后为每条待插入记录的AUTO_INCREMENT 修饰的列分配递增的值，在该语句执行结束后，再把AUTO-INC 锁释放掉。这样一个事务在持有AUTO-INC 锁的过程中，其他事务的插入语句都要被阻塞，可以保证一个语句中分配的递增值是连续的。如果我们的插入语句在执行前不可以确定具体要插入多少条记录（**无法预计即将插入记录的数量**），比方说使用INSERT ... SELECT 、REPLACE ... SELECT 或者LOAD DATA 这种插入语句，一般是使用AUTO-INC 锁为AUTO_INCREMENT 修饰的列生成对应的值。\r+ 采用一个轻量级的锁，**在为插入语句生成AUTO_INCREMENT 修饰的列的值时获取一下这个轻量级锁**，然后生成本次插入语句需要用到的AUTO_INCREMENT 列的值之后，就**把该轻量级锁释放掉**，并不需要等到整个插入语句执行完才释放锁。如果我们的插入语句在执行前就可以确定具体要插入多少条记录，比方说我们上边举的关于表t 的例子\r中，在语句执行前就可以确定要插入2条记录，那么一般采用轻量级锁的方式对AUTO_INCREMENT 修饰的列进行赋值。这种方式可以避免锁定表，可以提升插入性能。\r\u0026gt; 设计InnoDB的大叔提供了一个称之为innodb_autoinc_lock_mode的系统变量来控制到底使用上述两种方式中的哪种来为AUTO_INCREMENT修饰的列进行赋值，\r\u0026gt; \u0026gt; 当innodb_autoinc_lock_mode值为0时，一律采用AUTO-INC锁；\r\u0026gt; \u0026gt; 当innodb_autoinc_lock_mode值为2时，一律采用轻量级锁；\r\u0026gt; \u0026gt; 当innodb_autoinc_lock_mode值为1时，两种方式混着来（也就是在插入记录数量确定时采用轻量级锁，不确定时使用AUTO-INC锁）。不过当innodb_autoinc_lock_mode值为2时，可能会造成不同事务中的插入语句为AUTO_INCREMENT修饰的列生成的值是交叉的，在有主从复制的场景中是不安全的。\r##### InnoDB中的行级锁\r行锁，也称为记录锁，顾名思义就是**在记录上加的锁**。不过设计InnoDB 的大叔很有才，一个行锁玩出了各种花样，也就是把行锁分成了各种类型。换句话说即使对同一条记录加行锁，如果类型不同，起到的功效也是不同的\r+ Record Locks ：\r**单个行记录上的锁** Record Lock总是会去锁住索引记录，如果InnoDB存储引擎表建立的时候没有设置任何一个索引，这时InnoDB存储引擎会使用隐式的主键来进行锁定\r+ Gap Locks 间隙锁\r我们说MySQL 在REPEATABLE READ 隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用MVCC 方案解决，也可以采用加锁方案解决。但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上Record Lock。不过这难不倒设计InnoDB 的\r大叔，他们提出了一种称之为Gap Locks 的锁，官方的类型名称为： LOCK_GAP ，我们也可以简称为gap锁。比方说我们把number 值为8 的那条记录加一个gap锁的示意图如下：\r![image-20220210152223731](image-20220210152223731.png)\r如图中为number 值为8 的记录加了gap锁，意味着不允许别的事务在number 值为8 的记录前边的间隙插入新记录，其实就是number 列的值(3, 8) 这个区间的新记录是不允许立即插入的。比方说有另外一个事务再想插入一条number 值为4 的新记录，它定位到该条新记录的下一条记录的number 值为8，而这条记录上又有一个gap锁，所以就会阻塞插入操作，直到拥有这个gap锁的事务提交了之后， number 列的值在区间(3, 8) 中的新记录才可以被插入。\r这个gap锁的提出**仅仅是为了防止插入幻影记录而提出的**，虽然有共享gap锁和独占gap锁这样的说法，但是它们起到的作用都是相同的。而且如果你对一条记录加了gap锁（不论是共享gap锁还是独占gap锁），并不会限制其他事务对这条记录加正经记录锁或者继续加gap锁\r+ Next-Key Locks\r间隙锁+单行记录锁，有时候我们既想**锁住某条记录，又想阻止其他事务在该记录前边的间隙插入新记录**，所以设计InnoDB 的大叔们就提出了一种称之为Next-Key Locks 的锁，官方的类型名称为： LOCK_ORDINARY ，我们也可以简称为next-key锁。比方说我们把number 值为8 的那条记录加一个next-key锁的示意图如下：\r![image-20220210152631875](image-20220210152631875.png)\r+ Insert Intention Locks 我们说一个事务在插入一条记录时需要判断一下插入位置是不是被别的事务加了所谓的gap锁（ next-key锁也包含gap锁，后边就不强调了），如果有的话，插入操作需要等待，直到拥有gap锁的那个事务提交。设计InnoDB 的大叔规定事务在等待的时候也需要在内存中生成一个锁结构，表明有事务想在某个间隙中插入新记录，但是现在在等待。把这种类型的锁命名为Insert Intention Locks ，官方的类型名称为： LOCK_INSERT_INTENTION ，我们也可以称为**插入意向锁**\r比方说现在T1 为number 值为8 的记录加了一个gap锁，然后T2 和T3 分别想向hero 表中插入number 值分别为4 、5 的两条记录，所以现在为number 值为8 的记录加的锁的示意图就如下所示：\r![image-20220210152955329](image-20220210152955329.png)\r从图中可以看到，由于T1 持有gap锁，所以T2 和T3 需要生成一个插入意向锁的锁结构并且处于等待状态。当T1 提交后会把它获取到的锁都释放掉，这样T2 和T3 就能获取到对应的插入意向锁了（本质上就是把插入意向锁对应锁结构的is_waiting 属性改为false ）， T2 和T3 之间也并不会相互阻塞，它们可以同时获取到number 值为8的插入意向锁，然后执行插入操作。事实上**插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁**（ 插入意向锁就是这么鸡肋）。\r+ 隐式锁\r我们前边说一个事务在执行INSERT 操作时，如果即将插入的间隙已经被其他事务加了gap锁，那么本次INSERT 操作会阻塞，并且当前事务会在该间隙上加一个插入意向锁，否则一般情况下INSERT 操作是不加锁的。那如果一个事务首先插入了一条记录（此时并没有与该记录关联的锁结构），然后另一个事务\r+ 立即使用SELECT ... LOCK IN SHARE MODE 语句读取这条事务，也就是在要获取这条记录的S锁，或者使用SELECT ... FOR UPDATE 语句读取这条事务或者直接修改这条记录，也就是要获取这条记录的X锁，该咋办？如果允许这种情况的发生，那么可能产生**脏读**问题。\r+ 立即修改这条记录，也就是要获取这条记录的X锁，该咋办？\r如果允许这种情况的发生，那么可能产生**脏写**问题。\r这时候我们前边唠叨了很多遍的事务id 又要起作用了。我们把聚簇索引和二级索引中的记录分开看一下：\r+ 情景一：对于聚簇索引记录来说，**有一个trx_id 隐藏列，该隐藏列记录着最后改动该记录的事务id** 。\r那么如果在当前事务中新插入一条聚簇索引记录后，该记录的trx_id 隐藏列代表的的就是当前事务的事务id ，**如果其他事务此时想对该记录添加S锁或者X锁时，首先会看一下该记录的trx_id 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个X锁（也就是为当前事务创建一个锁结构， is_waiting 属性是false ），然后自己进入等待状态（也就是为自己也创建一个锁结构，is_waiting 属性是true ）**。\r+ 情景二：对于二级索引记录来说，本身并没有trx_id 隐藏列，但是在**二级索引页面的Page Header 部分有一个PAGE_MAX_TRX_ID 属性，该属性代表对该页面做改动的最大的事务id** ，如果**PAGE_MAX_TRX_ID 属性值小于当前最小的活跃事务id ，那么说明对该页面做修改的事务都已经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录**，然后再重复情景一的做法。\r**一个事务对新插入的记录可以不显式的加锁**（生成一个锁结构），但是由于事务id 这个牛逼的东东的存在，相当于加了一个**隐式锁**。**别的事务在对这条记录加S锁或者X锁时，由于隐式锁的存在，会先帮助当前事务生成一个锁结构，然后自己再生成一个锁结构后进入等待状态**。\r#### InnoDB锁的内存结构\r对一条记录加锁的本质就是在内存中创建一个锁结构与之关联，那么是不是一个事务对多条记录加锁，就要创建多个锁结构呢？设计InnoDB 的大叔本着勤俭节约的传统美德，决定在对不同记录加锁时，如果符合下边这些条件：\r+ 在同一个事务中进行加锁操作\r+ 被加锁的记录在同一个页面中\r+ 加锁的类型是一样的\r+ 等待状态是一样的\r那么这些记录的锁就可以被放到一个锁结构中\r![image-20220210161713733](image-20220210161713733.png)\r+ 锁所在的事务信息：\r不论是表锁还是行锁，都是在**事务执行过程中生成的**，哪个事务生成了这个锁结构，这里就记载着这个事务的信息。\r实际上这个所谓的`锁所在的事务信息`在内存结构中只是一个指针而已，所以不会占用多大内存\r空间，通过指针可以找到内存中关于该事务的更多信息，比方说事务id是什么。下边介绍的所谓的\r`索引信息`其实也是一个指针。\r+ 索引信息：\r对于行锁来说，需要记录一下加锁的记录是属于哪个索引的。\r+ 表锁／行锁信息：\r表锁结构和行锁结构在这个位置的内容是不同的：\r+ 表锁：\r记载着这是对哪个表加的锁，还有其他的一些信息。\r+ 行锁：\r记载了三个重要的信息：\r+ Space ID ：记录所在表空间。\r+ Page Number ：记录所在页号。\r+ n_bits ：对于行锁来说，一条记录就对应着一个比特位，一个页面中包含很多记录，用不同的比\r特位来区分到底是哪一条记录加了锁。为此在行锁结构的末尾放置了一堆比特位，这个n_bits 属\r性代表使用了多少比特位。\r+ type_mode ：\r这是一个32位的数，被分成了lock_mode 、lock_type 和rec_lock_type 三个部分，如图所示：\r![image-20220210162112856](image-20220210162112856.png)\r1. 锁的模式（ lock_mode ），占用低4位，可选的值如下：\r+ LOCK_IS （十进制的0 ）：表示共享意向锁，也就是IS锁。\r+ LOCK_IX （十进制的1 ）：表示独占意向锁，也就是IX锁。\r+ LOCK_S （十进制的2 ）：表示共享锁，也就是S锁。\r+ LOCK_X （十进制的3 ）：表示独占锁，也就是X锁。\r+ LOCK_AUTO_INC （十进制的4 ）：表示AUTO-INC锁。\r 在InnoDB存储引擎中，LOCK_IS，LOCK_IX，LOCK_AUTO_INC都算是表级锁的模式，LOCK_S和LOCK_X既可以算是表级锁的模式，也可以是行级锁的模式。\n\r2. 锁的类型（ lock_type ），占用第5～8位，不过现阶段只有第5位和第6位被使用\r+ LOCK_TABLE （十进制的16 ），也就是当第5个比特位置为1时，表示表级锁。\r+ LOCK_REC （十进制的32 ），也就是当第6个比特位置为1时，表示行级锁。\r3. 行锁的具体类型（ rec_lock_type ），使用其余的位来表示。只有在lock_type 的值为LOCK_REC 时，也就是只有在该锁为行级锁时，才会被细分为更多的类型：\r+ LOCK_ORDINARY （十进制的0 ）：表示next-key锁。\r+ LOCK_GAP （十进制的512 ）：也就是当第10个比特位置为1时，表示gap锁。\r+ LOCK_REC_NOT_GAP （十进制的1024 ）：也就是当第11个比特位置为1时，表示正经记录锁。\r+ LOCK_INSERT_INTENTION （十进制的2048 ）：也就是当第12个比特位置为1时，表示插入意向\r锁。\r怎么还没看见is_waiting 属性呢？这主要还是设计InnoDB 的大叔太抠门了，一个比特位也不想浪\r费，所以他们把is_waiting 属性也放到了type_mode 这个32位的数字中：\r+ LOCK_WAIT （十进制的256 ） ：也就是当第9个比特位置为1 时，表示is_waiting 为true ，也\r就是当前事务尚未获取到锁，处在等待状态；当这个比特位为0 时，表示is_waiting 为false ，\r也就是当前事务获取锁成功。\r4. 一堆比特位：\r如果是行锁结构的话，在该结构末尾还放置了一堆比特位，比特位的数量是由上边提到的n_bits 属性表示\r的。我们前边唠叨InnoDB记录结构的时候说过，页面中的每条记录在记录头信息中都包含一个heap_no 属\r性，伪记录Infimum 的heap_no 值为0 ， Supremum 的heap_no 值为1 ，之后每插入一条记录， heap_no\r值就增1。锁结构最后的一堆比特位就对应着一个页面中的记录，一个比特位映射一个heap_no ，不过为\r了编码方便，映射方式有点怪：\r ","id":8,"section":"posts","summary":"[TOC] 本篇文章是(Mysql是怎样运行的)阅读笔记，这本书网上很多人的评价都很高，看了下书也不是很厚，所以读读。 MySQL的架构 mysql 工作的整体流程","tags":["mysql"],"title":"Mysql是怎样运行的","uri":"https://wzgl998877.github.io/2022/01/mysql%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/","year":"2022"},{"content":"[TOC]\nJava 并发编程 ​\t并发编程之美的笔记\n什么是线程 ​\t进程是操作系统资源分配的最小单位，而线程是CPU任务调度和执行的最小单位\n具体参考 https://blog.csdn.net/ThinkWon/article/details/102021274\n五状态进程的主要有：\n 运行态：进程正在执行。 就绪态：进程做好了准备，随时接收调度。 阻塞态：进程在等待某些事件的发生，在事件发生前不能执行，如I/O操作。 新建态：刚刚新建的进程，操作系统还未将其加载至内存，通常是PCB已经创建但是还并未加载到内存中的新程序。 退出态：操作系统从可执行进程组中释放的进程。  Java 中对应的线程状态有：\n  new一个实例出来，线程就进入了初始状态\n  2.1. 就绪状态(RUNNABLE之READY)\n 就绪状态只是说你资格运行，调度程序没有挑选到你，你就永远是就绪状态。 调用线程的start()方法，此线程进入就绪状态 当前线程sleep()方法结束，其他线程join()结束，等待用户输入完毕，某个线程拿到对象锁，这些线程也将进入就绪状态。 当前线程时间片用完了，调用当前线程的yield()方法，当前线程进入就绪状态。锁池里的线程拿到对象锁后，进入就绪状态。    2.2. 运行中状态(RUNNABLE之RUNNING) 线程调度程序从可运行池中选择一个线程作为当前线程时线程所处的状态。这也是线程进入运行状态的唯一的一种方式。\n  阻塞状态是线程在等待获得synchronized锁。\n  等待状态（waiting）处于这种状态的线程不会被分配CPU执行时间，它们要等待被显式地唤醒，否则会处于无限期等待的状态，可以通过以下方式进入该状态\n wait() join() LockSupport.park()    超时等待(TIMED_WAITING) ,可以通过以下方式进入该状态\n Thread.sleep Object.wait with timeout Thread.join with timeout LockSupport.parkNanos LockSupport.parkUntil    终止状态(TERMINATED)\n  当线程的run()方法完成时，或者主线程的main()方法完成时，我们就认为它终止了。这个线程对象也许是活的，但是它已经不是一个单独执行的线程。线程一旦终止了，就不能复生。\n  在一个终止的线程上调用start()方法，会抛出java.lang.IllegalThreadStateException异常。\n    常用方法介绍 wait() 当一个线程调用一个共享变量的 wait() 方法时，该调用线程会被阻塞挂起，直到发生下面几件事情之一才返回：\n 其他线程调用了该共享对象的notify（）或者notifyAll（）方法； 其他线程调用了该线程的interrupt（）方法，该线程抛出InterruptedException异常返回。  如果调用wait（）方法的线程没有事先获取该对象的监视器锁，则调用wait（）方法时调用线程会抛出IllegalMonitorStateException异常。\n如何获得对象的监视器锁？\n 执行synchronized同步代码块时，使用该共享变量作为参数  synchronized (lock) {\rlock.wait();\r}\r 调用该共享变量的方法，并且该方法使用了synchronized修饰。  notify() ​\t一个线程调用共享对象的notify（）方法后，会唤醒一个在该共享变量上调用wait系列方法后被挂起的线程。一个共享变量上可能会有多个线程在等待，具体唤醒哪个等待的线程是随机的。此外，被唤醒的线程不能马上从wait方法返回并继续执行，它必须在获取了共享对象的监视器锁后才可以返回，也就是唤醒它的线程释放了共享变量上的监视器锁后，被唤醒的线程也不一定会获取到共享对象的监视器锁，这是因为该线程还需要和其他线程一起竞争该锁，只有该线程竞争到了共享变量的监视器锁后才可以继续执行。\nnotifyAll() 函数 不同于在共享变量上调用notify（）函数会唤醒被阻塞到该共享变量上的一个线程，notifyAll（）方法则会唤醒所有在该共享变量上由于调用wait系列方法而被挂起的线程。\n一个经典题目，n个线程按顺序交替打印数字\npackage com.zt.javastudy.grammar;\rimport java.util.concurrent.atomic.AtomicInteger;\r/**\r* n个线程按顺序打印从0 到N\r* @author zhengtao\r*/\rpublic class MultiThreadOrdering {\rprivate final AtomicInteger sign = new AtomicInteger(0);\rprivate static final Object lock = new Object();\rprivate int k;\rprivate int flag;\rpublic MultiThreadOrdering(int k, int flag) {\rthis.k = k;\rthis.flag = flag;\r}\rprivate void printAlpha(int num) {\rnew Thread(() -\u0026gt; {\rwhile (sign.intValue() \u0026lt; flag) {\rsynchronized (lock) {\rif (sign.intValue() % k != num) {\rtry {\rlock.wait();\r} catch (InterruptedException e) {\re.printStackTrace();\r}\r} else {\rSystem.out.print((char) (num + 'A'));\rsign.incrementAndGet();\rSystem.out.println(\u0026quot; 打印后的sign值为：\u0026quot; + sign);\rlock.notifyAll();\r}\r}\r}\r}\r).start();\r}\rpublic static void main(String[] args) throws InterruptedException {\rint k = 5;\rMultiThreadOrdering demo = new MultiThreadOrdering(k, 11);\rfor (int i = 0; i \u0026lt; k; i++) {\rdemo.printAlpha(i);\r}\r}\r}\r 信号量Semaphore Semaphore信号量也是Java中的一个同步器，与CountDownLatch和CycleBarrier不同的是，它内部的计数器是递增的，并且在一开始初始化Semaphore时可以指定一个初始值，但是并不需要知道需要同步的线程个数，而是在需要同步的地方调用acquire方法时指定需要同步的线程个数。\n主要方法\nacquire() ​\t当前线程调用该方法的目的是希望获取一个信号量资源。如果当前信号量个数大于0，则当前信号量的计数会减1，然后该方法直接返回。否则如果当前信号量个数等于0，则当前线程会被放入AQS的阻塞队列。当其他线程调用了当前线程的interrupt（）方法中断了当前线程时，则当前线程会抛出InterruptedException异常返回。\nrelease() ​\t该方法的作用是把当前Semaphore对象的信号量值增加1，如果当前有线程因为调用aquire方法被阻塞而被放入了AQS的阻塞队列，则会根据公平策略选择一个信号量个数能被满足的线程进行激活，激活的线程会尝试获取刚增加的信号量\nSemaphore是使用AQS实现的。Sync只是对AQS的一个修饰，并且Sync有两个实现类，用来指定获取信号量时是否采用公平策略，构造函数为\npublic Semaphore(int permits) {\rsync = new NonfairSync(permits);\r}\rpublic Semaphore(int permits, boolean fair) {\rsync = fair ? new FairSync(permits) : new NonfairSync(permits);\r}\r Semaphore默认采用非公平策略,其中的公平与非公平表现在，先调用aquire方法获取信号量的线程不一定比后来者先获取到信号量。考虑下面场景，如果线程A先调用了aquire（）方法获取信号量，但是当前信号量个数为0，那么线程A会被放入AQS的阻塞队列。过一段时间后线程C调用了release（）方法释放了一个信号量，如果当前没有其他线程获取信号量，那么线程A就会被激活，然后获取该信号量，但是假如线程C释放信号量后，线程C调用了aquire方法，那么线程C就会和线程A去竞争这个信号量资源。如果采用非公平策略，由nonfairTryAcquireShared的代码可知，线程C完全可以在线程A被激活前，或者激活后先于线程A获取到该信号量，也就是在这种模式下阻塞线程和当前请求的线程是竞争关系，而不遵循先来先得的策略。\n信号量解决经典题目，n个线程按顺序交替打印数字\npackage com.zt.javastudy.grammar;\rimport java.util.concurrent.Semaphore;\r/**\r* n个线程按顺序打印从0 到N 高级版\r* @author zhengtao\r*/\rpublic class LoopPrinter {\rprivate final static int THREAD_COUNT = 3;\rstatic int result = 0;\rstatic int maxNum = 10;\rpublic static void main(String[] args) throws InterruptedException {\rfinal Semaphore[] semaphores = new Semaphore[THREAD_COUNT];\rfor (int i = 0; i \u0026lt; THREAD_COUNT; i++) {\r//非公平信号量，每个信号量初始计数都为1\rsemaphores[i] = new Semaphore(1);\rif (i != THREAD_COUNT - 1) {\rSystem.out.println(i+\u0026quot;===\u0026quot;+semaphores[i].getQueueLength());\r//获取一个许可前线程将一直阻塞, for 循环之后只有 syncObjects[2] 没有被阻塞\rsemaphores[i].acquire();\r}\r}\rfor (int i = 0; i \u0026lt; THREAD_COUNT; i++) {\r// 初次执行，上一个信号量是 syncObjects[2]\rfinal Semaphore lastSemphore = i == 0 ? semaphores[THREAD_COUNT - 1] : semaphores[i - 1];\rfinal Semaphore currentSemphore = semaphores[i];\rfinal int index = i;\rnew Thread(() -\u0026gt; {\rtry {\rwhile (true) {\r// 初次执行，让第一个 for 循环没有阻塞的 syncObjects[2] 先获得令牌阻塞了\rlastSemphore.acquire();\rSystem.out.println(\u0026quot;thread\u0026quot; + index + \u0026quot;: \u0026quot; + result++);\rif (result \u0026gt; maxNum) {\rSystem.exit(0);\r}\r// 释放当前的信号量，syncObjects[0] 信号量此时为 1，下次 for 循环中上一个信号量即为syncObjects[0]\rcurrentSemphore.release();\r}\r} catch (Exception e) {\re.printStackTrace();\r}\r}).start();\r}\r}\r}\r join ​\t在项目实践中经常会遇到一个场景，就是需要等待某几件事情完成后才能继续往下执行，比如多个线程加载资源，需要等待多个线程全部加载完毕再汇总处理。Thread类中有一个join方法就可以做这个事情，前面介绍的等待通知方法是Object类中的方法，而join方法则是Thread类直接提供的。join是无参且返回值为void的方法。\npackage com.zt.javastudy.concurrent;\rimport java.util.Random;\rimport java.util.concurrent.ThreadLocalRandom;\r/**\r* 一些并发编程的源码研究\r*\r* @author zhengtao on 2021/10/29\r*/\rpublic class Day1 {\rpublic static void main(String[] args) throws InterruptedException {\rThread threadA = new Thread(() -\u0026gt; {\rtry {\rThread.sleep(2000);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rSystem.out.println(\u0026quot;Thread A 执行完了\u0026quot;);\r});\rThread threadB = new Thread(() -\u0026gt; {\rtry {\rThread.sleep(1000);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rSystem.out.println(\u0026quot;Thread B 执行完了\u0026quot;);\r});\rthreadA.start();\rthreadB.start();\rSystem.out.println(\u0026quot;123\u0026quot;);\rthreadA.join();\rSystem.out.println(\u0026quot;456\u0026quot;);\rthreadB.join();\rSystem.out.println(\u0026quot;主线程执行完了\u0026quot;);\r}\r}\r输出结果为：\r123\rThread B 执行完了\rThread A 执行完了\r456\r主线程执行完了，不太理解为什么会先输出b执行完了，而不是456\r sleep ​\tThread类中有一个静态的sleep方法，当一个执行中的线程调用了Thread的sleep方法后，调用线程会暂时让出指定时间的执行权，也就是在这期间不参与CPU的调度，但是该线程所拥有的监视器资源，比如锁还是持有不让出的。指定的睡眠时间到了后该函数会正常返回，线程就处于就绪状态，然后参与CPU的调度，获取到CPU资源后就可以继续运行了。如果在睡眠期间其他线程调用了该线程的interrupt（）方法中断了该线程，则该线程会在调用sleep方法的地方抛出InterruptedException异常而返回。\nyield ​\t当一个线程调用yield方法时，实际就是在暗示线程调度器当前线程请求让出自己的CPU使用，但是线程调度器可以无条件忽略这个暗示。我们知道操作系统是为每个线程分配一个时间片来占有CPU的，正常情况下当一个线程把分配给自己的时间片使用完后，线程调度器才会进行下一轮的线程调度，而当一个线程调用了Thread类的静态方法yield时，是在告诉线程调度器自己占有的时间片中还没有使用完的部分自己不想使用了，这暗示线程调度器现在就可以进行下一轮的线程调度。当一个线程调用yield方法时，当前线程会让出CPU使用权，然后处于就绪状态，线程调度器会从线程就绪队列里面获取一个线程优先级最高的线程，当然也有可能会调度到刚刚让出CPU的那个线程来获取CPU执行权.\n​\tsleep与yield方法的区别在于，当线程调用sleep方法时调用线程会被阻塞挂起指定的时间，在这期间线程调度器不会去调度该线程。而调用yield方法时，线程只是让出自己剩余的时间片，并没有被阻塞挂起，而是处于就绪状态，线程调度器下一次调度时就有可能调度到当前线程执行。\n并发和并行 ​\t并发是指同一个时间段内多个任务同时都在执行，并且都没有执行结束，而并行是说在单位时间内多个任务同时在执行。并发任务强调在一个时间段内同时执行，而一个时间段由多个单位时间累积而成，所以说并发的多个任务在单位时间内不一定同时在执行。在单CPU的时代多个任务都是并发执行的，这是因为单个CPU同时只能执行一个任务。\n线程死锁 ​\t死锁是指两个或两个以上的线程在执行过程中，因争夺资源而造成的互相等待的现象\n  互斥条件：指线程对已经获取到的资源进行排它性使用，即该资源同时只由一个线程占用。如果此时还有其他线程请求获取该资源，则请求者只能等待，直至占有资源的线程释放该资源。\n  请求并持有条件：指一个线程已经持有了至少一个资源，但又提出了新的资源请求，而新资源已被其他线程占有，所以当前线程会被阻塞，但阻塞的同时并不释放自己已经获取的资源。\n  不可剥夺条件：指线程获取到的资源在自己使用完之前不能被其他线程抢占，只有在自己使用完毕后才由自己释放该资源。\n  环路等待条件：指在发生死锁时，必然存在一个线程—资源的环形链，即线程集合{T0, T1, T2, …, Tn}中的T0正在等待一个T1占用的资源，T1正在等待T2占用的资源，……Tn正在等待已被T0占用的资源。\n  解决死锁 ​\t解决死锁，只需要破坏掉至少一个构造死锁的必要条件即可\n 破坏互斥条件: 我们需要允许进程同时访问某些资源，这种方法受制于实际场景，不太容易实现条件；java 中有很多类，采用了CAS的方法来实现，比如 AtomicInteger 等。 破坏请求并持有条件：这样需要允许进程强行从占有者那里夺取某些资源，或者简单一点理解，占有资源的进程不能再申请占有其他资源，必须释放手上的资源之后才能发起申请，这个其实也很难找到适用场景； 破坏不可剥夺条件: 进程在运行前申请得到所有的资源，否则该进程不能进入准备执行状态。 破坏环路等待条件: 通过对加锁的操作进行排序我们就能够破坏环路等待条件。例如当我们需要获取数组中某一个位置对应的锁来修改这个位置上保存的值时，如果需要同时获取多个位置对应的锁，那么我们就可以按位置在数组中的排列先后顺序统一从前往后加锁。  最有效的是使用资源申请的有序性原则来破坏环路等待条件解决死锁。\n内存可见性 ​\tJava 内存模型规定，将所有的变量都存放在主内存中，当线程使用变量时，会把主内存里面的变量复制到自己的工作空间或者叫作工作内存，然后对工作内存里的变量进行处理，处理完后将变量值更新到主内存，线程读写变量时操作的是自己工作内存中的变量。\n假如线程A和线程B同时处理一个共享变量，\n  线程A首先获取共享变量X的值，由于两级Cache都没有命中，所以加载主内存中X的值，假如为0。然后把X=0的值缓存到两级缓存，线程A修改X的值为1，然后将其写入两级Cache，并且刷新到主内存。线程A操作完毕后，线程A所在的CPU的两级Cache内和主内存里面的X的值都是1。\n  线程B获取X的值，首先一级缓存没有命中，然后看二级缓存，二级缓存命中了，所以返回X= 1；到这里一切都是正常的，因为这时候主内存中也是X=1。然后线程B修改X的值为2，并将其存放到线程2所在的一级Cache和共享二级Cache中，最后更新主内存中X的值为2；到这里一切都是好的。\n  线程A这次又需要修改X的值，获取时一级缓存命中，并且X=1，到这里问题就出现了，明明线程B已经把X的值修改为了2，为何线程A获取的还是1呢？这就是共享变量的内存不可见问题，也就是线程B写入的值对线程A不可见。\n  synchronized ​\tsynchronized 块是 Java 提供的一种原子性内置锁，Java 中的每个对象都可以把它当作一个同步锁来使用，这些 Java 内置的使用者看不到的锁被称为内部锁，也叫作监视器锁。线程的执行代码在进入synchronized代码块前会自动获取内部锁，这时候其他线程访问该同步代码块时会被阻塞挂起。拿到内部锁的线程会在正常退出同步代码块或者抛出异常后或者在同步块内调用了该内置锁资源的wait系列方法时释放该内置锁。内置锁是排它锁，也就是当一个线程获取这个锁后，其他线程必须等待该线程释放锁后才能获取该锁。\n由于Java中的线程是与操作系统的原生线程一一对应的，所以当阻塞一个线程时，需要从用户态切换到内核态执行阻塞操作，这是很耗时的操作，而synchronized的使用就会导致上下文切换。\n上下文切换：\n​\t在多线程编程中，线程个数一般都大于CPU个数，而每个CPU同一时刻只能被一个线程使用，为了让用户感觉多个线程是在同时执行的，CPU资源的分配采用了时间片轮转的策略，也就是给每个线程分配一个时间片，线程在时间片内占用CPU执行任务。当前线程使用完时间片后，就会处于就绪状态并让出CPU让其他线程占用，这就是上下文切换，从当前线程的上下文切换到了其他线程。那么就有一个问题，让出CPU的线程等下次轮到自己占有CPU时如何知道自己之前运行到哪里了？所以在切换线程上下文时需要保存当前线程的执行现场，当再次执行时根据保存的执行现场信息恢复执行现场。线程上下文切换时机有：当前线程的CPU时间片使用完处于就绪状态时，当前线程被其他线程中断时。\nsynchronized的内存语义 ​\t进入 synchronized 块的内存语义是把在 synchronized 块内使用到的变量从线程的工作内存中清除，这样在 synchronized 块内使用到该变量时就不会从线程的工作内存中获取，而是直接从主内存中获取。退出 synchronized 块的内存语义是把在 synchronized 块内对共享变量的修改刷新到主内存。\n​\t其实这也是加锁和释放锁的语义，当获取锁后会清空锁块内本地内存中将会被用到的共享变量，在使用这些共享变量时从主内存进行加载，在释放锁时将本地内存中修改的共享变量刷新到主内存。\n​\t除可以解决共享变量内存可见性问题外，synchronized经常被用来实现原子性操作。另外请注意，synchronized关键字会引起线程上下文切换并带来线程调度开销。\nvolatile ​\t该关键字可以确保对一个变量的更新对其他线程马上可见。当一个变量被声明为volatile时，线程在写入变量时不会把值缓存在寄存器或者其他地方，而是会把值刷新回主内存。当其他线程读取该共享变量时，会从主内存重新获取最新值，而不是使用当前线程的工作内存中的值。\nvolatile的内存语义 ​\t当线程写入了volatile变量值时就等价于线程退出synchronized同步块（把写入工作内存的变量值同步到主内存），读取volatile变量值时就相当于进入同步块（先清空本地内存变量值，再从主内存获取最新值）。volatile只能作用于变量。\nvolatile虽然提供了可见性保证，但并不保证操作的原子性。\n那么一般在什么时候才使用volatile关键字呢？\n 写入变量值不依赖变量的当前值时。因为如果依赖当前值，将是获取—计算—写入三步操作，这三步操作不是原子性的，而volatile不保证原子性。 读写变量值时没有加锁。因为加锁本身已经保证了内存可见性，这时候不需要把变量声明为volatile的。  原子性 ​\t是指执行一系列操作时，这些操作要么全部执行，要么全部不执行，不存在只执行其中一部分的情况。\n最简单的方法就是使用synchronized关键字进行同步，但是使用synchronized关键字效率比较低，有非阻塞的CAS算法可以实现原子性操作。\nCAS ​\t在Java中，锁在并发处理中占据了一席之地，但是使用锁有一个不好的地方，就是当一个线程没有获取到锁时会被阻塞挂起，这会导致线程上下文的切换和重新调度开销。Java提供了非阻塞的volatile关键字来解决共享变量的可见性问题，这在一定程度上弥补了锁带来的开销问题，但是volatile只能保证共享变量的可见性，不能解决读—改—写等的原子性问题。\n​\t而CAS即Compare and Swap，其是JDK提供的非阻塞原子性操作，它通过硬件保证了比较—更新操作的原子性，JDK里面的Unsafe类提供了一系列的compareAndSwap*方法，下面以compareAndSwapLong方法为例进行简单介绍。\n boolean compareAndSwapLong（Object obj, long valueOffset, long expect, longupdate）方法：其中compareAndSwap的意思是比较并交换。CAS有四个操作数，分别为：对象内存位置、对象中的变量的偏移量、变量预期值和新的值。其操作含义是，如果对象obj中内存偏移量为valueOffset的变量值为expect，则使用新的值update替换旧的值expect。这是处理器提供的一个原子性指令。\n ​\t关于CAS操作有个经典的ABA问题，具体如下：假如线程I使用CAS修改初始值为A的变量X，那么线程I会首先去获取当前变量X的值（为A），然后使用CAS操作尝试修改X的值为B，如果使用CAS操作成功了，那么程序运行一定是正确的吗？其实未必，这是因为有可能在线程I获取变量X的值A后，在执行CAS前，线程II使用CAS修改了变量X的值为B，然后又使用CAS修改了变量X的值为A。所以虽然线程I执行CAS时X的值是A，但是这个A已经不是线程I获取时的A了。\n有序性 ​\tJava内存模型允许编译器和处理器对指令重排序以提高运行性能，并且只会对不存在数据依赖性的指令重排序。在单线程下重排序可以保证最终执行的结果与程序顺序执行的结果一致，但是在多线程下就会存在问题。\npublic class OrderTest {\rprivate static int num = 0;\rprivate static boolean ready = false;\rpublic static void main(String[] args) throws InterruptedException {\rThread read = new Thread(new Runnable() {\r@Override\rpublic void run() {\rwhile (!Thread.currentThread().isInterrupted()) {\rif (ready) { // 1\rSystem.out.println(num + num);// 2\r}\r}\r}\r});\rThread write = new Thread(new Runnable() {\r@Override\rpublic void run() {\rnum = 2;// 3\rready = true;// 4\r}\r});\rread.start();\rwrite.start();\rThread.sleep(1);\rread.interrupt();\rSystem.out.println(\u0026quot;exit\u0026quot;);\r}\r}\r ​\t这段代码不一定输出是4，由于代码（1）（2）（3）（4）之间不存在依赖关系，所以写线程的代码（3）（4）可能被重排序为先执行（4）再执行（3），那么执行（4）后，读线程可能已经执行了（1）操作，并且在（3）执行前开始执行（2）操作，这时候输出结果为0而不是4。\n并发编程最主要的特性：原子性，有序性和可见性\nsynchronized: 具有原子性，有序性和可见性； volatile：具有有序性和可见性\n锁   乐观锁与悲观锁\n 悲观锁: 指对数据被外界修改持保守态度，认为数据很容易就会被其他线程修改，所以在数据被处理前先对数据进行加锁，并在整个数据处理过程中，使数据处于锁定状态。悲观锁的实现往往依靠数据库提供的锁机制，即在数据库中，在对数据记录操作前给记录加排它锁。如果获取锁失败，则说明数据正在被其他线程修改，当前线程则等待或者抛出异常。如果获取锁成功，则对记录进行操作，然后提交事务后释放排它锁。 乐观锁: 是相对悲观锁来说的，它认为数据在一般情况下不会造成冲突，所以在访问记录前不会加排它锁，而是在进行数据提交更新时，才会正式对数据冲突与否进行检测。乐观锁并不会使用数据库提供的锁机制，一般在表中添加version字段或者使用业务状态来实现。乐观锁直到提交时才锁定，所以不会产生任何死锁    公平锁与非公平锁\n 根据线程获取锁的抢占机制，锁可以分为公平锁和非公平锁，公平锁表示线程获取锁的顺序是按照线程请求锁的时间早晚来决定的，也就是最早请求锁的线程将最早获取到锁。而非公平锁则在运行时闯入，也就是先来不一定先得。 假设线程A已经持有了锁，这时候线程B请求该锁其将会被挂起。当线程A释放锁后，假如当前有线程C也需要获取该锁，如果采用非公平锁方式，则根据线程调度策略，线程B和线程C两者之一可能获取锁，这时候不需要任何其他干涉，而如果使用公平锁则需要把C挂起，让B获取当前锁。 在没有公平性需求的前提下尽量使用非公平锁，因为公平锁会带来性能开销。    独占锁与共享锁\n 根据锁只能被单个线程持有还是能被多个线程共同持有，锁可以分为独占锁和共享锁。 独占锁: 保证任何时候都只有一个线程能得到锁。 共享锁: 可以同时由多个线程持有，它允许一个资源可以被多线程同时进行读操作。 独占锁是一种悲观锁，由于每次访问资源都先加上互斥锁，这限制了并发性，因为读操作并不会影响数据的一致性，而独占锁只允许在同一时间由一个线程读取数据，其他线程必须等待当前线程释放锁才能进行读取。共享锁则是一种乐观锁，它放宽了加锁的条件，允许多个线程同时进行读操作。    可重入锁\n当一个线程要获取一个被其他线程持有的独占锁时，该线程会被阻塞，那么当一个线程再次获取它自己已经获取的锁时是否会被阻塞呢？如果不被阻塞，那么我们说该锁是可重入的，也就是只要该线程获取了该锁，那么可以无限次数（在高级篇中我们将知道，严格来说是有限次数）地进入被该锁锁住的代码。\nsynchronized内部锁是可重入锁。可重入锁的原理是在锁内部维护一个线程标示，用来标示该锁目前被哪个线程占用，然后关联一个计数器。一开始计数器值为0，说明该锁没有被任何线程占用。当一个线程获取了该锁时，计数器的值会变成1，这时其他线程再来获取该锁时会发现锁的所有者不是自己而被阻塞挂起。但是当获取了该锁的线程再次获取锁时发现锁拥有者是自己，就会把计数器值加+1，当释放锁后计数器值-1。当计数器值为0时，锁里面的线程标示被重置为null，这时候被阻塞的线程会被唤醒来竞争获取该锁。\n  自旋锁\n当一个线程在获取锁（比如独占锁）失败后，会被切换到内核状态而被挂起。当该线程获取到锁时又需要将其切换到用户态而唤醒该线程。而从用户状态切换到内核状态的开销是比较大的，在一定程度上会影响并发性能。自旋锁则是，当前线程在获取锁时，如果发现锁已经被其他线程占有，它不马上阻塞自己，在不放弃CPU使用权的情况下，多次尝试获取（默认次数是10，可以使用-XX:PreBlockSpinsh参数设置该值），很有可能在后面几次尝试中其他线程已经释放了锁。如果尝试指定的次数后仍没有获取到锁则当前线程才会被阻塞挂起。\n  锁的详细介绍参考博客：https://tech.meituan.com/2018/11/15/java-lock.html\nJdk 中一些并发编程的源码解析 Threadlocal ​\t它提供了线程本地变量，也就是如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的一个本地副本。当多个线程操作这个变量时，实际操作的是自己本地内存里面的变量，从而避免了线程安全问题。创建一个ThreadLocal变量后，每个线程都会复制一个变量到自己的本地内存。\n类图 ​\tThread 类中有一个 threadLocals 和一个 inheritableThreadLocals，它们都是 ThreadLocalMap 类型的变量，而ThreadLocalMap是一个定制化的Hashmap。在默认情况下，每个线程中的这两个变量都为null，只有当前线程第一次调用ThreadLocal的set或者get方法时才会创建它们。其实每个线程的本地变量不是存放在ThreadLocal实例里面，而是存放在调用线程的threadLocals变量里面。也就是说，ThreadLocal类型的本地变量存放在具体的线程内存空间中。ThreadLocal就是一个工具壳，它通过set方法把value值放入调用线程的threadLocals里面并存放起来，当调用线程调用它的get方法时，再从当前线程的threadLocals变量里面将其拿出来使用。如果调用线程一直不终止，那么这个本地变量会一直存放在调用线程的threadLocals变量里面，可能会造成内存溢出，因此使用完毕后要记得调用ThreadLocal的remove方法删除对应线程的threadLocals中的本地变量\nset public void set(T value) {\r// 获取当前线程\rThread t = Thread.currentThread();\r// 将当前线程作为key，去查找对应的线程变量\rThreadLocalMap map = getMap(t);\rif (map != null)\rmap.set(this, value);\relse\r// 第一次调用则创建当前线程对应的hashmap\rcreateMap(t, value);\r}\rThreadLocalMap getMap(Thread t) {\rreturn t.threadLocals;\r}\rvoid createMap(Thread t, T firstValue) {\rt.threadLocals = new ThreadLocalMap(this, firstValue);\r}\r get public T get() {\rThread t = Thread.currentThread();\rThreadLocalMap map = getMap(t);\rif (map != null) {\rThreadLocalMap.Entry e = map.getEntry(this);\rif (e != null) {\r@SuppressWarnings(\u0026quot;unchecked\u0026quot;)\rT result = (T)e.value;\rreturn result;\r}\r}\rreturn setInitialValue();\r}\rprivate T setInitialValue() {\rT value = initialValue();\rThread t = Thread.currentThread();\rThreadLocalMap map = getMap(t);\rif (map != null)\rmap.set(this, value);\relse\rcreateMap(t, value);\rreturn value;\r}\rprotected T initialValue() {\rreturn null;\r}\r get方法逻辑为\n 首先获取当前线程实例，如果当前线程的threadLocals变量不为null，则直接返回当前线程绑定的本地变量 否则进行初始化，如果当前线程的threadLocals变量不为空，则设置当前线程的本地变量值为null，否则调用createMap方法创建当前线程的createMap变量。   remove public void remove() {\rThreadLocalMap m = getMap(Thread.currentThread());\rif (m != null)\rm.remove(this);\r}\r 总结 在每个线程内部都有一个名为threadLocals的成员变量，该变量的类型为HashMap，其中key为我们定义的ThreadLocal变量的this引用，value则为我们使用set方法设置的值\npackage com.zt.javastudy.concurrent;\r/**\r* ThreadLocal 测试\r*\r* @author zhengtao on 2021/11/10\r*/\rpublic class ThreadLocalTest implements Runnable {\rpublic static ThreadLocal\u0026lt;Integer\u0026gt; threadLocal = new ThreadLocal\u0026lt;\u0026gt;();\rpublic static ThreadLocal\u0026lt;Integer\u0026gt; integerThreadLocal = new InheritableThreadLocal\u0026lt;\u0026gt;();\rpublic static int i = 0;\rpublic static void main(String[] args) {\rThreadLocalTest threadLocalTest = new ThreadLocalTest();\rthreadLocal.set(i++);\rintegerThreadLocal.set(i++);\rThread thread1 = new Thread(threadLocalTest);\rthread1.start();\rSystem.out.println(\u0026quot;maint中hreadLocal值为：\u0026quot; + threadLocal.get());\rSystem.out.println(\u0026quot;main中InheritableThreadLocal值为：\u0026quot; + integerThreadLocal.get());\r}\r@Override\rpublic void run() {\rSystem.out.println(Thread.currentThread().getName() + \u0026quot;threadLocal值为:\u0026quot; + threadLocal.get());\rSystem.out.println(Thread.currentThread().getName() + \u0026quot;InheritableThreadLocal值为:\u0026quot; + integerThreadLocal.get());\r}\r}\r# 结果为\rmaint中hreadLocal值为：0\rmain中InheritableThreadLocal值为：1\rThread-0threadLocal值为:null\rThread-0InheritableThreadLocal值为:1\r ​\t同一个ThreadLocal变量在父线程中被设置值后，在子线程中是获取不到的。根据上节的介绍，这应该是正常现象，因为在子线程thread里面调用get方法时当前线程为thread线程，而这里调用set方法设置线程变量的是main线程，两者是不同的线程，自然子线程访问时返回null\n而InheritableThreadLocal类解决了这个问题，流程为：\n 通过重写代码 getMap 和createMap让本地变量保存到了具体线程的inheritableThreadLocals变量里面，那么线程在通过InheritableThreadLocal类实例的set或者get方法设置变量时，就会创建当前线程的inheritableThreadLocals变量。 当父线程创建子线程时，构造函数会把父线程中inheritableThreadLocals变量里面的本地变量复制一份保存到子线程的inheritableThreadLocals变量里面。  Random 随机数正常使用为：\npublic static void main(String[] args) {\rRandom random = new Random();\rfor (int i = 0; i \u0026lt; 10; i++) {\rSystem.out.println(random.nextInt(5));\r}\r}\r public int nextInt(int bound) {\rif (bound \u0026lt;= 0)\rthrow new IllegalArgumentException(BadBound);\r// 根据旧种子生成新种子\rint r = next(31);\rint m = bound - 1;\r// 拿到新种子经过算法生成随机数\rif ((bound \u0026amp; m) == 0) // i.e., bound is a power of 2\rr = (int)((bound * (long)r) \u0026gt;\u0026gt; 31);\relse {\rfor (int u = r;\ru - (r = u % bound) + m \u0026lt; 0;\ru = next(31))\r;\r}\rreturn r;\r}\r 由此可见，新的随机数的生成需要两个步骤：\n  首先根据老的种子生成新的种子。\n  然后根据新的种子来计算新的随机数。\n在单线程情况下每次调用nextInt都是根据老的种子计算出新的种子，这是可以保证随机数产生的随机性的。但是在多线程下多个线程可能都拿同一个老的种子去计算新的种子，这会导致多个线程产生的新种子是一样的，由于算法是固定的，所以会导致多个线程产生相同的随机值。那就不随机了，jdk是怎么解决的呢？\n  protected int next(int bits) {\rlong oldseed, nextseed;\rAtomicLong seed = this.seed;\rdo {\roldseed = seed.get();\rnextseed = (oldseed * multiplier + addend) \u0026amp; mask;\r} while (!seed.compareAndSet(oldseed, nextseed));\rreturn (int)(nextseed \u0026gt;\u0026gt;\u0026gt; (48 - bits));\r}\r 每个Random实例里面都有一个原子性的种子变量用来记录当前的种子值，当要生成新的随机数时需要根据当前种子计算新的种子并更新回原子变量。在多线程下使用单个Random实例生成随机数时，当多个线程同时计算随机数来计算新的种子时，多个线程会竞争同一个原子变量的更新操作，由于原子变量的更新是CAS操作，同时只有一个线程会成功。这样每次的种子都是新的。但会造成大量线程进行自旋重试，这会降低并发性能。所以 ThreadLocalRandom 应运而生。\nThreadLocalRandom Random的缺点是多个线程会使用同一个原子性种子变量，从而导致对原子变量更新的竞争，流程如图所示\n而 ThreadLocalRandom 每个线程都维护一个种子变量，则每个线程生成随机数时都根据自己老的种子计算新的种子，并使用新种子更新老的种子，再根据新种子计算随机数，就不会存在竞争问题了，这会大大提高并发性能。\n明白了这个道理，看源码其实就很简单了。使用\nThreadLocalRandom threadLocalRandom = ThreadLocalRandom.current();\rfor (int i = 0; i \u0026lt; 10; i++) {\rSystem.out.println(threadLocalRandom.nextInt(5));\r}\r public int nextInt(int bound) {\rif (bound \u0026lt;= 0)\rthrow new IllegalArgumentException(BadBound);\rint r = mix32(nextSeed());\rint m = bound - 1;\rif ((bound \u0026amp; m) == 0) // power of two\rr \u0026amp;= m;\relse { // reject over-represented candidates\rfor (int u = r \u0026gt;\u0026gt;\u0026gt; 1;\ru + m - (r = u % bound) \u0026lt; 0;\ru = mix32(nextSeed()) \u0026gt;\u0026gt;\u0026gt; 1)\r;\r}\rreturn r;\r}\r nextInt 和 Random 基本一致，主要区别在生成种子这一步\nfinal long nextSeed() {\rThread t; long r; // read and update per-thread seed\rUNSAFE.putLong(t = Thread.currentThread(), SEED,\rr = UNSAFE.getLong(t, SEED) + GAMMA);\rreturn r;\r}\r 首先使用r = UNSAFE.getLong（t, SEED）获取当前线程中threadLocalRandomSeed变量的值，然后在种子的基础上累加GAMMA值作为新种子，而后使用UNSAFE的putLong方法把新种子放入当前线程的threadLocalRandomSeed变量中。这样就是每个线程维护了自己的种子，提高了并发性能。\nCopyOnWriteArrayList ​\t学习一样东西，先要知道它是为了什么而出现的。先看一段代码\npublic static void main(String[] args) {\rList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\rfor (int i = 0; i \u0026lt; 10; i++) {\rlist.add(i);\r}\rlist.forEach(i -\u0026gt; {\rif (i == 1) {\rlist.remove(i);\r}\r});\r}\r 这段代码，遍历时删除元素很明显会抛出异常，主要原因为list中有个属性为modCount，再进行增删时此值会加1，而进行遍历时会对比modcount是否发生了变化如果发了变化则抛出异常。\n​\t那么如果在并发情况下就有可能出现 一个线程在遍历而另一个线程确在删除，出现生产问题，类似于：\npackage com.zt.javastudy.concurrent;\rimport java.util.ArrayList;\rimport java.util.List;\r/**\r* copyonwritelist 学习\r*\r* @author zhengtao on 2021/11/26\r*/\rpublic class CopyOnWriteTest {\rpublic static void main(String[] args) {\r// 初始化一个list，放入5个元素\rfinal List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();\r// 使用CopyOnWriteArrayList就不会报错，原因看过后面就知道啦\r// final CopyOnWriteArrayList\u0026lt;Integer\u0026gt; list = new CopyOnWriteArrayList\u0026lt;\u0026gt;();\rfor(int i = 0; i \u0026lt; 5; i++) {\rlist.add(i);\r}\r// 线程一：通过Iterator遍历List\rnew Thread(new Runnable() {\r@Override\rpublic void run() {\rlist.forEach(item -\u0026gt; {\rSystem.out.println(\u0026quot;遍历元素：\u0026quot; + item);\r// 由于程序跑的太快，这里sleep了1秒来调慢程序的运行速度\rtry {\rThread.sleep(1000);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\r});\r}\r}).start();\r// 线程二：remove一个元素\rnew Thread(new Runnable() {\r@Override\rpublic void run() {\r// 由于程序跑的太快，这里sleep了1秒来调慢程序的运行速度\rtry {\rThread.sleep(1000);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rlist.remove(4);\rSystem.out.println(\u0026quot;list.remove(4)\u0026quot;);\r}\r}).start();\r}\r}\r 所以才有并发List CopyOnWriteArrayList\n概念 CopyOnWriteArrayList是一个线程安全的ArrayList，对其进行的修改操作都是在底层的一个复制的数组（快照）上进行的，也就是使用了写时复制策略。\n每个CopyOnWriteArrayList对象里面有一个array数组对象用来存放具体元素，ReentrantLock独占锁对象用来保证同时只有一个线程对array进行修改。\nadd public boolean add(E e) {\r// 获取独占锁\rfinal ReentrantLock lock = this.lock;\rlock.lock();\rtry {\r// 获取array\rObject[] elements = getArray();\r// 复制array到新数组\rint len = elements.length;\rObject[] newElements = Arrays.copyOf(elements, len + 1);\r// 添加元素到新数组\rnewElements[len] = e;\r// 使用新数组替换添加前的数组\rsetArray(newElements);\rreturn true;\r} finally {\r// 释放独占锁\rlock.unlock();\r}\r}\r 调用add方法的线程会去获取独占锁，如果多个线程都调用add方法则只有一个线程会获取到该锁，其他线程会被阻塞挂起直到锁被释放。所以一个线程获取到锁后，就保证了在该线程添加元素的过程中其他线程不会对array进行修改。线程获取锁后获取array，然后复制array到一个新数组（从这里可以知道新数组的大小是原来数组大小增加1，所以CopyOnWriteArrayList是无界list），并把新增的元素添加到新数组。然后使用新数组替换原数组，并在返回前释放锁。由于加了锁，所以整个add过程是个原子性操作。需要注意的是，在添加元素时，首先复制了一个快照，然后在快照上进行添加，而不是直接在原来数组上进行。\nget public E get(int index) {\rreturn get(getArray(), index);\r}\rfinal Object[] getArray() {\rreturn array;\r} private E get(Object[] a, int index) {\rreturn (E) a[index];\r}\r 当线程x调用get方法获取指定位置的元素时，分两步走，首先获取array数组（这里命名为步骤A），然后通过下标访问指定位置的元素（这里命名为步骤B），这是两步操作，但是在整个过程中并没有进行加锁同步。\n由于执行步骤A和步骤B没有加锁，这就可能导致在线程x执行完步骤A后执行步骤B前，另外一个线程y进行了remove操作，假设要删除元素1。remove操作首先会获取独占锁，然后进行写时复制操作，也就是复制一份当前array数组，然后在复制的数组里面删除线程x通过get方法要访问的元素1，之后让array指向复制的数组。而这时候array之前指向的数组的引用计数为1而不是0，因为线程x还在使用它，这时线程x开始执行步骤B，步骤B操作的数组是线程y删除元素之前的数组。\n所以，虽然线程y已经删除了index处的元素，但是线程x的步骤B还是会返回index处的元素，这其实就是写时复制策略产生的弱一致性问题\nset public E set(int index, E element) {\rfinal ReentrantLock lock = this.lock;\rlock.lock();\rtry {\rObject[] elements = getArray();\rE oldValue = get(elements, index);\rif (oldValue != element) {\rint len = elements.length;\rObject[] newElements = Arrays.copyOf(elements, len);\rnewElements[index] = element;\rsetArray(newElements);\r} else {\r// Not quite a no-op; ensures volatile write semantics\rsetArray(elements);\r}\rreturn oldValue;\r} finally {\rlock.unlock();\r}\r}\r set方法和add方法基本一样，步骤为先拿到锁，复制数组，替换数据，重新set数组值，释放锁。remove方法也一样。\n总结 CopyOnWriteArrayList使用写时复制的策略来保证list的一致性，而获取—修改—写入三步操作并不是原子性的，所以在增删改的过程中都使用了独占锁，来保证在某个时间只有一个线程能对list数组进行修改。另外CopyOnWriteArrayList提供了弱一致性的迭代器，从而保证在获取迭代器后，其他线程对list的修改是不可见的，迭代器遍历的数组是一个快照。\nAQS 概念介绍 AbstractQueuedSynchronizer抽象同步队列简称AQS，它是实现同步器的基础组件，并发包中锁的底层就是使用AQS实现的。\nAQS是一个FIFO的双向队列，其内部通过节点head和tail记录队首和队尾元素，队列元素的类型为Node。\n类图为：\n其中Node中的结构为：\nstatic final class Node {\r/** SHARED用来标记该线程是获取共享资源时被阻塞挂起后放入AQS队列的 */\rstatic final Node SHARED = new Node();\r/** EXCLUSIVE用来标记线程是获取独占资源时被挂起后放入AQS队列的 */\rstatic final Node EXCLUSIVE = null;\r/** waitStatus value to indicate thread has cancelled */\rstatic final int CANCELLED = 1;\r/** waitStatus value to indicate successor's thread needs unparking */\rstatic final int SIGNAL = -1;\r/** waitStatus value to indicate thread is waiting on condition */\rstatic final int CONDITION = -2;\r/**\r* waitStatus value to indicate the next acquireShared should\r* unconditionally propagate\r*/\rstatic final int PROPAGATE = -3;\r/**\rwaitStatus记录当前线程等待状态，可以为CANCELLED（线程被取消了）、SIGNAL（线程需要被唤醒）、CONDITION（线程在条件队列里面等待）、PROPAGATE（释放共享资源时需要通知其他节点）\r*/\rvolatile int waitStatus;\r/**\r* prev记录当前节点的前驱节点\r*/\rvolatile Node prev;\r/**\r* next记录当前节点的后继节点\r*/\rvolatile Node next;\r/**\r* thread变量用来存放进入AQS队列里面的线程\r*/\rvolatile Thread thread;\r/**\r* Link to next node waiting on condition, or the special\r* value SHARED. Because condition queues are accessed only\r* when holding in exclusive mode, we just need a simple\r* linked queue to hold nodes while they are waiting on\r* conditions. They are then transferred to the queue to\r* re-acquire. And because conditions can only be exclusive,\r* we save a field by using special value to indicate shared\r* mode.\r*/\rNode nextWaiter;\r/**\r* Returns true if node is waiting in shared mode.\r*/\rfinal boolean isShared() {\rreturn nextWaiter == SHARED;\r}\r/**\r* Returns previous node, or throws NullPointerException if null.\r* Use when predecessor cannot be null. The null check could\r* be elided, but is present to help the VM.\r*\r* @return the predecessor of this node\r*/\rfinal Node predecessor() throws NullPointerException {\rNode p = prev;\rif (p == null)\rthrow new NullPointerException();\relse\rreturn p;\r}\rNode() { // Used to establish initial head or SHARED marker\r}\rNode(Thread thread, Node mode) { // Used by addWaiter\rthis.nextWaiter = mode;\rthis.thread = thread;\r}\rNode(Thread thread, int waitStatus) { // Used by Condition\rthis.waitStatus = waitStatus;\rthis.thread = thread;\r}\r}\r 对于AQS来说，线程同步的关键是对状态值state进行操作。根据state是否属于一个线程，操作state的方式分为独\n占方式和共享方式。\n使用独占方式获取的资源是与具体线程绑定的，就是说如果一个线程获取到了资源，就会标记是这个线程获取到了，其他线程再尝试操作state获取资源时会发现当前该资源不是自己持有的，就会在获取失败后被阻塞。\n在独占方式下，获取与释放资源的流程如下：\n（1）当一个线程调用acquire（int arg）方法获取独占资源时，会首先使用tryAcquire方法尝试获取资源，具体是设置状态变量state的值，成功则直接返回，失败则将当前线程封装为类型为Node.EXCLUSIVE的Node节点后插入到AQS阻塞队列的尾部，并调用LockSupport.park（this）方法挂起自己。\n（2）当一个线程调用release（int arg）方法时会尝试使用tryRelease操作释放资源，这里是设置状态变量state的值，然后调用LockSupport.unpark（thread）方法激活AQS队列里面被阻塞的一个线程（thread）。被激活的线程则使用tryAcquire尝试，看当前状态变量state的值是否能满足自己的需要，满足则该线程被激活，然后继续向下运行，否则还是会被放入AQS队列并被挂起。\n在共享方式下，获取与释放资源的流程如下：\n（1）当线程调用acquireShared（int arg）获取共享资源时，会首先使用tryAcquireShared尝试获取资源，具体是设置状态变量state的值，成功则直接返回，失败则将当前线程封装为类型为Node.SHARED的Node节点后插入到AQS阻塞队列的尾部，并使用LockSupport.park（this）方法挂起自己。\n（2）当一个线程调用releaseShared（int arg）时会尝试使用tryReleaseShared操作释放资源，这里是设置状态变量state的值，然后使用LockSupport.unpark（thread）激活AQS队列里面被阻塞的一个线程（thread）。被激活的线程则使用tryReleaseShared查看当前状态变量state的值是否能满足自己的需要，满足则该线程被激活，然后继续向下运行，否则还是会被放入AQS队列并被挂起。\nAQS怎么实现双向队列的？ ● 入队操作：当一个线程获取锁失败后该线程会被转换为Node节点，然后就会使用enq（final Node node）方法将该节点插入到AQS的阻塞队列。\nprivate Node enq(final Node node) {\rfor (;;) {\rNode t = tail;\rif (t == null) { // Must initialize\rif (compareAndSetHead(new Node()))\rtail = head;\r} else {\rnode.prev = t;\rif (compareAndSetTail(t, node)) {\rt.next = node;\rreturn t;\r}\r}\r}\r}\r 下面结合代码和节点图（见图6-2）来讲解入队的过程。如上代码在第一次循环中，当要在AQS队列尾部插入元素时，AQS队列状态如图6-2中（default）所示。也就是队列头、尾节点都指向null；当执行代码（1）后节点t指向了尾部节点，这时候队列状态如图6-2中（I）所示。这时候t为null，故执行代码（2），使用CAS算法设置一个哨兵节点为头节点，如果CAS设置成功，则让尾部节点也指向哨兵节点，这时候队列状态如图6-2中（II）所示。到现在为止只插入了一个哨兵节点，还需要插入node节点，所以在第二次循环后执行到代码（1），这时候队列状态如图6-2（III）所示；然后执行代码（3）设置node的前驱节点为尾部节点，这时候队列状态如图6-2中（IV）所示；然后通过CAS算法设置node节点为尾部节点，CAS成功后队列状态如图6-2中（V）所示；CAS成功后再设置原来的尾部节点的后驱节点为node，这时候就完成了双向链表的插入，此时队列状态如图6-2中（VI）所示。\n条件变量 AQS有个内部类ConditionObject，用来结合锁实现线程同步。ConditionObject可以直接访问AQS对象内部的变量，比如state状态值和AQS队列。ConditionObject是条件变量，每个条件变量对应一个条件队列（单向链表队列），其用来存放调用条件变量的await方法后被阻塞的线程，条件变量本质上还是一个等待队列，AQS 中使用单向链表来实现，成员变量如下：\npublic class ConditionObject implements Condition, java.io.Serializable {\rprivate static final long serialVersionUID = 1173984872572414699L;\r/** First node of condition queue. */\rprivate transient Node firstWaiter;\r/** Last node of condition queue. */\rprivate transient Node lastWaiter;\r// ...\r}\r ​\tnotify和wait，是配合synchronized内置锁实现线程间同步的基础设施一样，条件变量的signal和await方法也是用来配合锁（使用AQS实现的锁）实现线程间同步的基础设施。synchronized同时只能与一个共享变量的notify或wait方法实现同步，而AQS的一个锁可以对应多个条件变量。也就是说 一个Lock对象可以创建多个条件变量\n​\t当线程调用条件变量的await（）方法时（必须先调用锁的lock（）方法获取锁），在内部会构造一个类型为Node.CONDITION的node节点，然后将该节点插入条件队列末尾，之后当前线程会释放获取的锁（也就是会操作锁对应的state变量的值），并被阻塞挂起。这时候如果有其他线程调用lock.lock（）尝试获取锁，就会有一个线程获取到锁，如果获取到锁的线程调用了条件变量的await（）方法，则该线程也会被放入条件变量的阻塞队列，然后释放获取到的锁，在await（）方法处阻塞。\npublic final void await() throws InterruptedException {\rif (Thread.interrupted())\rthrow new InterruptedException();\r// 创建新的node节点，并插入到条件队列末尾\rNode node = addConditionWaiter();\r// 释放当前线程获取的锁\rint savedState = fullyRelease(node);\rint interruptMode = 0;\r// 阻塞挂起当前线程\rwhile (!isOnSyncQueue(node)) {\rLockSupport.park(this);\rif ((interruptMode = checkInterruptWhileWaiting(node)) != 0)\rbreak;\r}\rif (acquireQueued(node, savedState) \u0026amp;\u0026amp; interruptMode != THROW_IE)\rinterruptMode = REINTERRUPT;\rif (node.nextWaiter != null) // clean up if cancelled\runlinkCancelledWaiters();\rif (interruptMode != 0)\rreportInterruptAfterWait(interruptMode);\r}\rprivate Node addConditionWaiter() {\rNode t = lastWaiter;\r// If lastWaiter is cancelled, clean out.\rif (t != null \u0026amp;\u0026amp; t.waitStatus != Node.CONDITION) {\runlinkCancelledWaiters();\rt = lastWaiter;\r}\rNode node = new Node(Thread.currentThread(), Node.CONDITION);\rif (t == null)\rfirstWaiter = node;\relse\rt.nextWaiter = node;\rlastWaiter = node;\rreturn node;\r}\r ​\t当另外一个线程调用条件变量的signal方法时（必须先调用锁的lock（）方法获取锁），在内部会把条件队列里面队头的一个线程节点从条件队列里面移除并放入AQS的阻塞队列里面，然后激活这个线程。\npublic final void signal() {\r// 判断是否获取锁\rif (!isHeldExclusively())\rthrow new IllegalMonitorStateException();\rNode first = firstWaiter;\rif (first != null)\r// 将条件队列头元素移动到aqs队列\rdoSignal(first);\r}\r 总结 ​\t当多个线程同时调用lock.lock（）方法获取锁时，只有一个线程获取到了锁，其他线程会被转换为Node节点插入到lock锁对应的AQS阻塞队列里面，并做自旋CAS尝试获取锁。如果获取到锁的线程又调用了对应的条件变量的await（）方法，则该线程会释放获取到的锁，并被转换为Node节点插入到条件变量对应的条件队列里面。这时候因为调用lock.lock（）方法被阻塞到AQS队列里面的一个线程会获取到被释放的锁，如果该线程也调用了条件变量的await（）方法则该线程也会被放入条件变量的条件队列里面。当另外一个线程调用条件变量的signal（）或者signalAll（）方法时，会把条件队列里面的一个或者全部Node节点移动到AQS的阻塞队列里面，等待时机获取锁。\naqs实现锁的原理为，aqs双向阻塞队列用于存放待获取锁的线程(调用lock.lock()没有获取到锁而被阻塞的线程，被signal，signalAll唤醒的线程), 条件变量的条件队列(单向链表队列),用来存放调用条件变量的await方法后被阻塞的线程\nReentrantLock 定义 ​\tReentrantLock 是可重入的独占锁，同时只能有一个线程可以获取该锁，其他获取该锁的线程会被阻塞而被放入该锁的 AQS 阻塞队列里面。\nReentrantLock最终还是使用AQS来实现，既然是基于aqs实现，实际上ReentrantLock只需要做到以下三步\n  明确state的定义\n  重写acquire（int arg）方法，用于获取资源\n  重写release方法，用于释放资源\n  state ​\tAQS的state状态值表示线程获取该锁的可重入次数，在默认情况下，state的值为0表示当前锁没有被任何线程持有。当一个线程第一次获取该锁时会尝试使用CAS设置state的值为1，如果CAS成功则当前线程获取了该锁，然后记录该锁的持有者为当前线程。在该线程没有释放锁的情况下第二次获取该锁后，状态值被设置为2，这就是可重入次数。在该线程释放该锁时，会尝试使用CAS让状态值减1，如果减1后状态值为0，则当前线程释放该锁\n获取锁 ​\tReentrantLock根据参数来决定其内部是一个公平还是非公平锁，默认是非公平锁。\n非公平锁获取锁的过程：\nfinal void lock() {\r// cas 设置状态值\rif (compareAndSetState(0, 1))\r// 设置该锁的持有者是当前线程\rsetExclusiveOwnerThread(Thread.currentThread());\relse\racquire(1);\r}\r 因为默认AQS的状态值为0，所以第一个调用Lock的线程会通过CAS设置状态值为1, CAS成功则表示当前线程获取到了锁，然后setExclusiveOwnerThread设置该锁持有者是当前线程。如果这时候有其他线程调用lock方法企图获取该锁，CAS会失败，然后会调用AQS的acquire方法。\npublic final void acquire(int arg) {\rif (!tryAcquire(arg) \u0026amp;\u0026amp;\r// 没有拿到锁，则放入aqs阻塞队列\racquireQueued(addWaiter(Node.EXCLUSIVE), arg))\rselfInterrupt();\r}\rprotected final boolean tryAcquire(int acquires) {\rreturn nonfairTryAcquire(acquires);\r}\rfinal boolean nonfairTryAcquire(int acquires) {\rfinal Thread current = Thread.currentThread();\rint c = getState();\r// 当前锁还没被占用\rif (c == 0) {\r// 直接更新为state为1，并设置该锁持有者是当前线程\rif (compareAndSetState(0, acquires)) {\rsetExclusiveOwnerThread(current);\rreturn true;\r}\r}\r// 锁已被占用，判断当前线程是否是该锁持有者\relse if (current == getExclusiveOwnerThread()) {\rint nextc = c + acquires;\rif (nextc \u0026lt; 0) // overflow\rthrow new Error(\u0026quot;Maximum lock count exceeded\u0026quot;);\rsetState(nextc);\rreturn true;\r}\rreturn false;\r}\r 公平锁的实现策略为：\nprotected final boolean tryAcquire(int acquires) {\rfinal Thread current = Thread.currentThread();\rint c = getState();\rif (c == 0) {\r// 唯一的不同就是先有个公平性检查\rif (!hasQueuedPredecessors() \u0026amp;\u0026amp;\rcompareAndSetState(0, acquires)) {\rsetExclusiveOwnerThread(current);\rreturn true;\r}\r}\relse if (current == getExclusiveOwnerThread()) {\rint nextc = c + acquires;\rif (nextc \u0026lt; 0)\rthrow new Error(\u0026quot;Maximum lock count exceeded\u0026quot;);\rsetState(nextc);\rreturn true;\r}\rreturn false;\r}\r/**\r公平性策略，主要是检查是否有该线程是否是最先来的获取锁的线程\r*/\rpublic final boolean hasQueuedPredecessors() {\r// The correctness of this depends on head being initialized\r// before tail and on head.next being accurate if the current\r// thread is first in queue.\rNode t = tail; // Read fields in reverse initialization order\rNode h = head;\rNode s;\rreturn h != t \u0026amp;\u0026amp;\r((s = h.next) == null || s.thread != Thread.currentThread());\r}\r 释放锁 尝试释放锁，如果当前线程持有该锁，则调用该方法会让该线程对该线程持有的AQS状态值减1，如果减去1后当前状态值为0，则当前线程会释放该锁，否则仅仅减1而已。如果当前线程没有持有该锁而调用了该方法则会抛出IllegalMonitorStateException异常。\nprotected final boolean tryRelease(int releases) {\rint c = getState() - releases;\r// 如果不是锁持有者抛异常\rif (Thread.currentThread() != getExclusiveOwnerThread())\rthrow new IllegalMonitorStateException();\rboolean free = false;\r// 如果可重入次数为0，则清空锁持有线程\rif (c == 0) {\rfree = true;\rsetExclusiveOwnerThread(null);\r}\r// state减1\rsetState(c);\rreturn free;\r}\r 总结 ​\tReentrantLock的底层是使用AQS实现的可重入独占锁。在这里AQS状态值为0表示当前锁空闲，为大于等于1的值则说明该锁已经被占用。该锁内部有公平与非公平实现，默认情况下是非公平的实现。另外，由于该锁是独占锁，所以某时只有一个线程可以获取该锁。\nReentrantReadWriteLock 解决线程安全问题使用ReentrantLock就可以，但是ReentrantLock是独占锁，某时只有一个线程可以获取该锁，而实际中会有写少读多的场景，显然ReentrantLock满足不了这个需求，所以ReentrantReadWriteLock应运而生。ReentrantReadWriteLock采用读写分离的策略，允许多个线程可以同时获取读锁。\n读写锁的内部维护了一个ReadLock和一个WriteLock，它们依赖Sync实现具体功能。而Sync继承自AQS，并且也提供了公平和非公平的实现。我们知道AQS中只维护了一个state状态，而ReentrantReadWriteLock则需要维护读状态和写状态，一个state怎么表示写和读两种状态呢？ReentrantReadWriteLock巧妙地使用state的高16位表示读状态，也就是获取到读锁的次数；使用低16位表示获取到写锁的线程的可重入次数。\n获取写锁 public void lock() {\rsync.acquire(1);\r}\r// aqs内部实现\rpublic final void acquire(int arg) {\rif (!tryAcquire(arg) \u0026amp;\u0026amp;\racquireQueued(addWaiter(Node.EXCLUSIVE), arg))\rselfInterrupt();\r}\rprotected final boolean tryAcquire(int acquires) {\r/*\r* Walkthrough:\r* 1. If read count nonzero or write count nonzero\r* and owner is a different thread, fail.\r* 2. If count would saturate, fail. (This can only\r* happen if count is already nonzero.)\r* 3. Otherwise, this thread is eligible for lock if\r* it is either a reentrant acquire or\r* queue policy allows it. If so, update state\r* and set owner.\r*/\rThread current = Thread.currentThread();\r// 获取锁状态\rint c = getState();\r// 获取低16位状态，即写锁状态\rint w = exclusiveCount(c);\rif (c != 0) {\r// (Note: if c != 0 and w == 0 then shared count != 0)\r// 如果w==0说明状态值的低16位为0，而AQS状态值不为0，则说明高16位不为0，这暗示已经有线程获取了读锁，所以直接返回false\r// 如果w! =0则说明当前已经有线程获取了该写锁，再看当前线程是不是该锁的持有者，如果不是则返回false\rif (w == 0 || current != getExclusiveOwnerThread())\rreturn false;\rif (w + exclusiveCount(acquires) \u0026gt; MAX_COUNT)\rthrow new Error(\u0026quot;Maximum lock count exceeded\u0026quot;);\r// Reentrant acquire\rsetState(c + acquires);\rreturn true;\r}\r// 锁标志位为0，则没有线程获取到读锁和写锁\rif (writerShouldBlock() ||\r!compareAndSetState(c, c + acquires))\rreturn false;\rsetExclusiveOwnerThread(current);\rreturn true;\r}\r 其中公平锁与非公平锁在writerShouldBlock这里体现\n// 非公平锁实现,抢占式执行CAS尝试获取写锁，获取成功则设置当前锁的持有者为当前线程并返回true，否则返回false 非公平锁，直接返回false，\rfinal boolean writerShouldBlock() {\rreturn false; // writers can always barge\r}\r// 公平锁实现，使用hasQueuedPredecessors来判断当前线程节点是否有前驱节点，如果有则当前线程放弃获取写锁的权限，直接返回false\rfinal boolean writerShouldBlock() {\rreturn hasQueuedPredecessors();\r}\rfinal boolean readerShouldBlock() {\rreturn hasQueuedPredecessors();\r}\r 获取写锁的步骤为:\n  如果读锁和写锁都没有被另一个线程持有，则获取写锁并立即返回，将写锁持有计数设置为 1。\n  如果当前线程已经持有写锁，那么持有计数加一并且该方法立即返回。\n  如果该锁由另一个线程持有，则当前请求写锁的线程会被阻塞挂起。\n  释放写锁 protected final boolean tryRelease(int releases) {\rif (!isHeldExclusively())\rthrow new IllegalMonitorStateException();\rint nextc = getState() - releases;\rboolean free = exclusiveCount(nextc) == 0;\rif (free)\rsetExclusiveOwnerThread(null);\rsetState(nextc);\rreturn free;\r}\r 释放锁，感觉都千篇一律，都是\n 先判断是否当前线程持有该锁 判断可重入次数 如果可重入次数大于0，则可重入次数减一，如果可重入次数等于0则释放锁  获取读锁 public void lock() {\rsync.acquireShared(1);\r}\rpublic final void acquireShared(int arg) {\rif (tryAcquireShared(arg) \u0026lt; 0)\rdoAcquireShared(arg);\r}\rprotected final int tryAcquireShared(int unused) {\r/*\r* Walkthrough:\r* 1. If write lock held by another thread, fail.\r* 2. Otherwise, this thread is eligible for\r* lock wrt state, so ask if it should block\r* because of queue policy. If not, try\r* to grant by CASing state and updating count.\r* Note that step does not check for reentrant\r* acquires, which is postponed to full version\r* to avoid having to check hold count in\r* the more typical non-reentrant case.\r* 3. If step 2 fails either because thread\r* apparently not eligible or CAS fails or count\r* saturated, chain to version with full retry loop.\r*/\rThread current = Thread.currentThread();\rint c = getState();\r// 判断写锁是否被占用\rif (exclusiveCount(c) != 0 \u0026amp;\u0026amp;\rgetExclusiveOwnerThread() != current)\rreturn -1;\r// 获取读锁计数\rint r = sharedCount(c);\rif (!readerShouldBlock() \u0026amp;\u0026amp;\rr \u0026lt; MAX_COUNT \u0026amp;\u0026amp;\rcompareAndSetState(c, c + SHARED_UNIT)) {\rif (r == 0) {\rfirstReader = current;\rfirstReaderHoldCount = 1;\r} else if (firstReader == current) {\rfirstReaderHoldCount++;\r} else {\rHoldCounter rh = cachedHoldCounter;\rif (rh == null || rh.tid != getThreadId(current))\rcachedHoldCounter = rh = readHolds.get();\relse if (rh.count == 0)\rreadHolds.set(rh);\rrh.count++;\r}\rreturn 1;\r}\rreturn fullTryAcquireShared(current);\r}\r 总结 ​\t本节介绍了读写锁ReentrantReadWriteLock的原理，它的底层是使用AQS实现的。ReentrantReadWriteLock巧妙地使用AQS的状态值的高16位表示获取到读锁的个数，低16位表示获取写锁的线程的可重入次数，并通过CAS对其进行操作实现了读写分离，这在读多写少的场景下比较适用。\nStampedLock StampedLock是并发包里面JDK8版本新增的一个锁，该锁提供了三种模式的读写控制，当调用获取锁的系列函数时，会返回一个long型的变量，我们称之为戳记（stamp），这个戳记代表了锁的状态。其中try系列获取锁的函数，当获取锁失败后会返回为0的stamp值。当调用释放锁和转换锁的方法时需要传入获取锁时返回的stamp值。StampedLock提供的三种读写模式的锁分别如下。\n 写锁writeLock：是一个排它锁或者独占锁，某时只有一个线程可以获取该锁，当一个线程获取该锁后，其他请求读锁和写锁的线程必须等待，这类似于ReentrantReadWriteLock的写锁（不同的是这里的写锁是不可重入锁）；当目前没有线程持有读锁或者写锁时才可以获取到该锁。请求该锁成功后会返回一个stamp变量用来表示该锁的版本，当释放该锁时需要调用unlockWrite方法并传递获取锁时的stamp参数。并且它提供了非阻塞的tryWriteLock方法。 悲观读锁readLock：是一个共享锁，在没有线程获取独占写锁的情况下，多个线程可以同时获取该锁。如果已经有线程持有写锁，则其他线程请求获取该读锁会被阻塞，这类似于ReentrantReadWriteLock的读锁（不同的是这里的读锁是不可重入锁）。这里说的悲观是指在具体操作数据前其会悲观地认为其他线程可能要对自己操作的数据进行修改，所以需要先对数据加锁，这是在读少写多的情况下的一种考虑。请求该锁成功后会返回一个stamp变量用来表示该锁的版本，当释放该锁时需要调用unlockRead方法并传递stamp参数。并且它提供了非阻塞的tryReadLock方法。 乐观读锁tryOptimisticRead：它是相对于悲观锁来说的，在操作数据前并没有通过CAS设置锁的状态，仅仅通过位运算测试。如果当前没有线程持有写锁，则简单地返回一个非0的stamp版本信息。获取该stamp后在具体操作数据前还需要调用validate方法验证该stamp是否已经不可用，也就是看当调用tryOptimisticRead返回stamp后到当前时间期间是否有其他线程持有了写锁，如果是则validate会返回0，否则就可以使用该stamp版本的锁对数据进行操作。由于tryOptimisticRead并没有使用CAS设置锁状态，所以不需要显式地释放该锁。该锁的一个特点是适用于读多写少的场景，因为获取读锁只是使用位操作进行检验，不涉及CAS操作，所以效率会高很多，但是同时由于没有使用真正的锁，在保证数据一致性上需要复制一份要操作的变量到方法栈，并且在操作数据时可能其他写线程已经修改了数据，而我们操作的是方法栈里面的数据，也就是一个快照，所以最多返回的不是最新的数据，但是一致性还是得到保障的  总结 StampedLock提供的读写锁与ReentrantReadWriteLock类似，只是前者提供的是不可重入锁。但是前者通过提供乐观读锁在多线程多读的情况下提供了更好的性能，这是因为获取乐观读锁时不需要进行CAS操作设置锁的状态，而只是简单地测试状态。没看懂这个哈哈。\n并发队列 ​\tJDK中提供了一系列场景的并发安全队列。总的来说，按照实现方式的不同可分为阻塞队列和非阻塞队列，前者使用锁实现，而后者则使用CAS非阻塞算法实现。\nConcurrentLinkedQueue 定义 ​\tConcurrentLinkedQueue是线程安全的无界非阻塞队列，其底层数据结构使用单向链表实现，对于入队和出队操作使用CAS来实现线程安全\n​\tConcurrentLinkedQueue内部的队列使用单向链表方式实现，其中有两个volatile类型的Node节点分别用来存放队列的首、尾节点。默认头、尾节点都是指向item为null的哨兵节点。新元素会被插入队列末尾，出队时从队列头部获取一个元素。在Node节点内部则维护一个使用volatile修饰的变量item，用来存放节点的值；next用来存放链表的下一个节点，从而链接为一个单向无界链表。其内部则使用UNSafe工具类提供的CAS算法来保证出入队时操作链表的原子性。\noffer public boolean offer(E e) {\rcheckNotNull(e);\r// 构造新的node节点\rfinal Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;E\u0026gt;(e);\r// 从尾结点开始插入\rfor (Node\u0026lt;E\u0026gt; t = tail, p = t;;) {\rNode\u0026lt;E\u0026gt; q = p.next;\r// 如果q==null说明p是尾结点，则执行插入\rif (q == null) {\r// 使用cas设置p节点的next节点\rif (p.casNext(null, newNode)) {\r// cas成功，说明新增节点已经被放入链表，设置当前节点为尾节点\rif (p != t) // hop two nodes at a time\rcasTail(t, newNode); // Failure is OK.\rreturn true;\r}\r// Lost CAS race to another thread; re-read next\r}\relse if (p == q)\r// 多线程操作时，由于poll操作移除元素后可能会把head变为自引用，重新找新的head\rp = (t != (t = tail)) ? t : head;\relse\r//寻找尾节点\rp = (p != t \u0026amp;\u0026amp; t != (t = tail)) ? t : q;\r}\r}\r ​\t通过原子CAS操作来控制某时只有一个线程可以追加元素到队列末尾。进行CAS竞争失败的线程会通过循环一次次尝试进行CAS操作，直到CAS成功才会返回，也就是通过使用无限循环不断进行CAS尝试方式来替代阻塞算法挂起调用线程。相比阻塞算法，这是使用CPU资源换取阻塞所带来的开销。\n注意：创建队列时头、尾节点指向一个item为null的哨兵节点，第一次执行offer后head指向的是哨兵节点\npoll poll操作是在队列头部获取并移除一个元素，如果队列为空则返回null\npublic E poll() {\r// 1\rrestartFromHead:\r// 2 无限循环\rfor (;;) {\rfor (Node\u0026lt;E\u0026gt; h = head, p = h, q;;) {\r// 3保存当前节点\rE item = p.item;\r// 4当前节点有值则cas变为null\rif (item != null \u0026amp;\u0026amp; p.casItem(item, null)) {\r// 5cas成功则标记当前节点并从链表中移除\rif (p != h) // hop two nodes at a time\rupdateHead(h, ((q = p.next) != null) ? q : p);\rreturn item;\r}\r// 6当前队列为空则返回null\relse if ((q = p.next) == null) {\rupdateHead(h, p);\rreturn null;\r}\r// 7如果当前节点被自引用了，则重新寻找新的队列头节点\relse if (p == q)\rcontinue restartFromHead;\relse\rp = q;\r}\r}\r}\rfinal void updateHead(Node\u0026lt;E\u0026gt; h, Node\u0026lt;E\u0026gt; p) {\rif (h != p \u0026amp;\u0026amp; casHead(h, p))\rh.lazySetNext(h);\r}\r 具体流程分析看书吧。\npoll方法在移除一个元素时，只是简单地使用CAS操作把当前节点的item值设置为null，然后通过重新设置头节点将该元素从队列里面移除，被移除的节点就成了孤立节点，这个节点会在垃圾回收时被回收掉。另外，如果在执行分支中发现头节点被修改了，要跳到外层循环重新获取新的头节点\npeek peek操作是获取队列头部一个元素（只获取不移除），如果队列为空则返回null\npublic E peek() {\r// 1\rrestartFromHead:\rfor (;;) {\rfor (Node\u0026lt;E\u0026gt; h = head, p = h, q;;) {\r// 2\rE item = p.item;\rif (item != null || (q = p.next) == null) {\r// 3\rupdateHead(h, p);\rreturn item;\r}\r// 4\relse if (p == q)\rcontinue restartFromHead;\relse\r// 5\rp = q;\r}\r}\r}\r ​\tPeek操作的代码结构与poll操作类似，不同之处在于少了castItem操作。即peek只获取队列头元素但是并不从队列里将它删除，而poll获取后需要从队列里面将它删除。另外，在第一次调用peek操作时，会删除哨兵节点，并让队列的head节点指向队列里面第一个元素或者null。\nLinkedBlockingQueue 定义 LinkedBlockingQueue是使用独占锁实现的阻塞队列\n​\tLinkedBlockingQueue也是使用单向链表实现的，其也有两个Node，分别用来存放首、尾节点，并且还有一个初始值为0的原子变量count，用来记录队列元素个数。另外还有两个ReentrantLock的实例，分别用来控制元素入队和出队的原子性，其中takeLock用来控制同时只有一个线程可以从队列头获取元素，其他线程必须等待，putLock控制同时只能有一个线程可以获取锁，在队列尾部添加元素，其他线程必须等待。另外，notEmpty是takeLock（出队）的条件变量和notFull是putLock（入队）的条件变量，它们内部都有一个条件队列用来存放进队和出队时被阻塞的线程，其实这是生产者—消费者模型\noffer public boolean offer(E e) {\rif (e == null) throw new NullPointerException();\rfinal AtomicInteger count = this.count;\r// 2,如果当前队列满则丢弃将要放入的元素，然后返回false\rif (count.get() == capacity)\rreturn false;\rint c = -1;\rNode\u0026lt;E\u0026gt; node = new Node\u0026lt;E\u0026gt;(e);\rfinal ReentrantLock putLock = this.putLock;\r// 3 获取独占锁\rputLock.lock();\rtry {\r// 4 队列不满则进入队列，并递增元素计数\rif (count.get() \u0026lt; capacity) {\renqueue(node);\rc = count.getAndIncrement();\r// 5\rif (c + 1 \u0026lt; capacity)\r// 唤醒入队线程\rnotFull.signal();\r}\r} finally {\r// 6 释放锁\rputLock.unlock();\r}\r// 7\rif (c == 0)\rsignalNotEmpty();\r// 8\rreturn c \u0026gt;= 0;\r}\r// 唤醒出队线程\rprivate void signalNotEmpty() {\rfinal ReentrantLock takeLock = this.takeLock;\rtakeLock.lock();\rtry {\rnotEmpty.signal();\r} finally {\rtakeLock.unlock();\r}\r}\r ​\toffer方法通过使用putLock锁保证了在队尾新增元素操作的原子性。另外，调用条件变量的方法前一定要记得获取对应的锁，并且注意进队时只操作队列链表的尾节点。\nput public void put(E e) throws InterruptedException {\rif (e == null) throw new NullPointerException();\r// Note: convention in all put/take/etc is to preset local var\r// holding count negative to indicate failure unless set.\rint c = -1;\rNode\u0026lt;E\u0026gt; node = new Node\u0026lt;E\u0026gt;(e);\rfinal ReentrantLock putLock = this.putLock;\rfinal AtomicInteger count = this.count;\rputLock.lockInterruptibly();\rtry {\r/*\r* Note that count is used in wait guard even though it is\r* not protected by lock. This works because count can\r* only decrease at this point (all other puts are shut\r* out by lock), and we (or some other waiting put) are\r* signalled if it ever changes from capacity. Similarly\r* for all other uses of count in other wait guards.\r*/\rwhile (count.get() == capacity) {\rnotFull.await();\r}\renqueue(node);\rc = count.getAndIncrement();\rif (c + 1 \u0026lt; capacity)\rnotFull.signal();\r} finally {\rputLock.unlock();\r}\rif (c == 0)\rsignalNotEmpty();\r}\r ThreadPoolExecutor 定义 ​\t线程池主要解决两个问题：一是当执行大量异步任务时线程池能够提供较好的性能。在不使用线程池时，每当需要执行异步任务时直接new一个线程来运行，而线程的创建和销毁是需要开销的。线程池里面的线程是可复用的，不需要每次执行异步任务时都重新创建和销毁线程。二是线程池提供了一种资源限制和管理的手段，比如可以限制线程的个数，动态新增线程等。每个ThreadPoolExecutor也保留了一些基本的统计数据，比如当前线程池完成的任务数目等。\nExecutors其实是个工具类，里面提供了好多静态方法，这些方法根据用户选择返回不同的线程池实例。ThreadPoolExecutor继承了AbstractExecutorService，成员变量ctl是一个Integer的原子变量，用来记录线程池状态和线程池中线程个数，类似于ReentrantReadWriteLock使用一个变量来保存两种信息。其中高3位用来表示线程池状态，后面29位用来记录线程池线程个数。\n线程池状态含义如下：\n  RUNNING：接受新任务并且处理阻塞队列里的任务。\n  SHUTDOWN：拒绝新任务但是处理阻塞队列里的任务。\n  STOP：拒绝新任务并且抛弃阻塞队列里的任务，同时会中断正在处理的任务。\n  TIDYING：所有任务都执行完（包含阻塞队列里面的任务）后当前线程池活动线程数为0，将要调用terminated方法。\n  TERMINATED：终止状态。terminated方法调用完成以后的状态。\n  线程池状态转换列举如下：\n  RUNNING -\u0026gt; SHUTDOWN ：显式调用shutdown（）方法，或者隐式调用了finalize（）方法里面的shutdown（）方法。\n  RUNNING或SHUTDOWN）-\u0026gt; STOP ：显式调用shutdownNow（）方法时。\n  SHUTDOWN -\u0026gt; TIDYING ：当线程池和任务队列都为空时。\n  STOP -\u0026gt; TIDYING ：当线程池为空时。\n  TIDYING -\u0026gt; TERMINATED：当terminated（）hook方法执行完成时。\n  线程池参数如下：\n  corePoolSize：线程池核心线程个数。\n  workQueue：用于保存等待执行的任务的阻塞队列，比如基于数组的有界ArrayBlockingQueue、基于链表的无界LinkedBlockingQueue、最多只有一个元素的同步队列SynchronousQueue及优先级队列PriorityBlockingQueue等。\n  maximunPoolSize：线程池最大线程数量。\n  ThreadFactory：创建线程的工厂。\n  RejectedExecutionHandler：饱和策略，当队列满并且线程个数达到maximunPoolSize后采取的策略，比如AbortPolicy（抛出异常）、CallerRunsPolicy（使用调用者所在线程来运行任务）、DiscardOldestPolicy（调用poll丢弃一个任务，执行当前任务）及DiscardPolicy（默默丢弃，不抛出异常）\n  keeyAliveTime：存活时间。如果当前线程池中的线程数量比核心线程数量多，并且是闲置状态，则这些闲置的线程能存活的最大时间。\n  TimeUnit：存活时间的时间单位。\n实际配置：\n  @Bean(\u0026quot;httpWorkThreadPool\u0026quot;)\rpublic ThreadPoolTaskExecutor flowThreadPool(){\rThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor();\r// 线程池核心线程个数\rthreadPoolTaskExecutor.setCorePoolSize(threadPoolTaskProperties.getCorePoolSize());\r// 线程池最大线程数量\rthreadPoolTaskExecutor.setMaxPoolSize(threadPoolTaskProperties.getMaxPoolSize());\r// 阻塞队列大小\rthreadPoolTaskExecutor.setQueueCapacity(threadPoolTaskProperties.getQueueCapacity());\r// 饱和策略 使用调用者所在线程来运行任务\rthreadPoolTaskExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());\r// 前缀\rthreadPoolTaskExecutor.setThreadNamePrefix(threadPoolTaskProperties.getThreadNamePrefix());\r// 存活时间。如果当前线程池中的线程数量比核心线程数量多，并且是闲置状态，则这些闲置的线程能存活的最大时间\rthreadPoolTaskExecutor.setKeepAliveSeconds(threadPoolTaskProperties.getKeepAliveSeconds());\rthreadPoolTaskExecutor.initialize();\rthreadPoolTaskProperties.printExecutorInfo(\u0026quot;httpWorkThreadPool\u0026quot;);\rreturn threadPoolTaskExecutor;\r}\r 线程池类型如下：\n newFixedThreadPool ：创建一个核心线程个数和最大线程个数都为nThreads的线程池，并且阻塞队列长度为Integer.MAX_VALUE。keeyAliveTime=0说明只要线程个数比核心线程个数多并且当前空闲则回收。 newSingleThreadExecutor：创建一个核心线程个数和最大线程个数都为1的线程池，并且阻塞队列长度为Integer.MAX_VALUE。keeyAliveTime=0说明只要线程个数比核心线程个数多并且当前空闲则回收。 newCachedThreadPool ：创建一个按需创建线程的线程池，初始线程个数为0，最多线程个数为Integer.MAX_VALUE，并且阻塞队列为同步队列。keeyAliveTime=60说明只要当前线程在60s内空闲则回收。这个类型的特殊之处在于，加入同步队列的任务会被马上执行，同步队列里面最多只有一个任务。  execute execute方法的作用是提交任务command到线程池进行执行，ThreadPoolExecutor的实现实际是一个生产消费模型，当用户添加任务到线程池时相当于生产者生产元素，workers线程工作集中的线程直接执行任务或者从任务队列里面获取任务时则相当于消费者消费元素。\npublic void execute(Runnable command) {\rif (command == null)\rthrow new NullPointerException();\r/*\r* Proceed in 3 steps:\r*\r* 1. If fewer than corePoolSize threads are running, try to\r* start a new thread with the given command as its first\r* task. The call to addWorker atomically checks runState and\r* workerCount, and so prevents false alarms that would add\r* threads when it shouldn't, by returning false.\r*\r* 2. If a task can be successfully queued, then we still need\r* to double-check whether we should have added a thread\r* (because existing ones died since last checking) or that\r* the pool shut down since entry into this method. So we\r* recheck state and if necessary roll back the enqueuing if\r* stopped, or start a new thread if there are none.\r*\r* 3. If we cannot queue task, then we try to add a new\r* thread. If it fails, we know we are shut down or saturated\r* and so reject the task.\r*/\r// 获取当前线程池的状态+线程个数变量的组合值\rint c = ctl.get();\r// 当前线程池中线程个数是否小于corePoolSize，小于则开启新线程运行\rif (workerCountOf(c) \u0026lt; corePoolSize) {\rif (addWorker(command, true))\rreturn;\rc = ctl.get();\r}\r// 如果线程池处于running状态，则添加任务到阻塞队列\rif (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) {\rint recheck = ctl.get();\rif (! isRunning(recheck) \u0026amp;\u0026amp; remove(command))\rreject(command);\relse if (workerCountOf(recheck) == 0)\raddWorker(null, false);\r}\relse if (!addWorker(command, false))\rreject(command);\r}\r 总体步骤为：\n 如果当前线程池中线程个数小于corePoolSize，会向workers里面新增一个核心线程（core线程）执行该任务 当当前线程池中线程个数大于等于corePoolSize时,如果当前线程池处于RUNNING状态则添加当前任务到任务队列。如果在非RUNNING状态下则抛弃新任务。 如果向任务队列添加任务成功，则对线程池状态进行二次校验，这是因为添加任务到任务队列后，执行代码（4.2）前有可能线程池的状态已经变化了。这里进行二次校验，如果当前线程池状态不是RUNNING了则把任务从任务队列移除，移除后执行拒绝策略；如果二次校验通过，则重新判断当前线程池里面是否还有线程，如果没有则新增一个线程 如果添加任务失败，则说明任务队列已满，那尝试新开启线程来执行该任务，如果当前线程池中线程个数\u0026gt;maximumPoolSize则执行拒绝策略  addWorker private boolean addWorker(Runnable firstTask, boolean core) {\rretry:\rfor (;;) {\rint c = ctl.get();\rint rs = runStateOf(c);\r// Check if queue empty only if necessary.\rif (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp;\r! (rs == SHUTDOWN \u0026amp;\u0026amp;\rfirstTask == null \u0026amp;\u0026amp;\r! workQueue.isEmpty()))\rreturn false;\rfor (;;) {\rint wc = workerCountOf(c);\rif (wc \u0026gt;= CAPACITY ||\rwc \u0026gt;= (core ? corePoolSize : maximumPoolSize))\rreturn false;\rif (compareAndIncrementWorkerCount(c))\rbreak retry;\rc = ctl.get(); // Re-read ctl\rif (runStateOf(c) != rs)\rcontinue retry;\r// else CAS failed due to workerCount change; retry inner loop\r}\r}\rboolean workerStarted = false;\rboolean workerAdded = false;\rWorker w = null;\rtry {\rw = new Worker(firstTask);\rfinal Thread t = w.thread;\rif (t != null) {\rfinal ReentrantLock mainLock = this.mainLock;\rmainLock.lock();\rtry {\r// Recheck while holding lock.\r// Back out on ThreadFactory failure or if\r// shut down before lock acquired.\rint rs = runStateOf(ctl.get());\rif (rs \u0026lt; SHUTDOWN ||\r(rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null)) {\rif (t.isAlive()) // precheck that t is startable\rthrow new IllegalThreadStateException();\rworkers.add(w);\rint s = workers.size();\rif (s \u0026gt; largestPoolSize)\rlargestPoolSize = s;\rworkerAdded = true;\r}\r} finally {\rmainLock.unlock();\r}\rif (workerAdded) {\rt.start();\rworkerStarted = true;\r}\r}\r} finally {\rif (! workerStarted)\raddWorkerFailed(w);\r}\rreturn workerStarted;\r}\r 第一部分双重循环的目的是通过CAS操作增加线程数\n第二部分主要是使用全局的独占锁来把新增的Worker添加到工作集workers中,如果新增工作线程成功，则启动工作线程\n总结 ​\t线程池巧妙地使用一个Integer类型的原子变量来记录线程池状态和线程池中的线程个数。通过线程池状态来控制任务的执行，每个Worker线程可以处理多个任务。线程池通过线程的复用减少了线程创建和销毁的开销。\n原理其实就是，当用户添加任务到线程池，通过判断看是直接添加到核心线程workers中，还是添加到任务队列，还是队列已满执行拒绝策略，workers线程池则执行线程。\n","id":9,"section":"posts","summary":"[TOC] Java 并发编程 ​ 并发编程之美的笔记 什么是线程 ​ 进程是操作系统资源分配的最小单位，而线程是CPU任务调度和执行的最小单位 具体参考 https://blog.csdn.net/ThinkWon/article/details/102021274 五状态进程的主","tags":["Java并发"],"title":"Java 并发编程","uri":"https://wzgl998877.github.io/2022/01/java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/","year":"2022"},{"content":"[TOC]\nJava 编程思想学习 第1章 对象导论 好像看不下去，。。。\n第2章 一切都是对象   用引用操作对象：每种编程语言都有自己的操纵内存中元素的方式。java中操纵的标识符是对象的一个“引用”，可以理解为用遥控器（引用）来操作电视机（对象），只要握住这个遥控器就能保持与电视机的连接，没有电视机，遥控器也可以存在，也就是说你拥有一个引用，并不一定需要有对象与他相关联。\n  存储到什么地方\n  寄存器：这是最快的存储区，位于处理器内部数量极其有限，不能直接控制\n  堆栈：位于通用 RAM（随机访问存储器）中，但通过堆栈指针可以从处理器那里获得直接支持，堆栈指针若向下移动则分配新的内存，向上移动则释放那些内存，创建程序时，Java 系统必须知道存储在堆栈内所有项的确切生命周期，所以虽然某些 Java 数据存储于堆栈中，特别是对象引用，但Java对象并不存储于其中\n  堆:一种通用的内存池（位于 RAM 中），用于存放所有的 Java 对象，相对于堆栈，编译器不需要知道存储的数据在堆里存活多久，需要一个对象时，只需要 new 一个对象，便会自动在堆里进行存储分配\n  常量存储：常量值通常被直接存放在程序代码内部，这样做是安全的，因为它们永远不会被改变\n  非 RAM 存储：如果数据完全存活于程序之外，那么它可以不受程序的任何控制，在程序没有运行时也可以存在，典型例子流对象和持久化对象，流对象转化为字节流，通常被发送给另一台机器，持久化对象被存放于磁盘上\n    特例：基本类型。在程序设计中经常要用到一系列类型，它们需要特殊对待，可以把它们想象成基本类型，之所以特殊对待，是因为 new 将对象存储在堆中，故用 new 创建一个对象特别是小的简单的变量，往往不是很有效，因此对于这些类型，Java 采用了和 c 相同的方法，也就是说不用 new 来创建变量，而是创建一个并非是引用的自动变量，这个变量直接存储值，并置于堆栈中，因此更加高效。Java 要确定每种基本类型所占存储空间的大小，它们的大小并不像其他大多数语言那样随机器硬件架构的变化而变化，这种所占存储空间大小的不变性是 Java 程序比其他程序更具有可移植性的原因之一。但boolean类型所占存储空间的大小没有明确指定，仅定义为能够取字面值true或false\n  基本成员默认值。 若类的某个成员是基本数据类型，即使没有进行初始化，Java 也会确保它获得一个默认值\n   基本类型 默认值     boolean false   char \u0026lsquo;\\uoooo\u0026rsquo;(null)   byte (byte)0   short (short)0   int 0   long oL   float 0.0f   double 0.0d      static 关键字。当声明一个事物是 static 时，就意味着这个域或者方法不会与包含它的那个类的任何对象实例关联在一起，所以，即使从未创建某个类的任何对象，也可以调用其 static 方法或访问其 static 域，使用类名是引用 static 变量的首选，这不仅强调变量的 static 结构，而且还为编译器进行优化提供了更好的机会，尽管当 static 作用于某个字段时，肯定会改变数据创建的方式（因为一个 static 字段对于每个类来说都只有一份存储空间，而非 static 字段是每一个对象就有一个存储空间）\n  第3章 操作符   赋值操作符=，它的意思是取右边的值复制给左边，在为对象赋值时，我们真正操作的是对对象的引用，所以倘若将一个对象赋值给另一个对象，实际上是将引用从一个地方复制到另一个地方，这意味着若对对象使用c=d，那么c和d都将指向原本d指向的那个对象，此时改变c属性的值，d中属性也会发生变化（指向同一个对象）\npackage com.zt.activemq_springboot.config;\rpublic class Example {\rint i;\rpublic int getI() {\rreturn i;\r}\rpublic void setI(int i) {\rthis.i = i;\r}\r}\r@Test\rvoid test9(){\rExample a=new Example();\rExample b=new Example();\ra.setI(1);\rb.setI(2);\rSystem.out.println(\u0026quot;a: \u0026quot;+a.getI()+\u0026quot; b: \u0026quot;+b.getI());\ra=b;\rSystem.out.println(\u0026quot;a: \u0026quot;+a.getI()+\u0026quot; b: \u0026quot;+b.getI());\ra.setI(3);\rSystem.out.println(\u0026quot;a: \u0026quot;+a.getI()+\u0026quot; b: \u0026quot;+b.getI());\rint i=0;\rf(i);\rd(a);\rSystem.out.println(\u0026quot;常量： \u0026quot;+i+\u0026quot; 对象：\u0026quot;+a.getI());\r}\rstatic void f(int i){\ri=4;\r}\rstatic void d(Example example){\rexample.setI(4);\r}\r//结果\ra: 1 b: 2\ra: 2 b: 2\ra: 3 b: 3\r常量： 0 对象：4\r   自动递增和自动递减。++ 就意味则增加一个单位，\u0026ndash; 就意味着减少一个单位，( ++a , \u0026ndash;a )先执行运算再生成值，( a++, a\u0026ndash; )先生成值，再执行运算\n  逻辑操作符：与（\u0026amp;\u0026amp;）、或（||) 、非（ ！），当时用逻辑操作符时，会出现短路现象，即一旦能够明确无误地确定整个表达式的值后面就不再计算表达式余下部分了。\n  按位操作符：按位操作符是用来操作整数基本数据类型中的单个bit的，即二进制位，按位操作符会对两个参数中对应的位执行布尔代数运算，并最终生成一个结果，按位与（\u0026amp;）、按位或（|）、按位非（~），按位异或（^）,按位操作符与逻辑操作符有同样的效果，只是不会短路\n  第4章 控制执行流程 noting\n第5章 初始化与清理   方法重载。构造器 (构造方法) 是强制重载方法名的一个原因，构造器的名字已经由类名所决定，所以就只能有一个构造器名，那么如果想要多种方式创建一个对象该怎么办呢？ 为了让方法名相同而形式参数不同的构造器同时存在，必须要有方法重载。\n 区分重载方法。通过参数列表区分，为什么不能以返回值区分重载方法？因为有时你并不关心方法的返回值，你想要的是方法调用的其他效果。    this 关键字。如果有同一个类型的两个对象，分别是 a 和 b ，那么如何才能让这两个对象都能调用 peel() 方法呢？\npackage com.zt.activemq_springboot.config;\rpublic class Example {\rvoid peel(int i){}\r}\r@Test\rvoid test13(){\rExample a=new Example();\rExample b=new Example();\ra.peel(1);\rb.peel(2);\r}\r//在java中他暗自把所操作对象的引用作为参数传递给peel();\r//即java内部为\rExample.peel(a,1)\rExample.peel(b,2)\r 如果要在方法的内部获得对当前对象的引用，由于这个引用是由编译器 “偷偷” 传入的，所以没有标识符可以用，为此有个专门的关键字： this。this关键字只能在方法内部使用 ,表示对 “调用方法的那个对象” 的引用 但是如果在方法内部调用同一个类的另一个方法，就不必使用 this，直接调用即可，这是因为方法中的this引用会自动应用于同一个类中的其他方法。\n  static 的含义。static 方法就是没有this的方法。 在 static 方法的内部不能调用非静态方法，反过来则可以。\n  清理：终结处理和垃圾回收。一旦垃圾回收器准备好释放对象占用的存储空间，将首先调用其 finalize() 方法，并且在下一次垃圾回收动作发生时，才会真正回收对象占用的内存。与 c++ 相比，Java 中：对象可能不被垃圾回收；垃圾回收并不等于析构 。这意味着在你不再需要某个对象之前，如果必须执行某些动作，那么你得自己去做，要做类似的清理工作，必须自己动手创建一个执行清理工作的普通方法。例如，假设某个对象在创建过程中会将自己绘制到屏幕上，如果不是明确地从屏幕上将其檫除，它有可能永远得不到清理，想要将其檫除应该定义一个檫除的普通方法，如果在 finalize() 中加入某种檫除的话，只有当垃圾回收发生时（不能保证一定会发生）finalize() 得到了调用图像才会被檫除。所以不该将 finalize()作为通用的清理方法。\n  finalize() 的用途何在。垃圾回收只与内存有关 也就是说，使用垃圾回收器的唯一原因是为了回收程序不再使用的内存。无论对象是如何创建的（对象中含有其他对象或者是其他情况）垃圾回收器都会负责释放对象所占据的所有内存。这就将 finalize() 的需求限制到一种特殊情况，即通过某种创建对象方式以外的方式为对象分配了存储空间，这种情况主要发生在使用 “本地方法” 的情况下，本地方法是一种在 Java 中调用非 Java 代码的方式，在非 Java 代码中，也许会调用 C 的 malloc() 函数系列来分配存储空间，而且除非调用了 free() 方法，否则存储空间得不到释放，从而造成内存泄漏，当然 free() 是非 Java 代码，所以需要在 finalize() 中用 “本地方法” 调用。\n  垃圾回收器如何工作。有点复杂。。。。\n  初始化顺序。在类的内部，变量定义的先后顺序决定了初始化的顺序。即使变量定义散布于方法定义间，它们仍旧会在任何方法（包括构造方法）被调用之前得到初始化。\n  静态数据的初始化。static 关键字不能应用于局部变量，因此它只能作用于域。如果一个域是静态的基本类型域，且没有初始化，那么它就会获得基本类型的标准初值，如果它是一个对象的引用，那么默认初始化值则为 null，静态初始化只有在必要时刻才会进行，并且静态初始化只会进行一次，在首次生成这个类的一个对象时或者首次访问属于这个类的静态变量或方法时。\n初始化的顺序是 父类静态变量 \u0026ndash;\u0026gt; 子类静态变量 \u0026ndash;\u0026gt; 父类实例变量 \u0026ndash;\u0026gt; 子类实例变量 \u0026ndash;\u0026gt; 父类构造函数 \u0026ndash;\u0026gt; 子类构造函数\n  对象的创建过程，假设有个Dog类\n  即使没有显示地使用 static 关键字，构造器实际上也是静态方法。当首次创建类型为 Dog 的对象时（构造器可以看成静态方法），或者 Dog 类的静态方法/静态域首次被访问时 Java 解释器必须查找类路径，以定位 Dog.class 文件。\n  然后载入 Dog.class (这将创建一个 Class 对象)，有关静态初始化的所有动作都会执行。因此，静态初始化只在 Class 对象首次加载的时候进行一次。\n  当用 new Dog() 创建对象时，首先在堆上为 Dog 对象分配足够的存储空间。\n  这块存储空间将会被清零，这就自动地将 Dog 对象中的所有基本类型数据都设置成了默认值，而引用则被设置成了 null。\n  执行所有出现于字段定义处的初始化动作。\n  执行构造器。\n    数组初始化。数组只是相同类型的、用一个标识符名称封装到一起的一个对象序列或基本类型数据序列。\nvoid test14(){\r// 这两种是等价的\rint[] a=new int[]{1,2,3};\rint[] a1={1,2,3};\r}\r 可变参数列表，应用于参数个数或类型未知的场合。\n@Test\rvoid test14(){\rprintArray(1,2,3);\rprintArray(\u0026quot;可变\u0026quot;,\u0026quot;参数\u0026quot;,\u0026quot;列表\u0026quot;);\r}\r/**\r*\r* @param args 类型加... 就代表了可变参数列表,可以使用任何类型的参数，包括基本类型\r*/\rstatic void printArray(Object... args){\rfor(Object object:args){\rSystem.out.print(object+\u0026quot; \u0026quot;);\r}\r}\r   枚举类。在你创建 enum 时，编译器会自动添加一些有用的特性。例如，它会创建 toString() 方法，以便你可以很方便地显示某个 enum 实例的name，编译器还会创建 ordinal() 方法，用来表示某个特定 enum 常量的声明顺序，以及 static values() 方法，用来按照 enum 常量的声明顺序，产生由这些常量值构成的数组。\npublic enum Fruit {\rAPPLE,BANANA,PEAR;\r}\r@Test\rvoid test16(){\rSystem.out.println(Fruit.APPLE);//默认的toString 打印出他的name\rfor(Fruit fruit:Fruit.values()){\rSystem.out.println(fruit+\u0026quot;, ordinal \u0026quot;+fruit.ordinal());\r}\r}\r/**\r结果\rAPPLE\rAPPLE, ordinal 0\rBANANA, ordinal 1\rPEAR, ordinal 2\r**/\r   第6章 访问权限控制   当编写一个 Java 源代码文件时，此文件通常被称为编译单元（转译单元）。每个编译单元必须有后缀名 .java,在编译单元内可以有一个 public 类，该类的名称必须与文件的名称相同，每一个编译单元只能有一个 public 类。\n  当编译一个 .java 文件时，在 .java 文件中的每个类都会有一个后缀名为 .class 的输出文件。\n  第7章 复用类 tips: foreach 的一些注意事项，结论：foreach 循环迭代数组元素时，不能改变“基本数组类型元素”的值，因此不要对 foreach 的循环变量进行赋值，引用类型数组（除 String 类型）可以改变。 举例：\n​\tforeach 不可以改变变量，即使使用集合存变量也不可以，foreach 循环中，是把容器中的数据交给了\t那个element相当于一个临时变量，系统会把数组元素依次赋给这个临时变量，而这个临时变量并不是\t数组元素，它只是保存了数组元素的值。因此当容器中装的是变量时，foreach是改变不了元数据的，\t想改变只能通过for循环\n@Test\rvoid test15(){\rint[] a={1,2,3};\rfor(int i:a){\ri++;\r}\rfor(int i:a){\rSystem.out.print(i+\u0026quot; \u0026quot;);\r}\rSystem.out.println();\r}\r//结果\r//1 2 3  ​\tforeach可以改变对象的值，但不能删除或添加对象（ foreach 循环中，是把容器中的数据交给了那个element，当容器中装的是对象时，对象的赋值（赋的是引用，即给的是原对象所在的地址））\n为什么不能删除或者添加变量，因为每次进入foreach是，就会调用java.util.LinkedList.Listltr.next()方法，方法对集合的长度进行了判断，所以会出现异常。\n@Test\rvoid test15(){\rList\u0026lt;Value\u0026gt; list=new ArrayList();\rlist.add(new Value(1));\rlist.add(new Value(2));\rlist.add(new Value(3));\rfor(Value value:list){\rvalue.setI(value.getI()+1);\r}\rfor (Value value:list){\rSystem.out.print(value.getI()+\u0026quot; \u0026quot;);\r}\r//结果\r//2 3 4\rfor(Value value:list){\rlist.remove(value);//报ConcurrentModificationException错\r}\r}\r   final 关键字。通常它指的是“这是无法改变的”，一个既是 static 又是 final 的 域只占据一段不能被改变的存储空间。\n final 数据。对于基本类型，final 使数值恒定不变；而对于对象，final 使引用恒定不变，一旦引用被初始化指向一个对象，就无法再把它改为指向另一个对象，然而，对象其自身却是可以被修改的，这一限制同样适用于数组，它也是对象。  package com.zt.activemq_springboot.config;\rimport java.util.Random;\rpublic class FinalData {\rprivate static Random random=new Random(47);\rprivate String id;\rpublic FinalData(String id){\rthis.id=id;\r}\rprivate final int valueOne=9;\rprivate static final int ValueTwo=99;\rpublic static final int ValueThree=39;\rprivate final int i4= random.nextInt(20);\rstatic final int INT_5=random.nextInt(20);\rprivate Value v1=new Value(11);\rprivate final Value v2=new Value(22);\rpublic static final Value VAL_3=new Value(33);\rprivate final int[] a={1,2,3};\r@Override\rpublic String toString(){\rreturn id+\u0026quot;:\u0026quot;+\u0026quot;i4:\u0026quot;+i4+\u0026quot;,INT_5=\u0026quot;+INT_5;\r}\rpublic static void main(String[] args) {\rFinalData fd1=new FinalData(\u0026quot;fd1\u0026quot;);\r// final的基本类型不能改变\r// fd1.valueOne++;\rfd1.v2.i++;\rfd1.v1=new Value(9);\rfor (int i=0;i\u0026lt;fd1.a.length;i++) {\rfd1.a[i]++;\r}\rfor(int i:fd1.a) {\rSystem.out.print(i+\u0026quot; \u0026quot;);\r}\rSystem.out.println();\rSystem.out.println(fd1.v2.i);\rSystem.out.println(fd1.toString());\rFinalData fd2=new FinalData(\u0026quot;fd2\u0026quot;);\rSystem.out.println(fd1.toString());\rSystem.out.println(fd2.toString());\r}\r}\r/**\r结果\r2 3 4 证明final类型的数组，值可以改变\r23\rfd1:i4:15,INT_5=18 因为i4 是final 的所以是不变的，只针对于当前对象，而对于INT_5是static的所以只会初始化一次所以不变，static可以参考第5章\rfd1:i4:15,INT_5=18\rfd2:i4:13,INT_5=18\r**/\r 对于 valueOne，ValueTwo 都是带有编译时数值的 final 基本类型，称为编译期常量，对于 ValueThree 定义为 public ，则可以被用于包之外；定义为 static，强调只有一份；定义为 final 说明是个常量。我们不能因为某数据是 final 的就认为在编译时可以知道它的值\n final 方法。使用 final 方法的原因：1、把方法锁定，以防任何继承类修改它的含义；2、出于效率，现在已经慢慢不用这种方式提高效率了。类中所有的 private 方法都隐式地指定为是 final 的。final 类 就表明了你不打算继承该类，而且也不允许别人这么做。final 类中的所有方法都隐式指定为 final 的。\n  初始化及类的加载。因为 Java 中的所有事物都是对象，每个类的编译代码都存在于它自己的独立文件中。该文件只在需要使用程序代码时 才会被加载。可以说，类的代码在初次使用时才加载，这通常是指加载发生于创建类的第一个对象之时，但是当访问 static 域 或方法时，也会发生加载，**初次使用之处也是 static 初始化发生之处。所有的 static 对象和 static 代码段都会在加载时，依程序的顺序（即，定义类时的书写顺序）而依次初始化。当然定义为 static 的东西只会被初始化一次。**构造器也是 static 方法，所以准确讲类是在其任何 static 成员被访问时加载的。\n  初始化全过程。\npackage com.zt.activemq_springboot.config;\rclass Insect {\rprivate int i=9;//4初始化父类实例变量，此时i=9，而j还是0\rprotected int j;\rInsect(){//5执行父类构造方法\rSystem.out.println(\u0026quot;我是父类构造方法\u0026quot;);\rSystem.out.println(\u0026quot;i= \u0026quot;+i+\u0026quot;, j= \u0026quot;+j);\rj=39;\r}\rprivate static int x1=printInit(\u0026quot;我是父类静态变量\u0026quot;);//1父类static初始化\rstatic int printInit(String s){\rSystem.out.println(s);\rreturn 47;\r}\r}\rpublic class Bettle extends Insect{\rprivate int k=printInit(\u0026quot;我是子类变量\u0026quot;);//6初始化子类实例变量\rpublic Bettle(){//7执行子类构造方法\rSystem.out.println(\u0026quot;我是子类构造方法\u0026quot;);\rSystem.out.println(\u0026quot;k= \u0026quot;+k);\rSystem.out.println(\u0026quot;j= \u0026quot;+j);\r}\rprivate static int x2=printInit(\u0026quot;我是子类静态变量\u0026quot;);//2 子类static初始化\r//3 将所有基本类型设为默认值，对象引用被设为null，这里是k，j，i都被初始化为0\rpublic static void main(String[] args) {\rSystem.out.println(\u0026quot;请开始你的表演\u0026quot;);\rBettle b=new Bettle();\r}\r}\r/**\r结果\r我是父类静态变量\r我是子类静态变量\r请开始你的表演\r我是父类构造方法\ri= 9, j= 0\r我是子类变量\r我是子类构造方法\rk= 47\rj= 39\r**/\r     第8章 多态   将一个方法调用同一个方法主体关联起来被称为绑定，若在程序执行前进行绑定，叫前期绑定，后期绑定：在运行时根据对象的类型进行绑定，后期绑定也叫做动态绑定或运行时绑定 编译器一直不知道对象的类型，但是方法调用机制能找到正确的方法。Java 中除了 static 方法和 final 方法（private 方法属于final方法）之外，所有方法都是后期绑定。\n  覆盖私有方法。由于 private 方法被自动认为就是一个全新的方法，而且对导出类是屏蔽的，如果在子类中覆盖 private 方法此时此方法就是一个全新的方法,\n如果某个方法是静态的，它的行为就不具有多态性。\n  通过组合和继承方法来创建新类时，永远不必担心对象的清理问题，子对象通常都会留给垃圾回收器进行处理，所以万一某个子对象要依赖于其他对象，销毁的顺序应该和初始化顺序相反，对于字段，则意味着于声明的顺序相反，对于基类应该先对其导出类进行清理然后才是基类。\n  第 9章 接口   包含抽象方法的类叫做抽象类。如果一个类包含一个或多个抽象方法，该类必须被限定为抽象的。从一个抽象类继承，并想创建该新类的对象，那么就必须为基类中的所有抽象方法提供方法定义。如果不这样做，那么导出类也是抽象类，抽象类可以不包含抽象方法。\n  interface 这个关键字产生一个完全抽象的类，没有提供任何具体实现，接口被用来建立类与类之间的协议。接口是实现多重继承的途径。\n  生成遵循某个接口的对象的典型方式就是工厂方法设计模式\n  第10章 内部类   可以将一个类的定义放在另一个类的定义内部，这就是内部类。内部类他能访问其外围对象的所有成员。当某个外围类的对象创建了一个内部类对象时，此内部类对象必定会秘密的捕获一个指向那个外围类对象的引用。在拥有外部类对象之前是不可能创建内部类对象的，这是因为内部类对象会暗暗的连接到创建它的外部类对象上，但是如果是静态内部类，则不需要对外部类对象的引用。\n  在方法的作用域内创建一个完整的类，称为局部内部类：\n//用到的类\rpublic class Wrapping {\rprivate int i;\rpublic Wrapping(int x){ i = x ;}\rpublic int value(){ return i;}\r}\rpublic interface Destination {\rString readLabel();\r}\rpublic interface Contents {\rint value();\r}\r public class InnerClass {\rpublic Destination destination(String s){\rclass PDestionation implements Destination{\rprivate String label;\rprivate PDestionation(String whereTo){\rlabel = whereTo;\r}\r@Override\rpublic String readLabel(){return label;}\r}\rreturn new PDestionation(s);\r}\rpublic static void main(String[] args) {\rInnerClass p = new InnerClass();\rDestination d = p.destination(\u0026quot;bob\u0026quot;);\rSystem.out.println(d.readLabel());\r}\r}\r PDestionation 是 destination 方法的一部分，而不是类的一部分，所以在方法外不能访问该类。\n  匿名内部类，将返回值的生成与表示这个返回值的类的定义结合在一起\n  package com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 内部类测试\r* @date 2020/10/13\r*/\rpublic class InnerClass {\r// 匿名内部类\rpublic Contents contents(){\rreturn new Contents() {\rprivate int i=11;\r@Override\rpublic int value() {\rreturn i;\r}\r};\r}\r// 相当于以下两段代码\rclass Mycontents implements Contents{\rprivate int i=11;\r@Override\rpublic int value() {\rreturn i;\r}\r}\rpublic Contents contents1(){\rreturn new Mycontents();\r}\rpublic static void main(String[] args) {\rInnerClass p = new InnerClass();\rContents c = p.contents();\rSystem.out.println(c.value());\rDestination d = p.destination(\u0026quot;bob\u0026quot;);\rSystem.out.println(d.readLabel());\r}\r}\r 有参数的构造器使用匿名内部类\npackage com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 内部类测试\r* @date 2020/10/13\r*/\rpublic class InnerClass {\rpublic Wrapping wrapping(int x){\rreturn new Wrapping(x){\r@Override\rpublic int value(){\rreturn super.value()*10;\r}\r};\r}\rpublic static void main(String[] args) {\rInnerClass p = new InnerClass();\rWrapping w = p.wrapping(10);\rSystem.out.println(w.value());//100\r}\r}\r 如果定义一个匿名内部类，并且希望它使用一个在其外部定义的对象，那么编译器会要求其参数引用是 final 的。匿名内部类不能同时实现接口，继承类，而且如果是实现接口也只能实现一个接口\npackage com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 内部类测试\r* @date 2020/10/13\r*/\rpublic class InnerClass {\r//在JDK8之前，如果我们在匿名内部类中需要访问局部变量，那么这个局部变量必须用final修饰符修饰\r//在JDK8中如果我们在匿名内部类中需要访问局部变量，那么这个局部变量不需要用final修饰符修饰。看似是一种编译机制的改变，实际上就是一个语法糖（底层还是帮你加了final）。但通过反编译没有看到底层为我们加上final，但我们无法改变这个局部变量的引用值，如果改变就会编译报错。\rpublic Destination destination1( final String dest){\rreturn new Destination() {\rprivate String label=dest;\r@Override\rpublic String readLabel() {\rreturn label;\r}\r};\r}\rpublic static void main(String[] args) {\rInnerClass p = new InnerClass();\rDestination D=p.destination1(\u0026quot;alice\u0026quot;);\rSystem.out.println(D.readLabel()); }\r}\r 为什么需要内部类。 内部类有效的实现了 多重继承 它允许继承多个非接口类型 (类或抽象类)。  package com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 测试内部类解决多继承\r* @date 2020/10/13\r*/\rclass D{}\rabstract class E{}\rclass Z extends D{\rE makeE(){\rreturn new E() {\r};\r}\r}\rpublic class MultiImplementation {\rstatic void takesD(D d){}\rstatic void takesE(E e){}\rpublic static void main(String[] args) {\rZ z=new Z();\rtakesD(z);\rtakesE(z.makeE());\r}\r}\r  由于每个类都会产生一个.class文件，其中包含了如何创建该类型的对象的全部信息（此信息产生一个 “meta-class”，叫做 Class 对象）内部类也必须生成一个 .class 文件以包含它们的 Class 对象信息。这些类文件的命名有严格的规则： 外围类的名字，加上 $ 再加上内部类的名字。如果是匿名内部类，就产生一个数字作为其标识符。\npackage com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 测试内部类解决多继承\r* @date 2020/10/13\r*/\rclass Egg2{\rprotected class Yolk{\rpublic Yolk(){\rSystem.out.println(\u0026quot;egg2.yolk\u0026quot;);\r}\rpublic void f(){\rSystem.out.println(\u0026quot;egg2.yolk.f\u0026quot;);\r}\r}\rprivate Yolk y = new Yolk();\rpublic Egg2(){\rSystem.out.println(\u0026quot;new Egg2\u0026quot;);\r}\rpublic void insertYolk(Yolk y) {\rthis.y = y;\r}\rpublic void g(){\ry.f();\r}\r}\rpublic class MultiImplementation extends Egg2{\rpublic class Yolk extends Egg2.Yolk{\rpublic Yolk(){\rSystem.out.println(\u0026quot;newegg2.yolk\u0026quot;);\r}\rpublic void f(){\rSystem.out.println(\u0026quot;newegg2.yolk.f\u0026quot;);\r}\r}\rpublic MultiImplementation(){\rinsertYolk(new Yolk());\r}\rpublic static void main(String[] args) {\rEgg2 egg2=new MultiImplementation();\regg2.g();\r}\r}\r/*\r打印出来为\regg2.yolk\rnew Egg2\regg2.yolk\rnewegg2.yolk\rnewegg2.yolk.f\r生成的class文件为 Egg2.class Egg2$Yolk.class MultiImplementation$Yolk.class MultiImplementation.class\r*/\r   第11章 持有对象  迭代器是一个对象，它的工作是遍历并选择序列中的对象，而不需要知道或关心该序列底层的结构。  @Test\rvoid test34(){\rList\u0026lt;Integer\u0026gt; list = new ArrayList();\rlist.add(1);\rlist.add(2);\rlist.add(3);\rIterator\u0026lt;Integer\u0026gt; iterator=list.iterator();// iterator()要求容器返回一个Iterator\rwhile (iterator.hasNext()){// 检查序列中是否还有元素\rInteger a = iterator.next();// 获得序列中的下一个元素\riterator.remove();// 将迭代器新近返回的元素删除,必须先调用next方法\rSystem.out.println(a);\r}\r}\r Stack 栈 后进先出，peek() 方法提供栈顶元素，pop() 将移除并返回栈顶元素。不能使用基本类型的容器。  第12章 通过异常处理错误   Java 的基本理念是 “结构不佳的代码不能运行”\n  异常情形是指阻止当前方法或作用域继续执行的问题。异常情形在当前环境下无法获得必要的信息来解决问题。你所能做的就是从当前环境跳出，并且把问题提交给上一级，这就是抛出异常的时候所发生的事情。当抛出异常后，有几件事情会随之发生。首先，同 Java 中其他对象的创建一样，将使用 new 在堆上创建异常对象，然后当前的执行路劲被终止，并且从当前的环境中弹出对异常对象的引用。此时，异常处理机制接管程序，并开始寻找一个恰当的地方来继续执行程序。这个恰当的地方就是异常处理程序，它的任务是将程序从错误状态中恢复，以使程序要么能换一种方式运行，要么继续运行下去。\n  try块。如果在方法内部抛出了异常（或者在方法内部调用的其他方法抛出了异常），这个方法将在抛出异常的过程中结束，要是不希望方法就此结束，可以在方法内设置一个特殊的块来捕获异常。因为在这个块里尝试各种可能产生异常的方法调用，所以称为 try 块。\n  异常处理程序。当然，抛出的异常必须在某处得到处理。这个地点就是异常处理程序，就是catch内的内容，每个 catch 子句看起来就像是接受一个且仅接受一个特殊类型的参数的方法，异常处理程序必须紧跟在 try 块之后，当异常被抛出时，异常处理机制将负责搜寻参数与异常类型相匹配的第一个处理程序，然后进入 catch 子句执行，此时认为异常得到了处理，一旦 catch 子句结束，则处理程序的查找过程结束。\n  终止与恢复。异常处理理论上有两种基本模型，终止模型，将假设错误非常关键，以至于程序无法返回到异常发生的地方继续执行。一旦异常被抛出，就表明错误已经无法挽回，也不难回来执行。恢复模型，意思是异常处理程序的工作是修订错误，通常希望异常被处理之后能继续执行程序。\n  自定义异常。printStackTrace方法将打印“从方法调用处直到异常抛出处”的方法调用序列，将被输出到标准错误流。\npackage com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 自定义异常类\r* @date 2020/10/15\r*/\rpublic class MyException extends Exception{\rprivate String code;\rpublic MyException(String code,String msg){\rsuper(msg);\rthis.code=code;\r}\rpublic MyException(){\r}\rpublic String getCode() {\rreturn code;\r}\rpublic void setCode(String code) {\rthis.code = code;\r}\r}\r@Test\rvoid test36(){\rtry {\rint a = 1/0;\r} catch (Exception e) {\rMyException myException = new MyException(\u0026quot;zz\u0026quot;,e.getMessage());\rSystem.out.println(myException.getMessage());\rmyException.printStackTrace(System.out);//将其发送到了标准输出流：System.out，所以不会报错\rmyException.printStackTrace();\r}\r}\r 结果\n   异常说明， java 鼓励人们把方法可能会抛出的异常告知使用此方法的调用者，java 提供了相应的语法（并强制使用这个语法），它属于方法声明的一部分，紧跟在形式参数列表之后，使用附加的关键字 throws，后面接一个所有潜在异常类型的列表。代码必须与异常说明保持一致。如果方法里的代码产生了异常却没有进行处理，编译器会发现问题并提醒你：要么处理这个异常，要么就在异常说明中表明此方法将产生异常。但可以声明方法将抛出异常，实际上却不抛出，编译器相信这个声明，并强制此方法的用户像真的抛出异常那样使用这个方法。这样做的好处是，为异常先占个位置，以后就可以抛出这种异常而不用修改已有的代码，在定义抽象基类和接口时这种能力很重要，这样派生类或接口实现就能够抛出这些预先声明的异常。\n  异常链，常常会想要在捕获一个异常后抛出另一个异常，并且希望把原始异常的信息保存下来，这被称为异常链，JDK 1.4 后 所有 Throwable 的子类在构造器中都可以接受一个cause对象作为参数，这个cause就表示原始异常，这样通过把原始异常传递给新的异常，使得即使在当前位置创建并抛出了新的异常，也能通过这个异常链追踪到异常最初发生的地方。只有Error，Exception，RuntimeException这三个类提供了带cause的构造方法。\n  java 标准异常，RuntimeException 运行时异常，它们会自动被 Java 虚拟机抛出，所以不必在异常说明中把它们列出来。但是可以在代码中抛出RuntimeException 异常而且不需要异常说明，因为它直接到达了main方法,再程序退出前将调用异常的printStackTrace方法。\n@Test\rvoid test36() throws MyException {\r// throw new MyException();//必须要异常说明或处理异常\rthrow new RuntimeException();//不需要\r}\r   第13章\t字符串   String 类中每一个看起来会修改 String 值的方法，实际上都是创建了一个全新的 String 对象，以包含修改后的字符串内容。而最初的 String 对象则丝毫未动\n  重载的 “+” 操作符，重载操作符的意思是，一个操作符在应用于特定的类时，被赋予了特殊的意义（用于 String 的 “+” 和 “+=” 是 Java 中仅有的两个重载过的操作符，而 Java 不允许程序员重载任何操作符）重载 + 操作符其实是使用了 StringBuilder的 append() 方法，然后在 toString，但是在循环体中使用 “+” 每次循环都会 new 出一个 StringBuilder 对象， 然后进行 append 操作，最后通过 toString 方法返回 String 对象，造成内存资源浪费。 所有循环体中建议直接使用 StringBuilder。\n  第15章\t泛型 ​\t只有知道了某个技术不能做到什么，你才能更好的做到所能做到的。\n  泛型类，我们更喜欢暂时不指定类型，而是稍后再决定具体使用什么类型，要达到这个目的，需要使用类型参数，用尖括号括住，放在类名后，然后在使用这个类的时候，再用实际的类型替换此参数类型。\npackage com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 泛型\r* @date 2020/10/20\r*/\rclass Automobile{\r}\rpublic class Holder\u0026lt;T\u0026gt; {// T 为类型参数\rprivate T a;\rpublic Holder(T a){\rthis.a=a;\r}\rpublic T getA() {\rreturn a;\r}\rpublic void setA(T a) {\rthis.a = a;\r}\rpublic static void main(String[] args) {\rHolder\u0026lt;Automobile\u0026gt; holder = new Holder\u0026lt;\u0026gt;(new Automobile());//创建对象时必须要指定想持有什么类型的对象，然后就只能在其中存入该类型或其子类，这就是 java 泛型的核心概念: 告诉编译器想使用什么类型，然后编译器帮你处理一切细节。\r}\r}\r   泛型接口， Java 泛型的局限性：基本类型无法作为类型参数。\n  泛型方法，是否拥有泛型方法，与其所在的类是不是泛型类没有关系，要定义泛型方法，只需将泛型参数列表置于返回值之前\npackage com.zt.activemq_springboot;\rimport sun.nio.cs.Surrogate;\r/**\r* @author zhengtao\r* @description 泛型\r* @date 2020/10/20\r*/\rclass Automobile{\r}\rpublic class Holder\u0026lt;T\u0026gt; {// T 为类型参数\rprivate T a;\rpublic Holder(T a){\rthis.a=a;\r}\rpublic T getA() {\rreturn a;\r}\rpublic void setA(T a) {\rthis.a = a;\r}\rpublic static void main(String[] args) {\rHolder\u0026lt;Automobile\u0026gt; holder = new Holder\u0026lt;\u0026gt;(new Automobile());\rholder.f(new Automobile());\rholder.f(holder);\rholder.f(1);\r}\rpublic \u0026lt;s\u0026gt; void f(s x){// s是泛型参数，可用任何字符代替,一般用T\rSystem.out.println(x.getClass().getName());\r}\r}\r   可变参数与泛型方法\npublic static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; makeList(T... args){\rList\u0026lt;T\u0026gt; result = new ArrayList\u0026lt;\u0026gt;();\rfor(T t:args){\rresult.add(t);\r}\rreturn result;\r}\r   檫除，在泛型代码内部，无法获得任何有关泛型参数类型的信息，Java的泛型是通过擦除来实现的，这意味着当你使用泛型时，任何具体的类型信息都被擦除了，你唯一知道的就是你在使用一个对象。\nList\u0026lt;String\u0026gt; l1 = new ArrayList\u0026lt;String\u0026gt;();\rList\u0026lt;Integer\u0026gt; l2 = new ArrayList\u0026lt;Integer\u0026gt;();\r// 这两种形式都被檫除成它们的原生类型，即List\rSystem.out.println(l1.getClass() == l2.getClass());// true\r package com.zt.activemq_springboot;\r/**\r* @author zhengtao\r* @description 擦除\r* @date 2020/10/20\r*/\rclass HasF{\rpublic void f(){\rSystem.out.println(\u0026quot;HasF.f()\u0026quot;);\r}\r}\rclass Manipulator\u0026lt;T\u0026gt;{\rprivate T obj;\rpublic Manipulator(T x){\robj=x;\r}\rpublic void manipulate(){\robj.f(); //因为擦除所以不知道这里是什么类型。\r}\r}\rclass Manipulator2\u0026lt;T extends HasF\u0026gt;{ // 指定了T 必须是HasF或者从HasF导出的类\rprivate T obj;\rpublic Manipulator2(T x){\robj=x;\r}\rpublic void manipulate(){\robj.f(); // 可以正确调用\r}\r}\rpublic class Manipulation {\r}\r   在基于檫除的实现中，泛型类型被当作第二类类型处理，即不能在某些重要的上下文环境中使用的类型，泛型类型只有在静态类型检查期间才出现，在此之后，程序中的所有泛型类型都将被檫除，替换为他们的非泛型上界，例如 List被檫除为 List。\n  檫除的问题，泛型不能用于显式地引用运行时类型的操作之中，例如转型、instanceof、new等，因为所有关于参数的类型信息都丢失了。\n  泛型数组，不能创建泛型的数组（T[] array = new T[10] 错滴） ，解决方案是在任何想要创建泛型数组的地方都是用 ArrayList\n  边界，因为檫除移除了类型信息，所以，可以用无界泛型参数调用的方法只是那些可以用 Object 调用的方法，但是，如果能够将这个参数限制为某个类型子集，那么你就可以用这些类型子集来调用方法，为了执行这种限制，Java 泛型重用了 extends 关键字。\n  通配符，数组的一种特殊行为：可以向导出类型的数组赋予其类型的数组引用\npackage com.zt.activemq_springboot.config;\rclass Fruit1{}\rclass Apple extends Fruit1{}\rclass Jonathan extends Apple{}\rclass Orange extends Fruit1{}\rpublic class Example {\rpublic static void main(String[] args) {\rFruit1[] fruit = new Apple[10];// 此时创建了一个 Apple 数组，并将其赋值给一个 Fruit 数组引用\rfruit[0] = new Apple();// 可以放置Apple或Apple的子类型\rfruit[1] = new Jonathan();\rfruit[2] = new Fruit1();// Apple有一个 Fruit[] 引用，所以编译器允许你将 fruit或fruit的子类 放置到这个数组中所以不会报错，但在运行时，数组机制知道它处理的是 Apple[] 所以就会抛出异常\rfruit[3] = new Orange();\r}\r}\r 使用泛型的主要目标之一是将这种错误检测移入到编译期。\nList\u0026lt;Fruit1\u0026gt; fruits = new ArrayList\u0026lt;Apple\u0026gt;();//直接报错，\r// 不能将一个涉及 Apple的泛型赋值给一个涉及 Fruit1的泛型。如果就像在数组中的情况一样，编译器对代码的了解足够多，可以确定所涉及到的容器，但是它不知道任何有关这方面的信息，因此它拒绝向上转型。但是这根本不是向上转型，Apple的list将持有Apple和apple的子类型;fruit的list将持有任何类型的fruit，虽然他包括apple，真正的问题在于我们是在谈论容器的类型而不是容器持有的类型，所以 fruit的list 在类型上不等价于Apple的list\r 但是，有时你想在两个类型之间建立某种类型的向上转型关系，这正是通配符所允许的\n//你可以将其读作 : 具有任何从 Fruit继承的类型的列表。但是,这实际上并不意味着这个List 将持有任何类型的 Fruit。\r//通配符引用的是明确的类型,因此它意味着 某种fruit引用没有指定的具体类型。因此这个被复制的List 必须持有诸如 Fruit 或 Apple 这样的某种执行类型,但是为了向上转型为 list ,这个类型是什么并没有人关心\rList\u0026lt;? extends Fruit1\u0026gt; fruit = new ArrayList\u0026lt;Apple\u0026gt;();\r// 你可能会认为,事情变得有点走极端了,因为现在你甚至不能向刚刚声明过将持有 Apple 对对象的List 中放置一个 Apple 对象了。是的,但是编译器并不知道这一点。 List\u0026lt;? extends Fruit\u0026gt; 可以合法地指向一个 List\u0026lt;Orange\u0026gt; 。一旦执行这种类型的向上转型,你就将丢失掉向其中传递任何对象的能力,甚至是传递Object 也不行。\rfruit.add(new Apple());// 错误\rfruit.add(new Fruit1());// 错误\rfruit.add(new Object());// 错误\rfruit.add(null);\r// 另一方面,如果你调用一个返回Fruit 的方法,则是安全的,因为你知道在这个List中的任何对象至少具有Fruit 类型,因此编译器将允许这么做。\rFruit1 fruit1 = fruit.get(0);\rApple apple = fruit.get(0);\r public class Holder\u0026lt;T\u0026gt; {\rprivate T t;\rpublic Holder(T t) {\rthis.t = t;\r}\rpublic Holder() {\r}\rpublic T getT() {\rreturn t;\r}\rpublic void setT(T t) {\rthis.t = t;\r}\r@Override\rpublic boolean equals(Object object) {\rreturn t.equals(object);\r}\rpublic static void main(String[] args) {\rHolder\u0026lt;Apple\u0026gt; holder = new Holder\u0026lt;\u0026gt;(new Apple());\rApple apple = holder.getT();\rholder.setT(apple);\r//Holder\u0026lt;Fruit\u0026gt; fruitHolder=holder; 无法向上转型\rHolder\u0026lt;? extends Fruit\u0026gt; fruit = holder;\rFruit fruit1 = fruit.getT();// 如果调用 getT() ,它只会返回一个 Fruit 这就是在给定任何扩展自Fruit的对象这一边界之后,它所能知道的一切了\r//返回的结果是 object\rapple= (Apple) fruit.getT();\rtry {\rOrange orange = (Orange) fruit.getT();\r}catch (Exception e){\rSystem.out.println(e);\r}\r//fruit.setT(new Apple()); setT() 方法不能工作于 Apple 或 Fruit ,因为 setT() 的参数也是 ? extends Fruit 这意味着它可以是任何事物,而编译器无法验证任何事物的类型安全性\r//fruit.setT(new Fruit());\rSystem.out.println(fruit.equals(apple));\r}\r}\r//运行结果为\rjava.lang.ClassCastException: generic.Apple cannot be cast to generic.Orange\rtrue\r 逆变，使用超类型通配符，可以声明通配符是由某个特定类的任何基类来界定的如\u0026lt;? super Fruit\u0026gt; 无界通配符，List 实际上表示持有任何 Object 类型的原生 List ，而 List\u0026lt;?\u0026gt; 表示 具有某种特定类型的非原生List，只是我们不知道那种类型是什么。    第16章 数组  Java 中数组是一种效率最高的存储和随机访问对象引用序列的方式 数组是第一级对象，无论使用哪种类型的数组，数组标识符其实只是一个引用，指向在堆中创建的一个真实对象，这个数组对象用以保存指向其他对象的引用，可以作为数组初始化语法的一部分隐式地创建此对象，或者用 new 表达式显示地创建。只读成员 length 是数组对象的一部分，事实上，这是唯一一个可以访问的字段或方法，表示此数组对象可以存储多少元素，“[]” 语法是访问对象唯一的方式。对象数组保存的是引用，基本类型数组直接保存基本类型的值。  第17章\t容器深入研究 第18章\tJava I/O系统 第19章\t枚举类型  编译器为你创建的 enum 类都继承自 Enum 类， 所以枚举类不能继承其他类，编译器会将枚举类标记为 final 类，所以无法继承自 enum。但枚举类仍然可以实现一个或多个接口。  第21章\t并发  线程可以驱动任务，因此你需要一种描述任务的方式，这可以由 Runnable 接口来提供，要想定义任务，只需实现 Runnable 接口并编写 run() 方法 将 Runnable 对象转变为工作任务的传统方式是把它提交给一个 Thread 构造器。  注解 注解的本质 注解的本质就是一个继承了 Annotation 接口的接口。\n「java.lang.annotation.Annotation」接口中有这么一句话，用来描述『注解』。\n The common interface extended by all annotation types\n所有的注解类型都继承自这个普通的接口（Annotation）\n @Override 的源码为：\n@Target(ElementType.METHOD)\r@Retention(RetentionPolicy.SOURCE)\rpublic @interface Override {\r}\r 本质上就相当于：\npublic interface Override extends Annotation{\r}\r 一个注解准确意义上来说，只不过是一种特殊的注释而已，如果没有解析它的代码，它可能连注释都不如。\n而解析一个类或者方法的注解往往有两种形式，一种是编译期直接的扫描，一种是运行期反射。反射的事情我们待会说，而编译器的扫描指的是编译器在对 java 代码编译字节码的过程中会检测到某个类或者方法被一些注解修饰，这时它就会对于这些注解进行某些处理。\n典型的就是注解 @Override，一旦编译器检测到某个方法被修饰了 @Override 注解，编译器就会检查当前方法的方法签名是否真正重写了父类的某个方法，也就是比较父类中是否具有一个同样的方法签名。\n这一种情况只适用于那些编译器已经熟知的注解类，比如 JDK 内置的几个注解，而你自定义的注解，编译器是不知道你这个注解的作用的，当然也不知道该如何处理，往往只是会根据该注解的作用范围来选择是否编译进字节码文件，仅此而已。\n元注解 『元注解』是用于修饰注解的注解，通常用在注解的定义上，JAVA 中有以下几个『元注解』：\n @Target：注解的作用目标，@Target 用于指明被修饰的注解最终可以作用的目标是谁，也就是指明，你的注解到底是用来修饰方法的？修饰类的？还是用来修饰字段属性的。具体取值为：  ElementType.TYPE：允许被修饰的注解作用在类、接口和枚举上 ElementType.FIELD：允许作用在属性字段上 ElementType.METHOD：允许作用在方法上 ElementType.PARAMETER：允许作用在方法参数上 ElementType.CONSTRUCTOR：允许作用在构造器上 ElementType.LOCAL_VARIABLE：允许作用在本地局部变量上 ElementType.ANNOTATION_TYPE：允许作用在注解上 ElementType.PACKAGE：允许作用在包上   @Retention：注解的生命周期。具体取值为：  RetentionPolicy.SOURCE：当前注解编译期可见，不会写入 class 文件 RetentionPolicy.CLASS：类加载阶段丢弃，会写入 class 文件 RetentionPolicy.RUNTIME：永久保存，可以反射获取   @Documented：注解是否应当被包含在 JavaDoc 文档中 @Inherited：是否允许子类继承该注解  注解工作流程 自定义注解：\n@Target(ElementType.TYPE)\r@Retention(RetentionPolicy.RUNTIME)\rpublic @interface Fruit {\rString genName() default \u0026quot;\u0026quot;;\rString genColor() default \u0026quot;\u0026quot;;\r}\r 作用于类：\n@Data\r@Fruit(genName = \u0026quot;富士康苹果\u0026quot;, genColor = \u0026quot;红色\u0026quot;)\rpublic class Apple {\rprivate String name;\rprivate String color;\r}\r 测试使用：\npublic class AnnotationTest {\rpublic static void main(String[] args) {\rApple apple = new Apple();\r// 通过动态代理生成实现注解的代理类, 将注解的值加入到map中\rFruit fruit = apple.getClass().getAnnotation(Fruit.class);\r// 实际上是根据方法名从map中将值取出\rSystem.out.println(fruit.genName());\rSystem.out.println(fruit.genColor());\r}\r}\r 对于一个类或者接口来说，Class 类中提供了以下一些方法用于反射注解。\n getAnnotation：返回指定的注解 isAnnotationPresent：判定当前元素是否被指定注解修饰 getAnnotations：返回所有的注解 getDeclaredAnnotation：返回本元素的指定注解 getDeclaredAnnotations：返回本元素的所有注解，不包含父类继承而来的   当使用反射获取到注解时即调用 getAnnotation() 方法后， JDK 通过动态代理机制生成一个实现我们注解（接口）的代理类。其构造方法执行的是，AnnotationInvocationHandler 这个类中  private static final long serialVersionUID = 6182022883658399397L;\rprivate final Class\u0026lt;? extends Annotation\u0026gt; type;\r// key为注解的方法名，value为属性的值。\rprivate final Map\u0026lt;String, Object\u0026gt; memberValues;\rprivate transient volatile Method[] memberMethods = null;\rAnnotationInvocationHandler(Class\u0026lt;? extends Annotation\u0026gt; var1, Map\u0026lt;String, Object\u0026gt; var2) {\rClass[] var3 = var1.getInterfaces();\rif (var1.isAnnotation() \u0026amp;\u0026amp; var3.length == 1 \u0026amp;\u0026amp; var3[0] == Annotation.class) {\rthis.type = var1;\rthis.memberValues = var2;\r} else {\rthrow new AnnotationFormatError(\u0026quot;Attempt to create proxy for a non-annotation type.\u0026quot;);\r}\r}\r 执行完构造方法后，memberValues map 中就有注解的所有方法名与值。\n 执行注解的任何方法都会调用 invoke 方法\npublic Object invoke(Object var1, Method var2, Object[] var3) {\rString var4 = var2.getName();\rClass[] var5 = var2.getParameterTypes();\rif (var4.equals(\u0026quot;equals\u0026quot;) \u0026amp;\u0026amp; var5.length == 1 \u0026amp;\u0026amp; var5[0] == Object.class) {\rreturn this.equalsImpl(var3[0]);\r} else if (var5.length != 0) {\rthrow new AssertionError(\u0026quot;Too many parameters for an annotation method\u0026quot;);\r} else {\rbyte var7 = -1;\rswitch(var4.hashCode()) {\rcase -1776922004:\rif (var4.equals(\u0026quot;toString\u0026quot;)) {\rvar7 = 0;\r}\rbreak;\rcase 147696667:\rif (var4.equals(\u0026quot;hashCode\u0026quot;)) {\rvar7 = 1;\r}\rbreak;\rcase 1444986633:\rif (var4.equals(\u0026quot;annotationType\u0026quot;)) {\rvar7 = 2;\r}\r}\rswitch(var7) {\rcase 0:\rreturn this.toStringImpl();\rcase 1:\rreturn this.hashCodeImpl();\rcase 2:\rreturn this.type;\rdefault:\r// 主要就是这个地方，根据方法名将值从map中取出来\rObject var6 = this.memberValues.get(var4);\rif (var6 == null) {\rthrow new IncompleteAnnotationException(this.type, var4);\r} else if (var6 instanceof ExceptionProxy) {\rthrow ((ExceptionProxy)var6).generateException();\r} else {\rif (var6.getClass().isArray() \u0026amp;\u0026amp; Array.getLength(var6) != 0) {\rvar6 = this.cloneArray(var6);\r}\rreturn var6;\r}\r}\r}\r}\r   ","id":10,"section":"posts","summary":"[TOC] Java 编程思想学习 第1章 对象导论 好像看不下去，。。。 第2章 一切都是对象 用引用操作对象：每种编程语言都有自己的操纵内存中元素的方式。java中操","tags":["Java基础"],"title":"Java 编程思想学习","uri":"https://wzgl998877.github.io/2022/01/java%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/","year":"2022"},{"content":"LeetCode 链表 链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针(Pointer)。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而顺序表相应的时间复杂度分别是O(logn)和O(1)。\n使用链表结构可以克服数组链表需要预先知道数据大小的缺点，链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理。但是链表失去了数组随机读取的优点，同时链表由于增加了结点的指针域，空间开销比较大。\n在计算机科学中，链表作为一种基础的数据结构可以用来生成其它类型的数据结构。链表通常由一连串节点组成，每个节点包含任意的实例数据（data fields）和一或两个用来指向上一个/或下一个节点的位置的链接（\u0026ldquo;links\u0026rdquo;）。链表最明显的好处就是，常规数组排列关联项目的方式可能不同于这些数据项目在记忆体或磁盘上顺序，数据的访问往往要在不同的排列顺序中转换。而链表是一种自我指示数据类型，因为它包含指向另一个相同类型的数据的指针（链接）。链表允许插入和移除表上任意位置上的节点，但是不允许随机存取。链表有很多种不同的类型：单向链表，双向链表以及循环链表\n单向链表 链表中最简单的一种是单向链表，它包含两个域，一个信息域和一个指针域。这个链接指向列表中的下一个节点，而最后一个节点则指向一个空值。\n 一个单向链表包含两个值: 当前节点的值和一个指向下一个节点的链接\n一个单向链表的节点被分成两个部分。第一个部分保存或者显示关于节点的信息，第二个部分存储下一个节点的地址。单向链表只可向一个方向遍历。\n链表最基本的结构是在每个节点保存数据和到下一个节点的地址，在最后一个节点保存一个特殊的结束标记，另外在一个固定的位置保存指向第一个节点的指针，有的时候也会同时储存指向最后一个节点的指针。一般查找一个节点的时候需要从第一个节点开始每次访问下一个节点，一直访问到需要的位置。但是也可以提前把一个节点的位置另外保存起来，然后直接访问。当然如果只是访问数据就没必要了，不如在链表上储存指向实际数据的指针。这样一般是为了访问链表中的下一个或者前一个（需要储存反向的指针，见下面的双向链表）节点。\n相对于下面的双向链表，这种普通的，每个节点只有一个指针的链表也叫单向链表，或者单链表，通常用在每次都只会按顺序遍历这个链表的时候（例如图的邻接表，通常都是按固定顺序访问的）。\n链表也有很多种不同的变化：\n双向链表 一种更复杂的链表是“双向链表”或“双面链表”。每个节点有两个连接：一个指向前一个节点，（当此“连接”为第一个“连接”时，指向空值或者空列表）；而另一个指向下一个节点，（当此“连接”为最后一个“连接”时，指向空值或者空列表）\n 一个双向链表有三个整数值: 数值, 向后的节点链接, 向前的节点链接\n在一些低级语言中, XOR-linking 提供一种在双向链表中通过用一个词来表示两个链接（前后），我们通常不提倡这种做法。\n双向链表也叫双链表。双向链表中不仅有指向后一个节点的指针，还有指向前一个节点的指针。这样可以从任何一个节点访问前一个节点，当然也可以访问后一个节点，以至整个链表。一般是在需要大批量的另外储存数据在链表中的位置的时候用。双向链表也可以配合下面的其他链表的扩展使用。\n由于另外储存了指向链表内容的指针，并且可能会修改相邻的节点，有的时候第一个节点可能会被删除或者在之前添加一个新的节点。这时候就要修改指向首个节点的指针。有一种方便的可以消除这种特殊情况的方法是在最后一个节点之后、第一个节点之前储存一个永远不会被删除或者移动的虚拟节点，形成一个下面说的循环链表。这个虚拟节点之后的节点就是真正的第一个节点。这种情况通常可以用这个虚拟节点直接表示这个链表，对于把链表单独的存在数组里的情况，也可以直接用这个数组表示链表并用第0个或者第-1个（如果编译器支持）节点固定的表示这个虚拟节点。\n循环链表 在一个 循环链表中, 首节点和末节点被连接在一起。这种方式在单向和双向链表中皆可实现。要转换一个循环链表，你开始于任意一个节点然后沿着列表的任一方向直到返回开始的节点。再来看另一种方法，循环链表可以被视为“无头无尾”。这种列表很利于节约数据存储缓存， 假定你在一个列表中有一个对象并且希望所有其他对象迭代在一个非特殊的排列下。\n指向整个列表的指针可以被称作访问指针。\n 用单向链表构建的循环链表\n循环链表中第一个节点之前就是最后一个节点，反之亦然。循环链表的无边界使得在这样的链表上设计算法会比普通链表更加容易。对于新加入的节点应该是在第一个节点之前还是最后一个节点之后可以根据实际要求灵活处理，区别不大(详见下面实例代码)。当然，如果只会在最后插入数据（或者只会在之前），处理也是很容易的。\n另外有一种模拟的循环链表，就是在访问到最后一个节点之后的时候，手工的跳转到第一个节点。访问到第一个节点之前的时候也一样。这样也可以实现循环链表的功能，在直接用循环链表比较麻烦或者可能会出现问题的时候可以用。\n动态规划 动态规划问题的一般形式就是求最值。动态规划其实是运筹学的一种最优化方法，只不过在计算机问题上应用比较多，比如说让你求最长递增子序列呀，最小编辑距离呀等等。\n既然是要求最值，核心问题是什么呢？求解动态规划的核心问题是穷举。因为要求最值，肯定要把所有可行的答案穷举出来，然后在其中找最值呗。\n首先，动态规划的穷举有点特别，因为这类问题存在「重叠子问题」，如果暴力穷举的话效率会极其低下，所以需要「备忘录」或者「DP table」来优化穷举过程，避免不必要的计算。\n而且，动态规划问题一定会具备「最优子结构」，才能通过子问题的最值得到原问题的最值。\n另外，虽然动态规划的核心思想就是穷举求最值，但是问题可以千变万化，穷举所有可行解其实并不是一件容易的事，只有列出 正确的「状态转移方程」实际上就是描述问题结构的数学形式，才能正确地穷举。\n以上提到的 重叠子问题、最优子结构、状态转移方程 就是动态规划三要素。\n如何列出正确的状态转移方程？\n  先确定「状态」，也就是原问题和子问题中变化的变量。\n  确定dp函数的定义\n  确定「选择」并择优，也就是对于每个状态，可以做出什么选择改变当前状态。\n  最后明确 base case\n  回溯算法 回溯法（back tracking）（探索与回溯法）是一种选优搜索法，又称为试探法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。\n\u0026gt; 加空格可以达到下面的效果\r  白话：回溯法可以理解为通过选择不同的岔路口寻找目的地，一个岔路口一个岔路口的去尝试找到目的地。如果走错了路，继续返回来找到岔路口的另一条路，直到找到目的地。\n 解决一个回溯问题，实际上就是一个决策树的遍历过程。你只需要思考 3 个问题：\n 路径：也就是已经做出的选择。 选择列表：也就是你当前可以做的选择。 结束条件：也就是到达决策树底层，无法再做选择的条件。  代码框架:\nresult = []\rdef backtrack(路径, 选择列表):\rif 满足结束条件:\rresult.add(路径)\rreturn\rfor 选择 in 选择列表:\r做选择\rbacktrack(路径, 选择列表)\r撤销选择\r 我们在高中的时候就做过排列组合的数学题，我们也知道n个不重复的数，全排列共有 n! 个。\nPS：为了简单清晰起见，我们这次讨论的全排列问题不包含重复的数字。\n那么我们当时是怎么穷举全排列的呢？比方说给三个数[1,2,3]，你肯定不会无规律地乱穷举，一般是这样：\n先固定第一位为 1，然后第二位可以是 2，那么第三位只能是 3；然后可以把第二位变成 3，第三位就只能是 2 了；然后就只能变化第一位，变成 2，然后再穷举后两位……\n其实这就是回溯算法，我们高中无师自通就会用，或者有的同学直接画出如下这棵回溯树：\n只要从根遍历这棵树，记录路径上的数字，其实就是所有的全排列。我们不妨把这棵树称为回溯算法的「决策树」。\n为啥说这是决策树呢，因为你在每个节点上其实都在做决策。比如说你站在下图的红色节点上：\n你现在就在做决策，可以选择 1 那条树枝，也可以选择 3 那条树枝。为啥只能在 1 和 3 之中选择呢？因为 2 这个树枝在你身后，这个选择你之前做过了，而全排列是不允许重复使用数字的。\n现在可以解答开头的几个名词：[2]就是「路径」，记录你已经做过的选择；[1,3]就是「选择列表」，表示你当前可以做出的选择；「结束条件」就是遍历到树的底层，在这里就是选择列表为空的时候。\n如果明白了这几个名词，可以把「路径」和「选择列表」作为决策树上每个节点的属性，比如下图列出了几个节点的属性：\n我们定义的backtrack函数其实就像一个指针，在这棵树上游走，同时要正确维护每个节点的属性，每当走到树的底层，其「路径」就是一个全排列。\n","id":11,"section":"posts","summary":"LeetCode 链表 链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是在每一个节点里存到下一个节点的","tags":["LeetCode"],"title":"LeetCode","uri":"https://wzgl998877.github.io/2022/01/leetcode/","year":"2022"},{"content":"mybatis 技术内幕学习 Jdbc 复习 通过JDBC查询数据库数据，一般需要以下七个步骤：\n  加载JDBC驱动； 建立并获取数据库连接； 创建 JDBC Statements 对象； 设置SQL语句的传入参数； 执行SQL语句并获得查询结果； 对查询结果进行转换处理并将处理结果返回； 释放相关资源（关闭Connection，关闭Statement，关闭ResultSet）；   代码如下：\npackage com.zt.javastudy.mybatis;\rimport com.alibaba.fastjson.JSONObject;\rimport lombok.extern.slf4j.Slf4j;\rimport java.sql.*;\rimport java.util.ArrayList;\rimport java.util.HashMap;\rimport java.util.List;\rimport java.util.Map;\r/**\r* jdbc连接数据库\r*\r* @author zhengtao on 2021/9/16\r*/\r@Slf4j\rpublic class JdbcTest {\rpublic static List\u0026lt;Map\u0026lt;String,Object\u0026gt;\u0026gt; queryForList(){\rConnection connection = null;\rResultSet rs = null;\rPreparedStatement stmt = null;\rList\u0026lt;Map\u0026lt;String,Object\u0026gt;\u0026gt; resultList = new ArrayList\u0026lt;\u0026gt;();\rtry {\r// 加载JDBC驱动\rClass.forName(\u0026quot;com.mysql.jdbc.Driver\u0026quot;).newInstance();\rString url = \u0026quot;jdbc:mysql://172.20.2.131:3506/journey?allowMultiQueries=true\u0026amp;Unicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;useSSL=false\u0026quot;;\rString user = \u0026quot;loan\u0026quot;;\rString password = \u0026quot;JLpaymysql8.0!\u0026quot;;\r// 获取数据库连接\rconnection = DriverManager.getConnection(url,user,password);\rString sql = \u0026quot;select * from t_user where user_id = ? \u0026quot;;\r// 创建Statement对象（每一个Statement为一次数据库执行请求）\rstmt = connection.prepareStatement(sql);\r// 设置传入参数\rstmt.setString(1, \u0026quot;1027604989\u0026quot;);\r// 执行SQL语句\rrs = stmt.executeQuery();\r// 处理查询结果（将查询结果转换成List\u0026lt;Map\u0026gt;格式）\rResultSetMetaData rsmd = rs.getMetaData();\rint num = rsmd.getColumnCount();\rwhile(rs.next()){\rMap map = new HashMap();\rfor(int i = 0;i \u0026lt; num;i++){\rString columnName = rsmd.getColumnName(i+1);\rmap.put(columnName,rs.getString(columnName));\r}\rresultList.add(map);\r}\r} catch (Exception e) {\re.printStackTrace();\r} finally {\rtry {\r// 关闭结果集\rif (rs != null) {\rrs.close();\r}\r// 关闭执行\rif (stmt != null) {\rstmt.close();\r}\rif (connection != null) {\rconnection.close();\r}\r} catch (SQLException e) {\re.printStackTrace();\r}\r}\rreturn resultList;\r}\rpublic static void main(String[] args) {\rList\u0026lt;Map\u0026lt;String,Object\u0026gt;\u0026gt; list = queryForList();\rlog.info(\u0026quot;jdbc查询结果为:{}\u0026quot;, JSONObject.toJSONString(list));\r}\r}\r Mybatis MyBatis 是一款优秀的持久层框架，它支持自定义 SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。MyBatis 可以通过简单的 XML 或注解来配置和映射原始类型、接口和 Java POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。\n使用 mybatis 一般有以下几步：\n  ","id":12,"section":"posts","summary":"mybatis 技术内幕学习 Jdbc 复习 通过JDBC查询数据库数据，一般需要以下七个步骤： 加载JDBC驱动； 建立并获取数据库连接； 创建 JDBC Statements 对象； 设置SQL语句的","tags":["mybatis"],"title":"mybatis 技术内幕学习","uri":"https://wzgl998877.github.io/2022/01/mybatis-%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E5%AD%A6%E4%B9%A0/","year":"2022"},{"content":"[toc]\nJdbc 复习 通过JDBC查询数据库数据，一般需要以下七个步骤：\n  加载JDBC驱动； 建立并获取数据库连接； 创建 JDBC Statements 对象； 设置SQL语句的传入参数； 执行SQL语句并获得查询结果； 对查询结果进行转换处理并将处理结果返回； 释放相关资源（关闭Connection，关闭Statement，关闭ResultSet）；   代码如下：\npackage com.zt.javastudy.mybatis;\rimport com.alibaba.fastjson.JSONObject;\rimport lombok.extern.slf4j.Slf4j;\rimport java.sql.*;\rimport java.util.ArrayList;\rimport java.util.HashMap;\rimport java.util.List;\rimport java.util.Map;\r/**\r* jdbc连接数据库\r*\r* @author zhengtao on 2021/9/16\r*/\r@Slf4j\rpublic class JdbcTest {\rpublic static List\u0026lt;Map\u0026lt;String,Object\u0026gt;\u0026gt; queryForList(){\rConnection connection = null;\rResultSet rs = null;\rPreparedStatement stmt = null;\rList\u0026lt;Map\u0026lt;String,Object\u0026gt;\u0026gt; resultList = new ArrayList\u0026lt;\u0026gt;();\rtry {\r// 加载JDBC驱动\rClass.forName(\u0026quot;com.mysql.jdbc.Driver\u0026quot;).newInstance();\rString url = \u0026quot;jdbc:mysql://172.20.2.131:3506/journey?allowMultiQueries=true\u0026amp;Unicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;useSSL=false\u0026quot;;\rString user = \u0026quot;loan\u0026quot;;\rString password = \u0026quot;JLpaymysql8.0!\u0026quot;;\r// 获取数据库连接\rconnection = DriverManager.getConnection(url,user,password);\rString sql = \u0026quot;select * from t_user where user_id = ? \u0026quot;;\r// 创建Statement对象（每一个Statement为一次数据库执行请求）\rstmt = connection.prepareStatement(sql);\r// 设置传入参数\rstmt.setString(1, \u0026quot;1027604989\u0026quot;);\r// 执行SQL语句\rrs = stmt.executeQuery();\r// 处理查询结果（将查询结果转换成List\u0026lt;Map\u0026gt;格式）\rResultSetMetaData rsmd = rs.getMetaData();\rint num = rsmd.getColumnCount();\rwhile(rs.next()){\rMap map = new HashMap();\rfor(int i = 0;i \u0026lt; num;i++){\rString columnName = rsmd.getColumnName(i+1);\rmap.put(columnName,rs.getString(columnName));\r}\rresultList.add(map);\r}\r} catch (Exception e) {\re.printStackTrace();\r} finally {\rtry {\r// 关闭结果集\rif (rs != null) {\rrs.close();\r}\r// 关闭执行\rif (stmt != null) {\rstmt.close();\r}\rif (connection != null) {\rconnection.close();\r}\r} catch (SQLException e) {\re.printStackTrace();\r}\r}\rreturn resultList;\r}\rpublic static void main(String[] args) {\rList\u0026lt;Map\u0026lt;String,Object\u0026gt;\u0026gt; list = queryForList();\rlog.info(\u0026quot;jdbc查询结果为:{}\u0026quot;, JSONObject.toJSONString(list));\r}\r}\r Mybatis ORM 对象关系映射 ,用于实现面向对象编程语言里不同类型系统的数据之间的转换，可以狭义的理解为，将对象与表对应起来，开发者只使用面向对象编程，与数据对象直接交互，crud就直接变成了面向对象编程。\n核心功能  将包含 if 等标签的复杂数据库操作语句解析为纯粹的 SQL 语句 将数据库操作节点和映射接口中的抽象方法进行绑定，在抽象方法被调用时执行数据库连接操作。 将输入参数对象转换为数据库操作语句中的参数 将数据库操作语句中的返回结果转化为对象  MyBatis 是一款优秀的持久层框架，它支持自定义 SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。MyBatis 可以通过简单的 XML 或注解来配置和映射原始类型、接口和 Java POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。\n使用 mybatis 一般有以下几步：\n  接口映射到数据库sql的实现-MapperMethod\n","id":13,"section":"posts","summary":"[toc] Jdbc 复习 通过JDBC查询数据库数据，一般需要以下七个步骤： 加载JDBC驱动； 建立并获取数据库连接； 创建 JDBC Statements 对象； 设置SQL语句的传入参数； 执","tags":["mybatis"],"title":"mybatis 技术内幕学习","uri":"https://wzgl998877.github.io/2022/01/mybatis/","year":"2022"},{"content":"[TOC]\nNetty 学习 虽然说netty，用的少但是感觉比较流行，值得研究！！！\nNetty 是一个 NIO 客户端-服务器框架，可以快速轻松地开发网络应用程序，它极大地简化了网络编程，比如TCP和UDP 套接字服务器。\n那么什么是NIO？或者说什么是IO？\nIO I/O（Input/Output），即输入／输出，通常指数据在存储器（内部和外部）或其他周边设备之间的输入和输出。\n那么什么是存储器或者说是内存呢？\n复习一下计算机组成原理\n冯诺依曼结构 执行过程：\n  通过输入设备到存储器\n  控制器读取程序计数器获得指令内存地址，通过操作地址总线从内存地址拿到数据。\n  控制器分析指令寄存器中的指令，分析完后送到存储器\n  存储器将数据送到运算器，运算器完成计算\n  自增后开始顺序执行下一条指令，不断循环执行直到程序结束。\n  输出到输出设备\n  现代计算机机构 I/O 其实就是 input 和 output 的缩写，即输入/输出。比如我们用键盘来敲代码其实就是输入，那显示器显示图案就是输出，这其实就是 I/O。\n  磁盘 I/O 指的是硬盘和内存之间的输入输出。读取本地文件的时候，要将磁盘的数据拷贝到内存中，修改本地文件的时候，需要把修改后的数据拷贝到磁盘中。\n  网络 I/O 指的是网卡与内存之间的输入输出。当网络上的数据到来时，网卡需要将数据拷贝到内存中。当要发送数据给网络上的其他人时，需要将数据从内存拷贝到网卡里。\n  那为什么都要跟内存交互呢?\n我们的指令最终是由 CPU 执行的，究其原因是 CPU 与内存交互的速度远高于 CPU 和这些外部设备直接交互的速度。\n因此都是和内存交互，当然假设没有内存，让 CPU 直接和外部设备交互，那也算 I/O。\n总结下：I/O 就是指内存与外部设备之间的交互（数据拷贝）。\n用户态和内核态 对 32 位操作系统而言，它的寻址空间（虚拟地址空间，或叫线性地址空间）为 4G（2的32次方）。也就是说一个进程的最大地址空间为 4G。操作系统的核心是内核(kernel)，它独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证内核的安全，现在的操作系统一般都强制用户进程不能直接操作内核。具体的实现方式基本都是由操作系统将虚拟地址空间划分为两部分，一部分为内核空间，另一部分为用户空间。\n 寻址空间一般指的是CPU对于内存寻址的能力。通俗地说，就是能最多用到多少内存的一个问题。数据在存储器(RAM)中存放是有规律的 ，CPU在运算的时候需要把数据提取出来就需要知道数据存放在哪里 ，这时候就需要挨家挨户的找，这就叫做寻址，但如果地址太多超出了CPU的能力范围，CPU就无法找到数据了。 CPU最大能查找多大范围的地址叫做寻址能力 ，CPU的寻址能力以字节为单位 ，如32位寻址的CPU可以寻址2的32次方大小的地址也就是4G，这也是为什么32位的CPU最大能搭配4G内存的原因 ，再多的话CPU就找不到了。\n 在说用户态与内核态之前，有必要说一下 C P U 指令集，指令集是 C P U 实现软件指挥硬件执行的媒介，具体来说每一条汇编语句都对应了一条 C P U 指令，同时 C P U 指令集 有权限分级，大家试想，C P U 指令集 可以直接操作硬件的，要是因为指令操作的不规范`，造成的错误会影响整个计算机系统的。好比你写程序，因为对硬件操作不熟悉，导致操作系统内核、及其他所有正在运行的程序，都可能会因为操作失误而受到不可挽回的错误，最后只能重启计算机才行。Intel的X86架构的CPU提供了0到3四个特权级，其中 ring 0 权限最高，可以使用所有 C P U 指令集，ring 3 权限最低，仅能使用常规 C P U 指令集，不能使用操作硬件资源的 C P U 指令集，比如 I O 读写、网卡访问、申请内存都不行，Linux系统仅采用ring 0 和 ring 3 这2个权限。\n分别对应的就是内核态(Kernel Mode)与用户态(User Mode)。\n 内核态：进程运行在内核空间中，此时 CPU 可以执行任何指令。运行的代码也不受任何的限制，可以自由地访问任何有效地址，也可以直接进行端口的访问 用户态：进程运行在用户空间中，被执行的代码要受到 CPU 的诸多检查，它们只能访问映射其地址空间的页表项中规定的在用户态下可访问页面的虚拟地址，且只能对任务状态段(TSS)中 I/O 许可位图(I/O Permission Bitmap)中规定的可访问端口进行直接访问。  Kernel 运行在超级权限模式（Supervisor Mode）下，所以拥有很高的权限。按照权限管理的原则，多数应用程序应该运行在最小权限下。因此，很多操作系统，将内存分成了两个区域：\n  内核空间（Kernal Space），这个空间只有内核程序可以访问；\n  用户空间（User Space），这部分内存专门给应用程序使用。\n  内核态和用户态的切换 从用户态到内核态切换可以通过三种方式：\n 系统调用，用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作 异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：缺页异常。 外设中断：当外设完成用户的请求时，会向CPU发送中断信号。  如上图所示：内核程序执行在内核态（Kernal Mode），用户程序执行在用户态（User Mode）。当发生系统调用时，用户态的程序发起系统调用。因为系统调用中牵扯特权指令，用户态程序权限不足，因此会中断执行，也就是 Trap（Trap 是一种中断）。\n发生中断后，当前 CPU 执行的程序会中断，跳转到中断处理程序。内核程序开始执行，也就是开始处理系统调用。内核处理完成后，主动触发 Trap，这样会再次发生中断，切换回用户态工作。\n根据UNIX网络编程对I/O模型的分类，UNIX提供了5种I/O模型，阻塞I/O模型、非阻塞I/O模型、I/O复用模型、信号驱动I/O模型、异步I/O\n要深入的理解各种IO模型，那么必须先了解下产生各种IO的原因是什么，要知道这其中的本质问题那么我们就必须要知一条消息是如何从过一个人发送到另外一个人的；\n以两个应用程序通讯为例，我们来了解一下当“A”向\u0026quot;B\u0026quot; 发送一条消息，简单来说会经过如下流程：\n第一步：应用A把消息发送到 TCP发送缓冲区。\n第二步： TCP发送缓冲区再把消息发送出去，经过网络传递后，消息会发送到B服务器的TCP接收缓冲区。\n第三步：B再从TCP接收缓冲区去读取属于自己的数据。\n阻塞I/O模型 因为应用之间发送消息是间断性的，也就是说在上图中TCP缓冲区还没有接收到属于应用B该读取的消息时，那么此时应用B向TCP缓冲区发起读取申请，TCP接收缓冲区是应该马上告诉应用B 现在没有你的数据，还是说让应用B在这里等着，直到有数据再把数据交给应用B。\n把这个问题应用到第一个步骤也是一样，应用A在向TCP发送缓冲区发送数据时，如果TCP发送缓冲区已经满了，那么是告诉应用A现在没空间了，还是让应用A等待着，等TCP发送缓冲区有空间了再把应用A的数据访拷贝到发送缓冲区。\n所谓阻塞IO就是当应用B发起读取数据申请时，在内核数据没有准备好之前，应用B会一直处于等待数据状态，直到内核把数据准备好了交给应用B才结束。\n术语描述：在应用调用recvfrom读取数据时，其系统调用直到数据包到达且被复制到应用缓冲区中或者发送错误时才返回，在此期间一直会等待，进程从调用到返回这段时间内都是被阻塞的称为阻塞IO；\n流程：\n1、应用进程向内核发起recfrom读取数据。\n2、准备数据包（应用进程阻塞）。\n3、将数据从内核空间复制到应用空间。\n4、复制完成后，返回成功。\n非阻塞IO 非阻塞IO就是当应用B发起读取数据申请时，如果内核数据没有准备好会即刻告诉应用B，不会让B在这里等待。\n术语：非阻塞IO是在应用调用recvfrom读取数据时，如果该缓冲区没有数据的话，就会直接返回一个EWOULDBLOCK错误，不会让应用一直等待中。在没有数据的时候会即刻返回错误标识，那也意味着如果应用要读取数据就需要不断的调用recvfrom请求，直到读取到它数据要的数据为止。\n流程：\n1、应用进程向内核发起recvfrom读取数据。\n2、没有数据包准备好，即刻返回EWOULDBLOCK错误码。\n3、应用进程向内核发起recvfrom读取数据。\n4、已有数据包准备好就进行一下步骤，否则还是返回错误码。\n5、将数据从内核空间拷贝到用户空间。\n6、完成后，返回成功提示。\nIO复用模型 我们还是把视角放到应用B从TCP缓冲区中读取数据这个环节来。如果在并发的环境下，可能会N个人向应用B发送消息，这种情况下我们的应用就必须创建多个线程去读取数据，每个线程都会自己调用recvfrom 去读取数据。那么此时情况可能如下图：\n如上图一样，并发情况下服务器很可能一瞬间会收到几十上百万的请求，这种情况下应用B就需要创建几十上百万的线程去读取数据，同时又因为应用线程是不知道什么时候会有数据读取，为了保证消息能及时读取到，那么这些线程自己必须不断的向内核发送recvfrom 请求来读取数据；\n那么问题来了，这么多的线程不断调用recvfrom 请求数据，先不说服务器能不能扛得住这么多线程，就算扛得住那么很明显这种方式是不是太浪费资源了，线程是我们操作系统的宝贵资源，大量的线程用来去读取数据了，那么就意味着能做其它事情的线程就会少。\n所以，有人就提出了一个思路，能不能提供一种方式，可以由一个线程监控多个网络请求（我们后面将称为fd文件描述符，linux系统把所有网络请求以一个fd来标识），这样就可以只需要一个或几个线程就可以完成数据状态询问的操作，当有数据准备就绪之后再分配对应的线程去读取数据，这么做就可以节省出大量的线程资源出来，这个就是IO复用模型的思路。\n正如上图，IO复用模型的思路就是系统提供了一种函数可以同时监控多个fd的操作，这个函数就是我们常说到的select、poll、epoll函数，有了这个函数后，应用线程通过调用select函数就可以同时监控多个fd，select函数监控的fd中只要有任何一个数据状态准备就绪了，select函数就会返回可读状态，这时询问线程再去通知处理数据的线程，对应线程此时再发起recvfrom请求去读取数据。\n 文件描述符fd Linux的内核将所有外部设备都可以看做一个文件来操作。那么我们对与外部设备的操作都可以看做对文件进行操作。我们对一个文件的读写，都通过调用内核提供的系统调用；内核给我们返回一个filede scriptor（fd,文件描述符）。而对一个socket的读写也会有相应的描述符，称为socketfd(socket描述符）。描述符就是一个数字，指向内核中一个结构体（文件路径，数据区，等一些属性）。那么我们的应用程序对文件的读写就通过对描述符的读写完成。\n 术语描述：进程通过将一个或多个fd传递给select，阻塞在select操作上，select帮我们侦测多个fd是否准备就绪，当有fd准备就绪时，select返回数据可读状态，应用程序再调用recvfrom读取数据。\n总结：复用IO的基本思路就是通过slect或poll、epoll 来监控多fd ，来达到不必为每个fd创建一个对应的监控线程，从而减少线程资源创建的目的。\nselect select 的设计思路是唤醒模式，「通过一个 socket 列表(fd_set)维护所有的 socket」，socket 对应文件列表中的 fd，select 会默认限制最大文件句柄数为 1024，间接控制 fd[] 最大为 1024。\n 需要将fd_set从用户空间拷贝到内核空间 然后内核用poll机制（此poll机制非IO多路复用的那个poll方法，可参加附录）直到有一个fd活跃，或者超时了，方法返回。 select方法返回后，需要轮询fd_set，以检查出发生IO事件的fd  poll poll 其实内部实现基本跟 select 一样，区别在于它们底层组织 fd[] 的数据结构不太一样，从而实现了 poll 的最大文件句柄数量限制去除了\nepoll epoll就是对select和poll的改进了。它的核心思想是基于事件驱动来实现的，实现起来也并不难，就是给每个fd注册一个回调函数，当fd对应的设备发生IO事件时，就会调用这个回调函数，将该fd放到一个链表中，然后由客户端从该链表中取出一个个fd，以此达到O（1）的时间复杂度\nepoll操作实际上对应着有三个函数：epoll_create，epoll_ctr，epoll_wait\nepoll_create epoll_create相当于在内核中创建一个存放fd的数据结构。在select和poll方法中，内核都没有为fd准备存放其的数据结构，只是简单粗暴地把数组或者链表复制进来；而epoll则不一样，epoll_create会在内核建立一颗专门用来存放fd结点的红黑树，后续如果有新增的fd结点，都会注册到这个epoll红黑树上。\nepoll_ctr 另一点不一样的是，select和poll会一次性将监听的所有fd都复制到内核中，而epoll不一样，当需要添加一个新的fd时，会调用epoll_ctr，给这个fd注册一个回调函数，然后将该fd结点注册到内核中的红黑树中。当该fd对应的设备活跃时，会调用该fd上的回调函数，将该结点存放在一个就绪链表中。这也解决了在内核空间和用户空间之间进行来回复制的问题。\nepoll_wait epoll_wait的做法也很简单，其实直接就是从就绪链表中取结点，这也解决了轮询的问题，时间复杂度变成O(1)\n所以综合来说，epoll的优点有：\n  没有最大并发连接的限制，远远比1024或者2048要大。\n  效率变高。epoll是基于事件驱动实现的，不会随着fd数量上升而效率下降\n  减少内存拷贝的次数\n  水平触发和边缘触发\n简单理解下\n水平触发的意思就是说，只要条件满足，对应的事件就会一直被触发。所以如果条件满足了但未进行处理，那么就会一直被通知\n边缘触发的意思就是说，条件满足后，对应的事件只会被触发一次，无论是否被处理，都只会触发一次。\n而对于select和poll来说，其触发都是水平触发。而epoll则有两种模式：·EPOLLLT和EPOLLET\n EPOLLLT（默认状态）：也就是水平触发。在该模式下，只要这个fd还有数据可读，那么epoll_wait函数就会返回该fd EPOLLET（高速模式）：也就是边缘触发。在该模式下，当被监控的fd上有可读写事件发生时，epoll_wait会通知程序去读写，若本次读写没有读完所有数据，或者甚至没有进行处理，那么下一次调用epoll_wait时，也不会获取到该fd。这种效率比水平触发的要高，系统中不会充斥着大量程序不感兴趣的fd，不感兴趣直接忽视就行，下次不会再触发  epoll就是对select和poll的改进了。它的核心思想是基于事件驱动来实现的\n  1.拆分:epoll 将添加等待队列和阻塞进程拆分成两个独立的操作，不用每次都去重新维护等待队列\n   先用 epoll_ctl 维护等待队列 eventpoll，它通过红黑树存储 socket 对象，实现高效的查找，删除和添加。 再调用 epoll_wait 阻塞进程，底层是一个双向链表。显而易见地，效率就能得到提升。    select 的添加等待队列和阻塞进程是合并在一起的，每次调用select()操作时都得执行一遍这两个操作，从而导致每次都要将fd[]传递到内核空间，并且遍历fd[]的每个fd的等待队列，将进程放入各个fd的等待队列中。\n 2.直接返回有数据的 fd[]:select 进程被唤醒后，是需要遍历一遍 socket 列表，手动获取有数据的 socket，而 epoll 是在唤醒时直接把有数据的 socket 返回给进程，不需要自己去进行遍历查询。   「直接返回有数据的 socket 是怎么实现的？」\n其实就是 epoll 会先注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知。\nepoll对文件描述符的操作有两种模式：「LT」（level trigger）和 「ET」（edge trigger）默认为 LT :\n  LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。\n  ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。\n  信号驱动IO模型 复用IO模型解决了一个线程可以监控多个fd的问题，但是select是采用轮询的方式来监控多个fd的，通过不断的轮询fd的可读状态来知道是否就可读的数据，而无脑的轮询就显得有点暴力，因为大部分情况下的轮询都是无效的，所以有人就想，能不能不要我总是去问你是否数据准备就绪，能不能我发出请求后等你数据准备好了就通知我，所以就衍生了信号驱动IO模型。\n于是信号驱动IO不是用循环请求询问的方式去监控数据就绪状态，而是在调用sigaction时候建立一个SIGIO的信号联系，当内核数据准备好之后再通过SIGIO信号通知线程数据准备好后的可读状态，当线程收到可读状态的信号后，此时再向内核发起recvfrom读取数据的请求，因为信号驱动IO的模型下应用线程在发出信号监控后即可返回，不会阻塞，所以这样的方式下，一个应用线程也可以同时监控多个fd。\n类似于下图描述：\n术语描述：首先开启套接口信号驱动IO功能，并通过系统调用sigaction执行一个信号处理函数，此时请求即刻返回，当数据准备就绪时，就生成对应进程的SIGIO信号，通过信号回调通知应用线程调用recvfrom来读取数据。\n总结： IO复用模型里面的select虽然可以监控多个fd了，但select其实现的本质上还是通过不断的轮询fd来监控数据状态， 因为大部分轮询请求其实都是无效的，所以信号驱动IO意在通过这种建立信号关联的方式，实现了发出请求后只需要等待数据就绪的通知即可，这样就可以避免大量无效的数据状态轮询操作。\n异步IO 其实经过了上面两个模型的优化，我们的效率有了很大的提升，但是我们当然不会就这样满足了，有没有更好的办法，通过观察我们发现，不管是IO复用还是信号驱动，我们要读取一个数据总是要发起两阶段的请求，第一次发送select请求，询问数据状态是否准备好，第二次发送recevform请求读取数据。\n思考一个问题：\n也许你一开始就有一个疑问，为什么我们明明是想读取数据，什么非得要先发起一个select询问数据状态的请求，然后再发起真正的读取数据请求,能不能有一种一劳永逸的方式，我只要发送一个请求我告诉内核我要读取数据，然后我就什么都不管了，然后内核去帮我去完成剩下的所有事情？\n当然既然你想得出来，那么就会有人做得到，有人设计了一种方案，应用只需要向内核发送一个read 请求,告诉内核它要读取数据后即刻返回；内核收到请求后会建立一个信号联系，当数据准备就绪，内核会主动把数据从内核复制到用户空间，等所有操作都完成之后，内核会发起一个通知告诉应用，我们称这种一劳永逸的模式为异步IO模型。\n术语描述： 应用告知内核启动某个操作，并让内核在整个操作完成之后，通知应用，这种模型与信号驱动模型的主要区别在于，信号驱动IO只是由内核通知我们合适可以开始下一个IO操作，而异步IO模型是由内核通知我们操作什么时候完成。\n总结：异步IO的优化思路是解决了应用程序需要先后发送询问请求、发送接收数据请求两个阶段的模式，在异步IO的模式下，只需要向内核发送一次请求就可以完成状态询问和数拷贝的所有操作。\n对比 阻塞IO模型、非阻塞IO模型、IO复用模型和信号驱动IO模型都是同步的IO模型。原因是因为，无论以上那种模型，真正的数据拷贝过程，都是同步进行的。还可以细分为，阻塞IO为同步阻塞IO，其他为同步非阻塞IO\nJava中的IO模型 在Java中，提供了一些关于使用IO的API，可以供开发者来读写外部数据和文件，我们称这些API为Java IO，主要有三种，BIO(同步阻塞I/O)、NIO(同步非阻塞模式)和AIO(异步非阻塞I/O模型)。\nBIO 对应着阻塞I/O模型,代码如下：\npackage com.zt.javastudy.netty.bio;\rimport java.io.IOException;\rimport java.io.InputStream;\rimport java.net.ServerSocket;\rimport java.net.Socket;\r/**\r* 同步阻塞服务端\r*\r* @author zhengtao on 2021/9/28\r*/\rpublic class BioServer {\rpublic static void main(String[] args) throws IOException {\rServerSocket serverSocket = new ServerSocket(3333);\r// 接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理\rnew Thread(() -\u0026gt; {\rwhile (true) {\rtry {\r// 阻塞方法获取新的连接\rSocket socket = serverSocket.accept();\r// 每一个新的连接都创建一个线程，负责读取数据\rnew Thread(() -\u0026gt; {\rtry {\rint len;\rbyte[] data = new byte[1024];\rInputStream inputStream = socket.getInputStream();\r// 按字节流方式读取数据\rwhile ((len = inputStream.read(data)) != -1) {\rSystem.out.println(new String(data, 0, len));\r}\r} catch (IOException e) {\r}\r}).start();\r} catch (IOException e) {\r}\r}\r}).start();\r}\r}\r package com.zt.javastudy.netty.bio;\rimport java.io.IOException;\rimport java.net.Socket;\rimport java.util.Date;\r/**\r* 同步阻塞客户端\r*\r* @author zhengtao on 2021/9/28\r*/\rpublic class BioClient {\rpublic static void main(String[] args) {\rnew Thread(() -\u0026gt; {\rtry {\rSocket socket = new Socket(\u0026quot;127.0.0.1\u0026quot;, 3333);\rwhile (true) {\rtry {\rsocket.getOutputStream().write((new Date() + \u0026quot;: hello world\u0026quot;).getBytes());\rThread.sleep(2000);\r} catch (Exception e) {\r}\r}\r} catch (IOException e) {\r}\r}).start();\r}\r}\r NIO 传统IO基于流(字节流和字符流)进行操作，而NIO基于Channel和Buffer(缓冲区)进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。\nNIO和传统IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。\nIO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变得可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）\nBuffer Buffer（缓冲区），它包含一些要写入或要读出的数据。关键Buffer实现有：ByteBuffer, CharBuffer, DoubleBuffer, FloatBuffer, IntBuffer, LongBuffer, ShortBuffer，分别对应基本数据类型: byte, char, double, float, int, long, short\nChannel channel（通道），它就像自来水管一样，数据通过channel读取和写入，通道和流的不同之处在于，通道是双向的（全双工的），既可以用来进行读操作，又可以用来进行写操作。流只是在一个方向上移动（要么写InputStream、要么读OutputStream）。\nSelector 多路复用器提供选择已经就绪的任务的能力，Selector会不断轮询注册在其上的Channel，如果某个Channel上面发生读或写事件，这个Channel就处于就绪状态，会被Selector轮询出来，然后通过SelectionKey可以获取就绪Channel的集合，进行后续的IO操作。\n通道注册到Selector时可以指定自己关注的事件\n SelectionKey.OP_CONNECT：连接就绪，某个channel成功连接到服务端 SelectionKey.OP_ACCEPT：接收就绪，服务端准备好接收新进入的连接 SelectionKey.OP_READ：读就绪，有数据可读了 SelectionKey.OP_WRITE：写就绪，待写数据  服务端代码\npackage com.zt.javastudy.netty.nio;\rimport java.io.IOException;\rimport java.net.InetSocketAddress;\rimport java.nio.ByteBuffer;\rimport java.nio.channels.SelectionKey;\rimport java.nio.channels.Selector;\rimport java.nio.channels.ServerSocketChannel;\rimport java.nio.channels.SocketChannel;\rimport java.nio.charset.StandardCharsets;\rimport java.util.Iterator;\rimport java.util.Set;\r/**\r* @author zhengtao on 2022/10/27\r*/\rpublic class NIOServer implements Runnable {\rpublic NIOServer(Selector selector) {\rthis.selector = selector;\r}\rprivate Selector selector;\r@Override\rpublic void run() {\rwhile (true) {\rtry {\rselector.select(1000);\rSet\u0026lt;SelectionKey\u0026gt; keys = selector.selectedKeys();\rIterator\u0026lt;SelectionKey\u0026gt; it = keys.iterator();\rSelectionKey key = null;\rwhile (it.hasNext()) {\rkey = it.next();\rit.remove();\rtry {\rhandleInput(key);\r} catch (Exception e) {\rkey.cancel();\rif (key.channel() != null) {\rkey.channel().close();\r}\r}\r}\r} catch (IOException e) {\re.printStackTrace();\r}\r}\r}\rprivate void handleInput(SelectionKey key) throws IOException {\rif (!key.isValid()) {\rreturn;\r}\r// 服务端ServerSocketChannel\rif (key.isAcceptable()) {\rServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel();\r// 与客户端建立连接\rSocketChannel socketChannel = serverSocketChannel.accept();\rsocketChannel.configureBlocking(false);\rsocketChannel.register(selector, SelectionKey.OP_READ);\rSystem.out.println(\u0026quot;客户端连接了\u0026quot;);\r}\rif (key.isReadable()) {\rSocketChannel socketChannel = (SocketChannel) key.channel();\rByteBuffer readBuffer = ByteBuffer.allocate(1024);\rint readBytes = socketChannel.read(readBuffer);\rif (readBytes \u0026gt; 0) {\rreadBuffer.flip();\rbyte[] bytes = new byte[readBuffer.remaining()];\rreadBuffer.get(bytes);\rString req = new String(bytes, StandardCharsets.UTF_8);\rSystem.out.println(\u0026quot;服务端收到客户端消息:\u0026quot; + req);\r// 写消息\rString msg = \u0026quot;客户端你好现在是:\u0026quot; + System.currentTimeMillis();\rbyte[] resp = msg.getBytes(StandardCharsets.UTF_8);\rByteBuffer writeBuffer = ByteBuffer.allocate(resp.length);\rwriteBuffer.put(resp);\rwriteBuffer.flip();\rsocketChannel.write(writeBuffer);\r} else if (readBytes \u0026lt; 0) {\rkey.cancel();\rsocketChannel.close();\r}\r}\r}\rpublic static void main(String[] args) throws IOException {\r// 打开服务端管道，用于监听客户端的连接，它是所有客户端连接的父管道\rServerSocketChannel socketChannel = ServerSocketChannel.open();\r// 绑定监听端口，设置连接为非阻塞模式\rsocketChannel.socket().bind(new InetSocketAddress(\u0026quot;localhost\u0026quot;, 8888));\rsocketChannel.configureBlocking(false);\r// 创建Reactor线程，创建多路复用器并启动线程\rSelector selector = Selector.open();\rnew Thread(new NIOServer(selector)).start();\r// 将服务端channel注册到多路复用器上，并监听连接事件\rsocketChannel.register(selector, SelectionKey.OP_ACCEPT);\rSystem.out.println(\u0026quot;服务端启动了\u0026quot;);\r}\r}\r 客户端代码\npackage com.zt.javastudy.netty.nio;\rimport java.io.IOException;\rimport java.net.InetSocketAddress;\rimport java.nio.ByteBuffer;\rimport java.nio.channels.SelectionKey;\rimport java.nio.channels.Selector;\rimport java.nio.channels.SocketChannel;\rimport java.nio.charset.StandardCharsets;\rimport java.util.Iterator;\rimport java.util.Set;\r/**\r* @author zhengtao\r*/\rpublic class NIOClient implements Runnable {\rprivate Selector selector;\rpublic NIOClient(Selector selector) {\rthis.selector = selector;\r}\r@Override\rpublic void run() {\rwhile (true) {\rtry {\rselector.select(1000);\rSet\u0026lt;SelectionKey\u0026gt; keys = selector.selectedKeys();\rIterator\u0026lt;SelectionKey\u0026gt; it = keys.iterator();\rSelectionKey key;\rwhile (it.hasNext()) {\rkey = it.next();\rit.remove();\rtry {\rhandleInput(key);\r} catch (Exception e) {\rkey.cancel();\rif (key.channel() != null) {\rkey.channel().close();\r}\r}\r}\r} catch (IOException e) {\re.printStackTrace();\r}\r}\r}\rprivate void handleInput(SelectionKey key) throws IOException {\rif (!key.isValid()) {\rreturn;\r}\r// 客户端SocketChannel\rSocketChannel socketChannel = (SocketChannel) key.channel();\rif (key.isConnectable()) {\rif (socketChannel.finishConnect()) {\rsocketChannel.register(selector, SelectionKey.OP_READ);\rdoWrite(socketChannel);\r} else {\rSystem.exit(1);\r}\r}\rif (key.isReadable()) {\rByteBuffer readBuffer = ByteBuffer.allocate(1024);\rint readBytes = socketChannel.read(readBuffer);\rif (readBytes \u0026gt; 0) {\rreadBuffer.flip();\rbyte[] bytes = new byte[readBuffer.remaining()];\rreadBuffer.get(bytes);\rString req = new String(bytes, StandardCharsets.UTF_8);\rSystem.out.println(\u0026quot;客户端收到服务端消息:\u0026quot; + req);\rdoWrite(socketChannel);\r} else if (readBytes \u0026lt; 0) {\rkey.cancel();\rsocketChannel.close();\r}\r}\r}\rprivate void doWrite(SocketChannel socketChannel) throws IOException {\r// 写消息\rString msg = \u0026quot;服务端你好现在是:\u0026quot; + System.currentTimeMillis();\rbyte[] resp = msg.getBytes(StandardCharsets.UTF_8);\rByteBuffer writeBuffer = ByteBuffer.allocate(resp.length);\rwriteBuffer.put(resp);\rwriteBuffer.flip();\rsocketChannel.write(writeBuffer);\r}\rpublic static void main(String[] args) throws IOException {\r// 打开客户端管道，设置连接为非阻塞模式\rSocketChannel socketChannel = SocketChannel.open();\rsocketChannel.configureBlocking(false);\r// 连接到服务端\rboolean isConnect = socketChannel.connect(new InetSocketAddress(\u0026quot;localhost\u0026quot;, 8888));\r// 创建Reactor线程，创建多路复用器并启动线程\rSelector selector = Selector.open();\rif (isConnect) {\rsocketChannel.register(selector, SelectionKey.OP_READ);\r} else {\rsocketChannel.register(selector, SelectionKey.OP_CONNECT);\r}\rnew Thread(new NIOClient(selector)).start();\r}\r}\r AIO 服务端代码\npackage com.zt.javastudy.aio;\rimport java.io.IOException;\rimport java.net.InetSocketAddress;\rimport java.nio.channels.AsynchronousServerSocketChannel;\rimport java.util.concurrent.CountDownLatch;\r/**\r* @author zhengtao\r* @create 2022/10/30 15:44\r*/\rpublic class AioServer implements Runnable {\rAsynchronousServerSocketChannel serverSocketChannel;\rCountDownLatch latch;\rpublic AioServer(AsynchronousServerSocketChannel serverSocketChannel) {\rthis.serverSocketChannel = serverSocketChannel;\r}\r@Override\rpublic void run() {\rlatch = new CountDownLatch(1);\rdoAccept();\rtry {\rlatch.await();\r} catch (InterruptedException e) {\re.printStackTrace();\r}\r}\rpublic void doAccept() {\rserverSocketChannel.accept(this, new AcceptCompletionHandler());\r}\rpublic static void main(String[] args) throws IOException {\r// 打开服务端管道，用于监听客户端的连接，它是所有客户端连接的父管道\rAsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open();\rserverSocketChannel.bind(new InetSocketAddress(8888));\rnew Thread(new AioServer(serverSocketChannel)).start();\rSystem.out.println(\u0026quot;服务端启动了\u0026quot;);\r}\r}\r package com.zt.javastudy.aio;\rimport java.nio.ByteBuffer;\rimport java.nio.channels.AsynchronousSocketChannel;\rimport java.nio.channels.CompletionHandler;\r/**\r* @author zhengtao\r* @create 2022/10/30 16:08\r*/\rpublic class AcceptCompletionHandler implements CompletionHandler\u0026lt;AsynchronousSocketChannel, AioServer\u0026gt; {\r@Override\rpublic void completed(AsynchronousSocketChannel result, AioServer attachment) {\r// 接收客户端连接\rattachment.serverSocketChannel.accept(attachment, this);\r// 读取消息\rByteBuffer buffer = ByteBuffer.allocate(1024);\rresult.read(buffer, buffer, new ReadCompletionHandler(result));\r}\r@Override\rpublic void failed(Throwable exc, AioServer attachment) {\rexc.printStackTrace();\rattachment.latch.countDown();\r}\r}\r package com.zt.javastudy.aio;\rimport java.io.IOException;\rimport java.nio.ByteBuffer;\rimport java.nio.channels.AsynchronousSocketChannel;\rimport java.nio.channels.CompletionHandler;\rimport java.nio.charset.StandardCharsets;\r/**\r* @author zhengtao\r* @create 2022/10/30 16:53\r*/\rpublic class ReadCompletionHandler implements CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt; {\rprivate AsynchronousSocketChannel channel;\rpublic ReadCompletionHandler(AsynchronousSocketChannel channel) {\rthis.channel = channel;\r}\r@Override\rpublic void completed(Integer result, ByteBuffer attachment) {\rattachment.flip();\rbyte[] body = new byte[attachment.remaining()];\rattachment.get(body);\rtry {\rString req = new String(body, StandardCharsets.UTF_8);\rSystem.out.println(\u0026quot;服务端收到客户端消息:\u0026quot; + req);\rdoWrite(channel);\r} catch (IOException e) {\re.printStackTrace();\r}\r}\r@Override\rpublic void failed(Throwable exc, ByteBuffer attachment) {\rtry {\rthis.channel.close();\r} catch (IOException e) {\re.printStackTrace();\r}\r}\rprivate void doWrite(AsynchronousSocketChannel channel) throws IOException {\r// 写消息\rString msg = \u0026quot;客户端你好现在是:\u0026quot; + System.currentTimeMillis();\rbyte[] resp = msg.getBytes(StandardCharsets.UTF_8);\rByteBuffer writeBuffer = ByteBuffer.allocate(resp.length);\rwriteBuffer.put(resp);\rwriteBuffer.flip();\rchannel.write(writeBuffer);\r}\r}\r 客户端代码\npackage com.zt.javastudy.aio;\rimport java.io.IOException;\rimport java.net.InetSocketAddress;\rimport java.nio.ByteBuffer;\rimport java.nio.channels.AsynchronousSocketChannel;\rimport java.nio.channels.CompletionHandler;\rimport java.nio.charset.StandardCharsets;\rimport java.util.concurrent.CountDownLatch;\r/**\r* @author zhengtao\r* @create 2022/10/30 17:53\r* aio 客户端\r*/\rpublic class AioClient implements CompletionHandler\u0026lt;Void, AioClient\u0026gt;, Runnable {\rprivate AsynchronousSocketChannel channel;\rprivate CountDownLatch latch;\rpublic AioClient(AsynchronousSocketChannel channel) {\rthis.channel = channel;\r}\r@Override\rpublic void run() {\rlatch = new CountDownLatch(1);\rchannel.connect(new InetSocketAddress(\u0026quot;localhost\u0026quot;, 8888), this, this);\rtry {\rlatch.await();\r} catch (InterruptedException e1) {\re1.printStackTrace();\r}\rtry {\rchannel.close();\r} catch (IOException e) {\re.printStackTrace();\r}\r}\r@Override\rpublic void completed(Void result, AioClient attachment) {\rString msg = \u0026quot;服务端你好现在是:\u0026quot; + System.currentTimeMillis();\rbyte[] req = msg.getBytes(StandardCharsets.UTF_8);\rByteBuffer writeBuffer = ByteBuffer.allocate(req.length);\rwriteBuffer.put(req);\rwriteBuffer.flip();\rchannel.write(writeBuffer, writeBuffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() {\r@Override\rpublic void completed(Integer result, ByteBuffer buffer) {\rif (buffer.hasRemaining()) {\rchannel.write(buffer, buffer, this);\r} else {\rByteBuffer readBuffer = ByteBuffer.allocate(1024);\rchannel.read(readBuffer, readBuffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() {\r@Override\rpublic void completed(Integer result, ByteBuffer buffer) {\rbuffer.flip();\rbyte[] bytes = new byte[buffer.remaining()];\rbuffer.get(bytes);\rString body;\rbody = new String(bytes, StandardCharsets.UTF_8);\rSystem.out.println(\u0026quot;客户端收到服务端消息 : \u0026quot; + body);\rlatch.countDown();\r}\r@Override\rpublic void failed(Throwable exc, ByteBuffer attachment) {\rtry {\rchannel.close();\rlatch.countDown();\r} catch (IOException e) {\r// ingnore on close\r}\r}\r});\r}\r}\r@Override\rpublic void failed(Throwable exc, ByteBuffer attachment) {\rtry {\rchannel.close();\rlatch.countDown();\r} catch (IOException e) {\r// ingnore on close\r}\r}\r});\r}\r@Override\rpublic void failed(Throwable exc, AioClient attachment) {\rexc.printStackTrace();\rtry {\rchannel.close();\rlatch.countDown();\r} catch (IOException e) {\re.printStackTrace();\r}\r}\rpublic static void main(String[] args) throws IOException {\r// 打开客户端管道\rAsynchronousSocketChannel channel = AsynchronousSocketChannel.open();\rnew Thread(new AioClient(channel)).start();\r}\r}\r 原生NIO和Netty的对比  NIO的类库比较复杂，需要搞懂Buffer、channel、Selector等 需要自己处理很多特殊情况，比如客户端断连，重连，网络闪断，半包读写，失败缓存，网络拥塞等问题 存在一些bug，比如epoll bug，它会导致Selector空轮询，最终导致cpu 100%  所以用netty哈哈，下面正式进入netty学习\nNetty对IO模式的支持   为什么不建议阻塞I/0 (BIO/OIO)\n连接数高的情况下:阻塞 -\u0026gt; 耗资源、效率低。\n  为什么删掉已经做好的AIO支持?\n Windows实现成熟，但其很少用做服务器 Linux常用做服务器，但其AlO实现不够成熟 Linux下AIO相比较NIO的性能提升不明显    所以netty选择支持NIO\nNetty核心组件 Netty 的逻辑处理架构为典型网络分层架构设计，共分为网络通信层、事件调度层、服务编排层，每一层各司其职\nBootstrap 网络通信层的职责是执行网络 I/O 的操作。它支持多种网络协议和 I/O 模型的连接操作。当网络数据读取到内核缓冲区后，会触发各种网络事件，这些网络事件会分发给事件调度层进行处理。\n网络通信层的核心组件包含BootStrap、ServerBootStrap、Channel三个组件。\nNetty 中的引导器共分为两种类型：一个为用于客户端的 Bootstrap，另一个为用于服务端的 ServerBootStrap。Bootstrap 作为整个 Netty 客户端和服务端的程序入口，可以把 Netty 的核心组件像搭积木一样组装在一起，串接了 Netty 所有的核心组件\nChannel Channel “通道”，它是网络通信的载体。Channel提供了基本的 API 用于网络 I/O 操作，如 register、bind、connect、read、write、flush 等。Netty 自己实现的 Channel 是以 JDK NIO Channel 为基础的，相比较于 JDK NIO，Netty 的 Channel 提供了更高层次的抽象，同时屏蔽了底层 Socket 的复杂性，赋予了 Channel 更加强大的功能，你在使用 Netty 时基本不需要再与 Java Socket 类直接打交道。\nEventLoopGroup EventLoopGroup 本质是一个Reactor 线程池，它实际就是EventLoop的数组，EventLoop的职责是处理所有注册到本线程多路复用器Selector上的channel，Selector的轮询操作由绑定的EventLoop线程run方法驱动。\n从上图中，我们可以总结出 EventLoopGroup、EventLoop、Channel 的几点关系。\n  一个 EventLoopGroup 往往包含一个或者多个 EventLoop。EventLoop 用于处理 Channel 生命周期内的所有 I/O 事件，如 accept、connect、read、write 等 I/O 事件。\n  EventLoop 同一时间会与一个线程绑定，每个 EventLoop 负责处理多个 Channel。\n  每新建一个 Channel，EventLoopGroup 会选择一个 EventLoop 与其绑定。该 Channel 在生命周期内都可以对 EventLoop 进行多次绑定和解绑。\n  Reactor 网络框架的设计离不开 I/O 线程模型，线程模型的优劣直接决定了系统的吞吐量、可扩展性、安全性等。目前主流的网络框架几乎都采用了 I/O 多路复用的方案。Reactor 模式作为其中的事件分发器，负责将读写事件分发给对应的读写事件处理者。大名鼎鼎的 Java 并发包作者 Doug Lea，在 Scalable I/O in Java 一文中阐述了服务端开发中 I/O 模型的演进过程。Netty 中三种 Reactor 线程模型也来源于这篇经典文章。下面我们对这三种 Reactor 线程模型做一个详细的分析。\n单线程模型 上图描述了 Reactor 的单线程模型结构，在 Reactor 单线程模型中，所有 I/O 操作（包括连接建立、数据读写、事件分发等），都是由一个线程完成的。单线程模型逻辑简单，缺陷也十分明显：\n 一个线程支持处理的连接数非常有限，CPU 很容易打满，性能方面有明显瓶颈； 当多个事件被同时触发时，只要有一个事件没有处理完，其他后面的事件就无法执行，这就会造成消息积压及请求超时； 线程在处理 I/O 事件时，Select 无法同时处理连接建立、事件分发等操作； 如果 I/O 线程一直处于满负荷状态，很可能造成服务端节点不可用。  多线程模型 由于单线程模型有性能方面的瓶颈，多线程模型作为解决方案就应运而生了。Reactor 多线程模型将业务逻辑交给多个线程进行处理。除此之外，多线程模型其他的操作与单线程模型是类似的，例如读取数据依然保留了串行化的设计。当客户端有数据发送至服务端时，Select 会监听到可读事件，数据读取完毕后提交到业务线程池中并发处理。\n主从多线程模型 主从多线程模型由多个 Reactor 线程组成，每个 Reactor 线程都有独立的 Selector 对象。MainReactor 仅负责处理客户端连接的 Accept 事件，连接建立成功后将新创建的连接对象注册至 SubReactor。再由 SubReactor 分配线程池中的 I/O 线程与其连接绑定，它将负责连接生命周期内所有的 I/O 事件。\nNetty 推荐使用主从多线程模型，这样就可以轻松达到成千上万规模的客户端连接。在海量客户端并发请求的场景下，主从多线程模式甚至可以适当增加 SubReactor 线程的数量，从而利用多核能力提升系统的吞吐量。\n介绍了上述三种 Reactor 线程模型，再结合它们各自的架构图，我们能大致总结出 Reactor 线程模型运行机制的四个步骤，分别为连接注册、事件轮询、事件分发、任务处理，如下图所示。\n 连接注册：Channel 建立后，注册至 Reactor 线程中的 Selector 选择器。 事件轮询：轮询 Selector 选择器中已注册的所有 Channel 的 I/O 事件。 事件分发：为准备就绪的 I/O 事件分配相应的处理线程。 任务处理：Reactor 线程还负责任务队列中的非 I/O 任务，每个 Worker 线程从各自维护的任务队列中取出任务异步执行。  以上介绍了 Reactor 线程模型的演进过程和基本原理，Netty 也同样遵循 Reactor 线程模型的运行机制，下面我们来了解一下 Netty 是如何实现 Reactor 线程模型的。\nNetty 通过创建不同的 EventLoopGroup 参数配置，就可以支持 Reactor 的三种线程模型：\n  单线程模型：EventLoopGroup 只包含一个 EventLoop，Boss 和 Worker 使用同一个EventLoopGroup；\n  多线程模型：EventLoopGroup 包含多个 EventLoop，Boss 和 Worker 使用同一个EventLoopGroup；\n  主从多线程模型：EventLoopGroup 包含多个 EventLoop，Boss 是主 Reactor，Worker 是从 Reactor，它们分别使用不同的 EventLoopGroup，主 Reactor 负责监听客户端的 Accept 事件,当事件触发时，将事件注册到从 Reactor。每新建一个 Channel， 只选择一个 NioEventLoop 与其绑定。所以说 Channel 生命周期的所有事件处理都是线程独立的，不同的 NioEventLoop 线程之间不会发生任何交集。\nNioEventLoop 完成数据读取后，会调用绑定的 ChannelPipeline 进行事件传播，ChannelPipeline 也是线程安全的，数据会被传递到 ChannelPipeline 的第一个 ChannelHandler 中。数据处理完成后，将加工完成的数据再传递给下一个 ChannelHandler，整个过程是串行化执行，不会发生线程上下文切换的问题\n  ChannelPipeline 一个负责处理网络事件的责任链，负责管理和执行ChannelHandler。网络事件以事件流的形式在ChannelPipeline中流转，并根据执行策略调度ChannelHandler执行。\n创建一个 Channel 都会绑定一个新的 ChannelPipeline，ChannelPipeline 中每加入一个 ChannelHandler 都会绑定一个 ChannelHandlerContext。ChannelHandlerContext 用于保存 ChannelHandler 上下文，通过 ChannelHandlerContext 我们可以知道 ChannelPipeline 和 ChannelHandler 的关联关系。\nChannelHandler 用户定制和扩展的关键接口。可以用于消息编解码、心跳、认证等\n总体流程 TCP粘包/拆包 ​ TCP是个“流”协议，所谓流就是没有界限的一串数据。大家可以想想河里的流水,它们是连成一片的，其间并没有分界线。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数椐包发送，这就是所谓的TCP粘包和拆包问题。\n​\t假设客户端分别发送两个数据包D1和D2给服务端，由于服务端一次性的字节数是不确定的.故可能存在以下4种情况。\n 服务端分两次读取到了两个独立的数据包，分别是DI和D2，没有粘包和拆包 服务端一次接收到了两个数裾包，D1和D2粘合在一起，被称为TCP粘包 服务端分两次读取到了 两个数据包，笫一次读取到了完整的D1包和D2包的部分内容，第二次读取到了 D2包的剩余内容，这被称为TCP拆包； 服务端分两次读取到了 两个数据包，第一次读取到了D1包的部分内容,第二次读取到了 D1 包的剩余内容和D2包的整包  如果此时服务端TCP接收滑窗非常小，而数裾包D1和D2比较大，很有可能会发生第5种可能，即服务端分多次才能将DI和D2包接收完全，期间发生多次拆包。\nTCP粘包/拆包发生的原因  应用程序write写入的字节大小大于套接口发送缓冲区大小 进行MSS大小的TCP分段； 以太网帧的payload大于MTU进行IP分片  解决策略 ​\t由于底层的TCP无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈设计来解决，归纳如下\n  消息定长.例如每个报文的大小为同定长度200字节，如果不够，空位补空格\n  在包尾增加回车换行符进行分割，例如FTP协议\n  将消息分为消息头和消息体，消息头中包含表示消息总长度（或者消息体长度） 的字段，通常设计思路为消息头的第一个字段使用int32来表示消息的总长度\n  更复杂的应用层协议\n  Netty的解决办法 ","id":14,"section":"posts","summary":"[TOC] Netty 学习 虽然说netty，用的少但是感觉比较流行，值得研究！！！ Netty 是一个 NIO 客户端-服务器框架，可以快速轻松地开发网络应用程序，它极大地简化了","tags":["Netty"],"title":"Netty 学习","uri":"https://wzgl998877.github.io/2022/01/netty/","year":"2022"},{"content":"spring源码学习 ​\t对于学习spring源码肯定最重要的学习spring的一些理念，比如控制翻转ioc，依赖注入di，面向切面编程aop等等。\n​\tBeanFactory、BeanDefinition、ApplicationContext\n​\tioc容器的初始化是通过refresh() 方法启动的，这个方法标志着ioc容器的正式启动，这个启动包括了BeanDefinition的Resource定位、载入和注册三个基本过程。在这个过程中一般不包括Bean依赖的注入，在spring ioc中bean定义的载入和依赖的注入是两个独立的过程，依赖注入一般发生在应用第一次通过getBean向容器索取Bean的时候。但如果对某个Bean设置了lazyinit属性，那么这个bean的依赖注入在ioc初始化时就完成了。@Lazy注解可能就是这个原理。\nspring 整体架构 核心类介绍 DefaultListableBeanFactory ​\tXmlBeanFactory继承自DefaultListableBeanFactory，而DefaultListableBeanFactory是整个bean加载的核心部分，是Spring注册及加载bean的默认实现，而对于XmlBeanFactory与DefaultListableBeanFactory不同的地方其实是在XmlBeanFactory中使用了自定义的XML读取器XmlBeanDefinitionReader，实现了个性化的BeanDefinitionReader读取，DefaultListableBeanFactory继承了AbstractAutowireCapableBeanFactory并实现了ConfigurableListableBeanFactory以及BeanDefinitionRegistry接口。\nXmlBeanDefinitionReader ​\tXML配置文件的读取是Spring中重要的功能。整个XML配置文件读取的大致流程为：\n1. 通过继承自AbstractBeanDefinitionReader中的方法，来使用ResourLoader将资源文件路径转换为对应的Resource文件。\r2. 通过DocumentLoader对Resource文件进行转换，将Resource文件转换为Document文件。\r3. 通过实现接口BeanDefinitionDocumentReader的DefaultBeanDefinitionDocumentReader类对Document进行解析，并使用BeanDefinitionParserDelegate对Element进行解析。\r 从配置文件到bean 获取bean总代码\nBeanFactory beanFactory = new XmlBeanFactory(new ClassPathResource(\u0026quot;applicationContext.xml\u0026quot;));\r public XmlBeanFactory(Resource resource, BeanFactory parentBeanFactory) throws BeansException {\rsuper(parentBeanFactory);\r// 加载bean\rthis.reader.loadBeanDefinitions(resource);\r}\r loadBeanDefinition() 方法具体实现\npublic int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException {\rAssert.notNull(encodedResource, \u0026quot;EncodedResource must not be null\u0026quot;);\rif (logger.isInfoEnabled()) {\rlogger.info(\u0026quot;Loading XML bean definitions from \u0026quot; + encodedResource.getResource());\r}\r// 记录已经加载的资源\rSet\u0026lt;EncodedResource\u0026gt; currentResources = this.resourcesCurrentlyBeingLoaded.get();\rif (currentResources == null) {\rcurrentResources = new HashSet\u0026lt;EncodedResource\u0026gt;(4);\rthis.resourcesCurrentlyBeingLoaded.set(currentResources);\r}\rif (!currentResources.add(encodedResource)) {\rthrow new BeanDefinitionStoreException(\r\u0026quot;Detected cyclic loading of \u0026quot; + encodedResource + \u0026quot; - check your import definitions!\u0026quot;);\r}\rtry {\rInputStream inputStream = encodedResource.getResource().getInputStream();\rtry {\rInputSource inputSource = new InputSource(inputStream);\rif (encodedResource.getEncoding() != null) {\rinputSource.setEncoding(encodedResource.getEncoding());\r}\r// 核心部分\rreturn doLoadBeanDefinitions(inputSource, encodedResource.getResource());\r}\rfinally {\rinputStream.close();\r}\r}\rcatch (IOException ex) {\rthrow new BeanDefinitionStoreException(\r\u0026quot;IOException parsing XML document from \u0026quot; + encodedResource.getResource(), ex);\r}\rfinally {\rcurrentResources.remove(encodedResource);\rif (currentResources.isEmpty()) {\rthis.resourcesCurrentlyBeingLoaded.remove();\r}\r}\r}\r doLoadBeanDefinitions 方法具体实现\nprotected int doLoadBeanDefinitions(InputSource inputSource, Resource resource)\rthrows BeanDefinitionStoreException {\rtry {\r// 获取document\rDocument doc = doLoadDocument(inputSource, resource);\r// 注册bean\rreturn registerBeanDefinitions(doc, resource);\r}\rcatch (BeanDefinitionStoreException ex) {\rthrow ex;\r}\rcatch (SAXParseException ex) {\rthrow new XmlBeanDefinitionStoreException(resource.getDescription(),\r\u0026quot;Line \u0026quot; + ex.getLineNumber() + \u0026quot; in XML document from \u0026quot; + resource + \u0026quot; is invalid\u0026quot;, ex);\r}\rcatch (SAXException ex) {\rthrow new XmlBeanDefinitionStoreException(resource.getDescription(),\r\u0026quot;XML document from \u0026quot; + resource + \u0026quot; is invalid\u0026quot;, ex);\r}\rcatch (ParserConfigurationException ex) {\rthrow new BeanDefinitionStoreException(resource.getDescription(),\r\u0026quot;Parser configuration exception parsing XML from \u0026quot; + resource, ex);\r}\rcatch (IOException ex) {\rthrow new BeanDefinitionStoreException(resource.getDescription(),\r\u0026quot;IOException parsing XML document from \u0026quot; + resource, ex);\r}\rcatch (Throwable ex) {\rthrow new BeanDefinitionStoreException(resource.getDescription(),\r\u0026quot;Unexpected exception parsing XML document from \u0026quot; + resource, ex);\r}\r}\r 在上面冗长的代码中假如不考虑异常类的代码，其实只做了三件事，这三件事的每一件都必不可少。\n 获取对XML文件的验证模式。 加载XML文件，并得到对应的Document。 根据返回的Document注册Bean信息。  1. 获取XML的验证模式 ​\t如果设定了验证模式则使用设定的验证模式（可以通过对调用XmlBeanDefinitionReader中的setValidationMode方法进行设定），否则使用自动检测的方式,Spring用来自动检测验证模式的办法就是判断是否包含DOCTYPE，如果包含就是DTD，否则就是XSD。这里主要是验证xml是否符合规范从而保证了XML文件的正确性。\n2. 获取Document ​\t读取 xml 文件并返回 Document 对象\n3. 解析及注册BeanDefinitions ​\t提取和注册 bean\n注册bean的主要代码为\n/**\r* Register each bean definition within the given root {@code \u0026lt;beans/\u0026gt;} element.\r*/\rprotected void doRegisterBeanDefinitions(Element root) {\r// Any nested \u0026lt;beans\u0026gt; elements will cause recursion in this method. In\r// order to propagate and preserve \u0026lt;beans\u0026gt; default-* attributes correctly,\r// keep track of the current (parent) delegate, which may be null. Create\r// the new (child) delegate with a reference to the parent for fallback purposes,\r// then ultimately reset this.delegate back to its original (parent) reference.\r// this behavior emulates a stack of delegates without actually necessitating one.\rBeanDefinitionParserDelegate parent = this.delegate;\rthis.delegate = createDelegate(getReaderContext(), root, parent);\rif (this.delegate.isDefaultNamespace(root)) {\rString profileSpec = root.getAttribute(PROFILE_ATTRIBUTE);\rif (StringUtils.hasText(profileSpec)) {\rString[] specifiedProfiles = StringUtils.tokenizeToStringArray(\rprofileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS);\rif (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) {\rreturn;\r}\r}\r}\r// 模版方法模式，如果继承自DefaultBeanDefinitionDocumentReader的子类需要在Bean解析前后做一些处理的话，那么只需要重写这两个方法就可以了\r// 解析前处理，留给子类实现\rpreProcessXml(root);\r// 解析并注册BeanDefinition\rparseBeanDefinitions(root, this.delegate);\r// 解析后处理，留给子类实现\rpostProcessXml(root);\r parseBeanDefinitions 方法具体实现\nprotected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) {\r// 处理beans\rif (delegate.isDefaultNamespace(root)) {\rNodeList nl = root.getChildNodes();\rfor (int i = 0; i \u0026lt; nl.getLength(); i++) {\rNode node = nl.item(i);\rif (node instanceof Element) {\rElement ele = (Element) node;\rif (delegate.isDefaultNamespace(ele)) {\r// 处理bean默认标签\rparseDefaultElement(ele, delegate);\r}\relse {\r// 处理bean自定义标签\rdelegate.parseCustomElement(ele);\r}\r}\r}\r}\relse {\rdelegate.parseCustomElement(root);\r}\r}\r parseDefaultElement()具体实现\nprivate void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) {\r// 处理import标签\rif (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) {\rimportBeanDefinitionResource(ele);\r}\r// 处理alias标签\relse if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) {\rprocessAliasRegistration(ele);\r}\r// 处理bean标签\relse if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) {\rprocessBeanDefinition(ele, delegate);\r}\r// 处理beans标签\relse if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) {\r// recurse\rdoRegisterBeanDefinitions(ele);\r}\r}\r bean标签的解析及注册 /**\r* Process the given bean element, parsing the bean definition\r* and registering it with the registry.\r*/\rprotected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) {\r// 解析BeanDefinition\rBeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele);\rif (bdHolder != null) {\r// 还需要再次对自定义标签进行解析\rbdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder);\rtry {\r// Register the final decorated instance.\r// 注册bean\rBeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry());\r}\rcatch (BeanDefinitionStoreException ex) {\rgetReaderContext().error(\u0026quot;Failed to register bean definition with name '\u0026quot; +\rbdHolder.getBeanName() + \u0026quot;'\u0026quot;, ele, ex);\r}\r// Send registration event.发出响应事件\rgetReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder));\r}\r}\r bean标签的解析及注册主要分为如下几步：\n 解析BeanDefinition。首先委托BeanDefinitionDelegate类的parseBeanDefinitionElement方法进行元素解析，返回BeanDefinitionHolder类型的实例bdHolder，经过这个方法后，bdHolder实例已经包含我们配置文件中配置的各种属性了，例如class、name、id、alias之类的属性。 **对自定义标签进行解析。**当返回的bdHolder不为空的情况下若存在默认标签的子节点下再有自定义属性，还需要再次对自定义标签进行解析。 注册解析的BeanDefinition。解析完成后，需要对解析后的bdHolder进行注册，同样，注册操作委托给了Bean DefinitionReaderUtils的registerBeanDefinition方法。 发出响应事件。最后发出响应事件，通知想关的监听器，这个bean已经加载完成了。  主要分析bean的注册：\n注册解析的BeanDefinition beanDefinition的注册就是使用beanDefinition作为value，使用beanName作为key、然后放入map中。\n/**\r* Register the given bean definition with the given bean factory.\r* @param definitionHolder the bean definition including name and aliases\r* @param registry the bean factory to register with\r* @throws BeanDefinitionStoreException if registration failed\r*/\rpublic static void registerBeanDefinition(\rBeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry)\rthrows BeanDefinitionStoreException {\r// Register bean definition under primary name. 使用beanName做唯一标识注册\rString beanName = definitionHolder.getBeanName();\rregistry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition());\r// Register aliases for bean name, if any.所有其他的别名\rString[] aliases = definitionHolder.getAliases();\rif (aliases != null) {\rfor (String alias : aliases) {\rregistry.registerAlias(beanName, alias);\r}\r}\r}\r 1.通过beanName注册BeanDefinition /** Map of bean definition objects, keyed by bean name */\r// 使用ConcurrentHashMap解决并发\rprivate final Map\u0026lt;String, BeanDefinition\u0026gt; beanDefinitionMap = new ConcurrentHashMap\u0026lt;String, BeanDefinition\u0026gt;(64);\r/** List of bean definition names, in registration order */\rprivate final List\u0026lt;String\u0026gt; beanDefinitionNames = new ArrayList\u0026lt;String\u0026gt;(64);\r/** List of names of manually registered singletons, in registration order */\rprivate final Set\u0026lt;String\u0026gt; manualSingletonNames = new LinkedHashSet\u0026lt;String\u0026gt;(16);\r/** Cached array of bean definition names in case of frozen configuration */\rprivate String[] frozenBeanDefinitionNames;\rpublic void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException {\rAssert.hasText(beanName, \u0026quot;Bean name must not be empty\u0026quot;);\rAssert.notNull(beanDefinition, \u0026quot;BeanDefinition must not be null\u0026quot;);\rif (beanDefinition instanceof AbstractBeanDefinition) {\rtry {\r// 注册前的最后一次校验\r((AbstractBeanDefinition)beanDefinition).validate();\r} catch (BeanDefinitionValidationException var4) {\rthrow new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \u0026quot;Validation of bean definition failed\u0026quot;, var4);\r}\r}\rBeanDefinition oldBeanDefinition = (BeanDefinition)this.beanDefinitionMap.get(beanName);\r// 处理已经注册过的bean\rif (oldBeanDefinition != null) {\rif (!this.isAllowBeanDefinitionOverriding()) {\rthrow new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \u0026quot;Cannot register bean definition [\u0026quot; + beanDefinition + \u0026quot;] for bean '\u0026quot; + beanName + \u0026quot;': There is already [\u0026quot; + oldBeanDefinition + \u0026quot;] bound.\u0026quot;);\r}\rif (oldBeanDefinition.getRole() \u0026lt; beanDefinition.getRole()) {\rif (this.logger.isWarnEnabled()) {\rthis.logger.warn(\u0026quot;Overriding user-defined bean definition for bean '\u0026quot; + beanName + \u0026quot;' with a framework-generated bean definition: replacing [\u0026quot; + oldBeanDefinition + \u0026quot;] with [\u0026quot; + beanDefinition + \u0026quot;]\u0026quot;);\r}\r} else if (this.logger.isInfoEnabled()) {\rthis.logger.info(\u0026quot;Overriding bean definition for bean '\u0026quot; + beanName + \u0026quot;': replacing [\u0026quot; + oldBeanDefinition + \u0026quot;] with [\u0026quot; + beanDefinition + \u0026quot;]\u0026quot;);\r}\r} else {\r// 记录beanName\rthis.beanDefinitionNames.add(beanName);\r// 在单例set中删除改bean，因为这是二次注册了\rthis.manualSingletonNames.remove(beanName);\rthis.frozenBeanDefinitionNames = null;\r}\r// 注册bean，也就是放入map中\rthis.beanDefinitionMap.put(beanName, beanDefinition);\rif (oldBeanDefinition != null || this.containsSingleton(beanName)) {\r// 重置所有beanName对应的缓存\rthis.resetBeanDefinition(beanName);\r}\r}\r 通过beanName注册bean主要流程\n 对AbstractBeanDefinition的校验。在解析XML文件的时候我们提过校验，但是此校验非彼校验，之前的校验时针对于XML格式的校验，而此时的校验时针是对于AbstractBean Definition的methodOverrides属性的。 对beanName已经注册的情况的处理。如果设置了不允许bean的覆盖，则需要抛出异常，如果oldBeanDefinition.getRole() \u0026lt; beanDefinition.getRole() 则打日志提醒，否则直接覆盖。 注册bean，也就是放入map中。 清除解析之前留下的对应beanName的缓存。  2. 通过别名注册BeanDefinition /** Map from alias to canonical name */\rprivate final Map\u0026lt;String, String\u0026gt; aliasMap = new ConcurrentHashMap\u0026lt;String, String\u0026gt;(16);\r@Override\rpublic void registerAlias(String name, String alias) {\rAssert.hasText(name, \u0026quot;'name' must not be empty\u0026quot;);\rAssert.hasText(alias, \u0026quot;'alias' must not be empty\u0026quot;);\r// 如果beanName与alias相同不记录alias，并删除对应的alias\rif (alias.equals(name)) {\rthis.aliasMap.remove(alias);\r}\relse {\rif (!allowAliasOverriding()) {\rString registeredName = this.aliasMap.get(alias);\rif (registeredName != null \u0026amp;\u0026amp; !registeredName.equals(name)) {\rthrow new IllegalStateException(\u0026quot;Cannot register alias '\u0026quot; + alias + \u0026quot;' for name '\u0026quot; +\rname + \u0026quot;': It is already registered for name '\u0026quot; + registeredName + \u0026quot;'.\u0026quot;);\r}\r}\r// 检查给定的名称是否指向另一个方向的别名，捕获一个循环引用并抛出相应的IllegalStateException。\r// 当b是a的别名，如果这时候有c是a的别名，b是c的别名，那么a就有引用指向了b，这时候就有两个指向b所以报错，if a-\u0026gt;b,exist a-\u0026gt;c-\u0026gt;b 报错\rcheckForAliasCircle(name, alias);\rthis.aliasMap.put(alias, name);\r}\r}\r 主要流程：\n alias与beanName相同情况处理。若alias与beanName并名称相同则不需要处理并删除掉原有alias。 alias覆盖处理。若aliasName已经使用并已经指向了另一beanName则需要用户的设置进行处理。 alias循环检查。当A-\u0026gt;B存在时，若再次出现A-\u0026gt;C-\u0026gt;B时候则会抛出异常。 注册alias。  bean 的加载 protected \u0026lt;T\u0026gt; T doGetBean(\rfinal String name, final Class\u0026lt;T\u0026gt; requiredType, final Object[] args, boolean typeCheckOnly)\rthrows BeansException {\rfinal String beanName = transformedBeanName(name);\rObject bean;\r// Eagerly check singleton cache for manually registered singletons.\r// 从三级缓存中获取单例bean\rObject sharedInstance = getSingleton(beanName);\rif (sharedInstance != null \u0026amp;\u0026amp; args == null) {\rif (logger.isDebugEnabled()) {\rif (isSingletonCurrentlyInCreation(beanName)) {\rlogger.debug(\u0026quot;Returning eagerly cached instance of singleton bean '\u0026quot; + beanName +\r\u0026quot;' that is not fully initialized yet - a consequence of a circular reference\u0026quot;);\r}\relse {\rlogger.debug(\u0026quot;Returning cached instance of singleton bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\r}\r// 从bean的实例中获取对象\rbean = getObjectForBeanInstance(sharedInstance, name, beanName, null);\r}\relse {\r// Fail if we're already creating this bean instance:\r// We're assumably within a circular reference.\r// 原型模式的bean 存在循环依赖直接报错\rif (isPrototypeCurrentlyInCreation(beanName)) {\rthrow new BeanCurrentlyInCreationException(beanName);\r}\r// Check if bean definition exists in this factory.\rBeanFactory parentBeanFactory = getParentBeanFactory();\rif (parentBeanFactory != null \u0026amp;\u0026amp; !containsBeanDefinition(beanName)) {\r// Not found -\u0026gt; check parent.\rString nameToLookup = originalBeanName(name);\rif (args != null) {\r// Delegation to parent with explicit args.\rreturn (T) parentBeanFactory.getBean(nameToLookup, args);\r}\relse {\r// No args -\u0026gt; delegate to standard getBean method.\rreturn parentBeanFactory.getBean(nameToLookup, requiredType);\r}\r}\rif (!typeCheckOnly) {\rmarkBeanAsCreated(beanName);\r}\rtry {\rfinal RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);\rcheckMergedBeanDefinition(mbd, beanName, args);\r// Guarantee initialization of beans that the current bean depends on.\rString[] dependsOn = mbd.getDependsOn();\rif (dependsOn != null) {\rfor (String dependsOnBean : dependsOn) {\rif (isDependent(beanName, dependsOnBean)) {\rthrow new BeanCreationException(mbd.getResourceDescription(), beanName,\r\u0026quot;Circular depends-on relationship between '\u0026quot; + beanName + \u0026quot;' and '\u0026quot; + dependsOnBean + \u0026quot;'\u0026quot;);\r}\rregisterDependentBean(dependsOnBean, beanName);\rgetBean(dependsOnBean);\r}\r}\r// Create bean instance.\rif (mbd.isSingleton()) {\r// 创建 bean 实例\rsharedInstance = getSingleton(beanName, new ObjectFactory\u0026lt;Object\u0026gt;() {\r@Override\rpublic Object getObject() throws BeansException {\rtry {\rreturn createBean(beanName, mbd, args);\r}\rcatch (BeansException ex) {\r// Explicitly remove instance from singleton cache: It might have been put there\r// eagerly by the creation process, to allow for circular reference resolution.\r// Also remove any beans that received a temporary reference to the bean.\rdestroySingleton(beanName);\rthrow ex;\r}\r}\r});\rbean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);\r}\relse if (mbd.isPrototype()) {\r// It's a prototype -\u0026gt; create a new instance.\rObject prototypeInstance = null;\rtry {\rbeforePrototypeCreation(beanName);\rprototypeInstance = createBean(beanName, mbd, args);\r}\rfinally {\rafterPrototypeCreation(beanName);\r}\rbean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);\r}\relse {\rString scopeName = mbd.getScope();\rfinal Scope scope = this.scopes.get(scopeName);\rif (scope == null) {\rthrow new IllegalStateException(\u0026quot;No Scope registered for scope '\u0026quot; + scopeName + \u0026quot;'\u0026quot;);\r}\rtry {\rObject scopedInstance = scope.get(beanName, new ObjectFactory\u0026lt;Object\u0026gt;() {\r@Override\rpublic Object getObject() throws BeansException {\rbeforePrototypeCreation(beanName);\rtry {\rreturn createBean(beanName, mbd, args);\r}\rfinally {\rafterPrototypeCreation(beanName);\r}\r}\r});\rbean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd);\r}\rcatch (IllegalStateException ex) {\rthrow new BeanCreationException(beanName,\r\u0026quot;Scope '\u0026quot; + scopeName + \u0026quot;' is not active for the current thread; \u0026quot; +\r\u0026quot;consider defining a scoped proxy for this bean if you intend to refer to it from a singleton\u0026quot;,\rex);\r}\r}\r}\rcatch (BeansException ex) {\rcleanupAfterBeanCreationFailure(beanName);\rthrow ex;\r}\r}\r// Check if required type matches the type of the actual bean instance.\rif (requiredType != null \u0026amp;\u0026amp; bean != null \u0026amp;\u0026amp; !requiredType.isAssignableFrom(bean.getClass())) {\rtry {\rreturn getTypeConverter().convertIfNecessary(bean, requiredType);\r}\rcatch (TypeMismatchException ex) {\rif (logger.isDebugEnabled()) {\rlogger.debug(\u0026quot;Failed to convert bean '\u0026quot; + name + \u0026quot;' to required type [\u0026quot; +\rClassUtils.getQualifiedName(requiredType) + \u0026quot;]\u0026quot;, ex);\r}\rthrow new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass());\r}\r}\rreturn (T) bean;\r}\r 缓存中获取单例bean ​\t单例在Spring的同一个容器内只会被创建一次，后续再获取bean直接从单例缓存中获取，当然这里也只是尝试加载，首先尝试从缓存中加载，然后再次尝试尝试从singletonFactories中加载。因为在创建单例bean的时候会存在依赖注入的情况，而在创建依赖的时候为了避免循环依赖， Spring创建bean的原则是不等bean创建完成就会将创建bean的ObjectFactory提早曝光加入到缓存中，一旦下一个bean创建时需要依赖上个bean，则直接使用ObjectFactory。\n主要代码：\n/** Cache of singleton objects: bean name --\u0026gt; bean instance */\r// singletonObjects：完成初始化的单例对象的cache（一级缓存）\rprivate final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(64);\r// 当前正在创建bean\r/** Names of beans that are currently in creation */\rprivate final Set\u0026lt;String\u0026gt; singletonsCurrentlyInCreation =\rCollections.newSetFromMap(new ConcurrentHashMap\u0026lt;String, Boolean\u0026gt;(16));\r// 完成实例化但是尚未初始化的，提前暴光的单例对象的Cache （二级缓存）\r/** Cache of early singleton objects: bean name --\u0026gt; bean instance */\rprivate final Map\u0026lt;String, Object\u0026gt; earlySingletonObjects = new HashMap\u0026lt;String, Object\u0026gt;(16);\r/** Cache of singleton factories: bean name --\u0026gt; ObjectFactory */\r// 进入实例化阶段的单例对象工厂的cache （三级缓存）\rprivate final Map\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt; singletonFactories = new HashMap\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt;(16);\rprotected Object getSingleton(String beanName, boolean allowEarlyReference) {\r// 从一级缓存中查找\rObject singletonObject = this.singletonObjects.get(beanName);\rif (singletonObject == null \u0026amp;\u0026amp; isSingletonCurrentlyInCreation(beanName)) {\rsynchronized (this.singletonObjects) {\r// 从二级缓存中查找\rsingletonObject = this.earlySingletonObjects.get(beanName);\rif (singletonObject == null \u0026amp;\u0026amp; allowEarlyReference) {\r// 从三级缓存中查找\rObjectFactory\u0026lt;?\u0026gt; singletonFactory = this.singletonFactories.get(beanName);\rif (singletonFactory != null) {\rsingletonObject = singletonFactory.getObject();\r// 放入二级缓存\rthis.earlySingletonObjects.put(beanName, singletonObject);\r// 从三级缓存移除\rthis.singletonFactories.remove(beanName);\r}\r}\r}\r}\rreturn (singletonObject != NULL_OBJECT ? singletonObject : null);\r}\r 这段代码解决了著名的循环依赖问题\n循环依赖：其实就是循环引用，也就是两个或者两个以上的bean互相持有对方，最终形成闭环。比如A依赖于B，B依赖于C，C又依赖于A。\n首先以自己写代码为例，出现循环引用的情况：\npackage com.zt.javastudy.spring;\r/**\r* @author zhengtao\r* @description 测试循环依赖\r* @date 2021/4/9\r*/\rpublic class TestXunHuan{\rpublic static void main(String[] args) {\rSystem.out.println(new StudentC());\r}\r}\r/**\r* StudentC与StudentD存在循环引用\r*/\rclass StudentC {\rpublic StudentC() {\rnew StudentD();\r}\r}\rclass StudentD {\rpublic StudentD() {\rnew StudentC();\r}\r}\r 结果栈溢出：\nException in thread \u0026quot;main\u0026quot; java.lang.StackOverflowError\r spring中循环依赖的三种情况：\n  构造器注入循环依赖   @Service\rpublic class StudentA {\rprivate StudentB b;\rpublic StudentB getB() {\rreturn b;\r}\rpublic void setB(StudentB b) {\rthis.b = b;\r}\r/**\r* 构造函数循环依赖\r* @param b\r*/\rpublic StudentA(StudentB b) {\rthis.b = b;\r}\r}\r@Service\rpublic class StudentB {\rprivate StudentA a;\rpublic StudentA getA() {\rreturn a;\r}\rpublic void setA(StudentA a) {\rthis.a = a;\r}\r/**\r* 构造函数循环依赖\r* @param a\r*/\rpublic StudentB(StudentA a) {\rthis.a = a;\r}\r}\r   field属性注入（setter方法注入）循环依赖 @Service\rpublic class StudentA {\r@Autowired\rprivate StudentB b;\rpublic StudentB getB() {\rreturn b;\r}\rpublic void setB(StudentB b) {\rthis.b = b;\r}\r}\r@Service\rpublic class StudentB {\r@Autowired\rprivate StudentA a;\rpublic StudentA getA() {\rreturn a;\r}\rpublic void setA(StudentA a) {\rthis.a = a;\r}\r}\r   prototype原型模式 field属性注入循环依赖 // 原型模式\r@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\r@Service\rpublic class StudentA {\r@Autowired\rprivate StudentB b;\rpublic StudentB getB() {\rreturn b;\r}\rpublic void setB(StudentB b) {\rthis.b = b;\r}\r}\r// 原型模式\r@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\r@Service\rpublic class StudentB {\r@Autowired\rprivate StudentA a;\rpublic StudentA getA() {\rreturn a;\r}\rpublic void setA(StudentA a) {\rthis.a = a;\r}\r}\r spring 中帮我们解决了第二种field属性注入（setter方法注入）循环依赖， 是使用了三级缓存的方式来解决的：\n 先从一级缓存singletonObjects中去获取。（如果获取到就直接return） 如果获取不到或者对象正在创建中（isSingletonCurrentlyInCreation()），那就再从二级缓存earlySingletonObjects中获取。（如果获取到就直接return） 如果还是获取不到，且允许singletonFactories（allowEarlyReference=true）通过getObject()获取。就从三级缓存singletonFactory.getObject()获取。（如果获取到了就从singletonFactories中移除，并且放进earlySingletonObjects。其实也就是从三级缓存移动（是剪切、不是复制哦~）**到了二级缓存）  加入singletonFactories三级缓存的前提是执行了构造器，所以构造器的循环依赖没法解决\n  这样的意义是什么呢？\n​\tA首先完成了初始化的第一步，并且将自己提前曝光到singletonFactories中也就是加入到三级缓存中，此时进行初始化的第二步，发现自己依赖对象B，此时就尝试去get(B)，发现B还没有被create，所以走create流程，B在初始化第一步的时候发现自己依赖了对象A，于是尝试get(A)，尝试一级缓存singletonObjects(肯定没有，因为A还没初始化完全)，尝试二级缓存earlySingletonObjects（也没有），尝试三级缓存singletonFactories，由于A通过ObjectFactory将自己提前曝光了，所以B能够通过ObjectFactory.getObject拿到A对象(虽然A还没有初始化完全，但是总比没有好呀)，B拿到A对象后顺利完成了初始化阶段1、2、3，完全初始化之后将自己放入到一级缓存singletonObjects中。此时返回A中，A此时能拿到B的对象顺利完成自己的初始化阶段2、3，最终A也完成了初始化，进去了一级缓存singletonObjects中，而且更加幸运的是，由于B拿到了A的对象引用而A现在已经完成了初始化，所以B现在拿到的A对象已经完成了初始化。\n从bean的实例中获取对象 ​\t调用该方法getObjectForBeanInstance()得到对象\n 对FactoryBean正确性的验证。 对非FactoryBean不做任何处理。 对bean进行转换。 将从Factory中解析bean的工作委托给getObjectFromFactoryBean。  获取单例 如果缓存中不存在已经加载的单例bean就需要从头开始bean的加载过程了，而Spring中使用getSingleton的重载方法实现bean的加载过程。\npublic Object getSingleton(String beanName, ObjectFactory\u0026lt;?\u0026gt; singletonFactory) {\rAssert.notNull(beanName, \u0026quot;Bean name must not be null\u0026quot;);\rsynchronized (this.singletonObjects) {\r// 一级缓存\rObject singletonObject = this.singletonObjects.get(beanName);\rif (singletonObject == null) {\r// 是否正在创建中\rif (this.singletonsCurrentlyInDestruction) {\rthrow new BeanCreationNotAllowedException(beanName,\r\u0026quot;Singleton bean creation not allowed while singletons of this factory are in destruction \u0026quot; +\r\u0026quot;(Do not request a bean from a BeanFactory in a destroy method implementation!)\u0026quot;);\r}\rif (logger.isDebugEnabled()) {\rlogger.debug(\u0026quot;Creating shared instance of singleton bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\r// 记录加载状态\rbeforeSingletonCreation(beanName);\rboolean newSingleton = false;\rboolean recordSuppressedExceptions = (this.suppressedExceptions == null);\rif (recordSuppressedExceptions) {\rthis.suppressedExceptions = new LinkedHashSet\u0026lt;\u0026gt;();\r}\rtry {\r// 获得实例\rsingletonObject = singletonFactory.getObject();\rnewSingleton = true;\r}\rcatch (IllegalStateException ex) {\r// Has the singleton object implicitly appeared in the meantime -\u0026gt;\r// if yes, proceed with it since the exception indicates that state.\rsingletonObject = this.singletonObjects.get(beanName);\rif (singletonObject == null) {\rthrow ex;\r}\r}\rcatch (BeanCreationException ex) {\rif (recordSuppressedExceptions) {\rfor (Exception suppressedException : this.suppressedExceptions) {\rex.addRelatedCause(suppressedException);\r}\r}\rthrow ex;\r}\rfinally {\rif (recordSuppressedExceptions) {\rthis.suppressedExceptions = null;\r}\r// 清除加载状态\rafterSingletonCreation(beanName);\r}\rif (newSingleton) {\r// 加入缓存\raddSingleton(beanName, singletonObject);\r}\r}\rreturn singletonObject;\r}\r}\r// this.singletonsCurrentlyInCreation.add(beanName) 将该bean加入正在加载的bean set集合中\rprotected void beforeSingletonCreation(String beanName) {\rif (!this.inCreationCheckExclusions.contains(beanName) \u0026amp;\u0026amp; !this.singletonsCurrentlyInCreation.add(beanName)) {\rthrow new BeanCurrentlyInCreationException(beanName);\r}\r}\r// this.singletonsCurrentlyInCreation.remove(beanName) 将该bean从正在加载bean的set集合中删除\rprotected void afterSingletonCreation(String beanName) {\rif (!this.inCreationCheckExclusions.contains(beanName) \u0026amp;\u0026amp; !this.singletonsCurrentlyInCreation.remove(beanName)) {\rthrow new IllegalStateException(\u0026quot;Singleton '\u0026quot; + beanName + \u0026quot;' isn't currently in creation\u0026quot;);\r}\r}\r// 清除二、三级缓存加入一级缓存\rprotected void addSingleton(String beanName, Object singletonObject) {\rsynchronized (this.singletonObjects) {\r// 加入一级缓存\rthis.singletonObjects.put(beanName, singletonObject);\r// 从三级缓存移除\rthis.singletonFactories.remove(beanName);\r// 从二级缓存移除\rthis.earlySingletonObjects.remove(beanName);\r// 加入单例注册set\rthis.registeredSingletons.add(beanName);\r}\r}\r 准备创建bean protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)\rthrows BeanCreationException {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Creating instance of bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\rRootBeanDefinition mbdToUse = mbd;\r// Make sure bean class is actually resolved at this point, and\r// clone the bean definition in case of a dynamically resolved Class\r// which cannot be stored in the shared merged bean definition.\r// 解析class\rClass\u0026lt;?\u0026gt; resolvedClass = resolveBeanClass(mbd, beanName);\rif (resolvedClass != null \u0026amp;\u0026amp; !mbd.hasBeanClass() \u0026amp;\u0026amp; mbd.getBeanClassName() != null) {\rmbdToUse = new RootBeanDefinition(mbd);\rmbdToUse.setBeanClass(resolvedClass);\r}\r// Prepare method overrides.\rtry {\r// 验证和准备 覆盖的方法\rmbdToUse.prepareMethodOverrides();\r}\rcatch (BeanDefinitionValidationException ex) {\rthrow new BeanDefinitionStoreException(mbdToUse.getResourceDescription(),\rbeanName, \u0026quot;Validation of method overrides failed\u0026quot;, ex);\r}\rtry {\r// Give BeanPostProcessors a chance to return a proxy instead of the target bean instance.\r// 给BeanPostProcessors一个返回代理而不是目标bean实例的机会\rObject bean = resolveBeforeInstantiation(beanName, mbdToUse);\rif (bean != null) {\rreturn bean;\r}\r}\rcatch (Throwable ex) {\rthrow new BeanCreationException(mbdToUse.getResourceDescription(), beanName,\r\u0026quot;BeanPostProcessor before instantiation of bean failed\u0026quot;, ex);\r}\rtry {\r// 真正创建bean\rObject beanInstance = doCreateBean(beanName, mbdToUse, args);\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Finished creating instance of bean '\u0026quot; + beanName + \u0026quot;'\u0026quot;);\r}\rreturn beanInstance;\r}\rcatch (BeanCreationException | ImplicitlyAppearedSingletonException ex) {\r// A previously detected exception with proper bean creation context already,\r// or illegal singleton state to be communicated up to DefaultSingletonBeanRegistry.\rthrow ex;\r}\rcatch (Throwable ex) {\rthrow new BeanCreationException(\rmbdToUse.getResourceDescription(), beanName, \u0026quot;Unexpected exception during bean creation\u0026quot;, ex);\r}\r}\r 这段代码可以分为：\n resolveBeanClass，根据设置的class属性或者根据className来解析Class。 mbdToUse.prepareMethodOverrides()，对override属性进行标记及验证。 Object bean = resolveBeforeInstantiation(beanName, mbdToUse)，应用初始化前的后处理器，解析指定bean是否存在初始化前的短路操作。aop功能就是基于这里的 Object beanInstance = doCreateBean(beanName, mbdToUse, args)，真正核心，创建bean。  创建bean protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args)\rthrows BeanCreationException {\r// Instantiate the bean.\rBeanWrapper instanceWrapper = null;\rif (mbd.isSingleton()) {\rinstanceWrapper = this.factoryBeanInstanceCache.remove(beanName);\r}\rif (instanceWrapper == null) {\r// 实例化，调用构造方法实例化对象\rinstanceWrapper = createBeanInstance(beanName, mbd, args);\r}\rObject bean = instanceWrapper.getWrappedInstance();\rClass\u0026lt;?\u0026gt; beanType = instanceWrapper.getWrappedClass();\rif (beanType != NullBean.class) {\rmbd.resolvedTargetType = beanType;\r}\r// Allow post-processors to modify the merged bean definition.\rsynchronized (mbd.postProcessingLock) {\rif (!mbd.postProcessed) {\rtry {\rapplyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName);\r}\rcatch (Throwable ex) {\rthrow new BeanCreationException(mbd.getResourceDescription(), beanName,\r\u0026quot;Post-processing of merged bean definition failed\u0026quot;, ex);\r}\rmbd.postProcessed = true;\r}\r}\r// Eagerly cache singletons to be able to resolve circular references\r// even when triggered by lifecycle interfaces like BeanFactoryAware.\rboolean earlySingletonExposure = (mbd.isSingleton() \u0026amp;\u0026amp; this.allowCircularReferences \u0026amp;\u0026amp;\risSingletonCurrentlyInCreation(beanName));\rif (earlySingletonExposure) {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Eagerly caching bean '\u0026quot; + beanName +\r\u0026quot;' to allow for resolving potential circular references\u0026quot;);\r}\r// 添加三级缓存\raddSingletonFactory(beanName, () -\u0026gt; getEarlyBeanReference(beanName, mbd, bean));\r}\r// Initialize the bean instance.\rObject exposedObject = bean;\rtry {\r// 填充属性\rpopulateBean(beanName, mbd, instanceWrapper);\r// 调用init方法初始化bean\rexposedObject = initializeBean(beanName, exposedObject, mbd);\r}\rcatch (Throwable ex) {\rif (ex instanceof BeanCreationException \u0026amp;\u0026amp; beanName.equals(((BeanCreationException) ex).getBeanName())) {\rthrow (BeanCreationException) ex;\r}\relse {\rthrow new BeanCreationException(\rmbd.getResourceDescription(), beanName, \u0026quot;Initialization of bean failed\u0026quot;, ex);\r}\r}\r// 循环依赖检查\rif (earlySingletonExposure) {\rObject earlySingletonReference = getSingleton(beanName, false);\rif (earlySingletonReference != null) {\rif (exposedObject == bean) {\rexposedObject = earlySingletonReference;\r}\relse if (!this.allowRawInjectionDespiteWrapping \u0026amp;\u0026amp; hasDependentBean(beanName)) {\rString[] dependentBeans = getDependentBeans(beanName);\rSet\u0026lt;String\u0026gt; actualDependentBeans = new LinkedHashSet\u0026lt;\u0026gt;(dependentBeans.length);\rfor (String dependentBean : dependentBeans) {\rif (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) {\ractualDependentBeans.add(dependentBean);\r}\r}\rif (!actualDependentBeans.isEmpty()) {\rthrow new BeanCurrentlyInCreationException(beanName,\r\u0026quot;Bean with name '\u0026quot; + beanName + \u0026quot;' has been injected into other beans [\u0026quot; +\rStringUtils.collectionToCommaDelimitedString(actualDependentBeans) +\r\u0026quot;] in its raw version as part of a circular reference, but has eventually been \u0026quot; +\r\u0026quot;wrapped. This means that said other beans do not use the final version of the \u0026quot; +\r\u0026quot;bean. This is often the result of over-eager type matching - consider using \u0026quot; +\r\u0026quot;'getBeanNamesForType' with the 'allowEagerInit' flag turned off, for example.\u0026quot;);\r}\r}\r}\r}\r// Register bean as disposable.\rtry {\rregisterDisposableBeanIfNecessary(beanName, bean, mbd);\r}\rcatch (BeanDefinitionValidationException ex) {\rthrow new BeanCreationException(\rmbd.getResourceDescription(), beanName, \u0026quot;Invalid destruction signature\u0026quot;, ex);\r}\rreturn exposedObject;\r}\r 创建bean的实例:createBeanInstance() 该方法作用为实例化 bean\n 如果存在工厂方法则使用工厂方法进行初始化。 一个类有多个构造函数，每个构造函数都有不同的参数，所以需要根据参数锁定构造函数并进行初始化。 如果既不存在工厂方法也不存在带有参数的构造函数，则使用默认的构造函数进行bean的实例化。  解决循环依赖:addSingletonFactory() protected void addSingletonFactory(String beanName, ObjectFactory\u0026lt;?\u0026gt; singletonFactory) {\rAssert.notNull(singletonFactory, \u0026quot;Singleton factory must not be null\u0026quot;);\rsynchronized (this.singletonObjects) {\rif (!this.singletonObjects.containsKey(beanName)) {\r// 加入三级缓存\rthis.singletonFactories.put(beanName, singletonFactory);\r// 删除二级缓存\rthis.earlySingletonObjects.remove(beanName);\rthis.registeredSingletons.add(beanName);\r}\r}\r}\r 创建完bean的实例后，将该单例提早曝光， 将创建该单例的工厂加入三级缓存\n属性注入:populateBean() 主要功能就是属性填充\n主要流程为：\n  根据注入类型（byName/byType），提取依赖的bean，并统一存入PropertyValues中。\nprotected void autowireByName(\rString beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) {\rString[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw);\rfor (String propertyName : propertyNames) {\rif (containsBean(propertyName)) {\r// 递归调用getBean()\rObject bean = getBean(propertyName);\rpvs.add(propertyName, bean);\rregisterDependentBean(propertyName, beanName);\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Added autowiring by name from bean name '\u0026quot; + beanName +\r\u0026quot;' via property '\u0026quot; + propertyName + \u0026quot;' to bean named '\u0026quot; + propertyName + \u0026quot;'\u0026quot;);\r}\r}\relse {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Not autowiring property '\u0026quot; + propertyName + \u0026quot;' of bean '\u0026quot; + beanName +\r\u0026quot;' by name: no matching bean found\u0026quot;);\r}\r}\r}\r}\r 这里面主要流程是在传入的参数pvs中找出已经加载的bean，并递归实例化，进而加入到pvs中\n  将所有PropertyValues中的属性填充至BeanWrapper中\n现在已经完成了对所有注入属性的获取，但是获取的属性是以PropertyValues形式存在的，还并没有应用到已经实例化的bean中，这一工作是在applyPropertyValues中完成。\n  初始化bean:initializeBean() ​\tSpring中程序已经执行过bean的实例化，并且进行了属性的填充，而就在这时将会调用用户设定的初始化方法。\n循环依赖检查 如果存在循环依赖，所以此时候代理对象还在二级缓存里~~~（备注：本利讲解的是自己被循环依赖了的情况） so，此处getSingleton()，就会把里面的对象拿出来，我们知道此时候它已经是个Proxy代理对象~~~\r最后赋值给 exposedObject 然后return出去，进而最终被addSingleton()添加进一级缓存里面去 这样就保证了我们容器里**最终实际上是代理对象**，而非原始对象~~~~~\r 从spring的启动开始 ​\t前面看了那么多觉得这本书讲的很多东西都没啥用，这一章讲的就是项目用到的spring的启动方式。\npublic class AccpServiceBoostrap {\rprivate static final Logger logger = Logger.getLogger(AccpServiceBoostrap.class);\rpublic static void main(String[] args) {\rlogger.info(\u0026quot;starting accp service\u0026quot;);\r@SuppressWarnings({ \u0026quot;resource\u0026quot;, \u0026quot;unused\u0026quot; })\rApplicationContext applicationContext = new ClassPathXmlApplicationContext(\u0026quot;classpath*:applicationContext.xml\u0026quot;);\rlogger.info(\u0026quot;accp service start ok.\u0026quot;);\r}\r}\r ApplicationContext和BeanFacotry两者都是用于加载Bean的，但是相比之下，ApplicationContext提供了更多的扩展功能，简单一点说：ApplicationContext包含BeanFactory的所有功能。通常建议比BeanFactory优先，除非在一些限制的场合，比如字节长度对内存有很大的影响时\n接下来，就跟着这本书将这句代码读通吧，冲啊。\n/**\r* Create a new ClassPathXmlApplicationContext, loading the definitions\r* from the given XML file and automatically refreshing the context.\r* @param configLocation resource location\r* @throws BeansException if context creation failed\r*/\rpublic ClassPathXmlApplicationContext(String configLocation) throws BeansException {\rthis(new String[] {configLocation}, true, null);\r}\r 注释翻译：创建一个新的ClassPathXmlApplicationContext，从给定的XML文件中加载定义，并自动刷新上下文\npublic ClassPathXmlApplicationContext(\rString[] configLocations, boolean refresh, @Nullable ApplicationContext parent)\rthrows BeansException {\r// 执行构造函数\rsuper(parent);\r// 设置配置路径,解析给定的路径数组\rsetConfigLocations(configLocations);\rif (refresh) {\r// 核心方法\rrefresh();\r}\r}\r public void setConfigLocations(@Nullable String... locations) {\rif (locations != null) {\rAssert.noNullElements(locations, \u0026quot;Config locations must not be null\u0026quot;);\rthis.configLocations = new String[locations.length];\rfor (int i = 0; i \u0026lt; locations.length; i++) {\rthis.configLocations[i] = resolvePath(locations[i]).trim();\r}\r}\relse {\rthis.configLocations = null;\r}\r}\r 此函数主要用于解析给定的路径数组，当然，如果数组中包含特殊符号，如${var}占位符，那么在resolvePath中会搜寻匹配的系统变量并替换。\n比如我们项目中\nApplicationContext applicationContext = new ClassPathXmlApplicationContext(\u0026quot;classpath*:applicationContext.xml\u0026quot;);\r 这里面classpath*就是占位符的使用相当于，\nclasspath：只会到你的class路径中查找找文件。\nclasspath*：不仅包含class路径，还包括jar文件中（class路径）进行查找。\n主要讲一下核心方法 refresh()\npublic void refresh() throws BeansException, IllegalStateException {\rsynchronized (this.startupShutdownMonitor) {\rStartupStep contextRefresh = this.applicationStartup.start(\u0026quot;spring.context.refresh\u0026quot;);\r// Prepare this context for refreshing.\r// 准备环境\rprepareRefresh();\r// Tell the subclass to refresh the internal bean factory.\r// 初始化BeanFactory，并进行xml文件读取\rConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\r// Prepare the bean factory for use in this context.\r// 对BeanFactory进行各种功能填充\rprepareBeanFactory(beanFactory);\rtry {\r// Allows post-processing of the bean factory in context subclasses.\rpostProcessBeanFactory(beanFactory);\rStartupStep beanPostProcess = this.applicationStartup.start(\u0026quot;spring.context.beans.post-process\u0026quot;);\r// Invoke factory processors registered as beans in the context.\rinvokeBeanFactoryPostProcessors(beanFactory);\r// Register bean processors that intercept bean creation.\rregisterBeanPostProcessors(beanFactory);\rbeanPostProcess.end();\r// Initialize message source for this context.\rinitMessageSource();\r// Initialize event multicaster for this context.\rinitApplicationEventMulticaster();\r// Initialize other special beans in specific context subclasses.\r// 留给子类来初始化其它的bean\ronRefresh();\r// Check for listener beans and register them.\rregisterListeners();\r// Instantiate all remaining (non-lazy-init) singletons.\r// 初始化剩下的单例bean(no-lazy)\rfinishBeanFactoryInitialization(beanFactory);\r// Last step: publish corresponding event.\r// 通知生命周期处理器刷新过程，启动bean的生命周期，同时发出事件通知监听者。\rfinishRefresh();\r}\rcatch (BeansException ex) {\rif (logger.isWarnEnabled()) {\rlogger.warn(\u0026quot;Exception encountered during context initialization - \u0026quot; +\r\u0026quot;cancelling refresh attempt: \u0026quot; + ex);\r}\r// Destroy already created singletons to avoid dangling resources.\rdestroyBeans();\r// Reset 'active' flag.\rcancelRefresh(ex);\r// Propagate exception to caller.\rthrow ex;\r}\rfinally {\r// Reset common introspection caches in Spring's core, since we\r// might not ever need metadata for singleton beans anymore...\rresetCommonCaches();\rcontextRefresh.end();\r}\r}\r}\r 环境准备 protected void prepareRefresh() {\r// Switch to active.\rthis.startupDate = System.currentTimeMillis();\rthis.closed.set(false);\rthis.active.set(true);\rif (logger.isDebugEnabled()) {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Refreshing \u0026quot; + this);\r}\relse {\rlogger.debug(\u0026quot;Refreshing \u0026quot; + getDisplayName());\r}\r}\r// Initialize any placeholder property sources in the context environment.\rinitPropertySources();\r// Validate that all properties marked as required are resolvable:\r// see ConfigurablePropertyResolver#setRequiredProperties\rgetEnvironment().validateRequiredProperties();\r// Store pre-refresh ApplicationListeners...\rif (this.earlyApplicationListeners == null) {\rthis.earlyApplicationListeners = new LinkedHashSet\u0026lt;\u0026gt;(this.applicationListeners);\r}\relse {\r// Reset local application listeners to pre-refresh state.\rthis.applicationListeners.clear();\rthis.applicationListeners.addAll(this.earlyApplicationListeners);\r}\r// Allow for the collection of early ApplicationEvents,\r// to be published once the multicaster is available...\rthis.earlyApplicationEvents = new LinkedHashSet\u0026lt;\u0026gt;();\r}\r 这个函数其实没什么逻辑，主要可以用来继承完成一些操作\n initPropertySources正符合Spring的开放式结构设计，给用户最大扩展Spring的能力。用户可以根据自身的需要重写initPropertySources方法，并在方法中进行个性化的属性处理及设置。 validateRequiredProperties则是对属性进行验证  一般在对一些配置文件进行检查时可以使用到。\n加载BeanFactory protected ConfigurableListableBeanFactory obtainFreshBeanFactory() {\r//核心逻辑\rrefreshBeanFactory();\rreturn getBeanFactory();\r}\rprotected final void refreshBeanFactory() throws BeansException {\rif (hasBeanFactory()) {\rdestroyBeans();\rcloseBeanFactory();\r}\rtry {\rDefaultListableBeanFactory beanFactory = createBeanFactory();\rbeanFactory.setSerializationId(getId());\r// 设置一些值\rcustomizeBeanFactory(beanFactory);\r// 加载BeanDefinition\rloadBeanDefinitions(beanFactory);\rthis.beanFactory = beanFactory;\r}\rcatch (IOException ex) {\rthrow new ApplicationContextException(\u0026quot;I/O error parsing bean definition source for \u0026quot; + getDisplayName(), ex);\r}\r}\r 这段代码的逻辑就是设置一些值，比如是否允许循环依赖等，然后加载BeanDefinition，其实就是进行bean的解析和注册，也就是最开始看的那些东西具体流程参考目录 解析及注册BeanDefinitions功能扩展 进入函数prepareBeanFactory前，Spring已经完成了对配置的解析，而ApplicationContext在功能上的扩展也由此展开。这里没太看懂啊，\n 增加对SPEL语言的支持。 增加对属性编辑器的支持。 增加对一些内置类，比如EnvironmentAware、MessageSourceAware的信息注入。 设置了依赖功能可忽略的接口。 注册一些固定依赖的属性。 增加AspectJ的支持（会在第7章中进行详细的讲解）。 将相关环境变量及属性注册以单例模式注册。  BeanFactory的后处理 主要是处理配置，Spring IoC容器允许BeanFactoryPostProcessor在容器实际实例化任何其他的bean之前读取配置元数据，并有可能修改它。当Spring加载任何实现了这个接口的bean的配置时，都会在bean工厂载入所有bean的配置之后执行postProcessBeanFactory方法。在PropertyResourceConfigurer类中实现了postProcessBeanFactory方法，在方法中先后调用了mergeProperties、convertProperties、processProperties这3个方法，分别得到配置，将得到的配置转换为合适的类型，最后将配置内容告知BeanFactory。正是通过实现BeanFactoryPostProcessor接口，BeanFactory会在实例化任何bean之前获得配置信息，从而能够正确解析bean描述文件中的变量引用。\n初始化非延迟加载单例 完成BeanFactory的初始化工作，其中包括ConversionService的设置、配置冻结以及非延迟加载的bean的初始化工作。\nprotected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) {\r// Initialize conversion service for this context.\rif (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) \u0026amp;\u0026amp;\rbeanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) {\rbeanFactory.setConversionService(\rbeanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class));\r}\r// Register a default embedded value resolver if no bean post-processor\r// (such as a PropertyPlaceholderConfigurer bean) registered any before:\r// at this point, primarily for resolution in annotation attribute values.\rif (!beanFactory.hasEmbeddedValueResolver()) {\rbeanFactory.addEmbeddedValueResolver(strVal -\u0026gt; getEnvironment().resolvePlaceholders(strVal));\r}\r// Initialize LoadTimeWeaverAware beans early to allow for registering their transformers early.\rString[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false);\rfor (String weaverAwareName : weaverAwareNames) {\rgetBean(weaverAwareName);\r}\r// Stop using the temporary ClassLoader for type matching.\rbeanFactory.setTempClassLoader(null);\r// Allow for caching all bean definition metadata, not expecting further changes.\r// 冻结所有的bean定义，说明注册的bean定义将不被修改或进行任何进一步的处理。\rbeanFactory.freezeConfiguration();\r// Instantiate all remaining (non-lazy-init) singletons.\r// 初始化剩下的单例bean(no - lazy)\rbeanFactory.preInstantiateSingletons();\r}\r 重点就是 preInstantiateSingletons() 这个方法，ApplicationContext实现的默认行为就是在启动时将所有单例bean提前进行实例化。提前实例化意味着作为初始化过程的一部分，ApplicationContext实例会创建并配置所有的单例bean。\npublic void preInstantiateSingletons() throws BeansException {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Pre-instantiating singletons in \u0026quot; + this);\r}\r// Iterate over a copy to allow for init methods which in turn register new bean definitions.\r// While this may not be part of the regular factory bootstrap, it does otherwise work fine.\rList\u0026lt;String\u0026gt; beanNames = new ArrayList\u0026lt;\u0026gt;(this.beanDefinitionNames);\r// Trigger initialization of all non-lazy singleton beans...\rfor (String beanName : beanNames) {\rRootBeanDefinition bd = getMergedLocalBeanDefinition(beanName);\rif (!bd.isAbstract() \u0026amp;\u0026amp; bd.isSingleton() \u0026amp;\u0026amp; !bd.isLazyInit()) {\rif (isFactoryBean(beanName)) {\rObject bean = getBean(FACTORY_BEAN_PREFIX + beanName);\rif (bean instanceof FactoryBean) {\rFactoryBean\u0026lt;?\u0026gt; factory = (FactoryBean\u0026lt;?\u0026gt;) bean;\rboolean isEagerInit;\rif (System.getSecurityManager() != null \u0026amp;\u0026amp; factory instanceof SmartFactoryBean) {\risEagerInit = AccessController.doPrivileged(\r(PrivilegedAction\u0026lt;Boolean\u0026gt;) ((SmartFactoryBean\u0026lt;?\u0026gt;) factory)::isEagerInit,\rgetAccessControlContext());\r}\relse {\risEagerInit = (factory instanceof SmartFactoryBean \u0026amp;\u0026amp;\r((SmartFactoryBean\u0026lt;?\u0026gt;) factory).isEagerInit());\r}\rif (isEagerInit) {\rgetBean(beanName);\r}\r}\r}\relse {\rgetBean(beanName);\r}\r}\r}\r// Trigger post-initialization callback for all applicable beans...\rfor (String beanName : beanNames) {\rObject singletonInstance = getSingleton(beanName);\rif (singletonInstance instanceof SmartInitializingSingleton) {\rStartupStep smartInitialize = this.getApplicationStartup().start(\u0026quot;spring.beans.smart-initialize\u0026quot;)\r.tag(\u0026quot;beanName\u0026quot;, beanName);\rSmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance;\rif (System.getSecurityManager() != null) {\rAccessController.doPrivileged((PrivilegedAction\u0026lt;Object\u0026gt;) () -\u0026gt; {\rsmartSingleton.afterSingletonsInstantiated();\rreturn null;\r}, getAccessControlContext());\r}\relse {\rsmartSingleton.afterSingletonsInstantiated();\r}\rsmartInitialize.end();\r}\r}\r}\r 这个方法核心就是getBean，其实又回到了bean 的加载finishRefresh ​\t在Spring中还提供了Lifecycle接口，Lifecycle中包含start/stop方法，实现此接口后Spring会保证在启动的时候调用其start方法开始生命周期，并在Spring关闭的时候调用stop方法来结束生命周期，通常用来配置后台程序，在启动后一直运行（如对MQ进行轮询等）。而finishRefresh正是保证了这一功能的实现\nprotected void finishRefresh() {\r// Clear context-level resource caches (such as ASM metadata from scanning).\rclearResourceCaches();\r// Initialize lifecycle processor for this context.\rinitLifecycleProcessor();\r// Propagate refresh to lifecycle processor first.\rgetLifecycleProcessor().onRefresh();\r// Publish the final event.\rpublishEvent(new ContextRefreshedEvent(this));\r// Participate in LiveBeansView MBean, if active.\rif (!IN_NATIVE_IMAGE) {\rLiveBeansView.registerApplicationContext(this);\r}\r}\r   当ApplicationContext启动或停止时，它会通过LifecycleProcessor来与所有声明的bean的周期做状态更新，而在LifecycleProcessor的使用前首先需要初始化,initLifecycleProcessor() 这个函数就是来初始化LifecycleProcessor这个bean的。\n  **onRefresh()**启动所有实现了Lifecycle接口的bean。\npublic void onRefresh() {\rstartBeans(true);\rthis.running = true;\r}\r// 这个函数的写法就很有意思\rprivate void startBeans(boolean autoStartupOnly) {\r// 获得bean\rMap\u0026lt;String, Lifecycle\u0026gt; lifecycleBeans = getLifecycleBeans();\rMap\u0026lt;Integer, LifecycleGroup\u0026gt; phases = new TreeMap\u0026lt;\u0026gt;();\rlifecycleBeans.forEach((beanName, bean) -\u0026gt; {\rif (!autoStartupOnly || (bean instanceof SmartLifecycle \u0026amp;\u0026amp; ((SmartLifecycle) bean).isAutoStartup())) {\rint phase = getPhase(bean);\rphases.computeIfAbsent(\rphase,\rp -\u0026gt; new LifecycleGroup(phase, this.timeoutPerShutdownPhase, lifecycleBeans, autoStartupOnly)\r).add(beanName, bean);\r}\r});\r// 启动bean\rif (!phases.isEmpty()) {\rphases.values().forEach(LifecycleGroup::start);\r}\r}\r   当完成ApplicationContext初始化的时候，要通过Spring中的事件发布机制来发出Context RefreshedEvent事件，以保证对应的监听器可以做进一步的逻辑处理， publishEvent 这个函数就是用来发布事件的。\n  bean的生命周期，课外知识 bean的生命周期主要是这些，实例化 -\u0026gt; 属性赋值 -\u0026gt; 初始化 -\u0026gt; 销毁， 网上那张比较流行的图其实是对这些步骤的扩展\n这几个过程分别对应的方法为：\n createBeanInstance() -\u0026gt; 实例化 populateBean() -\u0026gt; 属性赋值 initializeBean() -\u0026gt; 初始化  主要扩展点为\n将这几个扩展点分开来说就是网上这张图\nspring 管理bean 主要是注册 bean 和 装配bean\n读到这里其实就已经把spring的启动全部读完了，按照自己的理解其实就是可以分为几步：\n 注册bean，即将bean从配置文件中读取出来，注册到list中 提前实例化非延迟加载的单例 bean ，这就是把bean赋值了 启动bean的生命周期  整个流程中发现并没有对注解注入的bean进行注册也就是说目前spring中是不可能有@Service这些注解注解的bean的，那么为什么能启动呢？\n​\t\u0026lt; context:component-scan\u0026gt;隐式地启用了\u0026lt; context:annotation-config\u0026gt;的功能。\u0026lt; context:annotation-config\u0026gt;的作用是让Spring具备解析@Component等注解的功能，通过这个配置使得使用也可以得到注解注释的bean。\n\u0026lt;context:annotation-config /\u0026gt;\r ​\t同样 \u0026lt; aop:aspectj-autoproxy/\u0026gt; 开启支持 aop\nAOP 感觉这本书的aop讲的并不是很好，以后有机会可以再找书看看。\n首先一个使用aop的小栗子\n@Component\rpublic class AopStudy {\rpublic void test(){\rSystem.out.println(\u0026quot;真正的方法执行啦\u0026quot;);\r}\r}\r@Component\r@Aspect\rpublic class AopRun {\r// 定义切面\r@Pointcut(\u0026quot;execution(* com.zt.javastudy.grammar.*.test(..))\u0026quot;)\rpublic void test(){\r}\r@Before(\u0026quot;test()\u0026quot;)\rpublic void beforeTest(){\rSystem.out.println(\u0026quot;beforeTest\u0026quot;);\r}\r@After(\u0026quot;test()\u0026quot;)\rpublic void afterTest(){\rSystem.out.println(\u0026quot;afterTest\u0026quot;);\r}\r@Around(\u0026quot;test()\u0026quot;)\rpublic void arountTest(ProceedingJoinPoint point){\rSystem.out.println(\u0026quot;around1\u0026quot;);\rtry {\rpoint.proceed();\r} catch (Throwable throwable) {\rthrowable.printStackTrace();\r}\rSystem.out.println(\u0026quot;around2\u0026quot;);\r}\r}\r 测试\n@Slf4j\r@RunWith(SpringRunner.class)\r@SpringBootTest()\rpublic class AopStudyTest {\r@Autowired\rprivate AopStudy aopStudy;\r@Test\rpublic void test(){\raopStudy.test();\r}\r}\r/**\raround1\rbeforeTest\r真正的方法执行啦\rafterTest\raround2\r*/\r aop使用起来确实是方便，只需要在代理bean上加上@Aspect 注解，然后在类中定义切面，再使用相对应的注解，就可以在方法执行的不同时刻拓展不同的功能。但是有句话叫做简曰但不简单，aop的源码确实比较复杂，没怎么看明白。\n在xml中\u0026lt; aop:aspectj-autoproxy/\u0026gt; 通过这个注解开启对aop的支持，之前讲过Spring中的自定义注解，如果声明了自定义的注解，那么就一定会在程序中的某个地方注册了对应的解析器。\naop部分的解析器由AopNamespaceHandler注册，其init方法\npublic void init() {\r// In 2.0 XSD as well as in 2.5+ XSDs\rregisterBeanDefinitionParser(\u0026quot;config\u0026quot;, new ConfigBeanDefinitionParser());\rregisterBeanDefinitionParser(\u0026quot;aspectj-autoproxy\u0026quot;, new AspectJAutoProxyBeanDefinitionParser());\rregisterBeanDefinitionDecorator(\u0026quot;scoped-proxy\u0026quot;, new ScopedProxyBeanDefinitionDecorator());\r// Only in 2.0 XSD: moved to context namespace in 2.5+\rregisterBeanDefinitionParser(\u0026quot;spring-configured\u0026quot;, new SpringConfiguredBeanDefinitionParser());\r}\r 在解析配置文件的时候，一旦遇到aspectj-autoproxy注解时就会使用解析器AspectJAutoProxyBeanDefinitionParser进行解析,所有解析器，因为是对BeanDefinitionParser接口的统一实现，入口都是从parse函数开始的，AspectJAutoProxyBeanDefinitionParser的parse函数如下\npublic BeanDefinition parse(Element element, ParserContext parserContext) {\r// 注册 核心代码\rAopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element);\r// 对于注解中子类的处理\rextendBeanDefinition(element, parserContext);\rreturn null;\r}\rpublic static void registerAspectJAnnotationAutoProxyCreatorIfNecessary(\rParserContext parserContext, Element sourceElement) {\r// 注册或者升级AnnotationAwareAspectJAutoProxyCreator\rBeanDefinition beanDefinition = AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(\rparserContext.getRegistry(), parserContext.extractSource(sourceElement));\r// 处理proxy-target-class以及expose-proxy属性\ruseClassProxyingIfNecessary(parserContext.getRegistry(), sourceElement);\r// 注册组件并通知，便于监听器做进一步处理\rregisterComponentIfNecessary(beanDefinition, parserContext);\r}\r 注册或者升级AnnotationAwareAspectJAutoProxyCreator ​\t这里的东西感觉书上讲的不是很清楚，简单来说就是实现了自动注册AnnotationAwareAspectJAutoProxyCreator类的功能。\n处理proxy-target-class以及expose-proxy属性   proxy-target-class\n这个属性的配置其实就是说采用哪种代理的方式，默认为false，Spring AOP部分使用JDK动态代理或者CGLIB来为目标对象创建代理，当这个属性为true时就会使用cglib来创建代理对象，\n  expose-proxy\n代表是否将代理bean暴露给用户，如果暴露，可以通过Spring AopContext类获得，默认不暴露，典型的可以用来事务@Transactional失效的问题详情见：\nhttps://www.cnblogs.com/2YSP/p/11748389.html\n  经过这个代码的处理，完成了对AnnotationAwareAspectJAutoProxyCreator类型的自动注册。\nAnnotationAwareAspectJAutoProxyCreator这个类就为我们进行aop的逻辑处理，但是到这里为止其实只是证明了spring现在有处理aop的能力了，也知道了spring有处理的能力是因为AnnotationAwareAspectJAutoProxyCreator这个类，但是怎么处理的？何时调用这个类的方法进行对象功能的增加并返回代理对象的？下面将进行这些的解释。\n​\t让我们把思绪拉到spring bean的注册在bean的注册中有这么一段代码\ntry {\r// Give BeanPostProcessors a chance to return a proxy instead of the target bean instance.\rObject bean = resolveBeforeInstantiation(beanName, mbdToUse);\rif (bean != null) {\rreturn bean;\r}\rprotected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) {\rObject bean = null;\rif (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) {\r// Make sure bean class is actually resolved at this point.\rif (!mbd.isSynthetic() \u0026amp;\u0026amp; hasInstantiationAwareBeanPostProcessors()) {\rClass\u0026lt;?\u0026gt; targetType = determineTargetType(beanName, mbd);\rif (targetType != null) {\rbean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName);\rif (bean != null) {\rbean = applyBeanPostProcessorsAfterInitialization(bean, beanName);\r}\r}\r}\rmbd.beforeInstantiationResolved = (bean != null);\r}\rreturn bean;\r}\r 其中有两个方法，applyBeanPostProcessorsBeforeInstantiation、applyBeanPostProcessorsAfterInitialization这两个方法里面去调用了BeanPostProcessor中的postProcessBeforeInitialization和postProcessAfterInitialization这两个方法，而AnnotationAwareAspectJAutoProxyCreator实现了BeanPostProcessor接口**，所以当Spring加载这个Bean时会在实例化前调用它的postProcessBeforeInitialization和postProcessAfterInitialization方法**\n这两个方法也就是做了一件事情创建AOP代理\n创建AOP代理 这两个方法基本逻辑 一样，这里分析postProcessAfterInitialization这个方法\n/**\r* Create a proxy with the configured interceptors if the bean is\r* identified as one to proxy by the subclass.\r* @see #getAdvicesAndAdvisorsForBean\r*/\r// 注释就告诉我们这是创建aop代理的地方\r@Override\rpublic Object postProcessAfterInitialization(@Nullable Object bean, String beanName) {\rif (bean != null) {\rObject cacheKey = getCacheKey(bean.getClass(), beanName);\rif (this.earlyProxyReferences.remove(cacheKey) != bean) {\r// 关键方法 如果需要被代理那么就封装指定的bean\rreturn wrapIfNecessary(bean, beanName, cacheKey);\r}\r}\rreturn bean;\r}\rprotected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) {\rif (StringUtils.hasLength(beanName) \u0026amp;\u0026amp; this.targetSourcedBeans.contains(beanName)) {\rreturn bean;\r}\rif (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) {\rreturn bean;\r}\r// 给定的bean是否一个基础设施类如果是就不需要增强\rif (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) {\rthis.advisedBeans.put(cacheKey, Boolean.FALSE);\rreturn bean;\r}\r// Create proxy if we have advice.\r// 获取增加方法或增强器\rObject[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null);\rif (specificInterceptors != DO_NOT_PROXY) {\rthis.advisedBeans.put(cacheKey, Boolean.TRUE);\r// 创建代理\rObject proxy = createProxy(\rbean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean));\rthis.proxyTypes.put(cacheKey, proxy.getClass());\rreturn proxy;\r}\rthis.advisedBeans.put(cacheKey, Boolean.FALSE);\rreturn bean;\r}\r 基础设施类包括：Spring跳过的是适用于当前bean的Advisor的Advice/Aspect对象，说人话就是我们定义的切面注解@AspectJ\n获取增强方法或者增强器 protected Object[] getAdvicesAndAdvisorsForBean(\rClass\u0026lt;?\u0026gt; beanClass, String beanName, @Nullable TargetSource targetSource) {\rList\u0026lt;Advisor\u0026gt; advisors = findEligibleAdvisors(beanClass, beanName);\rif (advisors.isEmpty()) {\rreturn DO_NOT_PROXY;\r}\rreturn advisors.toArray();\r}\rprotected List\u0026lt;Advisor\u0026gt; findEligibleAdvisors(Class\u0026lt;?\u0026gt; beanClass, String beanName) {\r// 获取所有的增强\rList\u0026lt;Advisor\u0026gt; candidateAdvisors = findCandidateAdvisors();\r// 寻找所有增强中适用于bean的增强并应用\rList\u0026lt;Advisor\u0026gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName);\rextendAdvisors(eligibleAdvisors);\rif (!eligibleAdvisors.isEmpty()) {\religibleAdvisors = sortAdvisors(eligibleAdvisors);\r}\rreturn eligibleAdvisors;\r}\r 获取增强器 protected List\u0026lt;Advisor\u0026gt; findCandidateAdvisors() {\r// Add all the Spring advisors found according to superclass rules.\rList\u0026lt;Advisor\u0026gt; advisors = super.findCandidateAdvisors();\r// Build Advisors for all AspectJ aspects in the bean factory.\rif (this.aspectJAdvisorsBuilder != null) {\radvisors.addAll(this.aspectJAdvisorsBuilder.buildAspectJAdvisors());\r}\rreturn advisors;\r}\r 这里的代码过于复杂，主要思路就是\n 获取所有beanName，这一步骤中所有在beanFacotry中注册的Bean都会被提取出来。 遍历所有beanName，并找出声明AspectJ注解的类，进行进一步的处理。 对标记为AspectJ注解的类进行增强器的提取。 将提取结果加入缓存  代码首先完成了对增强器的获取，包括获取注解以及根据注解生成增强的步骤，然后考虑到在配置中可能会将增强配置成延迟初始化，那么需要在首位加入同步实例化增强器以保证增强使用之前的实例化，最后是对DeclareParents注解的获取。\n寻找匹配的增强器 protected List\u0026lt;Advisor\u0026gt; findAdvisorsThatCanApply(\rList\u0026lt;Advisor\u0026gt; candidateAdvisors, Class\u0026lt;?\u0026gt; beanClass, String beanName) {\rProxyCreationContext.setCurrentProxiedBeanName(beanName);\rtry {\rreturn AopUtils.findAdvisorsThatCanApply(candidateAdvisors, beanClass);\r}\rfinally {\rProxyCreationContext.setCurrentProxiedBeanName(null);\r}\r}\r 上个函数中已经完成了所有增强器的解析，但是对于所有增强器来讲，并不一定都适用于当前的Bean，还要挑取出适合的增强器，也就是满足我们配置的通配符的增强器，这个函数就完成了这一工作。\n创建代理 ​\t就是真正创建代理的地方\nprotected Object createProxy(Class\u0026lt;?\u0026gt; beanClass, @Nullable String beanName,\r@Nullable Object[] specificInterceptors, TargetSource targetSource) {\rif (this.beanFactory instanceof ConfigurableListableBeanFactory) {\rAutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass);\r}\rProxyFactory proxyFactory = new ProxyFactory();\rproxyFactory.copyFrom(this);\rif (!proxyFactory.isProxyTargetClass()) {\rif (shouldProxyTargetClass(beanClass, beanName)) {\rproxyFactory.setProxyTargetClass(true);\r}\relse {\revaluateProxyInterfaces(beanClass, proxyFactory);\r}\r}\rAdvisor[] advisors = buildAdvisors(beanName, specificInterceptors);\rproxyFactory.addAdvisors(advisors);\rproxyFactory.setTargetSource(targetSource);\rcustomizeProxyFactory(proxyFactory);\rproxyFactory.setFrozen(this.freezeProxy);\rif (advisorsPreFiltered()) {\rproxyFactory.setPreFiltered(true);\r}\rreturn proxyFactory.getProxy(getProxyClassLoader());\r}\r public Object getProxy(@Nullable ClassLoader classLoader) {\rreturn createAopProxy().getProxy(classLoader);\r}\rprotected final synchronized AopProxy createAopProxy() {\rif (!this.active) {\ractivate();\r}\rreturn getAopProxyFactory().createAopProxy(this);\r}\r// 真正的代码\rpublic AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException {\rif (!IN_NATIVE_IMAGE \u0026amp;\u0026amp;\r(config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config))) {\rClass\u0026lt;?\u0026gt; targetClass = config.getTargetClass();\rif (targetClass == null) {\rthrow new AopConfigException(\u0026quot;TargetSource cannot determine target class: \u0026quot; +\r\u0026quot;Either an interface or a target is required for proxy creation.\u0026quot;);\r}\rif (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) {\rreturn new JdkDynamicAopProxy(config);\r}\rreturn new ObjenesisCglibAopProxy(config);\r}\relse {\rreturn new JdkDynamicAopProxy(config);\r}\r}\r 上面的代码主要是确定使用哪种代理方式进行代理，总的来说就是：\n 如果指定了(proxy-target-classs设为true)使用Cglib，那么就会使用Cglib的方式 如果没有指定(或为false)，那么先会检测被代理类是否实现了自己的接口，如果实现了，那么就采用JDK动态代理的方式，如果没有实现那么就走Cglib。  先简单学习 一下两种代理的使用方式\njdk动态代理：\n/**\r* @author zhengtao\r* @description jdk 动态代理学习\r* @date 2021/4/29\r*/\rpublic interface IJdkProxyStudy {\r/**\r* 目标方法\r*/\rvoid add();\r}\rpublic class JdkProxyStudyImpl implements IJdkProxyStudy {\r@Override\rpublic void add() {\rSystem.out.println(\u0026quot;add\u0026quot;);\r}\r}\rpublic class MyInvocationHandler implements InvocationHandler {\r// 目标对象\rprivate Object target;\rpublic MyInvocationHandler(Object target) {\rsuper();\rthis.target = target;\r}\r/**\r* 执行目标对象的方法\r* @param proxy\r* @param method\r* @param args\r* @return\r* @throws Throwable\r*/\r@Override\rpublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\rSystem.out.println(\u0026quot;before\u0026quot;);\r// 执行目标对象的方法\rObject result = method.invoke(target, args);\rSystem.out.println(\u0026quot;after\u0026quot;);\rreturn result;\r}\r/**\r* 获得目标对象的代理对象\r* @return 代理对象\r*/\rpublic Object getProxy(){\rreturn Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), target.getClass().getInterfaces(), this);\r}\r}\r 测试jdk动态代理：\n@Test\rpublic void testJdkProxy(){\r// 接口\rIJdkProxyStudy jdkProxyStudy = new JdkProxyStudyImpl();\rMyInvocationHandler invocationHandler = new MyInvocationHandler(jdkProxyStudy);\rIJdkProxyStudy proxy = (IJdkProxyStudy) invocationHandler.getProxy();\rproxy.add();\r// 没有实现接口的类，使用jdk代理报错\rCglibTest cglibTest = new CglibTest();\rMyInvocationHandler invocationHandler1 = new MyInvocationHandler(cglibTest);\rCglibTest proxy1 = (CglibTest) invocationHandler1.getProxy();\rproxy1.test();\r}\r/** 结果\rbefore\radd\rafter\r*/\r 我们再次来回顾一下使用JDK代理的方式，在整个创建过程中，对于InvocationHandler的创建是最为核心的，在自定义的InvocationHandler中需要重写3个函数。\n 构造函数，将代理的对象传入。 invoke方法，此方法中实现了AOP增强的所有逻辑。 getProxy方法，此方法千篇一律，但是必不可少。  Spring中JDK代理实现：\n invoke方法:  /**\r* Implementation of {@code InvocationHandler.invoke}.\r* \u0026lt;p\u0026gt;Callers will see exactly the exception thrown by the target,\r* unless a hook method throws an exception.\r*/\r@Override\r@Nullable\rpublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\rObject oldProxy = null;\rboolean setProxyContext = false;\rTargetSource targetSource = this.advised.targetSource;\rObject target = null;\rtry {\rif (!this.equalsDefined \u0026amp;\u0026amp; AopUtils.isEqualsMethod(method)) {\r// The target does not implement the equals(Object) method itself.\rreturn equals(args[0]);\r}\relse if (!this.hashCodeDefined \u0026amp;\u0026amp; AopUtils.isHashCodeMethod(method)) {\r// The target does not implement the hashCode() method itself.\rreturn hashCode();\r}\relse if (method.getDeclaringClass() == DecoratingProxy.class) {\r// There is only getDecoratedClass() declared -\u0026gt; dispatch to proxy config.\rreturn AopProxyUtils.ultimateTargetClass(this.advised);\r}\relse if (!this.advised.opaque \u0026amp;\u0026amp; method.getDeclaringClass().isInterface() \u0026amp;\u0026amp;\rmethod.getDeclaringClass().isAssignableFrom(Advised.class)) {\r// Service invocations on ProxyConfig with the proxy config...\rreturn AopUtils.invokeJoinpointUsingReflection(this.advised, method, args);\r}\rObject retVal;\rif (this.advised.exposeProxy) {\r// Make invocation available if necessary.\roldProxy = AopContext.setCurrentProxy(proxy);\rsetProxyContext = true;\r}\r// Get as late as possible to minimize the time we \u0026quot;own\u0026quot; the target,\r// in case it comes from a pool.\rtarget = targetSource.getTarget();\rClass\u0026lt;?\u0026gt; targetClass = (target != null ? target.getClass() : null);\r// Get the interception chain for this method.\rList\u0026lt;Object\u0026gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);\r// Check whether we have any advice. If we don't, we can fallback on direct\r// reflective invocation of the target, and avoid creating a MethodInvocation.\rif (chain.isEmpty()) {\r// We can skip creating a MethodInvocation: just invoke the target directly\r// Note that the final invoker must be an InvokerInterceptor so we know it does\r// nothing but a reflective operation on the target, and no hot swapping or fancy proxying.\rObject[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args);\rretVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse);\r}\relse {\r// We need to create a method invocation...\rMethodInvocation invocation =\rnew ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain);\r// Proceed to the joinpoint through the interceptor chain.\rretVal = invocation.proceed();\r}\r// Massage return value if necessary.\rClass\u0026lt;?\u0026gt; returnType = method.getReturnType();\rif (retVal != null \u0026amp;\u0026amp; retVal == target \u0026amp;\u0026amp;\rreturnType != Object.class \u0026amp;\u0026amp; returnType.isInstance(proxy) \u0026amp;\u0026amp;\r!RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) {\r// Special case: it returned \u0026quot;this\u0026quot; and the return type of the method\r// is type-compatible. Note that we can't help if the target sets\r// a reference to itself in another returned object.\rretVal = proxy;\r}\relse if (retVal == null \u0026amp;\u0026amp; returnType != Void.TYPE \u0026amp;\u0026amp; returnType.isPrimitive()) {\rthrow new AopInvocationException(\r\u0026quot;Null return value from advice does not match primitive return type for: \u0026quot; + method);\r}\rreturn retVal;\r}\rfinally {\rif (target != null \u0026amp;\u0026amp; !targetSource.isStatic()) {\r// Must have come from TargetSource.\rtargetSource.releaseTarget(target);\r}\rif (setProxyContext) {\r// Restore old proxy.\rAopContext.setCurrentProxy(oldProxy);\r}\r}\r}\r 上面的函数中最主要的工作就是创建了一个拦截器链，并使用ReflectiveMethodInvocation类进行了链的封装，而在ReflectiveMethodInvocation类的proceed方法中实现了拦截器的逐一调用\ngetProxy方法  public Object getProxy(@Nullable ClassLoader classLoader) {\rif (logger.isTraceEnabled()) {\rlogger.trace(\u0026quot;Creating JDK dynamic proxy: \u0026quot; + this.advised.getTargetSource());\r}\rreturn Proxy.newProxyInstance(classLoader, this.proxiedInterfaces, this);\r}\r Cglib代理学习：\npublic class CglibTest {\rpublic void test(){\rSystem.out.println(\u0026quot;test\u0026quot;);\r}\r}\rpublic class CgLibProxy implements MethodInterceptor {\r// 目标对象\rprivate Object target;\rpublic CgLibProxy(Object target) {\rsuper();\rthis.target = target;\r}\r@Override\rpublic Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {\rSystem.out.println(\u0026quot;before test\u0026quot;);\rObject result = methodProxy.invokeSuper(o, objects);\rSystem.out.println(\u0026quot;after test\u0026quot;);\rreturn result;\r}\rpublic Object getProxy(){\rEnhancer enhancer = new Enhancer();\renhancer.setSuperclass(this.target.getClass());\renhancer.setCallback(this);\rObject proxy = enhancer.create();\rreturn proxy;\r}\r}\r 测试：\n@Test\rpublic void testCglibProxy(){\rCglibTest cglibTest = new CglibTest();\rCgLibProxy cgLibProxy = new CgLibProxy(cglibTest);\rCglibTest proxy = (CglibTest) cgLibProxy.getProxy();\rproxy.test();\r}\r结果\rbefore test\rtest\rafter test\r spring中怎么实现的就不多讲了。\n静态代理  public interface StaticProxy {\rvoid test();\r}\rpublic class StaticProxyImpl implements StaticProxy {\r@Override\rpublic void test() {\rSystem.out.println(\u0026quot;test\u0026quot;);\r}\r}\rpublic class StaticProxyTest implements StaticProxy {\rprivate StaticProxy staticProxy;\rpublic StaticProxyTest(StaticProxy staticProxy) {\rthis.staticProxy = staticProxy;\r}\r@Override\rpublic void test() {\rSystem.out.println(\u0026quot;before test\u0026quot;);\rthis.staticProxy.test();\rSystem.out.println(\u0026quot;after test\u0026quot;);\r}\r}\r 测试\n@Test\rpublic void testStaticProxy(){\rStaticProxy staticProxy = new StaticProxyTest(new StaticProxyImpl());\rstaticProxy.test();\r}\r结果\rbefore test\rtest\rafter test\r 静态代理感觉起来就是每个类都必须有一个代理类来具体实现，所以就效率不高。\n","id":15,"section":"posts","summary":"spring源码学习 ​ 对于学习spring源码肯定最重要的学习spring的一些理念，比如控制翻转ioc，依赖注入di，面向切面编程aop等","tags":["spring"],"title":"spring源码学习","uri":"https://wzgl998877.github.io/2022/01/spring%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","year":"2022"},{"content":"thrift+zk RPC调用框架 zk 在这个框架中zk其实只是作为了注册中心来使用 (客户端只需要在zk上获得地址，服务端只需要将自己注册在zk上)，所以先了解一下zk的节点的划分。\nZooKeeper 节点是有生命周期的，这取决于节点的类型。在 ZooKeeper 中，节点类型可以分为持久节点（PERSISTENT ）、临时节点（EPHEMERAL），以及时序节点（SEQUENTIAL ），具体在节点创建过程中，一般是组合使用，可以生成以下 4 种节点类型。\n持久节点（PERSISTENT） 所谓持久节点，是指在节点创建后，就一直存在，直到有删除操作来主动清除这个节点——不会因为创建该节点的客户端会话失效而消失。 持久顺序节点（PERSISTENT_SEQUENTIAL） 这类节点的基本特性和上面的节点类型是一致的。额外的特性是，在ZK中，每个父节点会为他的第一级子节点维护一份时序，会记录每个子节点创建的先后顺序。基于这个特性，在创建子节点的时候，可以设置这个属性，那么在创建节点过程中，ZK会自动为给定节点名加上一个数字后缀，作为新的节点名。这个数字后缀的范围是整型的最大值。 临时节点（EPHEMERAL） 和持久节点不同的是，临时节点的生命周期和客户端会话绑定。也就是说，如果客户端会话失效，那么这个节点就会自动被清除掉。注意，这里提到的是会话失效，而非连接断开。另外，在临时节点下面不能创建子节点。\n临时顺序节点（EPHEMERAL_SEQUENTIAL）\n可以用来实现分布式锁\n客户端有关zk的操作：\n","id":16,"section":"posts","summary":"thrift+zk RPC调用框架 zk 在这个框架中zk其实只是作为了注册中心来使用 (客户端只需要在zk上获得地址，服务端只需要将自己注册在zk上)，所以先了解一","tags":["RPC"],"title":"thrift+zk RPC调用框架","uri":"https://wzgl998877.github.io/2022/01/thrift-zk-rpc%E8%B0%83%E7%94%A8%E6%A1%86%E6%9E%B6/","year":"2022"},{"content":"Websocket学习 WebSocket是一种网络传输协议，可在单个TCP连接上进行全双工通信，位于应用层。\n 全双工：  单工（simplex ） : 只能有一个方向的通信而没有反方向的交互。 半双工（half duplex ） ：通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。 全双工（full duplex） : 通信的双方可以同时发送和接收信息。    建立websocket连接的步骤：\n 使用http进行握手，告知服务器通信协议变为websocket  GET /chat HTTP/1.1\rHost: server.example.com\rUpgrade: websocket\rConnection: Upgrade\rSec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==\rOrigin: http://example.com\rSec-WebSocket-Protocol: chat, superchat\rSec-WebSocket-Version: 13\r# 服务器响应\rHTTP/1.1 101 Switching Protocols\rUpgrade: websocket\rConnection: Upgrade\rSec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=\rSec-WebSocket-Protocol: chat\r# ng配置\rlocation / {\rproxy_pass http://127.0.0.1:8110; proxy_set_header Host $http_host; proxy_http_version 1.1; proxy_set_header Upgrade \u0026quot;websocket\u0026quot;;\rproxy_set_header Connection \u0026quot;Upgrade\u0026quot;; }  进行websocket通信  ","id":17,"section":"posts","summary":"Websocket学习 WebSocket是一种网络传输协议，可在单个TCP连接上进行全双工通信，位于应用层。 全双工： 单工（simplex ） :","tags":["Websocket"],"title":"Websocket学习","uri":"https://wzgl998877.github.io/2022/01/websocket%E5%AD%A6%E4%B9%A0/","year":"2022"},{"content":"[TOC]\n图解http 网络模型   OSI 模型(具体每层的定义还需要看书确定这个只是网上看的，可以回去看计网)\n  应用层（application）：最接近终端用户的OSI层，这就意味着OSI应用层与用户之间是通过应用软件直接相互作用的。网络进程访问应用层；提供接口服务。\n  表示层（presention）：数据表现形式；特定功能的实现-比如加密模式确保原始设备上加密的数据可以在目标设备上正确地解密。\n  会话层（session）：主机间通信；对应用会话管理，同步。\n  传输层（transport）:实现端到端传输；分可靠与不可靠传输；在传输前实现错误检测与流量控制，定义端口号（标记相应的服务）。\n  网络层（network）（单位类型：报文）：数据传输；提供逻辑地址，选择路由数据包，负责在源和终点之间建立连接。\n  数据链路层（date link）（单位类型：帧）：访问介质；数据在该层封装成帧；用MAC地址作为访问媒介；具有错误检测与修正功能。MAC描述在共享介质环境中如何进行站的调度、发生和接收数据。MAC确保信息跨链路的可靠传输，对数据传输进行同步，识别错误和控制数据的流向。一般地讲，MAC只在共享介质环境中才是重要的，只有在共享介质环境中多个节点才能连接到同一传输介质上。\n  物理层（physical）（单位类型：比特）：实现比特流的透明传输，物理接口，具有电气特性。\n    TCP/IP 四层模型\n 应用层：应用层决定了向用户提供应用服务时通信的活动。TCP/IP协议族内预存了各类通用的应用服务。比如，FTP（File Transfer Protocol，文件传输协议）和DNS（Domain Name System，域名系统）服务就是其中两类。HTTP协议也处于该层。 传输层：传输层对上层应用层，提供处于网络连接中的两台计算机之间的数据传输。在传输层有两个性质不同的协议：TCP（Transmission Control Protocol，传输控制协议）和UDP（User Data Protocol，用户数据报协议）。 网络层：（又名网络互连层）网络层用来处理在网络上流动的数据包。数据包是网络传输的最小数据单位。该层规定了通过怎样的路径（所谓的传输路线）到达对方计算机，并把数据包传送给对方。与对方计算机之间通过多台计算机或网络设备进行传输时，网络层所起的作用就是在众多的选项内选择一条传输路线。 链路层：（又名数据链路层，网络接口层）用来处理连接网络的硬件部分。包括控制操作系统、硬件的设备驱动、NIC（Network InterfaceCard，网络适配器，即网卡），及光纤等物理可见部分（还包括连接器等一切传输媒介）。硬件上的范畴均在链路层的作用范围之内。    五层模型\nOSI是一个完整的、完善的宏观理论模型;而TCP/IP(参考)模型，更加侧重的是互联网通信核心(也是就是围绕TCP/IP协议展开的一系列通信协议)的分层，因此它不包括物理层，以及其他一些不想干的协议; 而五层模型就是将物理层加进来了而已。\n     OSI TCP/IP 五层模型     应用层 应用层 应用层   表示层 应用层 应用层   会话层 应用层 应用层   传输层 传输层 传输层   网络层 网络层 网络层   数据链路层 数据链路层 数据链路层   物理层 数据链路层 并没有给出具体的物理层的实现 物理层    IP协议 ​\t是TCP/IP协议栈中最核心的协议之一，通过IP地址，保证了联网设备的唯一性，实现了网络通信的面向无连接和不可靠的传输功能。\nARP 地址解析协议 ​\t网络层使用的是IP地址，但在实际网络的链路上传送数据帧时，最终还是必须使用该网络的硬件地址，ARP协议就 实现了从 IP 地址到 MAC 地址的映射，即询问目标 IP 对应的 MAC 地址\nTCP/UDP 协议 ​\tTCP协议全称是传输控制协议是一种面向连接的、可靠的、基于字节流的传输层通信协议。\n​\tUDP协议全称是用户数据报协议是面向无连接、不可靠的、基于报文的的传输层通信协议。\n​\t经典知识点：三次握手，四次挥手\n B的TCP服务器进程先创建传输控制块 TCB，准备接受客户进程的连接请求。然后服务器进程就处于LISTEN（收听）状态，等待客户的连接请求。如有，即作出响应。A的TCP客户进程也是首先创建传输控制模块TCB，然后向B发出连接请求报文段，这时首部中的同步位SYN = 1，同时选择一个初始序号seq = x。TCP规定，SYN报文段（即SYN = 1的报文段）不能携带数据，但要消耗掉一个序号。这时，TCP客户进程进入SYN-SENT（同步已发送）状态。 B收到连接请求报文段后，如同意建立连接，则向A发送确认。在确认报文段中应把SYN位和ACK位都置1，确认号是ack = x + 1，同时也为自己选择一个初始序号seq = y。请注意，这个报文段也不能携带数据，但同样要消耗掉一个序号。这时TCP服务器进程进入SYN-RCVD（同步收到）状态。 TCP客户进程收到B的确认后，还要向B给出确认。确认报文段的ACK置1，确认号ack = y + 1，而自己的序号seq = x + 1。TCP的标准规定，ACK报文段可以携带数据。但如果不携带数据则不消耗序号，在这种情况下，下一个数据报文段的序号仍是seq = x + 1。这时，TCP连接已经建立，A进入ESTABLISHED（已建立连接）状态。当B收到A的确认后，也进入ESTABLISHED状态。  为什么要三次握手？\n​\t这主要是为了防止已失效的连接请求报文段突然又传送到了B，因而产生错误。\n所谓“已失效的连接请求报文段”是这样产生的。考虑一种正常情况。A发出连接请求，但因连接请求报文丢失而未收到确认。于是A再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接。A共发送了两个连接请求报文段，其中第一个丢失，第二个到达了B。没有“已失效的连接请求报文段”。现假定出现一种异常情况，即A发出的第一个连接请求报文段并没有丢失，而是在某些网络结点长时间滞留了，以致延误到连接释放以后的某个时间才到达B。本来这是一个早已失效的报文段。但B收到此失效的连接请求报文段后，就误认为是A又发出一次新的连接请求。于是就向A发出确认报文段，同意建立连接。假定不采用三次握手，那么只要B发出确认，新的连接就建立了。由于现在A并没有发出建立连接的请求，因此不会理睬B的确认，也不会向B发送数据。但B却以为新的运输连接已经建立了，并一直等待A发来数据。B的许多资源就这样白白浪费了。\n 数据传输结束后，通信的双方都可释放连接。现在A和B都处于ESTABLISHED状态（图5-32）。A的应用进程先向其TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把连接释放报文段首部的终止控制位FIN置1，其序号seq = u，它等于前面已传送过的数据的最后一个字节的序号加1。这时A进入FIN-WAIT-1（终止等待1）状态，等待B的确认。请注意，TCP规定，FIN报文段即使不携带数据，它也消耗掉一个序号。 B收到连接释放报文段后即发出确认，确认号是ack = u + 1，而这个报文段自己的序号是v，等于B前面已传送过的数据的最后一个字节的序号加1。然后B就进入CLOSE-WAIT（关闭等待）状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的连接就释放了，这时的TCP连接处于半关闭(half-close)状态，即A已经没有数据要发送了，但B若发送数据，A仍要接收。也就是说，从B到A这个方向的连接并未关闭，这个状态可能会持续一些时间。 A收到来自B的确认后，就进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接。这时B发出的连接释放报文段必须使FIN = 1。现假定B的序号为w（在半关闭状态B可能又发送了一些数据）。B还必须重复上次已发送过的确认号ack = u + 1。这时B就进入LAST-ACK（最后确认）状态，等待A的确认。 A在收到B的连接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ack = w + 1，而自己的序号是seq = u + 1（根据TCP标准，前面发送过的FIN报文段要消耗一个序号）。然后进入到TIME-WAIT（时间等待）状态。请注意，现在TCP连接还没有释放掉。必须经过时间等待计时器(TIME-WAIT timer)设置的时间2MSL后，A才进入到CLOSED状态。  为什么A在TIME-WAIT状态必须等待2MSL的时间呢？这有两个理由。\n第一，为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的B收不到对已发送的FIN + ACK报文段的确认。B会超时重传这个FIN + ACK报文段，而A就能在2MSL时间内收到这个重传的FIN + ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后，A和B都正常进入到CLOSED状态。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后立即释放连接，那么就无法收到B重传的FIN + ACK报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常步骤进入CLOSED状态。第二，防止上一节提到的“已失效的连接请求报文段”出现在本连接中。A在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。B只要收到了A发出的确认，就进入CLOSED状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。我们注意到，B结束TCP连接的时间要比A早一些。\n除时间等待计时器外，TCP还设有一个保活计时器(keepalive timer)。设想有这样的情况：客户已主动与服务器建立了TCP连接。但后来客户端的主机突然出故障。显然，服务器以后就不能再收到客户发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就是使用保活计时器。服务器每收到一次客户的数据，就重新设置保活计时器，时间的设置通常是两小时。若两小时没有收到客户的数据，服务器就发送一个探测报文段，以后则每隔75分钟发送一次。若一连发送10个探测报文段后仍无客户的响应，服务器就认为客户端出了故障，接着就关闭这个连接。\nTCP 重传、滑动窗口、流量控制、拥塞控制 TCP 实现可靠传输的方式之一，是通过序列号与确认应答。\n在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。当上一个数据包收到了应答了， 再发送下一个。\n这个模式就有点像我和你面对面聊天，你一句我一句。但这种方式的缺点是效率比较低的。\n如果你说完一句话，我在处理其他事情，没有及时回复你，那你不是要干等着我做完其他事情后，我回复你，你才能说下一句话，很显然这不现实。\n所以，这样的传输方式有一个缺点：数据包的往返时间越长，通信的效率就越低。\n为了解决这个问题，tcp使用了窗口的概念。\n滑动窗口 有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值。\n发送窗口表示：在没有收到B的确认的情况下，A可以连续把窗口内的数据都发送出去。凡是已经发送过的数据，在未收到确认之前都必须暂时保留（p1~p2），以便在超时重传时使用。\n接收窗口大小是20。在接收窗口外面，到30号为止的数据是已经发送过确认，并且已经交付主机了。因此在B可以不再保留这些数据。接收窗口内的序号（31～50）是允许接收的。B收到了序号为32和33的数据。这些数据没有按序到达，因为序号为31的数据没有收到（也许丢失了，也许滞留在网络中的某处）。请注意，B只能对按序收到的数据中的最高序号给出确认，因此B发送的确认报文段中的确认号仍然是31（即期望收到的序号），而不能是32或33。\n现在假定B收到了序号为31的数据，并把序号为31～33的数据交付主机，然后B删除这些数据。接着把接收窗口向前移动3个序号，同时给A发送确认，其中窗口值仍为20，但确认号是34。这表明B已经收到了到序号33为止的数据。我们注意到，B还收到了序号为37, 38和40的数据，但这些都没有按序到达，只能先暂存在接收窗口中。A收到B的确认后，就可以把发送窗口向前滑动3个序号，但指针P2不动。可以看出，现在A的可用窗口增大了，可发送的序号范围是42～53。\n这就是所谓的滑动窗口。\n重传方式 TCP 针对数据包丢失的情况，会用重传机制解决，重传方式有：\n 超时重传：就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据。 快速重传：是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。 SACK 方法：这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。 Duplicate SACK： 又称 D-SACK，其主要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。  具体文章参考博文：https://www.cnblogs.com/xiaolincoding/p/12732052.html\n流量控制 一般说来，我们总是希望数据传输得更快一些。但如果发送方把数据发送得过快，接收方就可能来不及接收，这就会造成数据的丢失。所谓流量控制(flow control)就是让发送方的发送速率不要太快，要让接收方来得及接收。\n利用滑动窗口机制可以很方便地在TCP连接上实现对发送方的流量控制。\n设A向B发送数据。在连接建立时，B告诉了A：“我的接收窗口rwnd = 400”（这里rwnd表示receiver window）。因此，发送方的发送窗口不能超过接收方给出的接收窗口的数值。请注意，TCP的窗口单位是字节，不是报文段。TCP连接建立时的窗口协商过程在图中没有显示出来。再设每一个报文段为100字节长，而数据报文段序号的初始值设为1（见图中第一个箭头上面的序号seq =1。图中右边的注释可帮助理解整个的过程）。请注意，图中箭头上面大写ACK表示首部中的确认位ACK，小写ack表示确认字段的值。\n我们应注意，接收方的主机B进行了三次流量控制。第一次把窗口减小到rwnd = 300，第二次又减到rwnd = 100，最后减到rwnd = 0，即不允许发送方再发送数据了。这种使发送方暂停发送的状态将持续到主机B重新发出一个新的窗口值为止。我们还应注意到，B向A发送的三个报文段都设置了ACK= 1，只有在ACK = 1时确认号字段才有意义。现在我们考虑一种情况。在图5-22中，B向A发送了零窗口的报文段后不久，B的接收缓存又有了一些存储空间。于是B向A发送了rwnd = 400的报文段。然而这个报文段在传送过程中丢失了。A一直等待收到B发送的非零窗口的通知，而B也一直等待A发送的数据。如果没有其他措施，这种互相等待的死锁局面将一直延续下去。为了解决这个问题，TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口探测报文段（仅携带1字节的数据），而对方就在确认这个探测报文段时给出了现在的窗口值[插图]。如果窗口仍然是零，那么收到这个报文段的一方就重新设置持续计时器。如果窗口仍然是零，那么收到这个报文段的一方就重新设置持续计时器。如果窗口不是零，那么死锁的僵局就可以打破了。\n拥塞控制 在计算机网络中的链路容量（即带宽）、交换结点中的缓存和处理机等，都是网络的资源。在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫做拥塞(congestion)。\n拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞窗口 cwnd是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。\n发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。\n拥塞控制的具体方法有：\n  慢开始(slow-start)：由小到大逐渐增大拥塞窗口数值，通常在刚刚开始发送报文段时，先把拥塞窗口cwnd设置为一个最大报文段MSS的数值[插图]。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。\n  拥塞避免(congestion avoidance)：是让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1\n  快重传(fast retransmit)：首先要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等待自己发送数据时才进行捎带确认，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，由于发送方能尽早重传未被确认的报文段，因此采用快重传后可以使整个网络的吞吐量提高约20%。\n  快恢复(fast recovery)：(1) 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把慢开始门限ssthresh减半。这是为了预防网络发生拥塞。请注意，接下去不执行慢开始算法。(2) 由于发送方现在认为网络很可能没有发生拥塞（如果网络发生了严重的拥塞，就不会一连有好几个报文段连续到达接收方，也就不会导致接收方连续发送重复确认），因此与慢开始不同之处是现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。不懂\n  DNS 域名解析服务 输入网址需要经过哪些协议 ​\t从输入URL到页面加载发生了什么？\n  DNS解析 ，将域名解析为ip地址\n  TCP连接，与服务器建立tcp连接，将http报文分割成报文段，并把每个报文段可靠地传给服务器\n  发送HTTP请求\n  服务器处理请求并返回HTTP报文\n  浏览器解析渲染页面\n  连接结束\n具体可以参考 https://segmentfault.com/a/1190000006879700\n  ​\t​\t表格版\n​\t上图有一个错误，请注意，是OSPF不是OPSF。 OSPF（Open Shortest Path First，ospf）开放最短路径优先协议,是由Internet工程任务组开发的路由选择协议\nhttps http的缺点：\n 通信使用明文（不加密），内容可能会被窃听 不验证通信方的身份，因此有可能遭遇伪装 无法证明报文的完整性，所以有可能已遭篡改  为了统一解决上述这些问题，我们把添加了加密及认证机制的HTTP称为HTTPS（HTTP Secure）。\nHTTPS并非是应用层的一种新协议。只是HTTP通信接口部分用SSL（SecureSocket Layer）和TLS（Transport Layer Security）协议代替而已。通常，HTTP直接和TCP通信。当使用SSL时，则演变成先和SSL通信，再由SSL和TCP通信了。简言之，所谓HTTPS，其实就是身披SSL协议这层外壳的HTTP。\nSSL是独立于HTTP的协议，所以不光是HTTP协议，其他运行在应用层的SMTP和Telnet等协议均可配合SSL协议使用。可以说SSL是当今世界上应用最为广泛的网络安全技术。\n加密方式   对称加密：加密和解密同用一个密钥的方式称为共享密钥加密（Common key cryptosystem），也被叫做对称密钥加密。优点：速度快 缺点：不安全\n  非对称加密：公开密钥加密使用一对非对称的密钥。一把叫做私有密钥（private key），另一把叫做公开密钥（public key）。优点：安全 缺点：速度慢。\n  HTTPS采用共享密钥加密和公开密钥加密两者并用的混合加密机制，在交换密钥环节使用公开密钥加密方式，之后的建立通信交换报文阶段则使用共享密钥加密方式。\n  遗憾的是，公开密钥加密方式还是存在一些问题的。那就是无法证明公开密钥本身就是货真价实的公开密钥。\n比如，正准备和某台服务器建立公开密钥加密方式下的通信时，如何证明收到的公开密钥就是原本预想的那台服务器发行的公开密钥。或许在公开密钥传输途中，真正的公开密钥已经被攻击者替换掉了。\n为了解决上述问题，可以使用由数字证书认证机构（CA,CertificateAuthority）和其相关机关颁发的公开密钥证书。数字证书认证机构的业务流程：\n  首先，服务器的运营人员向数字证书认证机构提出公开密钥的申请。数字证书认证机构在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公钥证书后绑定在一起。\n  服务器会将这份由数字证书认证机构颁发的公钥证书发送给客户端，以进行公开密钥加密方式通信。公钥证书也可叫做数字证书或直接称为证书。\n  接到证书的客户端可使用数字证书认证机构的公开密钥，对那张证书上的数字签名进行验证，一旦验证通过，客户端便可明确两件事：一，认证服务器的公开密钥的是真实有效的数字证书认证机构。二，服务器的公开密钥是值得信赖的。\n  此处认证机关的公开密钥必须安全地转交给客户端。使用通信方式时，如何安全转交是一件很困难的事，因此，多数浏览器开发商发布版本时，会事先在内部植入常用认证机关的公开密钥。\nHTTPS的安全通信机制 步骤1： 客户端通过发送Client Hello报文开始SSL通信。报文中包含客户端支持的SSL的指定版本、加密组件（Cipher Suite）列表（所使用的加密算法及密钥长度等）。\n步骤2： 服务器可进行SSL通信时，会以Server Hello报文作为应答。和客户端一样，在报文中包含SSL版本以及加密组件。服务器的加密组件内容是从接收到的客户端加密组件内筛选出来的。\n步骤3： 之后服务器发送Certificate报文。报文中包含公开密钥证书。\n步骤4： 最后服务器发送Server Hello Done报文通知客户端，最初阶段的SSL握手协商部分结束。\n步骤5: SSL第一次握手结束之后，客户端以Client Key Exchange报文作为回应。报文中包含通信加密中使用的一种被称为Pre-master secret的随机密码串。该报文已用步骤3中的公开密钥进行加密。\n步骤6： 接着客户端继续发送Change Cipher Spec报文。该报文会提示服务器，在此报文之后的通信会采用Pre-master secret密钥加密。\n步骤7： 客户端发送Finished报文。该报文包含连接至今全部报文的整体校验值。这次握手协商是否能够成功，要以服务器是否能够正确解密该报文作为判定标准。\n步骤8： 服务器同样发送Change Cipher Spec报文。\n步骤9： 服务器同样发送Finished报文。\n步骤10： 服务器和客户端的Finished报文交换完毕之后，SSL连接就算建立完成。当然，通信会受到SSL的保护。从此处开始进行应用层协议的通信，即发送HTTP请求。\n步骤11： 应用层协议通信，即发送HTTP响应。\n步骤12： 最后由客户端断开连接。断开连接时，发送close_notify报文。上图做了一些省略，这步之后再发送TCP FIN报文来关闭与TCP的通信。在以上流程中，应用层发送数据。\nmaster key 就是https通信中使用的共享密钥\nhttps的问题  一种是指通信慢。和使用HTTP相比，网络负载可能会变慢2到100倍。除去和TCP连接、发送HTTP请求·响应以外，还必须进行SSL通信，因此整体上处理通信量不可避免会增加。 另一种是指由于大量消耗CPU及内存等资源，导致处理速度变慢。SSL必须进行加密处理。在服务器和客户端都需要进行加密和解密的运算处理。因此从结果上讲，比起HTTP会更多地消耗服务器和客户端的硬件资源，导致负载增强。  ","id":18,"section":"posts","summary":"[TOC] 图解http 网络模型 OSI 模型(具体每层的定义还需要看书确定这个只是网上看的，可以回去看计网) 应用层（application）：最接近终端用户","tags":["http"],"title":"图解http","uri":"https://wzgl998877.github.io/2022/01/%E5%9B%BE%E8%A7%A3http/","year":"2022"},{"content":"数据库知识 一些函数 orcale中的decode() 函数 ​\tdecode(条件,值1，返回值1，值2，返回值2,\u0026hellip;.值n，返回值n，缺省值)；\n​\tdecode(字段或字段的运算，值1，值2，值3）\nselect decode(COUPON_AMOUNT,'40.00','1','2'),COUPON_AMOUNT from T_CHN_WECHAT_LIST;--如果是40.00那么就是1否则就是2\r ​\t用来代替case when then else end\nselect decode(name,'a',id,0) id_1,\rdecode(name,'b',id,0) id_2,\rdecode(name,'c',id,0) id_3 from t_decode;\r-- 相等于：case when then else end\rselect case name when 'a' then id else 0 end as id_1,\rcase name when 'b' then id else 0 end as id_2,\rcase name when 'c' then id else 0 end as id_3 from t_decode;\r 与order by连用\n order by 1,代表按第一个栏位排序，order by 加数字代表按第几个栏位排序。 order by 中认为null是最大值，所以如果是ASC升序则排在最后，DESC降序则排在最前。 order by 与 decode连用可以达到自定义排序的效果。  SELECT t.rowid rd, row_number() over (PARTITION BY t.chn_check_id ORDER BY decode(t.status, '2', '5', t.status), t.trans_time DESC) rn FROM settle_user.t_check_trans_tmp_list t\rWHERE t.chn_check_id IS NOT NULL -- 先根据chn_check_id 分区，然后根据状态和时间排序，其中如果status是2则按照5来排序，其他status就按照本来的值来排序，这里status的取值为0,1,2,3,4；所以5排在最后，所以当status为2时排在最后\r 参考博客：https://blog.csdn.net/qichangjian/article/details/88975499\nhttps://blog.csdn.net/qq_35029061/article/details/82795804?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control\u0026amp;dist_request_id=1328696.1178.16166729427143225\u0026amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control\ncase when CASE WHEN用于SQL语句中的条件判断，Case具有两种格式，简单Case函数和Case搜索函数\n--简单Case函数\rCASE sex WHEN '1' THEN '男'\rWHEN '2' THEN '女'\rELSE '其他' END\r--Case搜索函数\rCASE WHEN sex = '1' THEN '男'\rWHEN sex = '2' THEN '女'\rELSE '其他' END\r 几个注意的点\n 在case when中若不加else，那么不满足条件就为null。 执行到end，如果end后还有语句会继续执行。  CASE WHEN sex = '1' THEN '男'\rWHEN sex = '2' THEN '女'\rELSE '其他' END '太监'\r-- 那么无论是什么都会变为太监 哈哈\r orcale 中的序列 sequence是序列号生成器，可以为表中的行自动生成序列号，产生一组等间隔的数值(类型为数字)。其主要的用途是生成表的主键值，可以在插入语句中引用，在插入之前，获取序列号nextval值，然后进行插入。也可以通过查询检查当前值，或使序列增至下一个值。\n创建序列 create sequence t_mng_user_user_id\rincrement by 1 -- 每次加几个\rstart with 1 -- 从几开始计数\rnomaxvalue -- 不设置最大值\rnocycle -- 一直累加，不循环\rcache 10;\r 这里创建了一个序列，序列名称叫做t_mng_user_user_id 从1开始每次加一\n使用序列 select t_mng_user_user_id.nextval from DUAL; -- 获得增加后的值\rselect t_mng_user_user_id.currval from DUAL; -- 获得当前值\r 使用场景   不包含子查询、snapshot、VIEW的 SELECT 语句\n  INSERT语句的子查询中\n  INSERT语句的VALUES中\n  UPDATE 的 SET中\n例如：\n  update T_MNG_USER set USER_ID = t_mng_user_user_id.nextval where USER_ID = '2021030510101722';\r 集合函数  union 并集运算  select USER_ID from T_MNG_USER union select USER_ID from T_MNG_USER_ROLE ;\r 注意：如果union前面是n列，那么后面也必须是n列，即union前后列数必须相同。而且查询结果的列名是按照union前面n列的名称命名\nINTERSECT 交集运算  select USER_ID from T_MNG_USER intersect select USER_ID from T_MNG_USER_ROLE where USER_ID = '2017112710000009';\r 注意：intersect前面列名和后面列名要相同\nMINUS 差集  select USER_ID from T_MNG_USER where USER_ID in ('2017112710000009','2013061210007957') minus select USER_ID from T_MNG_USER_ROLE where USER_ID = '2017112710000009';\r ","id":19,"section":"posts","summary":"数据库知识 一些函数 orcale中的decode() 函数 ​ decode(条件,值1，返回值1，值2，返回值2,\u0026hellip;.值n，返回值n","tags":null,"title":"数据库知识","uri":"https://wzgl998877.github.io/2022/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9F%A5%E8%AF%86/","year":"2022"},{"content":"[TOC]\n日常问题总结 工作中总是会遇到各种各样的问题，只有总结下来才是一笔财富。\nfastjson的一些技巧 在工作中总是遇到给前端的字段需要是下划线的，这时候可以通过全局配置来实现。\n添加这个配置bean后，所有的http请求都会进行转换，即会将返回参数改为下划线\n@Bean\rpublic HttpMessageConverters fastJsonHttpMessageConverters(){\rFastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter();\rFastJsonConfig fastJsonConfig = new FastJsonConfig();\r// 格式化输出，也就是换行等处理\rfastJsonConfig.setSerializerFeatures(SerializerFeature.PrettyFormat);\rSerializeConfig config = new SerializeConfig();\r// 转为下划线\rconfig.propertyNamingStrategy = PropertyNamingStrategy.SnakeCase;\rfastJsonConfig.setSerializeConfig(config);\rconverter.setFastJsonConfig(fastJsonConfig);\rreturn new HttpMessageConverters(converter);\r}\rCamelCase策略，Java对象属性：personId，序列化后属性：persionId\rPascalCase策略，Java对象属性：personId，序列化后属性：PersonId\rSnakeCase策略，Java对象属性：personId，序列化后属性：person_id\rKebabCase策略，Java对象属性：personId，序列化后属性：person-id\r 例子：\n@Data\rpublic class TestResponse {\rprivate String userName;\rprivate String teacherName;\r}\r @RequestMapping(\u0026quot;/group/test\u0026quot;)\rpublic Object test(HttpServletRequest request, HttpServletResponse response) {\rTestResponse response1 = new TestResponse();\rresponse1.setUserName(\u0026quot;zt\u0026quot;);\rresponse1.setTeacherName(\u0026quot;zt\u0026quot;);\rreturn response1;\r}\r// 返回结果就自动转为了下划线了\r{\r\u0026quot;teacher_name\u0026quot;: \u0026quot;zt\u0026quot;,\r\u0026quot;user_name\u0026quot;: \u0026quot;zt\u0026quot;\r}\r 除了这种全局配置的方式，也可以进行代码层面的配置\n转为string\n@RequestMapping(\u0026quot;/test\u0026quot;)\rpublic String test(HttpServletRequest request, HttpServletResponse response) {\rTestResponse response1 = new TestResponse();\rresponse1.setUserName(\u0026quot;zt\u0026quot;);\rresponse1.setTeacherName(\u0026quot;zt\u0026quot;);\rSerializeConfig config = new SerializeConfig();\rconfig.propertyNamingStrategy = PropertyNamingStrategy.SnakeCase;\r// 返回的json就是下划线的\rreturn JSON.toJSONString(response1, config);\r}\r{\u0026quot;teacher_name\u0026quot;:\u0026quot;zt\u0026quot;,\u0026quot;user_name\u0026quot;:\u0026quot;zt\u0026quot;}\r string转为对象\n@RequestMapping(\u0026quot;/test2\u0026quot;)\rpublic Object test2(HttpServletRequest request, HttpServletResponse response) {\rString response1 = \u0026quot;{\\\u0026quot;teacherName\\\u0026quot;:\\\u0026quot;zt\\\u0026quot;,\\\u0026quot;userName\\\u0026quot;:\\\u0026quot;zt\\\u0026quot;}\u0026quot;;\r// 转为下划线\rParserConfig parserConfig = new ParserConfig();\rparserConfig.propertyNamingStrategy = PropertyNamingStrategy.SnakeCase;\rreturn JSON.parseObject(response1, TestResponse.class, parserConfig);\r}\r// 很奇怪应该不是这样的\r{\r\u0026quot;userName\u0026quot;: \u0026quot;zt\u0026quot;,\r\u0026quot;teacherName\u0026quot;: \u0026quot;zt\u0026quot;\r}\r 这个本来应该也是的，但是现在有点尴尬，不得行\nJava 中是“值传递” java中方法参数传递方式是按值传递,只不过值不同。\r如果参数是基本类型，传递的是基本类型的字面量值的拷贝。\r如果参数是引用类型，传递的是该参量所引用的对象在堆中地址值的拷贝。\r 举例：\npackage com.zt.javastudy.grammar;\r/**\r* @author zhengtao\r* @description java中方法参数传递方式是按值传递。\r* 如果参数是基本类型，传递的是基本类型的字面量值的拷贝。\r* 如果参数是引用类型，传递的是该参量所引用的对象在堆中地址值的拷贝。\r* @date 2021/4/25\r*/\rpublic class QuoteStudy {\rpublic static void main(String[] args) {\rint a = 0;\radd(a);\rSystem.out.println(a);\rString b = \u0026quot;hello\u0026quot;;\radd(b);\rSystem.out.println(b);\rStringBuilder c = new StringBuilder(\u0026quot;hello\u0026quot;);\radd(c);\rSystem.out.println(c);\rStringBuilder d = new StringBuilder(\u0026quot;hello\u0026quot;);\rmove(d);\rSystem.out.println(d);\r}\r/**\r* 基本类型传递的值的拷贝，所以不影响原值\r* @param a\r*/\rprivate static void add(int a){\ra = 1;\r}\r/**\r* 对象传递的是对象的引用的拷贝，所以如果改变对象的属性是可以改变，如果将引用赋值给另一个对象，则不会改变原对象的引用\r* @param b\r*/\rprivate static void add(String b){\r// 在字符中 = ，就相当于重新new对象，因为string类型是不可变的，等价于b = new String(\u0026quot;helloWorld\u0026quot;)\rb = \u0026quot;helloWorld\u0026quot;;\r}\r/**\r* 对象传递的是对象的引用的拷贝,改变对象属性可以成功\r* @param c\r*/\rprivate static void add(StringBuilder c){\rc = c.append(\u0026quot;world!\u0026quot;);\r}\r/**\r* 改变引用的指向，不成功\r* @param c\r*/\rprivate static void move(StringBuilder c){\rc = new StringBuilder(\u0026quot;helloworld!\u0026quot;);\r}\r}\r// 结果\r0\rhello\rhelloworld!\rhello\r @Autowired 的一点发现 在日常写代码中基本上都是，写一个service，写一个impl，然后将@Service注解加在impl类上，在代码中直接使用@Autowired 自动注入。\n@autowired注释可以对类成员变量、方法、构造函数进行标注，完成自动装配功能。@autowired查找bean首先是先通过byType查，如果发现找到有很多bean，则按照byName方式对比获取，若有名称一样的则可以加上@Qualifier(\u0026ldquo;XXX\u0026rdquo;)配置使用。\n所以说当一个service只有一个一个impl实现时，自动注入根据byType发现只有一个实现，所以就能正确进行装配，但是如果有多个实现则会报错。\npublic interface TestService {\rvoid test();\r}\r@Service\rpublic class TestServiceImpl implements TestService {\r@Override\rpublic void test() {\rSystem.out.println(\u0026quot;实现1\u0026quot;);\r}\r}\r@Service\rpublic class TestServiceI2mpl implements TestService {\r@Override\rpublic void test() {\rSystem.out.println(\u0026quot;实现2\u0026quot;);\r}\r}\r 这时可以使用@Qualifier注解来完成正确的装配\n@Slf4j\r@RunWith(SpringRunner.class)\r@SpringBootTest()\rpublic class AopStudyTest {\r// 多个实现类，使用@Qudalifier使用byName注入\r@Qualifier(\u0026quot;testServiceImpl\u0026quot;)\r@Autowired\rprivate TestService testService;\r@Qualifier(\u0026quot;testServiceI2mpl\u0026quot;)\r@Autowired\rprivate TestService testService2;\r@Test\rpublic void testService(){\rtestService.test();\rtestService2.test();\r}\r}\r 公司的一套框架 web容器同步请求 Web容器（比如tomcat）默认情况下会为每个请求分配一个请求处理线程（在tomcat7/8中，能够同时处理到达的请求的线程数量默认为200），默认情况下，在响应完成前，该线程资源都不会被释放。如图所示：\n处理HTTP请求和执行具体业务代码的线程是同一个线程！\n如果业务代码处理时间比较长，那么请求处理线程将一直被占用，直到任务结束，这种情况下，随着并发请求数量的增加，将可能导致处理请求线程全部被占用，此时tomcat会将后来的请求堆积到内部阻塞队列容器中，如果存放请求的阻塞队列也满了，那么后续的进来请求将会遭遇拒绝服务，直到有线程资源可以处理请求为止。\n实践是检验真理的唯一标准 将工作线程设为1，方便测试\nserver:\rport: 19003\rtomcat:\ruri-encoding: UTF-8\rmax-threads: 1 #最大工作线程数量\rmin-spare-threads: 1 #最小工作线程数量\r#max-connections: 10000 #一瞬间最大支持的并发的连接数\raccept-count: 1 #等待队列长度\r @RequestMapping(\u0026quot;/testCommon\u0026quot;)\rpublic String testCommon() throws InterruptedException {\rlog.info(\u0026quot;请求开始！\u0026quot;);\rstart = System.currentTimeMillis();\rThread.sleep(5000);\rlog.info(\u0026quot;请求处理时间:{}ms\u0026quot;, (System.currentTimeMillis() - start));\rreturn \u0026quot;hello world!\u0026quot;;\r}\r 返回结果为：\n很明显的看到当有两个请求过来时，第二个会阻塞，直到第一个请求完成后才会开始处理，而且执行请求的线程和处理业务的线程是同一个线程。\nweb容器异步请求 有同步请求当然就有异步请求，Servlet 3.0开始支持异步处理请求。在接收到请求之后，Servlet线程可以将耗时的操作委派给另一个线程来完成，自己在不生成响应的情况下返回至容器，以便能处理另一个请求。此时当前请求的响应将被延后，在异步处理完成后时再对客户端进行响应（异步线程拥有 ServletRequest 和 ServletResponse 对象的引用）。开启异步请求处理之后，Servlet 线程不再是一直处于阻塞状态以等待业务逻辑的处理，而是启动异步线程之后可以立即返回。异步处理的特性可以帮助应用节省容器中的线程。如图所示：\n 我们还能发现，实际上这里的异步请求处理对于客户端浏览器来说仍然是同步输出，它并没有提升响应速度，用户是没有感知的，但是异步请求处理解放了服务器端的请求处理线程的使用，处理请求线程并没有卡在业务代码那里等待，当前的业务逻辑被转移给其他线程去处理了，能够让tomcat同时接受更多的请求，从而提升了并发处理请求的能力！\n代码说话\npackage com.zt.javastudy.async;\rimport brave.Tracing;\rimport lombok.extern.slf4j.Slf4j;\rimport org.springframework.beans.factory.annotation.Autowired;\rimport org.springframework.beans.factory.annotation.Qualifier;\rimport org.springframework.beans.factory.config.ConfigurableBeanFactory;\rimport org.springframework.cloud.sleuth.SpanNamer;\rimport org.springframework.cloud.sleuth.instrument.async.TraceRunnable;\rimport org.springframework.context.annotation.Scope;\rimport org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;\rimport org.springframework.util.ObjectUtils;\rimport org.springframework.web.bind.annotation.RequestMapping;\rimport org.springframework.web.bind.annotation.RestController;\rimport javax.servlet.AsyncContext;\rimport javax.servlet.ServletOutputStream;\rimport javax.servlet.http.HttpServletRequest;\rimport javax.servlet.http.HttpServletResponse;\rimport java.io.IOException;\r/**\r* 测试异步http请求\r*\r* @author zhengtao on 2021/9/23\r*/\r@RestController\r@Slf4j\r@Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE)\rpublic class AsyncController {\r@Autowired\r@Qualifier(\u0026quot;httpWorkThreadPool\u0026quot;)\rprivate ThreadPoolTaskExecutor executor;\r@Autowired\rprivate Tracing tracing;\r@Autowired\rprivate SpanNamer defaultSpanNamer;\r// private static LongAdder start = new LongAdder();\rprivate volatile long start;\r@RequestMapping(\u0026quot;/testAsync\u0026quot;)\rpublic void test(HttpServletRequest request, HttpServletResponse response) {\rlog.info(\u0026quot;请求开始！\u0026quot;);\rstart = System.currentTimeMillis();\rAsyncContext asyncContext = request.startAsync(request, response);\r// 设置监听\rasyncContext.addListener(new HttpAsyncListener());\rexecutor.execute(new TraceRunnable(tracing, defaultSpanNamer, () -\u0026gt; {\rtry {\rdoInvoke(asyncContext);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\r}));\r}\r/**\r* 处理业务\r*\r* @param asyncContext\r*/\rprivate void doInvoke(AsyncContext asyncContext) throws InterruptedException {\rThread.sleep(5000);\rcompleteResponse(\u0026quot;这是一个异步的http请求\u0026quot;, 200, asyncContext);\r}\r/**\r* 将\r* @param context\r* @param status\r* @param asyncContext\r*/\rprivate void completeResponse(String context, int status, AsyncContext asyncContext) {\rHttpServletResponse servletResponse = (HttpServletResponse) asyncContext.getResponse();\rif (!ObjectUtils.isEmpty(context)) {\rservletResponse.setContentType(asyncContext.getRequest().getContentType());\rservletResponse.setStatus(status);\rcompleteResponse(servletResponse, context);\r}\r// 调用了complete方法后才算请求完成\rasyncContext.complete();\rlog.info(\u0026quot;请求处理时间:{}ms\u0026quot;, (System.currentTimeMillis() - start));\r}\rprivate void completeResponse(HttpServletResponse servletResponse, String context) {\rServletOutputStream out = null;\rtry {\rbyte[] buff = context.getBytes();\rservletResponse.setContentLength(buff.length);\rout = servletResponse.getOutputStream();\rout.write(buff);\rout.flush();\r} catch (IOException e) {\rlog.error(\u0026quot;complete http reqeust error\u0026quot;, e);\r} finally {\rif (out != null) {\rtry {\rout.close();\r} catch (Exception e) {\rlog.error(e.getMessage(), e);\r}\r}\r}\r}\r}\r package com.zt.javastudy.async;\rimport lombok.extern.slf4j.Slf4j;\rimport javax.servlet.AsyncEvent;\rimport javax.servlet.AsyncListener;\rimport java.io.IOException;\r/**\r* 异步监听器\r*\r* @author zhengtao on 2021/9/23\r*/\r@Slf4j\rpublic class HttpAsyncListener implements AsyncListener {\r@Override\rpublic void onComplete(AsyncEvent event) throws IOException {\rlog.info(\u0026quot;http异步请求完成\u0026quot;);\r}\r@Override\rpublic void onTimeout(AsyncEvent event) throws IOException {\rlog.info(\u0026quot;http请求超时\u0026quot;);\r}\r@Override\rpublic void onError(AsyncEvent event) throws IOException {\rlog.info(\u0026quot;http请求失败\u0026quot;);\r}\r@Override\rpublic void onStartAsync(AsyncEvent event) throws IOException {\rlog.info(\u0026quot;http异步请求开始\u0026quot;);\r}\r}\r 测试，同样是发两个请求：\n可以很明显的看到，请求线程是同一个，但是第一个请求还没结束第二个请求就已经开始处理了。业务代码则是自定义的线程池处理的。\n搞懂这个之后看公司的代码就很简单了\n静态方法中使用bean 写一些工具类时，可能会用到其他的bean。第一眼是这样写的\n@Component\rpublic class MerchUtils {\r@Autowired\rprivate static IAgentService iAgentService;\rprivate static ConcurrentHashMap\u0026lt;String, Agent\u0026gt; agentMap = new ConcurrentHashMap\u0026lt;\u0026gt;();\rpublic static String getAgentName(String agentId) {\rAgent agent = agentMap.get(agentId);\rif (agent == null \u0026amp;\u0026amp; !agentMap.containsKey(agentId)) {\ragent = iAgentService.get(agentId);\rif (agent != null \u0026amp;\u0026amp; StringUtils.isNotEmpty(agentId)) {\ragentMap.put(agentId, agent);\r}\r}\rif (agent == null) {\rreturn agentId;\r}\rreturn agent.getAgentNameCn();\r}\r}\r 这样会报null，因为静态方法优先于bean的注入。\n正确写法为：\npublic class MerchUtils {\rprivate static IAgentService iAgentService;\r@Autowired\rpublic MerchUtils(IAgentService iAgentService) {\rMerchUtils.iAgentService = iAgentService;\r}\rprivate static ConcurrentHashMap\u0026lt;String, Agent\u0026gt; agentMap = new ConcurrentHashMap\u0026lt;\u0026gt;();\rpublic static String getAgentName(String agentId) {\rAgent agent = agentMap.get(agentId);\rif (agent == null \u0026amp;\u0026amp; !agentMap.containsKey(agentId)) {\ragent = iAgentService.get(agentId);\rif (agent != null \u0026amp;\u0026amp; StringUtils.isNotEmpty(agentId)) {\ragentMap.put(agentId, agent);\r}\r}\rif (agent == null) {\rreturn agentId;\r}\rreturn agent.getAgentNameCn();\r}\r}\r @ConditionalOnProperty 有时需要根据配置文件来决定是否创建 bean\npackage com.zt.javastudy.grammar;\rimport lombok.extern.slf4j.Slf4j;\rimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\rimport org.springframework.context.annotation.Bean;\rimport org.springframework.context.annotation.Configuration;\r/**\r* 注解学习\r*\r* @author zhengtao on 2021/10/27\r*/\r@Configuration\r@ConditionalOnProperty(\rprefix = \u0026quot;test\u0026quot;,\rname = {\u0026quot;enable\u0026quot;},\rhavingValue = \u0026quot;true\u0026quot;,\rmatchIfMissing = false\r)\r@Slf4j\rpublic class ConditionalTest {\r@Bean(initMethod = \u0026quot;start\u0026quot;, destroyMethod = \u0026quot;shutdown\u0026quot;)\rpublic ConditionalTest buildProducer() {\rlog.info(\u0026quot;创建bean了\u0026quot;);\rConditionalTest conditionalTest = new ConditionalTest();\rreturn conditionalTest;\r}\rprivate void start() {\r}\rprivate void shutdown() {\r}\r}\r 注解含义详解\n@Retention(RetentionPolicy.RUNTIME)\r@Target({ ElementType.TYPE, ElementType.METHOD })\r@Documented\r@Conditional(OnPropertyCondition.class)\rpublic @interface ConditionalOnProperty {\r// 数组，获取对应property名称的值，与name不可同时使用\rString[] value() default {};\r// 配置属性名称的前缀，比如spring.http.encoding\rString prefix() default \u0026quot;\u0026quot;;\r// 数组，配置属性完整名称或部分名称\r// 可与prefix组合使用，组成完整的配置属性名称，与value不可同时使用\rString[] name() default {};\r// 可与name组合使用，比较获取到的属性值与havingValue给定的值是否相同，相同才加载配置\rString havingValue() default \u0026quot;\u0026quot;;\r// 缺少该配置属性时是否可以加载。如果为true，没有该配置属性时也会正常加载；反之则不会生效\rboolean matchIfMissing() default false;\r}\r 因此在这个代码中只有test.enable=true,才会创建bean。\n分布式id解决方案 uuid ​\tUUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：550e8400-e29b-41d4-a716-446655440000\n优点：\n 性能非常高：本地生成，没有网络消耗。  缺点：\n  不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。\n  信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。\n  ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用：\n① MySQL官方有明确的建议主键要尽量越短越好，36个字符长度的UUID不符合要求。\n② 对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能。\n  snowflake(雪花算法) ​\t雪花算法（Snowflake）是twitter公司内部分布式项目采用的ID生成算法，具体规则如下：\n 1位，不用。二进制中最高位为1的都是负数，但是我们生成的id一般都使用整数，所以这个最高位固定是0 41位，用来记录时间戳（毫秒）。 - 41位可以表示 2^{41}-1 个数字，  如果只用来表示正整数（计算机中正数包含0），可以表示的数值范围是：0 至 2^{41}-1，减1是因为可表示的数值范围是从0开始算的，而不是1。 - 也就是说41位可以表示 2^{41}-1 个毫秒的值，转化成单位年则是 (2^{41}-1) / (1000 60 60 24 365) = 69 年   10位，用来记录工作机器id。 - 可以部署在 2^{10} = 1024 个节点，包括 5位 datacenterId 和 5位 workerId - 5位（bit）可以表示的最大正整数是 2^{5}-1 = 31 ，即可以用 0、1、2、3、\u0026hellip;.31 这 32 个数字，来表示不同的 datecenterId 或 workerId 12位，序列号，用来记录同毫秒内产生的不同id。 - 12位（bit）可以表示的最大正整数是 2^{12}-1 = 4095 ，即可以用 0、1、2、3、\u0026hellip;.4094 这 4095 个数字，来表示同一机器同一时间截（毫秒)内产生的 4095 个 ID 序号。  由于在 Java 中 64bit 的整数是 long 类型，所以在 Java 中 SnowFlake 算法生成的 id 就是 long 来存储的。\n优点：\n 毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。 可以根据自身业务特性分配bit位，非常灵活。  缺点：\n 强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。  基于Redis模式 Redis也同样可以实现，原理就是利用redis的 incr命令实现ID的原子性自增。\n127.0.0.1:6379\u0026gt; set seq_id 1 // 初始化自增ID为1\rOK\r127.0.0.1:6379\u0026gt; incr seq_id // 增加1，并返回递增后的数值\r(integer) 2\r 用redis实现需要注意一点，要考虑到redis持久化的问题。redis有两种持久化方式RDB和AOF\n RDB会定时打一个快照进行持久化，假如连续自增但redis没及时持久化，而这会Redis挂掉了，重启Redis后会出现ID重复的情况。 AOF会对每条写命令进行持久化，即使Redis挂掉了也不会出现ID重复的情况，但由于incr命令的特殊性，会导致Redis重启恢复的数据时间过长。  简单代码演示：\nLong serialNo = RedisTemplateUtil.incrBy(REDIS_KEY_PREFIX + seq.getSequenceName(), seq.getStep());\rpublic static Long incrBy(String key, long delta){\rreturn getRedisTemplate().opsForValue().increment(key, delta);\r}\r mybatis - 动态数据源 动态数据源，主要是为了解决读写分离的场景。\n那么创建一个数据源主要有哪几步呢？\n 配置 dao，model(bean)，xml mapper文件的扫描路径 注入数据源配置属性，创建数据源。 将数据源设置到SQL会话工厂和事务管理器。  这样当进行数据库操作时，就会通过我们创建的动态数据源去获取要操作的数据源了。一步一步操作完成功能\n创建多个数据源 package com.jlpay.saas.common.db.datasource;\rimport com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceBuilder;\rimport lombok.extern.slf4j.Slf4j;\rimport org.springframework.beans.factory.annotation.Qualifier;\rimport org.springframework.boot.context.properties.ConfigurationProperties;\rimport org.springframework.context.annotation.Bean;\rimport org.springframework.context.annotation.Configuration;\rimport org.springframework.context.annotation.Primary;\rimport javax.sql.DataSource;\rimport java.util.HashMap;\rimport java.util.Map;\r/**\r* @Description: 配置多数据源\r* @Author shuqingzhou\r* @Date 2021/11/15 9:09\r* @Version 1.0\r*/\r@Configuration\r@Slf4j\rpublic class DynamicDataSourceConfig {\r/**\r* 创建 DataSource Bean\r*/\r@Primary\r@Bean(\u0026quot;masterDataSource\u0026quot;)\r@ConfigurationProperties(prefix = \u0026quot;spring.datasource.druid.master\u0026quot;)\rpublic DataSource masterDataSource(){\rDataSource dataSource = DruidDataSourceBuilder.create().build();\rreturn dataSource;\r}\r/**\r* 创建 DataSource Bean\r*/\r@Bean(\u0026quot;slaveDataSource\u0026quot;)\r@ConfigurationProperties(prefix = \u0026quot;spring.datasource.druid.salve\u0026quot;)\rpublic DataSource slaveDataSource(){\rDataSource dataSource = DruidDataSourceBuilder.create().build();\rreturn dataSource;\r}\r/**\r* 如果还有数据源,在这继续添加 DataSource Bean\r*/\r@Bean(\u0026quot;dynamicDataSource\u0026quot;)\r@Primary\rpublic DynamicDataSource dynamicDataSource(@Qualifier(\u0026quot;masterDataSource\u0026quot;) DataSource masterDataSource, @Qualifier(\u0026quot;slaveDataSource\u0026quot;)DataSource slaveDataSource) {\rMap\u0026lt;Object, Object\u0026gt; targetDataSources = new HashMap\u0026lt;\u0026gt;(2);\rtargetDataSources.put(DataSourceNames.MASTER, masterDataSource);\rtargetDataSources.put(DataSourceNames.SLAVE, slaveDataSource);\r// 还有数据源,在targetDataSources中继续添加\rlog.info(\u0026quot;DataSources:{}\u0026quot;, targetDataSources);\rreturn new DynamicDataSource(masterDataSource, targetDataSources);\r}\r}\r 将数据源设置到SQL会话工厂和事务管理器 package com.jlpay.saas.common.db.datasource;\rimport org.apache.ibatis.session.SqlSessionFactory;\rimport org.mybatis.spring.SqlSessionFactoryBean;\rimport org.mybatis.spring.SqlSessionTemplate;\rimport org.mybatis.spring.boot.autoconfigure.MybatisProperties;\rimport org.springframework.beans.factory.annotation.Autowired;\rimport org.springframework.beans.factory.annotation.Qualifier;\rimport org.springframework.boot.context.properties.EnableConfigurationProperties;\rimport org.springframework.context.annotation.Bean;\rimport org.springframework.context.annotation.Configuration;\rimport org.springframework.core.io.*;\rimport org.springframework.jdbc.datasource.DataSourceTransactionManager;\rimport org.springframework.transaction.PlatformTransactionManager;\rimport java.io.File;\rimport java.io.FileInputStream;\rimport java.io.InputStream;\r/**\r* @Description: Mybatis数据源配置\r* @Author shuqingzhou\r* @Date 2021/11/15 9:09\r* @Version 1.0\r*/\r@Configuration\r@EnableConfigurationProperties(MybatisProperties.class)\rpublic class MybatisConfig {\rprivate MybatisProperties mybatisProperties;\rpublic MybatisConfig(MybatisProperties properties) {\rthis.mybatisProperties = properties;\r}\r@Autowired\r@Qualifier(value = \u0026quot;dynamicDataSource\u0026quot;)\rprivate DynamicDataSource dynamicDataSource;\r/**\r* 配置mybatis的sqlSession连接动态数据源\r* @throws Exception\r*/\r@Bean\rpublic SqlSessionFactory sqlSessionFactory() throws Exception {\rSqlSessionFactoryBean bean = new SqlSessionFactoryBean();\rbean.setDataSource(dynamicDataSource);\rbean.setMapperLocations(mybatisProperties.resolveMapperLocations());\rbean.setTypeAliasesPackage(mybatisProperties.getTypeAliasesPackage());\rResource resource = new DefaultResourceLoader().getResource(mybatisProperties.getConfigLocation());\rbean.setConfigLocation(resource);\rbean.setConfiguration(mybatisProperties.getConfiguration());\rreturn bean.getObject();\r}\r@Bean\rpublic SqlSessionTemplate sqlSessionTemplate() throws Exception {\rreturn new SqlSessionTemplate(sqlSessionFactory());\r}\r/**\r* 将动态数据源添加到事务管理器中，并生成新的bean\r* @return the platform transaction manager\r*/\r@Bean\rpublic PlatformTransactionManager transactionManager() {\rreturn new DataSourceTransactionManager(dynamicDataSource);\r}\r}\r tips：在访问数据库时会调用 determineCurrentLookupKey() 方法获取数据库实例的 key\nprotected DataSource determineTargetDataSource() {\rAssert.notNull(this.resolvedDataSources, \u0026quot;DataSource router not initialized\u0026quot;);\rObject lookupKey = determineCurrentLookupKey();\rDataSource dataSource = this.resolvedDataSources.get(lookupKey);\rif (dataSource == null \u0026amp;\u0026amp; (this.lenientFallback || lookupKey == null)) {\rdataSource = this.resolvedDefaultDataSource;\r}\rif (dataSource == null) {\rthrow new IllegalStateException(\u0026quot;Cannot determine target DataSource for lookup key [\u0026quot; + lookupKey + \u0026quot;]\u0026quot;);\r}\rreturn dataSource;\r}\r@Nullable\rprotected abstract Object determineCurrentLookupKey();\r 所以我们可以通过重写 determineCurrentLookupKey 方法来实现更改数据源\npackage com.jlpay.saas.common.db.datasource;\rimport org.springframework.jdbc.datasource.lookup.AbstractRoutingDataSource;\rimport javax.sql.DataSource;\rimport java.util.Map;\r/**\r* @Description: 动态多数据源\r* @Author shuqingzhou\r* @Date 2021/11/15 9:09\r* @Version 1.0\r*/\rpublic class DynamicDataSource extends AbstractRoutingDataSource {\rprivate static final ThreadLocal\u0026lt;String\u0026gt; CONTEXT_HOLDER = new ThreadLocal\u0026lt;\u0026gt;();\r/**\r* 配置DataSource, defaultTargetDataSource为主数据库\r*/\rpublic DynamicDataSource(DataSource defaultTargetDataSource, Map\u0026lt;Object, Object\u0026gt; targetDataSources) {\rsuper.setDefaultTargetDataSource(defaultTargetDataSource);\rsuper.setTargetDataSources(targetDataSources);\rsuper.afterPropertiesSet();\r}\r@Override\rprotected Object determineCurrentLookupKey() {\rreturn getDataSource();\r}\rpublic static void setDataSource(String dataSource) {\rCONTEXT_HOLDER.set(dataSource);\r}\rpublic static String getDataSource() {\rreturn CONTEXT_HOLDER.get();\r}\rpublic static void remove() {\rCONTEXT_HOLDER.remove();\r}\r}\r 这里使用一个ThreadLocal的变量达到每个线程都有自己的数据源的效果。我们只需要在要需要切换数据源时\nDynamicDataSource.setDataSource(),即可动态的更改数据源。\n但是这样做未免太low了，所以运用切面来升级一波！！！\n 定义注解  package com.jlpay.saas.common.db.datasource;\rimport java.lang.annotation.*;\r/**\r* @Description: 数据源注解\r* @Author shuqingzhou\r* @Date 2021/11/15 9:09\r* @Version 1.0\r*/\r@Target({ElementType.METHOD})\r@Retention(RetentionPolicy.RUNTIME)\r@Documented\rpublic @interface DataSource {\rString value() default DataSourceNames.MASTER;\r}\rpackage com.jlpay.saas.common.db.datasource;\r/**\r* @Description: 主备库名\r* @Author shuqingzhou\r* @Date 2021/11/15 9:09\r* @Version 1.0\r*/\rpublic interface DataSourceNames {\rString MASTER = \u0026quot;master\u0026quot;;\rString SLAVE = \u0026quot;slave\u0026quot;;\r}\r 创建一个AOP切面，拦截带 @DataSource 注解的方法，在方法执行前切换至目标数据源，执行完成后恢复到默认数据源。  package com.jlpay.saas.common.db.datasource;\rimport lombok.extern.slf4j.Slf4j;\rimport org.aspectj.lang.JoinPoint;\rimport org.aspectj.lang.annotation.After;\rimport org.aspectj.lang.annotation.AfterThrowing;\rimport org.aspectj.lang.annotation.Aspect;\rimport org.aspectj.lang.annotation.Before;\rimport org.aspectj.lang.reflect.MethodSignature;\rimport org.springframework.stereotype.Component;\rimport java.lang.reflect.Method;\r/**\r* @Description: 数据源切面处理\r* @Author shuqingzhou\r* @Date 2021/11/15 9:09\r* @Version 1.0\r*/\r@Aspect\r@Component\r@Slf4j\rpublic class DataSourceAspect {\r@Before(\u0026quot;execution(* com.jlpay.saas.*.db.service..*(..))\u0026quot;)\rpublic void before(JoinPoint point) throws Throwable {\rMethodSignature signature = (MethodSignature) point.getSignature();\rMethod method = signature.getMethod();\rDataSource dataSource = method.getAnnotation(DataSource.class);\rif (dataSource == null) {\rlog.debug(\u0026quot;【默认数据源】，切入点：{}\u0026quot;, signature.toShortString());\rDynamicDataSource.setDataSource(DataSourceNames.MASTER);\r} else {\rlog.debug(\u0026quot;【切换数据源】：{}，切入点：{}\u0026quot;, dataSource.value(), signature.toShortString());\rDynamicDataSource.setDataSource(dataSource.value());\r}\r}\r@After(\u0026quot;execution(* com.jlpay.saas.*.db.service..*(..))\u0026quot;)\rpublic void after() throws Throwable{\rDynamicDataSource.remove();\r}\r@AfterThrowing(\u0026quot;execution(* com.jlpay.saas.*.db.service..*(..))\u0026quot;)\rpublic void afterThrowing() {\rlog.info(\u0026quot;数据源异常，切换主库数据源\u0026quot;);\rDynamicDataSource.remove();\rDynamicDataSource.setDataSource(DataSourceNames.MASTER);\r}\r}\r 到现在如何实现动态数据源已经很透彻了。主要流程有：\n 创建数据源 将数据源设置到SQL会话工厂和事务管理器 重写 determineCurrentLookupKey 方法来更改数据源（到此，其实已经完成了动态数据源的功能，调用set方法即可动态的更改数据源） 自定义注解，并创建一个切面来调用set方法动态的更改数据源  公司的jwt公共包 Cookie、Session、Token、JWT（JSON Web Token） 这些都是登录认证中经常用到的东西，\n具体链接可见https://juejin.cn/post/6844904034181070861\nJWT消息构成 一个token分3部分，按顺序:\n头部（header): 一般为固定的，加密方式等\r载荷（payload): 是一个 JSON 对象，用来存放实际需要传递的数据\r签证（signature): 对前两部分进行签名，防止数据篡改\r 其中载荷（payload)中JWT 规定了7个官方字段\riss (issuer)：签发人\rexp (expiration time)：过期时间，这个很重要\rsub (subject)：主题\raud (audience)：受众\rnbf (Not Before)：生效时间\riat (Issued At)：签发时间\rjti (JWT ID)：编号\r   对象为一个很长的字符串，字符之间通过\u0026quot;.\u0026ldquo;分隔符分为三个子串。注意JWT对象为一个长字串，各字串之间也没有换行符，一般格式为：xxxxx.yyyyy.zzzzz 。 例如 ：\n  yJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\r   JWT的认证流程图 流程说明：\r1，浏览器发起请求登陆，携带用户名和密码；\r2，服务端验证身份，根据算法，将用户标识符打包生成 token,\r3，服务器返回JWT信息给浏览器，JWT不包含敏感信息；\r4，浏览器发起请求获取用户资料，把刚刚拿到的 token一起发送给服务器；\r5，服务器发现数据中有 token，验明正身；\r6，服务器返回该用户的用户资料；\r 公司代码应用 熟悉之前两部分后，只要抱着是怎样加密json、实现过期时间、单点登录去看即可代码即可，整体流程如下\n登录 @Service(LoginReq.COMMAND_SERVICE_NAME)\rpublic class LoginCmd extends BaseCmdService\u0026lt;LoginReq\u0026gt; {\rprivate static final String JWT_HEADER = \u0026quot;{ \\\u0026quot;alg\\\u0026quot;: \\\u0026quot;SM2SM3\\\u0026quot;, \\\u0026quot;typ\\\u0026quot;: \\\u0026quot;JWT\\\u0026quot;, \\\u0026quot;ver\\\u0026quot;: \\\u0026quot;1\\\u0026quot;}\u0026quot;;\rprivate static final String PLAT_CODE = \u0026quot;MERCH\u0026quot;;\r@Autowired\rprivate StaffService merchUserService;\r@Autowired\rprivate JwtSessionUtils jwtSessionUtils;\r@Autowired\rprivate IMonitorService monitorService;\r@Value(\u0026quot;${jlpay.framework.jwt.expire.plat:86400}\u0026quot;)\rprivate int jwtWxExpireSeconds;\r/**\r* 执行命令将请求参数传给实现类\r*\r* @param req 请求参数\r* @return\r* @throws Exception\r*/\r@Override\rprotected BaseCmdResp invoke(LoginReq req) throws Exception {\r// 登录逻辑\r//生成JWT ACCESS TOKEN\rJwtHeaderInfo jwtHeaderInfo = JSON.parseObject(JWT_HEADER, JwtHeaderInfo.class);\rjwtHeaderInfo.setPlat(plat);\rJwtPayloadInfo payloadInfo = buildJwtPayloadInfo(req, resp);\rString jwtToken = jwtSessionUtils.getAccessToken(jwtHeaderInfo, payloadInfo, jwtWxExpireSeconds, TimeUnit.SECONDS, monitorService);\rresp.setAccessToken(jwtToken);\r// 逻辑处理\r//返回\rreturn resp;\r}\r/**\r* 构造需要传递的数据\r* @param req\r* @param resp\r* @return\r*/\rprivate JwtPayloadInfo buildJwtPayloadInfo(LoginReq req, LoginResp resp) {\rJwtPayloadInfo payloadInfo = new JwtPayloadInfo();\rpayloadInfo.setUid(resp.getUserId());\rpayloadInfo.setUname(resp.getUserName());\rpayloadInfo.setUtype(resp.getUserType());\rpayloadInfo.setCid(resp.getMerchId());\rpayloadInfo.setMno(resp.getShopId());\rpayloadInfo.setMname(resp.getShopName());\rpayloadInfo.setAud(resp.getMerchType());\rpayloadInfo.setNbf(DateEasyUtil.format(new Date(), DateEasyUtil.NEW_FORMAT));\rreturn payloadInfo;\r}\r}\r 其中主要代码就在getAccessToken 这个方法内\n/**\r* 按默认封装的header,payload生成JWT Token\r*\r* @param headerInfo\r* @param payloadInfo\r* @param expireTime\r* @param timeUnit\r* @return\r*/\rpublic String getAccessToken(JwtHeaderInfo headerInfo, JwtPayloadInfo payloadInfo, int expireTime, TimeUnit timeUnit, IMonitorService monitorService) {\rgetTokenPreProcess(headerInfo, payloadInfo);\rJSONObject headerJson = JSON.parseObject(JSON.toJSONString(headerInfo));\rJSONObject payloadJson = JSON.parseObject(JSON.toJSONString(payloadInfo));\r// 生成token，并添加exp失效时间参数\rString accessToken = jwtHelper.generateToken(headerJson, payloadJson, expireTime, timeUnit);\r// 放redis\rsetJwtCacheInfo(headerInfo, payloadInfo, monitorService);\rreturn accessToken;\r}\r 生成token，就是使用国密算法加密得到token，放入redis中\n/**\r* 获取token前做的前置操作：设置签发时间，设置jti编号\r*\r* @param headerInfo\r* @param payloadInfo\r*/\rprivate void getTokenPreProcess(JwtHeaderInfo headerInfo, JwtPayloadInfo payloadInfo) {\rString deviceId = payloadInfo.getDeviceId();\r// 统一签发时间为当前执行时间\rString currentDate = DateUtil.getLongDateString(new Date());\rpayloadInfo.setIat(currentDate);\r// 有设备号jti存设备号，否则取随机数\rif (StringUtils.isNotEmpty(deviceId)) {\rpayloadInfo.setJti(deviceId);\r} else {\rpayloadInfo.setJti(generateJti(JTI_BIT));\r}\r// 检查入参\rcheckRequest(headerInfo, payloadInfo);\r}\r/**\r* 设置JwtCacheInfo\r*\r* @param headerInfo\r* @param payloadInfo\r* @param monitorService\r*/\rprivate void setJwtCacheInfo(JwtHeaderInfo headerInfo, JwtPayloadInfo payloadInfo, IMonitorService monitorService) {\rString loginJwtKey = getLoginJwtKey(headerInfo.getPlat(), payloadInfo.getUid());\rJwtCacheInfo jwtCacheInfo = new JwtCacheInfo();\rjwtCacheInfo.setIat(payloadInfo.getIat());\rjwtCacheInfo.setJti(payloadInfo.getJti());\r// 存这个redis并没有加入失效时间是否可取？\rboolean setJwtCacheResult = persistService.setString(loginJwtKey, JSON.toJSONString(jwtCacheInfo));\rif (monitorService != null \u0026amp;\u0026amp; !setJwtCacheResult) {\rmonitorService.monitor(\u0026quot;写入JWT失败, 请排查Xpipe-redis-write节点是否故障\u0026quot;);\r}\r}\r 保证了，key为前缀+用户id，value为 签发时间(iat)，唯一编号(jti)组成\n校验jwt 主要为verifyJwtAccessToken这个方法\npublic JwtInfo verifyJwtAccessToken(String accessToken, IMonitorService monitorService, IJwtCacheCheckService cacheCheckService) throws JwtException {\rJwtInfo jwtInfo = new JwtInfo();\rtry {\r// 校验jwt\rDecodedJWT jwtDecoded = jwtHelper.verify(accessToken);\rJwtHeaderInfo jwtHeaderInfo = JSON.parseObject(jwtHelper.getHeader(jwtDecoded), JwtHeaderInfo.class);\rJwtPayloadInfo payloadInfo = JSON.parseObject(jwtHelper.getPayloadClaims(jwtDecoded), JwtPayloadInfo.class);\rjwtInfo.setHeaderInfo(jwtHeaderInfo);\rjwtInfo.setPayloadInfo(payloadInfo);\rString loginJwtKey = getLoginJwtKey(jwtHeaderInfo.getPlat(), payloadInfo.getUid());\rString jwtCacheStr = persistService.getString(loginJwtKey);\rJwtCacheInfo jwtCacheInfo = JSON.parseObject(jwtCacheStr, JwtCacheInfo.class);\r// 获取到jwtCacheInfo，则校验，获取不到则不校验并告警(降级处理)\rif (jwtCacheInfo == null) {\rmonitorService.monitor(\u0026quot;读取JWT失败, 请排查Xpipe-redis-read节点是否故障\u0026quot;);\rreturn jwtInfo;\r}\r//jwtCacheInfo及JWT是否过期\rcacheCheckService.check(jwtCacheInfo, payloadInfo, iatDiff);\r} catch (JwtException e) {\rlog.warn(\u0026quot;JWT缓存时间校验不通过\u0026quot;, e.getMessage());\rthrow new JwtException(JwtExceptionCode.SESSION_KICK_OUT);\r} catch (JWTVerificationException e) {\rlog.warn(\u0026quot;JWT校验错误\u0026quot;, e);\rthrow new JwtException(JwtExceptionCode.SESSION_TIME_OUT);\r} catch (Exception e) {\rlog.error(\u0026quot;未知错误\u0026quot;, e);\rthrow new JwtException(JwtExceptionCode.SESSION_TIME_OUT);\r}\rreturn jwtInfo;\r}\r 这里主要实现，如果token过期，则踢出登录\n/**\r校验jwt是否合法\r*/\rpublic DecodedJWT verify(DecodedJWT jwt) throws JWTVerificationException {\r// 验签校验\rverifyAlgorithm(jwt, algorithm);\ralgorithm.verify(jwt);\r// 校验参数，主要校验过期时间，实现踢出登录\rverifyClaims(jwt, claims);\rreturn jwt;\r}\r 这里主要实现单点登录\n/**\r* 默认校验规则\r*\r* @param jwtCacheInfo\r* @param payloadInfo\r* @param diff 签发时间iat允许的误差，单位秒\r* @throws JwtException\r*/\r@Override\rpublic void check(JwtCacheInfo jwtCacheInfo, JwtPayloadInfo payloadInfo, int diff) throws JwtException {\rDate cacheIat = DateUtil.parseDateLongFormat(jwtCacheInfo.getIat());\rif (cacheIat == null) {\rthrow new JwtException(\u0026quot;iat not in xpipe-redis\u0026quot;);\r}\r// 目前的时间\rDate payloadIat = DateUtil.parseDateLongFormat(payloadInfo.getIat());\rif (payloadIat == null) {\rthrow new JwtException(\u0026quot;iat not in jwt payload\u0026quot;);\r}\r// 通过uid和plat取用户最后一次登录生成JWT TOKEN的生效时间(多机房通过XPIPE同步)，\r// 如果当前TOKEN生效时间早于取到的时间则判断为失效，如果没取到最后一次生效时间，则认为是TOKEN是有效的\rif (payloadIat.before(cacheIat)) {\rthrow new JwtException(\u0026quot;iat(payload) is before iat(xpipe-redis)\u0026quot;);\r}\r// 如果当前TOKEN的签发时间早于或者晚于XPIPE中取到的签发时间3秒(可配置)以内，则jti编号必须一致才认为TOKEN是有效的\rif (cacheIat.before(DateUtil.addSeconds(payloadIat, diff)) \u0026amp;\u0026amp; cacheIat.after(DateUtil.addSeconds(payloadIat, -diff)) \u0026amp;\u0026amp; !payloadInfo.getJti().equals(jwtCacheInfo.getJti())) {\rthrow new JwtException(\u0026quot;jti(payload) and jti(xpipe-redis) must be same\u0026quot;);\r}\r}\r ","id":20,"section":"posts","summary":"[TOC] 日常问题总结 工作中总是会遇到各种各样的问题，只有总结下来才是一笔财富。 fastjson的一些技巧 在工作中总是遇到给前端的字段需要是下划线的","tags":null,"title":"日常问题总结","uri":"https://wzgl998877.github.io/2022/01/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","year":"2022"},{"content":"[TOC]\n消息队列 消息队列主要应用场景为异步、削峰、解耦。\n异步解耦 场景一：用户注册，注册时需要发送短信及邮件，并且需要写库\n 串行模式。流程如下：   ​ 缺点：流程不断加长，影响用户体验。\n  显然就会想到使用异步解决，发邮件的同时可以发短信，流程如下：\n缺点：流程越多，在注册时需要调很多其他接口，代码耦合严重，出现问题不好排查\n  使用消息队列，流程如下：\n  注册接口只需要写库，并且推送到消息队列，至于其他业务订阅消息队列即可，达到了异步并且解耦\n削峰 场景二：秒杀活动，一般由于瞬时访问量过大，服务器接收过大，会导致流量暴增，相关系统无法处理请求甚至崩溃。而加入消息队列后，系统作为消费者，根据自身应用的能力进行消息的消费，不受大流量的影响。流程如下：\n常见消息队列 消息模型 最初的消息队列，就是一个严格意义上的队列。在计算机领域，“队列（Queue）”是一种数据结构，有完整而严格的定义。队列的定义是这样的：队列是先进先出（FIFO, First-In-First-Out）的线性表（Linear List）。在具体应用中通常用链表或者数组来实现。队列只允许在后端（称为 rear）进行插入操作，在前端（称为 front）进行删除操作。\n这个定义里面包含几个关键点，第一个是先进先出，这里面隐含着的一个要求是，在消息入队出队过程中，需要保证这些消息严格有序，按照什么顺序写进队列，必须按照同样的顺序从队列中读出来。不过，队列是没有“读”这个操作的，“读”就是出队，也就是从队列中“删除”这条消息。\n**早期的消息队列，就是按照“队列”的数据结构来设计的。**我们一起看下这个图，生产者（Producer）发消息就是入队操作，消费者（Consumer）收消息就是出队也就是删除操作，服务端存放消息的容器自然就称为“队列”。\n这就是最初的一种消息模型：队列模型。\n如果有多个生产者往同一个队列里面发送消息，这个队列中可以消费到的消息，就是这些生产者生产的所有消息的合集。消息的顺序就是这些生产者发送消息的自然顺序。如果有多个消费者接收同一个队列的消息，这些消费者之间实际上是竞争的关系，每个消费者只能收到队列中的一部分消息，也就是说任何一条消息只能被其中的一个消费者收到。\n如果需要将一份消息数据分发给多个消费者，要求每个消费者都能收到全量的消息，例如，对于一份订单数据，风控系统、分析系统、支付系统等都需要接收消息。这个时候，单个队列就满足不了需求，一个可行的解决方式是，为每个消费者创建一个单独的队列，让生产者发送多份。\n显然这是个比较蠢的做法，同样的一份消息数据被复制到多个队列中会浪费资源，更重要的是，生产者必须知道有多少个消费者。为每个消费者单独发送一份消息，这实际上违背了消息队列“解耦”这个设计初衷。\n为了解决这个问题，演化出了另外一种消息模型：发布 - 订阅模型（Publish-Subscribe Pattern）\n在发布 - 订阅模型中，消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先“订阅主题”。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息。\n在消息领域的历史上很长的一段时间，队列模式和发布 - 订阅模式是并存的，有些消息队列同时支持这两种消息模型，比如 ActiveMQ。我们仔细对比一下这两种模型，生产者就是发布者，消费者就是订阅者，队列就是主题，并没有本质的区别。它们最大的区别其实就是，一份消息数据能不能被消费多次的问题。\n实际上，在这种发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。也就是说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。\n现代的消息队列产品使用的消息模型大多是这种发布 - 订阅模型。\nRabbitMQ 的消息模型 RabbitMQ，它是少数依然坚持使用队列模型的产品之一。那它是怎么解决多个消费者的问题呢？在 RabbitMQ 中，Exchange 位于生产者和队列之间，生产者并不关心将消息发送给哪个队列，而是将消息发送给 Exchange，由 Exchange 上配置的策略来决定将消息投递到哪些队列中。\n同一份消息如果需要被多个消费者来消费，需要配置 Exchange 将消息发送到多个队列，每个队列中都存放一份完整的消息数据，可以为一个消费者提供消费服务。这也可以变相地实现新发布 - 订阅模型中，“一份消息数据可以被多个订阅者来多次消费”这样的功能。\nRocketMQ 的消息模型 RocketMQ 使用的消息模型是标准的发布 - 订阅模型，在 RocketMQ 的术语表中，生产者、消费者和主题与我在上面讲的发布 - 订阅模型中的概念是完全一样的。\n但是，在 RocketMQ 也有队列（Queue）这个概念，并且队列在 RocketMQ 中是一个非常重要的概念，那队列在 RocketMQ 中的作用是什么呢？这就要从消息队列的消费机制说起。\n几乎所有的消息队列产品都使用一种非常朴素的“请求 - 确认”机制，确保消息不会在传递过程中由于网络或服务器故障丢失。具体的做法也非常简单。在生产端，生产者先将消息发送给服务端，也就是 Broker，服务端在收到消息并将消息写入主题或者队列中后，会给生产者发送确认的响应。\n如果生产者没有收到服务端的确认或者收到失败的响应，则会重新发送消息；在消费端，消费者在收到消息并完成自己的消费业务逻辑（比如，将数据保存到数据库中）后，也会给服务端发送消费成功的确认，服务端只有收到消费确认后，才认为一条消息被成功消费，否则它会给消费者重新发送这条消息，直到收到对应的消费成功确认。\n这个确认机制很好地保证了消息传递过程中的可靠性，但是，引入这个机制在消费端带来了一个不小的问题。什么问题呢？为了确保消息的有序性，在某一条消息被成功消费之前，下一条消息是不能被消费的，否则就会出现消息空洞，违背了有序性这个原则。\n也就是说，每个主题在任意时刻，至多只能有一个消费者实例在进行消费，那就没法通过水平扩展消费者的数量来提升消费端总体的消费性能。为了解决这个问题，RocketMQ 在主题下面增加了队列的概念。\n**每个主题包含多个队列，通过多个队列来实现多实例并行生产和消费。**需要注意的是，RocketMQ 只在队列上保证消息的有序性，主题层面是无法保证消息的严格顺序的。\nRocketMQ 中，订阅者的概念是通过消费组（Consumer Group）来体现的。每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响，也就是说，一条消息被 Consumer Group1 消费过，也会再给 Consumer Group2 消费。\n消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息。如果一条消息被消费者 Consumer1 消费了，那同组的其他消费者就不会再收到这条消息。\n在 Topic 的消费过程中，由于消息需要被不同的组进行多次消费，所以消费完的消息并不会立即被删除，这就需要 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset），这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一。这个消费位置是非常重要的概念，我们在使用消息队列的时候，丢消息的原因大多是由于消费位置处理不当导致的。\nRocketMQ 的消息模型如图所示：\n例子 假设有一个主题 MyTopic，我们为主题创建 5 个队列，分布到 2 个 Broker 中。\n先说消息生产这一端，假设我们有 3 个生产者实例：Produer0，Produer1 和 Producer2。\n这 3 个生产者是如何对应到 2 个 Broker 的，又是如何对应到 5 个队列的呢？这个很简单，不用对应，随便发。每个生产者可以在 5 个队列中轮询发送，也可以随机选一个队列发送，或者只往某个队列发送，这些都可以。比如 Producer0 要发 5 条消息，可以都发到队列 Q0 里面，也可以 5 个队列每个队列发一条。\n然后说消费端，很多同学没有搞清楚消费组、消费者和队列这几个概念的对应关系。\n每个消费组就是一份订阅，它要消费主题 MyTopic 下，所有队列的全部消息。注意，队列里的消息并不是消费掉就没有了，这里的“消费”，只是去队列里面读了消息，并没有删除，消费完这条消息还是在队列里面。\n多个消费组在消费同一个主题时，消费组之间是互不影响的。比如我们有 2 个消费组：G0 和 G1。G0 消费了哪些消息，G1 是不知道的，也不用知道。G0 消费过的消息，G1 还可以消费。即使 G0 积压了很多消息，对 G1 来说也没有任何影响。\n然后我们再说消费组的内部，一个消费组中可以包含多个消费者的实例。比如说消费组 G1，包含了 2 个消费者 C0 和 C1，那这 2 个消费者又是怎么和主题 MyTopic 的 5 个队列对应的呢？\n由于消费确认机制的限制，这里面有一个原则是，在同一个消费组里面，每个队列只能被一个消费者实例占用。至于如何分配，这里面有很多策略，我就不展开说了。总之保证每个队列分配一个消费者就行了。比如，我们可以让消费者 C0 消费 Q0，Q1 和 Q2，C1 消费 Q3 和 Q4，如果 C0 宕机了，会触发重新分配，这时候 C1 同时消费全部 5 个队列。\n再强调一下，队列占用只是针对消费组内部来说的，对于其他的消费组来说是没有影响的。比如队列 Q2 被消费组 G1 的消费者 C1 占用了，对于消费组 G2 来说，是完全没有影响的，G2 也可以分配它的消费者来占用和消费队列 Q2。\n最后说一下消费位置，每个消费组内部维护自己的一组消费位置，每个队列对应一个消费位置。消费位置在服务端保存，并且，消费位置和消费者是没有关系的。每个消费位置一般就是一个整数，记录这个消费组中，这个队列消费到哪个位置了，这个位置之前的消息都成功消费了，之后的消息都没有消费或者正在消费。\n我把咱们这个例子的消费位置整理成下面的表格，便于你理解。\n你可以看到，这个表格中并没有消费者这一列，也就是说消费者和消费位置是没有关系的。\nKafka 的消息模型 我们再来看看另一种常见的消息队列 Kafka，Kafka 的消息模型和 RocketMQ 是完全一样的，我刚刚讲的所有 RocketMQ 中对应的概念，和生产消费过程中的确认机制，都完全适用于 Kafka。唯一的区别是，在 Kafka 中，队列这个概念的名称不一样，Kafka 中对应的名称是“分区（Partition）”，含义和功能是没有任何区别的。\nRocketMQ详解 技术架构 RocketMQ架构上主要分为四部分，如上图所示:\n  Producer：消息发布的角色，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。\n  Consumer：消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。\n  NameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。主要包括两个功能：\n Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活； 路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。  然后Producer和Consumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer和Consumer仍然可以动态感知Broker的路由的信息。\n  BrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。\n Remoting Module：整个Broker的实体，负责处理来自Client端的请求。 Client Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息。 Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。 HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。 Index Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。    部署架构 RocketMQ 网络部署特点  NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave 的对应关系通过指定相同的BrokerName，不同的BrokerId 来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。 注意：当前RocketMQ版本在部署架构上支持一Master多Slave，但只有BrokerId=1的从服务器才会参与消息的读负载。 Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，消费者在向Master拉取消息时，Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。  结合部署架构图，描述集群工作流程：\n 启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。 Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。 收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。 Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。 Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。  代码详解 个人认为如果想了解一个中间件，可以先由整体架构、思想进行学习，然后对每部分进行深入研究，后面就对每一个部分进行详解，由简入繁，先看NameServer是怎么实现的\nNameServer 作为注册中心，基本通讯流程如下：\n(1) Broker启动后需要完成一次将自己注册至NameServer的操作；随后每隔30s时间定时向NameServer上报Topic路由信息。\n(2) 消息生产者Producer作为客户端发送消息时候，需要根据消息的Topic从本地缓存的TopicPublishInfoTable获取路由信息。如果没有则更新路由信息会从NameServer上重新拉取，同时Producer会默认每隔30s向NameServer拉取一次路由信息。\n(3) 消息生产者Producer根据2）中获取的路由信息选择一个队列（MessageQueue）进行消息发送；Broker作为消息的接收者接收消息并落盘存储。\n(4) 消息消费者Consumer根据2）中获取的路由信息，并再完成客户端的负载均衡后，选择其中的某一个或者某几个消息队列来拉取消息并进行消费。\n作为注册中心主要包含以下几个部分：路由注册、 路由删除、路由发现，主要由以下几个map实现\nprivate final Map\u0026lt;String/* topic */, Map\u0026lt;String, QueueData\u0026gt;\u0026gt; topicQueueTable;\rprivate final Map\u0026lt;String/* brokerName */, BrokerData\u0026gt; brokerAddrTable;\rprivate final Map\u0026lt;String/* clusterName */, Set\u0026lt;String/* brokerName */\u0026gt;\u0026gt; clusterAddrTable;\rprivate final Map\u0026lt;BrokerAddrInfo/* brokerAddr */, BrokerLiveInfo\u0026gt; brokerLiveTable;\rprivate final Map\u0026lt;BrokerAddrInfo/* brokerAddr */, List\u0026lt;String\u0026gt;/* Filter Server */\u0026gt; filterServerTable;\rprivate final Map\u0026lt;String/* topic */, Map\u0026lt;String/*brokerName*/, TopicQueueMappingInfo\u0026gt;\u0026gt; topicQueueMappingInfoTable;\r   topicQueueTable:Topic消息队列路由信息，消息发送时根据路由表进行负载均衡。\n  brokerAddrTable:Broker基础信息，包含brokerName、所属集群名称、主备Broker地址。\n  clusterAddrTable:Broker集群信息，存储集群中所有Broker名称。\n  brokerLiveTable:Broker状态信息。NameServer每次收到心跳包时会替换该信息。\n  filterServerTable:Broker上的FilterServer列表，用于类模式消息过滤。\n  路由注册 路由注册是通过Broker与NameServer的心跳功能实现的。\n  Broker启动时向集群中所有的NameServer发送心跳语句(心跳包中包含BrokerId、Broker地址、Broker名称、Broker所属集群名称、Broker关联的FilterServer列表)，每隔30s向集群中所有NameServer发送心跳包\n  NameServer收到Broker心跳包时会更新brokerLiveTable缓存中BrokerLiveInfo的lastUpdateTimestamp，然后Name Server每隔10s扫描brokerLiveTable，如果连续120s没有收到心跳包，NameServer将移除该Broker的路由信息同时关闭Socket连接\n  broker 发送心跳包 //每隔30s上报Broker信息到NameServer\rscheduledFutures.add(this.scheduledExecutorService.scheduleAtFixedRate(new AbstractBrokerRunnable(this.getBrokerIdentity()) {\r@Override\rpublic void run2() {\rtry {\rif (System.currentTimeMillis() \u0026lt; shouldStartTime) {\rBrokerController.LOG.info(\u0026quot;Register to namesrv after {}\u0026quot;, shouldStartTime);\rreturn;\r}\rif (isIsolated) {\rBrokerController.LOG.info(\u0026quot;Skip register for broker is isolated\u0026quot;);\rreturn;\r}\rBrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister());\r} catch (Throwable e) {\rBrokerController.LOG.error(\u0026quot;registerBrokerAll Exception\u0026quot;, e);\r}\r}\r}, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS));\r 这个其实就是使用了定时任务来实现每隔30s向集群中所有NameServer发送心跳包\nNameServer收到Broker心跳包 public RegisterBrokerResult registerBroker(\rfinal String clusterName,\rfinal String brokerAddr,\rfinal String brokerName,\rfinal long brokerId,\rfinal String haServerAddr,\rfinal Long timeoutMillis,\rfinal Boolean enableActingMaster,\rfinal TopicConfigSerializeWrapper topicConfigWrapper,\rfinal List\u0026lt;String\u0026gt; filterServerList,\rfinal Channel channel) {\rRegisterBrokerResult result = new RegisterBrokerResult();\rtry {\rtry {\r// 加锁\rthis.lock.writeLock().lockInterruptibly();\r// 维护clusterAddrTable\rSet\u0026lt;String\u0026gt; brokerNames = this.clusterAddrTable.computeIfAbsent(clusterName, k -\u0026gt; new HashSet\u0026lt;\u0026gt;());\rbrokerNames.add(brokerName);\rboolean registerFirst = false;\r// 维护brokerAddrTable\rBrokerData brokerData = this.brokerAddrTable.get(brokerName);\r// 第一次注册,则创建brokerData\rif (null == brokerData) {\rregisterFirst = true;\rbrokerData = new BrokerData(clusterName, brokerName, new HashMap\u0026lt;\u0026gt;());\rthis.brokerAddrTable.put(brokerName, brokerData);\r}\r// 非第一次注册,更新Broker\rboolean isOldVersionBroker = enableActingMaster == null;\rbrokerData.setEnableActingMaster(!isOldVersionBroker \u0026amp;\u0026amp; enableActingMaster);\rMap\u0026lt;Long, String\u0026gt; brokerAddrsMap = brokerData.getBrokerAddrs();\rboolean isMinBrokerIdChanged = false;\rlong prevMinBrokerId = 0;\rif (!brokerAddrsMap.isEmpty()) {\rprevMinBrokerId = Collections.min(brokerAddrsMap.keySet());\r}\rif (brokerId \u0026lt; prevMinBrokerId) {\risMinBrokerIdChanged = true;\r}\r//Switch slave to master: first remove \u0026lt;1, IP:PORT\u0026gt; in namesrv, then add \u0026lt;0, IP:PORT\u0026gt;\r//The same IP:PORT must only have one record in brokerAddrTable\rbrokerAddrsMap.entrySet().removeIf(item -\u0026gt; null != brokerAddr \u0026amp;\u0026amp; brokerAddr.equals(item.getValue()) \u0026amp;\u0026amp; brokerId != item.getKey());\r//If Local brokerId stateVersion bigger than the registering one,\rString oldBrokerAddr = brokerAddrsMap.get(brokerId);\rif (null != oldBrokerAddr \u0026amp;\u0026amp; !oldBrokerAddr.equals(brokerAddr)) {\rBrokerLiveInfo oldBrokerInfo = brokerLiveTable.get(new BrokerAddrInfo(clusterName, oldBrokerAddr));\rif (null != oldBrokerInfo) {\rlong oldStateVersion = oldBrokerInfo.getDataVersion().getStateVersion();\rlong newStateVersion = topicConfigWrapper.getDataVersion().getStateVersion();\rif (oldStateVersion \u0026gt; newStateVersion) {\rlog.warn(\u0026quot;Registered Broker conflicts with the existed one, just ignore.: Cluster:{}, BrokerName:{}, BrokerId:{}, \u0026quot; +\r\u0026quot;Old BrokerAddr:{}, Old Version:{}, New BrokerAddr:{}, New Version:{}.\u0026quot;,\rclusterName, brokerName, brokerId, oldBrokerAddr, oldStateVersion, brokerAddr, newStateVersion);\r//Remove the rejected brokerAddr from brokerLiveTable.\rbrokerLiveTable.remove(new BrokerAddrInfo(clusterName, brokerAddr));\rreturn result;\r}\r}\r}\rString oldAddr = brokerAddrsMap.put(brokerId, brokerAddr);\rregisterFirst = registerFirst || (StringUtils.isEmpty(oldAddr));\rboolean isMaster = MixAll.MASTER_ID == brokerId;\rboolean isPrimeSlave = !isOldVersionBroker \u0026amp;\u0026amp; !isMaster\r\u0026amp;\u0026amp; brokerId == Collections.min(brokerAddrsMap.keySet());\r// 维护topicQueueTable\rif (null != topicConfigWrapper \u0026amp;\u0026amp; (isMaster || isPrimeSlave)) {\rConcurrentMap\u0026lt;String, TopicConfig\u0026gt; tcTable =\rtopicConfigWrapper.getTopicConfigTable();\rif (tcTable != null) {\rfor (Map.Entry\u0026lt;String, TopicConfig\u0026gt; entry : tcTable.entrySet()) {\rif (registerFirst || this.isTopicConfigChanged(clusterName, brokerAddr,\rtopicConfigWrapper.getDataVersion(), brokerName,\rentry.getValue().getTopicName())) {\rfinal TopicConfig topicConfig = entry.getValue();\rif (isPrimeSlave) {\r// Wipe write perm for prime slave\rtopicConfig.setPerm(topicConfig.getPerm() \u0026amp; (~PermName.PERM_WRITE));\r}\rthis.createAndUpdateQueueData(brokerName, topicConfig);\r}\r}\r}\rif (this.isBrokerTopicConfigChanged(clusterName, brokerAddr, topicConfigWrapper.getDataVersion()) || registerFirst) {\rTopicConfigAndMappingSerializeWrapper mappingSerializeWrapper = TopicConfigAndMappingSerializeWrapper.from(topicConfigWrapper);\rMap\u0026lt;String, TopicQueueMappingInfo\u0026gt; topicQueueMappingInfoMap = mappingSerializeWrapper.getTopicQueueMappingInfoMap();\r//the topicQueueMappingInfoMap should never be null, but can be empty\rfor (Map.Entry\u0026lt;String, TopicQueueMappingInfo\u0026gt; entry : topicQueueMappingInfoMap.entrySet()) {\rif (!topicQueueMappingInfoTable.containsKey(entry.getKey())) {\rtopicQueueMappingInfoTable.put(entry.getKey(), new HashMap\u0026lt;String, TopicQueueMappingInfo\u0026gt;());\r}\r//Note asset brokerName equal entry.getValue().getBname()\r//here use the mappingDetail.bname\rtopicQueueMappingInfoTable.get(entry.getKey()).put(entry.getValue().getBname(), entry.getValue());\r}\r}\r}\r// 维护brokerLiveTable\rBrokerAddrInfo brokerAddrInfo = new BrokerAddrInfo(clusterName, brokerAddr);\rBrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddrInfo,\rnew BrokerLiveInfo(\rSystem.currentTimeMillis(),\rtimeoutMillis == null ? DEFAULT_BROKER_CHANNEL_EXPIRED_TIME : timeoutMillis,\rtopicConfigWrapper == null ? new DataVersion() : topicConfigWrapper.getDataVersion(),\rchannel,\rhaServerAddr));\rif (null == prevBrokerLiveInfo) {\rlog.info(\u0026quot;new broker registered, {} HAService: {}\u0026quot;, brokerAddrInfo, haServerAddr);\r}\r// 维护filterServerList\rif (filterServerList != null) {\rif (filterServerList.isEmpty()) {\rthis.filterServerTable.remove(brokerAddrInfo);\r} else {\rthis.filterServerTable.put(brokerAddrInfo, filterServerList);\r}\r}\rif (MixAll.MASTER_ID != brokerId) {\rString masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID);\rif (masterAddr != null) {\rBrokerAddrInfo masterAddrInfo = new BrokerAddrInfo(clusterName, masterAddr);\rBrokerLiveInfo masterLiveInfo = this.brokerLiveTable.get(masterAddrInfo);\rif (masterLiveInfo != null) {\rresult.setHaServerAddr(masterLiveInfo.getHaServerAddr());\rresult.setMasterAddr(masterAddr);\r}\r}\r}\rif (isMinBrokerIdChanged \u0026amp;\u0026amp; namesrvConfig.isNotifyMinBrokerIdChanged()) {\rnotifyMinBrokerIdChanged(brokerAddrsMap, null,\rthis.brokerLiveTable.get(brokerAddrInfo).getHaServerAddr());\r}\r} finally {\rthis.lock.writeLock().unlock();\r}\r} catch (Exception e) {\rlog.error(\u0026quot;registerBroker Exception\u0026quot;, e);\r}\rreturn result;\r}\r 这个代码看起来很复杂，实际上很简单\n因为NameServe与Broker保持长连接，所以需要心跳机制来检测连接是否存在，做法为，Broker状态存储在brokerLiveTable中，NameServer每收到一个心跳包，将更新brokerLiveTable中关于Broker的信息(lastUpdateTimestamp主要是上次更新时间)以及路由表（topicQueueTable、brokerAddrTable、brokerLiveTable、filterServerTable）。更新上述路由表（HashTable）使用了锁粒度较少的读写锁，允许多个消息发送者（Producer）并发读，保证消息发送时的高并发。但同一时刻NameServer只处理一个Broker心跳包，多个心跳包请求串行执行\n路由删除 NameServer会每隔10s扫描brokerLiveTable状态表，如果BrokerLive的lastUpdateTimestamp的时间戳距当前时间超过120s，则认为Broker失效，移除该Broker，关闭与Broker的长连接，并同时更新topicQueueTable、brokerAddrTable、brokerLiveTable、filterServerTable。RocktMQ有两个触发点来触发路由删除。\n NameServer每隔10s扫描brokerLiveTable检测上次心跳包与当前系统时间的时间差，如果时间戳大于120s，则需要移除该Broker信息。 Broker在正常被关闭的情况下，会执行unregisterBroker指令。  // 启动NameServer后，每隔10s扫描brokerLiveTable\rthis.scanExecutorService.scheduleAtFixedRate(NamesrvController.this.routeInfoManager::scanNotActiveBroker,\r5, this.namesrvConfig.getScanNotActiveBrokerInterval(), TimeUnit.MILLISECONDS);  public void scanNotActiveBroker() {\rtry {\rlog.info(\u0026quot;start scanNotActiveBroker\u0026quot;);\r// 遍历brokerLiveTable，如果BrokerLive的lastUpdateTimestamp的时间戳距当前时间超过120s，关闭Channel，然后删除与该Broker相关的路由信息\rfor (Entry\u0026lt;BrokerAddrInfo, BrokerLiveInfo\u0026gt; next : this.brokerLiveTable.entrySet()) {\rlong last = next.getValue().getLastUpdateTimestamp();\rlong timeoutMillis = next.getValue().getHeartbeatTimeoutMillis();\rif ((last + timeoutMillis) \u0026lt; System.currentTimeMillis()) {\rRemotingUtil.closeChannel(next.getValue().getChannel());\rlog.warn(\u0026quot;The broker channel expired, {} {}ms\u0026quot;, next.getKey(), timeoutMillis);\rthis.onChannelDestroy(next.getKey());\r}\r}\r} catch (Exception e) {\rlog.error(\u0026quot;scanNotActiveBroker exception\u0026quot;, e);\r}\r}\r 看下\npublic void onChannelDestroy(BrokerAddrInfo brokerAddrInfo) {\rUnRegisterBrokerRequestHeader unRegisterRequest = new UnRegisterBrokerRequestHeader();\rboolean needUnRegister = false;\rif (brokerAddrInfo != null) {\rtry {\rtry {\rthis.lock.readLock().lockInterruptibly();\rneedUnRegister = setupUnRegisterRequest(unRegisterRequest, brokerAddrInfo);\r} finally {\rthis.lock.readLock().unlock();\r}\r} catch (Exception e) {\rlog.error(\u0026quot;onChannelDestroy Exception\u0026quot;, e);\r}\r}\rif (needUnRegister) {\rboolean result = this.submitUnRegisterBrokerRequest(unRegisterRequest);\rlog.info(\u0026quot;the broker's channel destroyed, submit the unregister request at once, \u0026quot; +\r\u0026quot;broker info: {}, submit result: {}\u0026quot;, unRegisterRequest, result);\r}\r}\r// 把要删除的路由信息放入到队列中\rpublic boolean submitUnRegisterBrokerRequest(UnRegisterBrokerRequestHeader unRegisterRequest) {\rreturn this.unRegisterService.submit(unRegisterRequest);\r}\r// 启动时，启动了该线程\rpublic void run() {\rwhile (!this.isStopped()) {\rtry {\r// 每三秒钟从队列中拉取然后进行路由删除逻辑\rfinal UnRegisterBrokerRequestHeader request = unRegisterQueue.poll(3, TimeUnit.SECONDS);\rif (request != null) {\rSet\u0026lt;UnRegisterBrokerRequestHeader\u0026gt; unRegisterRequests = new HashSet\u0026lt;\u0026gt;();\runRegisterQueue.drainTo(unRegisterRequests);\r// Add polled request\runRegisterRequests.add(request);\rthis.routeInfoManager.unRegisterBroker(unRegisterRequests);\r}\r} catch (Throwable e) {\rlog.error(\u0026quot;Handle unregister broker request failed\u0026quot;, e);\r}\r}\r}\r 这里为什么要先放到队列，然后再每隔三秒钟去队列拉取并不是很理解\npublic void unRegisterBroker(Set\u0026lt;UnRegisterBrokerRequestHeader\u0026gt; unRegisterRequests) {\rtry {\rtry {\rSet\u0026lt;String\u0026gt; removedBroker = new HashSet\u0026lt;\u0026gt;();\rSet\u0026lt;String\u0026gt; reducedBroker = new HashSet\u0026lt;\u0026gt;();\rMap\u0026lt;String, BrokerStatusChangeInfo\u0026gt; needNotifyBrokerMap = new HashMap\u0026lt;\u0026gt;();\rthis.lock.writeLock().lockInterruptibly();\rfor (final UnRegisterBrokerRequestHeader unRegisterRequest : unRegisterRequests) {\rfinal String brokerName = unRegisterRequest.getBrokerName();\rfinal String clusterName = unRegisterRequest.getClusterName();\rBrokerAddrInfo brokerAddrInfo = new BrokerAddrInfo(clusterName, unRegisterRequest.getBrokerAddr());\rBrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.remove(brokerAddrInfo);\rlog.info(\u0026quot;unregisterBroker, remove from brokerLiveTable {}, {}\u0026quot;,\rbrokerLiveInfo != null ? \u0026quot;OK\u0026quot; : \u0026quot;Failed\u0026quot;,\rbrokerAddrInfo\r);\rthis.filterServerTable.remove(brokerAddrInfo);\rboolean removeBrokerName = false;\rboolean isMinBrokerIdChanged = false;\rBrokerData brokerData = this.brokerAddrTable.get(brokerName);\rif (null != brokerData) {\rif (!brokerData.getBrokerAddrs().isEmpty() \u0026amp;\u0026amp;\runRegisterRequest.getBrokerId().equals(Collections.min(brokerData.getBrokerAddrs().keySet()))) {\risMinBrokerIdChanged = true;\r}\rString addr = brokerData.getBrokerAddrs().remove(unRegisterRequest.getBrokerId());\rlog.info(\u0026quot;unregisterBroker, remove addr from brokerAddrTable {}, {}\u0026quot;,\raddr != null ? \u0026quot;OK\u0026quot; : \u0026quot;Failed\u0026quot;,\rbrokerAddrInfo\r);\rif (brokerData.getBrokerAddrs().isEmpty()) {\rthis.brokerAddrTable.remove(brokerName);\rlog.info(\u0026quot;unregisterBroker, remove name from brokerAddrTable OK, {}\u0026quot;,\rbrokerName\r);\rremoveBrokerName = true;\r} else if (isMinBrokerIdChanged) {\rneedNotifyBrokerMap.put(brokerName, new BrokerStatusChangeInfo(\rbrokerData.getBrokerAddrs(), addr, null));\r}\r}\rif (removeBrokerName) {\rSet\u0026lt;String\u0026gt; nameSet = this.clusterAddrTable.get(clusterName);\rif (nameSet != null) {\rboolean removed = nameSet.remove(brokerName);\rlog.info(\u0026quot;unregisterBroker, remove name from clusterAddrTable {}, {}\u0026quot;,\rremoved ? \u0026quot;OK\u0026quot; : \u0026quot;Failed\u0026quot;,\rbrokerName);\rif (nameSet.isEmpty()) {\rthis.clusterAddrTable.remove(clusterName);\rlog.info(\u0026quot;unregisterBroker, remove cluster from clusterAddrTable {}\u0026quot;,\rclusterName\r);\r}\r}\rremovedBroker.add(brokerName);\r} else {\rreducedBroker.add(brokerName);\r}\r}\rcleanTopicByUnRegisterRequests(removedBroker, reducedBroker);\rif (!needNotifyBrokerMap.isEmpty() \u0026amp;\u0026amp; namesrvConfig.isNotifyMinBrokerIdChanged()) {\rnotifyMinBrokerIdChanged(needNotifyBrokerMap);\r}\r} finally {\rthis.lock.writeLock().unlock();\r}\r} catch (Exception e) {\rlog.error(\u0026quot;unregisterBroker Exception\u0026quot;, e);\r}\r}\r 这段代码就是，执行删除路由的逻辑，具体逻辑没什么好说的，就是在维护几个map，但需要注意又用到了写锁，这可能就是高手的代码，处处有锁。\n路由发现 路由发现是非实时的，当Topic路由出现变化后，NameServer不主动推送给客户端，而是由客户端定时拉取主题最新的路由\npublic RemotingCommand getRouteInfoByTopic(ChannelHandlerContext ctx,\rRemotingCommand request) throws RemotingCommandException {\rfinal RemotingCommand response = RemotingCommand.createResponseCommand(null);\rfinal GetRouteInfoRequestHeader requestHeader =\r(GetRouteInfoRequestHeader) request.decodeCommandCustomHeader(GetRouteInfoRequestHeader.class);\rif (requestHeader.getTopic().indexOf(GetRouteInfoRequestHeader.split) \u0026lt; 0) {\rTopicRouteData topicRouteData = this.namesrvController.getRouteInfoManager().pickupTopicRouteData(requestHeader.getTopic());\rif (topicRouteData != null) {\rif (this.namesrvController.getNamesrvConfig().isOrderMessageEnable()) {\rString orderTopicConf =\rthis.namesrvController.getKvConfigManager().getKVConfig(NamesrvUtil.NAMESPACE_ORDER_TOPIC_CONFIG,\rrequestHeader.getTopic());\rtopicRouteData.setOrderTopicConf(orderTopicConf);\r}\rbyte[] content = topicRouteData.encode();\rresponse.setBody(content);\rresponse.setCode(ResponseCode.SUCCESS);\rresponse.setRemark(null);\rreturn response;\r}\rresponse.setCode(ResponseCode.TOPIC_NOT_EXIST);\rresponse.setRemark(\u0026quot;No topic route info in name server for the topic: \u0026quot; + requestHeader.getTopic()\r+ FAQUrl.suggestTodo(FAQUrl.APPLY_TOPIC_URL));\rreturn response;\r}\rString[] topics = requestHeader.getTopic().split(String.valueOf(GetRouteInfoRequestHeader.split));\rTopicRouteDatas topicRouteDatas = new TopicRouteDatas();\rfor (String topic : topics) {\r//调用RouteInfoManager的方法,从路由表topicQueueTable、brokerAddrTable、filterServerTable中分别填充TopicRouteData的List\u0026lt;QueueData\u0026gt;、List\u0026lt;BrokerData\u0026gt;、filterServer\rTopicRouteData topicRouteData = this.namesrvController.getRouteInfoManager().pickupTopicRouteData(topic);\rif (topicRouteData == null) {\rcontinue;\r}\r// 如果找到主题对应你的路由信息并且该主题为顺序消息，则从NameServer KVConfig中获取关于顺序消息相关的配置填充路由信息\rif (this.namesrvController.getNamesrvConfig().isOrderMessageEnable()) {\rString orderTopicConf =\rthis.namesrvController.getKvConfigManager().getKVConfig(NamesrvUtil.NAMESPACE_ORDER_TOPIC_CONFIG, topic);\rtopicRouteData.setOrderTopicConf(orderTopicConf);\r}\rtopicRouteDatas.getTopics().put(topic, topicRouteData);\r}\rresponse.setBody(topicRouteDatas.encode());\rresponse.setCode(ResponseCode.SUCCESS);\rresponse.setRemark(null);\rreturn response;\r}\r 总结 NameServer是一种典型的心跳机制，即定时任务发起心跳，收到心跳记录心跳时间，再使用定时任务扫描判断心跳是否超过设置时间。\nProducer 作为生产者，需要搞清的东西是，消息的发送过程，即发到哪个broker哪个Topic哪个队列\nProducer端在发送消息的时候，会先根据Topic找到指定的TopicPublishInfo，在获取了TopicPublishInfo路由信息后，RocketMQ的客户端在默认方式下selectOneMessageQueue()方法会从TopicPublishInfo中的messageQueueList中选择一个队列（MessageQueue）进行发送消息。具体的容错策略均在MQFaultStrategy这个类中定义。这里有一个sendLatencyFaultEnable开关变量，如果开启，在随机递增取模的基础上，再过滤掉not available的Broker代理。所谓的\u0026quot;latencyFaultTolerance\u0026quot;，是指对之前失败的，按一定的时间做退避。例如，如果上次请求的latency超过550Lms，就退避3000Lms；超过1000L，就退避60000L；如果关闭，采用随机递增取模的方式选择一个队列（MessageQueue）来发送消息，latencyFaultTolerance机制是实现消息发送高可用的核心关键所在。\n启动流程 这个生产者的启动流程看起来比较复杂，所以需要看下,代码入口在DefaultMQProducerImpl的start方法\npublic void start(final boolean startFactory) throws MQClientException {\rswitch (this.serviceState) {\rcase CREATE_JUST:\rthis.serviceState = ServiceState.START_FAILED;\r// 检查生产者组是否满足要求\rthis.checkConfig();\r// 更改当前instanceName为进程ID\rif (!this.defaultMQProducer.getProducerGroup().equals(MixAll.CLIENT_INNER_PRODUCER_GROUP)) {\rthis.defaultMQProducer.changeInstanceNameToPID();\r}\r// 获得MQ客户端实例\rthis.mQClientFactory = MQClientManager.getInstance().getOrCreateMQClientInstance(this.defaultMQProducer, rpcHook);\r// 注册实例到生产者表中\rboolean registerOK = mQClientFactory.registerProducer(this.defaultMQProducer.getProducerGroup(), this);\rif (!registerOK) {\rthis.serviceState = ServiceState.CREATE_JUST;\rthrow new MQClientException(\u0026quot;The producer group[\u0026quot; + this.defaultMQProducer.getProducerGroup()\r+ \u0026quot;] has been created before, specify another name please.\u0026quot; + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL),\rnull);\r}\rthis.topicPublishInfoTable.put(this.defaultMQProducer.getCreateTopicKey(), new TopicPublishInfo());\r// 启动生产者\rif (startFactory) {\rmQClientFactory.start();\r}\rlog.info(\u0026quot;the producer [{}] start OK. sendMessageWithVIPChannel={}\u0026quot;, this.defaultMQProducer.getProducerGroup(),\rthis.defaultMQProducer.isSendMessageWithVIPChannel());\rthis.serviceState = ServiceState.RUNNING;\rbreak;\rcase RUNNING:\rcase START_FAILED:\rcase SHUTDOWN_ALREADY:\rthrow new MQClientException(\u0026quot;The producer service state not OK, maybe started once, \u0026quot;\r+ this.serviceState\r+ FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK),\rnull);\rdefault:\rbreak;\r}\rthis.mQClientFactory.sendHeartbeatToAllBrokerWithLock();\rRequestFutureHolder.getInstance().startScheduledTask(this);\r}\r 这段代码跟着流程图来看就好了，下面这个代码有点意思的\npublic MQClientInstance getOrCreateMQClientInstance(final ClientConfig clientConfig, RPCHook rpcHook) {\r// 构建客户端ID\rString clientId = clientConfig.buildMQClientId();\r// 根据clientId获取\rMQClientInstance instance = this.factoryTable.get(clientId);\r// 如果为空就创建实例，并放入实例表中\rif (null == instance) {\rinstance =\rnew MQClientInstance(clientConfig.cloneClientConfig(),\rthis.factoryIndexGenerator.getAndIncrement(), clientId, rpcHook);\r// 这里可以学习下，在上面已经判断为空的基础上，再判断一次\rMQClientInstance prev = this.factoryTable.putIfAbsent(clientId, instance);\rif (prev != null) {\rinstance = prev;\rlog.warn(\u0026quot;Returned Previous MQClientInstance for clientId:[{}]\u0026quot;, clientId);\r} else {\rlog.info(\u0026quot;Created new MQClientInstance for clientId:[{}]\u0026quot;, clientId);\r}\r}\rreturn instance;\r}\r  MQClientInstance prev = this.factoryTable.putIfAbsent(clientId, instance);说有意思是因为，自己在写代码时，进行了为空判断后在循环内，基本不会再进行判断，而这个代码就解决了并发出现的问题\n 发送流程 根据上图一步步的进行看源码即可，看下来就觉得比较长，但是也没什么比较精妙的地方感觉\nprivate SendResult sendDefaultImpl(\rMessage msg,\rfinal CommunicationMode communicationMode,\rfinal SendCallback sendCallback,\rfinal long timeout\r) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {\r// 校验生产者状态\rthis.makeSureStateOK();\r// 参数校验\rValidators.checkMessage(msg, this.defaultMQProducer);\rfinal long invokeID = random.nextLong();\rlong beginTimestampFirst = System.currentTimeMillis();\rlong beginTimestampPrev = beginTimestampFirst;\rlong endTimestamp = beginTimestampFirst;\r// 尝试获取主题的路由信息\rTopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic());\rif (topicPublishInfo != null \u0026amp;\u0026amp; topicPublishInfo.ok()) {\rboolean callTimeout = false;\rMessageQueue mq = null;\rException exception = null;\rSendResult sendResult = null;\rint timesTotal = communicationMode == CommunicationMode.SYNC ? 1 + this.defaultMQProducer.getRetryTimesWhenSendFailed() : 1;\rint times = 0;\rString[] brokersSent = new String[timesTotal];\rfor (; times \u0026lt; timesTotal; times++) {\rString lastBrokerName = null == mq ? null : mq.getBrokerName();\r// 选择消息队列\rMessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName);\rif (mqSelected != null) {\rmq = mqSelected;\rbrokersSent[times] = mq.getBrokerName();\rtry {\rbeginTimestampPrev = System.currentTimeMillis();\rif (times \u0026gt; 0) {\r//Reset topic with namespace during resend.\rmsg.setTopic(this.defaultMQProducer.withNamespace(msg.getTopic()));\r}\rlong costTime = beginTimestampPrev - beginTimestampFirst;\rif (timeout \u0026lt; costTime) {\rcallTimeout = true;\rbreak;\r}\rsendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime);\rendTimestamp = System.currentTimeMillis();\r//\rthis.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false);\rswitch (communicationMode) {\rcase ASYNC:\rreturn null;\rcase ONEWAY:\rreturn null;\rcase SYNC:\rif (sendResult.getSendStatus() != SendStatus.SEND_OK) {\rif (this.defaultMQProducer.isRetryAnotherBrokerWhenNotStoreOK()) {\rcontinue;\r}\r}\rreturn sendResult;\rdefault:\rbreak;\r}\r} catch (RemotingException e) {\rendTimestamp = System.currentTimeMillis();\rthis.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true);\rlog.warn(String.format(\u0026quot;sendKernelImpl exception, resend at once, InvokeID: %s, RT: %sms, Broker: %s\u0026quot;, invokeID, endTimestamp - beginTimestampPrev, mq), e);\rlog.warn(msg.toString());\rexception = e;\rcontinue;\r} catch (MQClientException e) {\rendTimestamp = System.currentTimeMillis();\rthis.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true);\rlog.warn(String.format(\u0026quot;sendKernelImpl exception, resend at once, InvokeID: %s, RT: %sms, Broker: %s\u0026quot;, invokeID, endTimestamp - beginTimestampPrev, mq), e);\rlog.warn(msg.toString());\rexception = e;\rcontinue;\r} catch (MQBrokerException e) {\rendTimestamp = System.currentTimeMillis();\rthis.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true);\rlog.warn(String.format(\u0026quot;sendKernelImpl exception, resend at once, InvokeID: %s, RT: %sms, Broker: %s\u0026quot;, invokeID, endTimestamp - beginTimestampPrev, mq), e);\rlog.warn(msg.toString());\rexception = e;\rif (this.defaultMQProducer.getRetryResponseCodes().contains(e.getResponseCode())) {\rcontinue;\r} else {\rif (sendResult != null) {\rreturn sendResult;\r}\rthrow e;\r}\r} catch (InterruptedException e) {\rendTimestamp = System.currentTimeMillis();\rthis.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false);\rlog.warn(String.format(\u0026quot;sendKernelImpl exception, throw exception, InvokeID: %s, RT: %sms, Broker: %s\u0026quot;, invokeID, endTimestamp - beginTimestampPrev, mq), e);\rlog.warn(msg.toString());\rthrow e;\r}\r} else {\rbreak;\r}\r}\rif (sendResult != null) {\rreturn sendResult;\r}\rString info = String.format(\u0026quot;Send [%d] times, still failed, cost [%d]ms, Topic: %s, BrokersSent: %s\u0026quot;,\rtimes,\rSystem.currentTimeMillis() - beginTimestampFirst,\rmsg.getTopic(),\rArrays.toString(brokersSent));\rinfo += FAQUrl.suggestTodo(FAQUrl.SEND_MSG_FAILED);\rMQClientException mqClientException = new MQClientException(info, exception);\rif (callTimeout) {\rthrow new RemotingTooMuchRequestException(\u0026quot;sendDefaultImpl call timeout\u0026quot;);\r}\rif (exception instanceof MQBrokerException) {\rmqClientException.setResponseCode(((MQBrokerException) exception).getResponseCode());\r} else if (exception instanceof RemotingConnectException) {\rmqClientException.setResponseCode(ClientErrorCode.CONNECT_BROKER_EXCEPTION);\r} else if (exception instanceof RemotingTimeoutException) {\rmqClientException.setResponseCode(ClientErrorCode.ACCESS_BROKER_TIMEOUT);\r} else if (exception instanceof MQClientException) {\rmqClientException.setResponseCode(ClientErrorCode.BROKER_NOT_EXIST_EXCEPTION);\r}\rthrow mqClientException;\r}\rvalidateNameServerSetting();\rthrow new MQClientException(\u0026quot;No route info of this topic: \u0026quot; + msg.getTopic() + FAQUrl.suggestTodo(FAQUrl.NO_TOPIC_ROUTE_INFO),\rnull).setResponseCode(ClientErrorCode.NOT_FOUND_TOPIC_EXCEPTION);\r}\r 这个方法是主流程，参数校验包括主题名称、消息体不能为空、消息长度不能等于0且默认不能超过允许发送消息的最大长度4M（maxMessageSize=1024 * 1024 * 4）\ntryToFindTopicPublishInfo 这个方法是查找主题路由信息\nprivate TopicPublishInfo tryToFindTopicPublishInfo(final String topic) {\r// 从map中取\rTopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic);\rif (null == topicPublishInfo || !topicPublishInfo.ok()) {\rthis.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo());\r// 向NameServer查询该topic的路由信息\rthis.mQClientFactory.updateTopicRouteInfoFromNameServer(topic);\rtopicPublishInfo = this.topicPublishInfoTable.get(topic);\r}\rif (topicPublishInfo.isHaveTopicRouterInfo() || topicPublishInfo.ok()) {\rreturn topicPublishInfo;\r} else {\r// 用默认主题去查询\rthis.mQClientFactory.updateTopicRouteInfoFromNameServer(topic, true, this.defaultMQProducer);\rtopicPublishInfo = this.topicPublishInfoTable.get(topic);\rreturn topicPublishInfo;\r}\r}\r 流程很简单，先在map中查，然后去NameServer去查，如果还没查到，那就使用默认主题去查\nselectOneMessageQueue 选择消息队列，选择消息队列有两种方式。\n sendLatencyFaultEnable=false，默认不启用Broker故障延迟机制。 sendLatencyFaultEnable=true，启用Broker故障延迟机制  为什么要有这两种机制呢？其实就是为了填之前nameServer路由发现时没有主动推送过来的坑，NameServer检测Broker是否可用是有延迟的，最短为一次心跳检测间隔（10s）；其次，NameServer不会检测到Broker宕机后马上推送消息给消息生产者，而是消息生产者每隔30s更新一次路由信息，所以消息生产者最快感知Broker最新的路由信息也需要30s\npublic MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) {\r// 启用Broker故障延迟机制\rif (this.sendLatencyFaultEnable) {\rtry {\rint index = tpInfo.getSendWhichQueue().incrementAndGet();\rfor (int i = 0; i \u0026lt; tpInfo.getMessageQueueList().size(); i++) {\rint pos = Math.abs(index++) % tpInfo.getMessageQueueList().size();\rif (pos \u0026lt; 0)\rpos = 0;\rMessageQueue mq = tpInfo.getMessageQueueList().get(pos);\r// 根据时间戳判断broker是否可用\rif (latencyFaultTolerance.isAvailable(mq.getBrokerName()))\rreturn mq;\r}\rfinal String notBestBroker = latencyFaultTolerance.pickOneAtLeast();\rint writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker);\rif (writeQueueNums \u0026gt; 0) {\rfinal MessageQueue mq = tpInfo.selectOneMessageQueue();\rif (notBestBroker != null) {\rmq.setBrokerName(notBestBroker);\rmq.setQueueId(tpInfo.getSendWhichQueue().incrementAndGet() % writeQueueNums);\r}\rreturn mq;\r} else {\r// 移除broker不可用队列\rlatencyFaultTolerance.remove(notBestBroker);\r}\r} catch (Exception e) {\rlog.error(\u0026quot;Error occurred when selecting message queue\u0026quot;, e);\r}\r// 默认不启用Broker故障延迟机制\rreturn tpInfo.selectOneMessageQueue();\r}\rreturn tpInfo.selectOneMessageQueue(lastBrokerName);\r}\r// 默认不启用Broker故障延迟机制的方法\rpublic MessageQueue selectOneMessageQueue(final String lastBrokerName) {\rif (lastBrokerName == null) {\rreturn selectOneMessageQueue();\r} else {\rfor (int i = 0; i \u0026lt; this.messageQueueList.size(); i++) {\r// 随机选取一个队列\rint index = this.sendWhichQueue.incrementAndGet();\rint pos = Math.abs(index) % this.messageQueueList.size();\rif (pos \u0026lt; 0)\rpos = 0;\rMessageQueue mq = this.messageQueueList.get(pos);\rif (!mq.getBrokerName().equals(lastBrokerName)) {\rreturn mq;\r}\r}\rreturn selectOneMessageQueue();\r}\r}\r 默认不启用Broker故障延迟机制的方法，就是与上次失败进行判断，而启用延迟方法就比较复杂，具体是，再发送失败后，首先会调用updateFaultItem方法\npublic void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) {\rif (this.sendLatencyFaultEnable) {\r// 计算broker不可用的时长\rlong duration = computeNotAvailableDuration(isolation ? 30000 : currentLatency);\r// 更新时间戳\rthis.latencyFaultTolerance.updateFaultItem(brokerName, currentLatency, duration);\r}\r}\rprivate long computeNotAvailableDuration(final long currentLatency) {\r// 从数组{50L, 100L, 550L, 1000L, 2000L, 3000L, 15000L}中拿接近的一个时间\rfor (int i = latencyMax.length - 1; i \u0026gt;= 0; i--) {\rif (currentLatency \u0026gt;= latencyMax[i])\rreturn this.notAvailableDuration[i];\r}\rreturn 0;\r}\rpublic void updateFaultItem(final String name, final long currentLatency, final long notAvailableDuration) {\rFaultItem old = this.faultItemTable.get(name);\rif (null == old) {\rfinal FaultItem faultItem = new FaultItem(name);\rfaultItem.setCurrentLatency(currentLatency);\r// 关键就在于这里，broker超过这个时间才可用\rfaultItem.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration);\rold = this.faultItemTable.putIfAbsent(name, faultItem);\rif (old != null) {\rold.setCurrentLatency(currentLatency);\rold.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration);\r}\r} else {\rold.setCurrentLatency(currentLatency);\rold.setStartTimestamp(System.currentTimeMillis() + notAvailableDuration);\r}\r}\r 这个代码也很简单，就是根据发送失败的时间差，从{50L, 100L, 550L, 1000L, 2000L, 3000L, 15000L}拿出一个最接近的最大的睡眠时间，并把时间戳更新进去\nsendKernelImpl 消息发送\nprivate SendResult sendKernelImpl(final Message msg,\rfinal MessageQueue mq,\rfinal CommunicationMode communicationMode,\rfinal SendCallback sendCallback,\rfinal TopicPublishInfo topicPublishInfo,\rfinal long timeout) throws MQClientException, RemotingException, MQBrokerException, InterruptedException {\rlong beginStartTime = System.currentTimeMillis();\rString brokerName = this.mQClientFactory.getBrokerNameFromMessageQueue(mq);\rString brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(brokerName);\rif (null == brokerAddr) {\rtryToFindTopicPublishInfo(mq.getTopic());\rbrokerName = this.mQClientFactory.getBrokerNameFromMessageQueue(mq);\rbrokerAddr = this.mQClientFactory.findBrokerAddressInPublish(brokerName);\r}\rSendMessageContext context = null;\rif (brokerAddr != null) {\rbrokerAddr = MixAll.brokerVIPChannel(this.defaultMQProducer.isSendMessageWithVIPChannel(), brokerAddr);\rbyte[] prevBody = msg.getBody();\rtry {\r//for MessageBatch,ID has been set in the generating process\rif (!(msg instanceof MessageBatch)) {\r// 生成消息唯一全局id\rMessageClientIDSetter.setUniqID(msg);\r}\rboolean topicWithNamespace = false;\rif (null != this.mQClientFactory.getClientConfig().getNamespace()) {\rmsg.setInstanceId(this.mQClientFactory.getClientConfig().getNamespace());\rtopicWithNamespace = true;\r}\rint sysFlag = 0;\rboolean msgBodyCompressed = false;\rif (this.tryToCompressMessage(msg)) {\rsysFlag |= MessageSysFlag.COMPRESSED_FLAG;\rsysFlag |= compressType.getCompressionFlag();\rmsgBodyCompressed = true;\r}\rfinal String tranMsg = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED);\rif (Boolean.parseBoolean(tranMsg)) {\rsysFlag |= MessageSysFlag.TRANSACTION_PREPARED_TYPE;\r}\r// 增强，执行勾子函数\rif (hasCheckForbiddenHook()) {\rCheckForbiddenContext checkForbiddenContext = new CheckForbiddenContext();\rcheckForbiddenContext.setNameSrvAddr(this.defaultMQProducer.getNamesrvAddr());\rcheckForbiddenContext.setGroup(this.defaultMQProducer.getProducerGroup());\rcheckForbiddenContext.setCommunicationMode(communicationMode);\rcheckForbiddenContext.setBrokerAddr(brokerAddr);\rcheckForbiddenContext.setMessage(msg);\rcheckForbiddenContext.setMq(mq);\rcheckForbiddenContext.setUnitMode(this.isUnitMode());\rthis.executeCheckForbiddenHook(checkForbiddenContext);\r}\r// 增强，执行勾子函数\rif (this.hasSendMessageHook()) {\rcontext = new SendMessageContext();\rcontext.setProducer(this);\rcontext.setProducerGroup(this.defaultMQProducer.getProducerGroup());\rcontext.setCommunicationMode(communicationMode);\rcontext.setBornHost(this.defaultMQProducer.getClientIP());\rcontext.setBrokerAddr(brokerAddr);\rcontext.setMessage(msg);\rcontext.setMq(mq);\rcontext.setNamespace(this.defaultMQProducer.getNamespace());\rString isTrans = msg.getProperty(MessageConst.PROPERTY_TRANSACTION_PREPARED);\rif (isTrans != null \u0026amp;\u0026amp; isTrans.equals(\u0026quot;true\u0026quot;)) {\rcontext.setMsgType(MessageType.Trans_Msg_Half);\r}\rif (msg.getProperty(\u0026quot;__STARTDELIVERTIME\u0026quot;) != null || msg.getProperty(MessageConst.PROPERTY_DELAY_TIME_LEVEL) != null) {\rcontext.setMsgType(MessageType.Delay_Msg);\r}\rthis.executeSendMessageHookBefore(context);\r}\rSendMessageRequestHeader requestHeader = new SendMessageRequestHeader();\r// 生产者组\rrequestHeader.setProducerGroup(this.defaultMQProducer.getProducerGroup());\r// 主题名称\rrequestHeader.setTopic(msg.getTopic());\r// 默认创建主题key\rrequestHeader.setDefaultTopic(this.defaultMQProducer.getCreateTopicKey());\rrequestHeader.setDefaultTopicQueueNums(this.defaultMQProducer.getDefaultTopicQueueNums());\rrequestHeader.setQueueId(mq.getQueueId());\rrequestHeader.setSysFlag(sysFlag);\rrequestHeader.setBornTimestamp(System.currentTimeMillis());\rrequestHeader.setFlag(msg.getFlag());\rrequestHeader.setProperties(MessageDecoder.messageProperties2String(msg.getProperties()));\rrequestHeader.setReconsumeTimes(0);\rrequestHeader.setUnitMode(this.isUnitMode());\rrequestHeader.setBatch(msg instanceof MessageBatch);\rif (requestHeader.getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) {\rString reconsumeTimes = MessageAccessor.getReconsumeTime(msg);\rif (reconsumeTimes != null) {\rrequestHeader.setReconsumeTimes(Integer.valueOf(reconsumeTimes));\rMessageAccessor.clearProperty(msg, MessageConst.PROPERTY_RECONSUME_TIME);\r}\rString maxReconsumeTimes = MessageAccessor.getMaxReconsumeTimes(msg);\rif (maxReconsumeTimes != null) {\rrequestHeader.setMaxReconsumeTimes(Integer.valueOf(maxReconsumeTimes));\rMessageAccessor.clearProperty(msg, MessageConst.PROPERTY_MAX_RECONSUME_TIMES);\r}\r}\rSendResult sendResult = null;\rswitch (communicationMode) {\rcase ASYNC:\rMessage tmpMessage = msg;\rboolean messageCloned = false;\rif (msgBodyCompressed) {\r//If msg body was compressed, msgbody should be reset using prevBody.\r//Clone new message using commpressed message body and recover origin massage.\r//Fix bug:https://github.com/apache/rocketmq-externals/issues/66\rtmpMessage = MessageAccessor.cloneMessage(msg);\rmessageCloned = true;\rmsg.setBody(prevBody);\r}\rif (topicWithNamespace) {\rif (!messageCloned) {\rtmpMessage = MessageAccessor.cloneMessage(msg);\rmessageCloned = true;\r}\rmsg.setTopic(NamespaceUtil.withoutNamespace(msg.getTopic(), this.defaultMQProducer.getNamespace()));\r}\rlong costTimeAsync = System.currentTimeMillis() - beginStartTime;\rif (timeout \u0026lt; costTimeAsync) {\rthrow new RemotingTooMuchRequestException(\u0026quot;sendKernelImpl call timeout\u0026quot;);\r}\rsendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage(\rbrokerAddr,\rbrokerName,\rtmpMessage,\rrequestHeader,\rtimeout - costTimeAsync,\rcommunicationMode,\rsendCallback,\rtopicPublishInfo,\rthis.mQClientFactory,\rthis.defaultMQProducer.getRetryTimesWhenSendAsyncFailed(),\rcontext,\rthis);\rbreak;\rcase ONEWAY:\rcase SYNC:\rlong costTimeSync = System.currentTimeMillis() - beginStartTime;\rif (timeout \u0026lt; costTimeSync) {\rthrow new RemotingTooMuchRequestException(\u0026quot;sendKernelImpl call timeout\u0026quot;);\r}\rsendResult = this.mQClientFactory.getMQClientAPIImpl().sendMessage(\rbrokerAddr,\rbrokerName,\rmsg,\rrequestHeader,\rtimeout - costTimeSync,\rcommunicationMode,\rcontext,\rthis);\rbreak;\rdefault:\rassert false;\rbreak;\r}\r// 增强，钩子函数\rif (this.hasSendMessageHook()) {\rcontext.setSendResult(sendResult);\rthis.executeSendMessageHookAfter(context);\r}\rreturn sendResult;\r} catch (RemotingException e) {\rif (this.hasSendMessageHook()) {\rcontext.setException(e);\rthis.executeSendMessageHookAfter(context);\r}\rthrow e;\r} catch (MQBrokerException e) {\rif (this.hasSendMessageHook()) {\rcontext.setException(e);\rthis.executeSendMessageHookAfter(context);\r}\rthrow e;\r} catch (InterruptedException e) {\rif (this.hasSendMessageHook()) {\rcontext.setException(e);\rthis.executeSendMessageHookAfter(context);\r}\rthrow e;\r} finally {\rmsg.setBody(prevBody);\rmsg.setTopic(NamespaceUtil.withoutNamespace(msg.getTopic(), this.defaultMQProducer.getNamespace()));\r}\r}\rthrow new MQClientException(\u0026quot;The broker[\u0026quot; + brokerName + \u0026quot;] not exist\u0026quot;, null);\r}\r 这个消息发送实际上并没有太多的可以讲的东西，可以学习的地方有通过钩子函数实现增强，真正发送消息的代码在sendMessage\npublic SendResult sendMessage(\rfinal String addr,\rfinal String brokerName,\rfinal Message msg,\rfinal SendMessageRequestHeader requestHeader,\rfinal long timeoutMillis,\rfinal CommunicationMode communicationMode,\rfinal SendCallback sendCallback,\rfinal TopicPublishInfo topicPublishInfo,\rfinal MQClientInstance instance,\rfinal int retryTimesWhenSendFailed,\rfinal SendMessageContext context,\rfinal DefaultMQProducerImpl producer\r) throws RemotingException, MQBrokerException, InterruptedException {\rlong beginStartTime = System.currentTimeMillis();\rRemotingCommand request = null;\rString msgType = msg.getProperty(MessageConst.PROPERTY_MESSAGE_TYPE);\rboolean isReply = msgType != null \u0026amp;\u0026amp; msgType.equals(MixAll.REPLY_MESSAGE_FLAG);\rif (isReply) {\rif (sendSmartMsg) {\rSendMessageRequestHeaderV2 requestHeaderV2 = SendMessageRequestHeaderV2.createSendMessageRequestHeaderV2(requestHeader);\rrequest = RemotingCommand.createRequestCommand(RequestCode.SEND_REPLY_MESSAGE_V2, requestHeaderV2);\r} else {\rrequest = RemotingCommand.createRequestCommand(RequestCode.SEND_REPLY_MESSAGE, requestHeader);\r}\r} else {\rif (sendSmartMsg || msg instanceof MessageBatch) {\rSendMessageRequestHeaderV2 requestHeaderV2 = SendMessageRequestHeaderV2.createSendMessageRequestHeaderV2(requestHeader);\rrequest = RemotingCommand.createRequestCommand(msg instanceof MessageBatch ? RequestCode.SEND_BATCH_MESSAGE : RequestCode.SEND_MESSAGE_V2, requestHeaderV2);\r} else {\rrequest = RemotingCommand.createRequestCommand(RequestCode.SEND_MESSAGE, requestHeader);\r}\r}\rrequest.setBody(msg.getBody());\rswitch (communicationMode) {\rcase ONEWAY:\rthis.remotingClient.invokeOneway(addr, request, timeoutMillis);\rreturn null;\rcase ASYNC:\rfinal AtomicInteger times = new AtomicInteger();\rlong costTimeAsync = System.currentTimeMillis() - beginStartTime;\rif (timeoutMillis \u0026lt; costTimeAsync) {\rthrow new RemotingTooMuchRequestException(\u0026quot;sendMessage call timeout\u0026quot;);\r}\rthis.sendMessageAsync(addr, brokerName, msg, timeoutMillis - costTimeAsync, request, sendCallback, topicPublishInfo, instance,\rretryTimesWhenSendFailed, times, context, producer);\rreturn null;\rcase SYNC:\rlong costTimeSync = System.currentTimeMillis() - beginStartTime;\rif (timeoutMillis \u0026lt; costTimeSync) {\rthrow new RemotingTooMuchRequestException(\u0026quot;sendMessage call timeout\u0026quot;);\r}\rreturn this.sendMessageSync(addr, brokerName, msg, timeoutMillis - costTimeSync, request);\rdefault:\rassert false;\rbreak;\r}\rreturn null;\r}j\r 几种发送方式都不同，主要看下同步发送\n\r ","id":21,"section":"posts","summary":"[TOC] 消息队列 消息队列主要应用场景为异步、削峰、解耦。 异步解耦 场景一：用户注册，注册时需要发送短信及邮件，并且需要写库 串行模式。流程如下： ​ 缺点","tags":["消息队列"],"title":"消息队列介绍","uri":"https://wzgl998877.github.io/2022/01/mq%E6%80%BB%E7%BB%93/","year":"2022"},{"content":"消息队列介绍 消息队列主要应用场景为异步、削峰、解耦。\n异步解耦 场景一：用户注册，注册时需要发送短信及邮件，并且需要写库\n 串行模式。流程如下：   ​ 缺点：流程不断加长，影响用户体验。\n  显然就会想到使用异步解决，发邮件的同时可以发短信，流程如下：\n缺点：流程越多，在注册时需要调很多其他接口，代码耦合严重，出现问题不好排查\n  使用消息队列，流程如下：\n  注册接口只需要写库，并且推送到消息队列，至于其他业务订阅消息队列即可，达到了异步并且解耦\n削峰 场景二：秒杀活动，一般由于瞬时访问量过大，服务器接收过大，会导致流量暴增，相关系统无法处理请求甚至崩溃。而加入消息队列后，系统作为消费者，根据自身应用的能力进行消息的消费，不受大流量的影响。流程如下：\n常见消息队列 RocketMQ 架构 RocketMQ架构上主要分为四部分，如上图所示:\n  Producer：生产者，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。RocketMQ 提供了三种方式发送消息：同步、异步和单向\n  同步发送：同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。\n  异步发送：异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。\n  单向发送：单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。\n     Consumer：消费者，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。\n  Pull：拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。\n  Push：推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。\n  集群消费: 默认就为该模式，相同Consumer Group的每个Consumer实例平均分摊消息。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。\n  广播消费：消息会发给消费者组中的每一个消费者进行消费。\n    NameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。主要包括两个功能：Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。\n  BrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。\n Remoting Module：整个Broker的实体，负责处理来自clients端的请求。 Client Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息 Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。 HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。 Index Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。    \n名词解释 Message Message（消息）就是要传输的信息。\n一条消息必须有一个主题（Topic），主题可以看做是你的信件要邮寄的地址。\n一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key(messageId) 并在 Broker 上查找此消息以便在开发期间查找问题。\nTopic Topic（主题）可以看做消息的规类，它是消息的第一级类型。比如一个电商系统可以分为：交易消息、物流消息等，一条消息必须有一个 Topic 。\nTopic 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。\n一个 Topic 也可以被 0个、1个、多个消费者订阅。\nTag Tag（标签）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。\nGroup 分组，一个组可以订阅多个Topic。\n分为ProducerGroup，ConsumerGroup，代表某一类的生产者和消费者，一般来说同一个服务可以作为Group，同一个Group一般来说发送和消费的消息都是一样的\nQueue 在Kafka中叫Partition，每个Queue内部是有序的，在RocketMQ中分为读和写两种队列，一般来说读写队列数量一致，如果不一致就会出现很多问题。\nMessage Queue Message Queue（消息队列），主题被划分为一个或多个子主题，即消息队列。\n一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic ，RocketMQ 会轮询该 Topic 下的所有队列将消息发出去。\n消息的物理管理单位。一个Topic下可以有多个Queue，Queue的引入使得消息的存储可以分布式集群化，具有了水平扩展能力。\nOffset 在RocketMQ 中，所有消息队列都是持久化，长度无限的数据结构，所谓长度无限是指队列中的每个存储单元都是定长，访问其中的存储单元使用Offset 来访问，Offset 为 java long 类型，64 位，理论上在 100年内不会溢出，所以认为是长度无限。\n也可以认为 Message Queue 是一个长度无限的数组，Offset 就是下标。\nMessage Order Message Order（消息顺序）有两种：Orderly（顺序消费）和Concurrently（并行消费）。\n  消息有序指的是可以按照消息的发送顺序来消费(FIFO)。RocketMQ可以严格的保证消息有序，可以分为分区有序或者全局有序。\n顺序消费的原理解析，在默认的情况下消息发送会采取Round Robin轮询方式把消息发送到不同的queue(分区队列)；而消费消息的时候从多个queue上拉取消息，这种情况发送和消费是不能保证顺序。但是如果控制发送的顺序消息只依次发送到同一个queue中，消费的时候只从这个queue上依次拉取，则就保证了顺序。当发送和消费参与的queue只有一个，则是全局有序；如果多个queue参与，则为分区有序，即相对每个queue，消息都是有序的。\npublic static void main(String[] args) throws Exception {\rDefaultMQProducer producer = new DefaultMQProducer(\u0026quot;order_queue\u0026quot;);\rproducer.setNamesrvAddr(\u0026quot;127.0.0.1:9876\u0026quot;);\rproducer.start();\rString[] tags = new String[]{\u0026quot;TagA\u0026quot;, \u0026quot;TagC\u0026quot;, \u0026quot;TagD\u0026quot;};\r// 订单列表\rList\u0026lt;OrderStep\u0026gt; orderList = new Producer().buildOrders();\rDate date = new Date();\rSimpleDateFormat sdf = new SimpleDateFormat(\u0026quot;yyyy-MM-dd HH:mm:ss\u0026quot;);\rString dateStr = sdf.format(date);\rfor (int i = 0; i \u0026lt; 10; i++) {\r// 加个时间前缀\rString body = dateStr + \u0026quot; Hello RocketMQ \u0026quot; + orderList.get(i);\rMessage msg = new Message(\u0026quot;TopicTest\u0026quot;, tags[i % tags.length], \u0026quot;KEY\u0026quot; + i, body.getBytes());\rSendResult sendResult = producer.send(msg, new MessageQueueSelector() {\r@Override\rpublic MessageQueue select(List\u0026lt;MessageQueue\u0026gt; mqs, Message msg, Object arg) {\rLong id = (Long) arg; //根据订单id选择发送queue\r// 通过取余保证相同id的订单进入同一个队列，消费时会顺序消费，从而保证了订单的创建过程（创建，完成，付款）\rlong index = id % mqs.size(); return mqs.get((int) index);\r}\r}, orderList.get(i).getOrderId());//订单id\rSystem.out.println(String.format(\u0026quot;SendResult status:%s, queueId:%d, body:%s\u0026quot;,\rsendResult.getSendStatus(),\rsendResult.getMessageQueue().getQueueId(),\rbody));\r}\rproducer.shutdown();\r}\r   并行消费不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制。\n  更多名词解释参考官网：https://github.com/apache/rocketmq/blob/master/docs/cn/concept.md\n通信机制 RocketMQ消息队列集群主要包括NameServer、Broker(Master/Slave)、Producer、Consumer4个角色，基本通讯流程如下：\n(1) Broker启动后需要完成一次将自己注册至NameServer的操作；随后每隔30s时间定时向NameServer上报Topic路由信息。\n(2) 消息生产者Producer作为客户端发送消息时候，需要根据消息的Topic从本地缓存的TopicPublishInfoTable获取路由信息。如果没有则更新路由信息会从NameServer上重新拉取，同时Producer会默认每隔30s向NameServer拉取一次路由信息。\n(3) 消息生产者Producer根据2）中获取的路由信息选择一个队列（MessageQueue）进行消息发送；Broker作为消息的接收者接收消息并落盘存储。\n(4) 消息消费者Consumer根据2）中获取的路由信息，并在完成客户端的负载均衡后，选择其中的某一个或者某几个消息队列来拉取消息并进行消费。\nRocketMQ中的负载均衡 RocketMQ中的负载均衡都在Client端完成，具体来说的话，主要可以分为Producer端发送消息时候的负载均衡和Consumer端订阅消息的负载均衡。\n  producer端\nProducer端在发送消息的时候，会先根据Topic找到指定的TopicPublishInfo，在获取了TopicPublishInfo路由信息后，RocketMQ的客户端在默认方式下selectOneMessageQueue()方法会从TopicPublishInfo中的messageQueueList中选择一个队列（MessageQueue）进行发送消息。具体的容错策略均在MQFaultStrategy这个类中定义。这里有一个sendLatencyFaultEnable开关变量，如果开启，在随机递增取模的基础上，再过滤掉not available的Broker代理。所谓的\u0026quot;latencyFaultTolerance\u0026quot;，是指对之前失败的，按一定的时间做退避。例如，如果上次请求的latency超过550Lms，就退避3000Lms；超过1000L，就退避60000L；如果关闭，采用随机递增取模的方式选择一个队列（MessageQueue）来发送消息，latencyFaultTolerance机制是实现消息发送高可用的核心关键所在。\n    Consumer的负载均衡 在RocketMQ中，Consumer端的两种消费模式（Push/Pull）都是基于拉模式来获取消息的，而在Push模式只是对pull模式的一种封装，其本质实现为消息拉取线程在从服务器拉取到一批消息后，然后提交到消息消费线程池后，又“马不停蹄”的继续向服务器再次尝试拉取消息。如果未拉取到消息，则延迟一下又继续拉取。在两种基于拉模式的消费方式（Push/Pull）中，均需要Consumer端在知道从Broker端的哪一个消息队列—队列中去获取消息。因此，有必要在Consumer端来做负载均衡，即Broker端中多个MessageQueue分配给同一个ConsumerGroup中的哪些Consumer消费。\n1、Consumer端的心跳包发送\n在Consumer启动后，它就会通过定时任务不断地向RocketMQ集群中的所有Broker实例发送心跳包（其中包含了，消息消费分组名称、订阅关系集合、消息通信模式和客户端id的值等信息）。Broker端在收到Consumer的心跳消息后，会将它维护在ConsumerManager的本地缓存变量—consumerTable，同时并将封装后的客户端网络通道信息保存在本地缓存变量—channelInfoTable中，为之后做Consumer端的负载均衡提供可以依据的元数据信息。\n2、Consumer端实现负载均衡的核心类—RebalanceImpl\n在Consumer实例的启动流程中的启动MQClientInstance实例部分，会完成负载均衡服务线程—RebalanceService的启动（每隔20s执行一次）。通过查看源码可以发现，RebalanceService线程的run()方法最终调用的是RebalanceImpl类的rebalanceByTopic()方法，该方法是实现Consumer端负载均衡的核心。这里，rebalanceByTopic()方法会根据消费者通信类型为“广播模式”还是“集群模式”做不同的逻辑处理。这里主要来看下集群模式下的主要处理流程：\n(1) 从rebalanceImpl实例的本地缓存变量—topicSubscribeInfoTable中，获取该Topic主题下的消息消费队列集合（mqSet）；\n(2) 根据topic和consumerGroup为参数调用mQClientFactory.findConsumerIdList()方法向Broker端发送获取该消费组下消费者Id列表的RPC通信请求（Broker端基于前面Consumer端上报的心跳包数据而构建的consumerTable做出响应返回，业务请求码：GET_CONSUMER_LIST_BY_GROUP）；\n(3) 先对Topic下的消息消费队列、消费者Id排序，然后用消息队列分配策略算法（默认为：消息队列的平均分配算法），计算出待拉取的消息队列。这里的平均分配算法，类似于分页的算法，将所有MessageQueue排好序类似于记录，将所有消费端Consumer排好序类似页数，并求出每一页需要包含的平均size和每个页面记录的范围range，最后遍历整个range而计算出当前Consumer端应该分配到的记录（这里即为：MessageQueue）。\n\n(4) 然后，调用updateProcessQueueTableInRebalance()方法，具体的做法是，先将分配到的消息队列集合（mqSet）与processQueueTable做一个过滤比对。\n\n 上图中processQueueTable标注的红色部分，表示与分配到的消息队列集合mqSet互不包含。将这些队列设置Dropped属性为true，然后查看这些队列是否可以移除出processQueueTable缓存变量，这里具体执行removeUnnecessaryMessageQueue()方法，即每隔1s 查看是否可以获取当前消费处理队列的锁，拿到的话返回true。如果等待1s后，仍然拿不到当前消费处理队列的锁则返回false。如果返回true，则从processQueueTable缓存变量中移除对应的Entry； 上图中processQueueTable的绿色部分，表示与分配到的消息队列集合mqSet的交集。判断该ProcessQueue是否已经过期了，在Pull模式的不用管，如果是Push模式的，设置Dropped属性为true，并且调用removeUnnecessaryMessageQueue()方法，像上面一样尝试移除Entry；  最后，为过滤后的消息队列集合（mqSet）中的每个MessageQueue创建一个ProcessQueue对象并存入RebalanceImpl的processQueueTable队列中（其中调用RebalanceImpl实例的computePullFromWhere(MessageQueue mq)方法获取该MessageQueue对象的下一个进度消费值offset，随后填充至接下来要创建的pullRequest对象属性中），并创建拉取请求对象—pullRequest添加到拉取列表—pullRequestList中，最后执行dispatchPullRequest()方法，将Pull消息的请求对象PullRequest依次放入PullMessageService服务线程的阻塞队列pullRequestQueue中，待该服务线程取出后向Broker端发起Pull消息的请求。其中，可以重点对比下，RebalancePushImpl和RebalancePullImpl两个实现类的dispatchPullRequest()方法不同，RebalancePullImpl类里面的该方法为空，这样子也就回答了上一篇中最后的那道思考题了。\n消息消费队列在同一消费组不同消费者之间的负载均衡，其核心设计理念是在一个消息消费队列在同一时间只允许被同一消费组内的一个消费者消费，一个消息消费者能同时消费多个消息队列。\n  ","id":22,"section":"posts","summary":"消息队列介绍 消息队列主要应用场景为异步、削峰、解耦。 异步解耦 场景一：用户注册，注册时需要发送短信及邮件，并且需要写库 串行模式。流程如下： ​ 缺","tags":["消息队列"],"title":"消息队列介绍","uri":"https://wzgl998877.github.io/2022/01/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","year":"2022"},{"content":"深入理解Java虚拟机 第2章 Java内存区域与内存溢出异常 程序计数器 ​\t程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。\n​\t由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。\n​\t此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。\nJava虚拟机栈 ​\t与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。\n​\t局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。\n​\t局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。\n本地方法栈 ​\t本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。\nJava堆 ​\tJava堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。\n方法区 ​\t它用于存储已被虚拟机加载的类信息(又叫对象类型数据，类的全路径名、类的直接超类的全限定名、类(接口)的类型、类的直接接口全限定名的有序列表)、常量、静态变量、即时编译器编译后的代码等数据。\n运行时常量池 ​\t运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。\n​\t运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。\n​\t在JDK 1.6中，intern()方法会把首次遇到的字符串实例复制到永久代中，返回的也是永久代中这个字符串实例的引用，而由StringBuilder创建的字符串实例在Java堆上，所以必然不是同一个引用，将返回false。而JDK 1.7（以及部分其他虚拟机，例如JRockit）的intern()实现不会再复制实例，只是在运行时常量池中记录首次出现的实例引用，因此intern()返回的引用和由StringBuilder创建的那个字符串实例是同一个。\n几个概念：\n 常量池：位于Class文件中，用于存放编译期生成的各种字面量(文本字符串 、八种基本类型的值 、被声明为final的常量等)和符号引用(类和方法的全限定名 、字段的名称和描述符 、方法的名称和描述符)，这部分内容将在类加载后进入方法区的运行时常量池中存放。 运行时常量池：位于方法区中，用于存放各种字面量和符号引用。 字符串常量池：从jdk1.7开始，将属于方法区（永久代）中的字符串常量池放入了堆中。  public static void main(String[] args) {\r// s位于常量池在class文件中，在类加载后就到了运行时常量池，jdk1.6后就属于了字符串常量池，所以是在堆中的\rString s =\u0026quot;1234\u0026quot;;\r// 在jvm中第一次出现，于是s.intern会返回一个指向字符串常量池的引用指向s\rSystem.out.println(s.intern()==s);\r// 字符串常量池已经有了,不会新建对象，直接返回引用\rString s1 = \u0026quot;1234\u0026quot;;\r// s1不是首次出现的，所以s1返回的就是指向堆里字符串常量池的引用也就是第一次出现的s\rSystem.out.println(s1.intern()==s1);\r// 因为在jvm运行时，java这个字符串就已经在字符串常量池了\rString s2 = new StringBuilder(\u0026quot;ja\u0026quot;).append(\u0026quot;va\u0026quot;).toString();\rSystem.out.println(s2.intern()==s2);\r// 在jvm中第一次出现，于是s3.intern只会在运行时常量池里保存第一个的引用指向s3\rString s3 = new StringBuilder(\u0026quot;123456\u0026quot;).append(\u0026quot;789\u0026quot;).toString();\r// 这已经不是第一次出现了，不会保存这个引用，而是继续指向s3\rString s4 = new StringBuilder(\u0026quot;123456\u0026quot;).append(\u0026quot;789\u0026quot;).toString();\rSystem.out.println(s3.intern()==s3);\rSystem.out.println(s4.intern()==s4);\rSystem.out.println(s4.intern()==s3);\r// 创建了两个对象，在直接new指令前先去字符串常量池去检查是否有这个对象如果没有那么先在字符串常量池创建这个对象然后再去堆上创建对象\rString s5 = new String(\u0026quot;hello\u0026quot;);\r// Byte,Short,Integer,Long,Character这5种整型的包装类也实现了常量池技术，很显然在堆上，默认-128 ~ 127 缓存\rInteger a = 127;\rInteger b = 127;\rSystem.out.println(a==b);\rInteger c = new Integer(127);\rSystem.out.println(c==a);\r// 超出了包装类的缓存区，所以在常量池里没有\rInteger d = 128;\rInteger e = 128;\rSystem.out.println(d==e);\r}\r/**\r结果\rture\rtrue\rfasle\rtrue\rfasle\rtrue\rtrue\rfalse\rfalse\r**/\r 直接内存 ​\t直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。\n​\t在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。\n对象的创建 Java是一门面向对象的编程语言，在Java程序运行过程中无时无刻都有对象被创建出来。在语言层面上，创建对象（例如克隆、反序列化）通常仅仅是一个new关键字而已，而在虚拟机中，对象（文中讨论的对象限于普通Java对象，不包括数组和Class对象等）的创建又是怎样一个过程呢？\n  虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。\n  在类加载检查通过后，接下来虚拟机将为新生对象分配内存。\n为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。假设Java堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为“指针碰撞”（Bump the Pointer）。如果Java堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那就没有办法简单地进行指针碰撞了，虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空闲列表”（Free List）。\n  内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值。这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。（零值就是默认值比如int为0）。\n  虚拟机要对对象进行必要的设置。例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头（Object Header）之中。\n  在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从Java程序的视角来看，对象创建才刚刚开始——方法还没有执行，所有的字段都还为零。所以，一般来说（由字节码中是否跟随invokespecial指令所决定），执行new指令之后会接着执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。\n  对象的内存布局 ​\t在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。\n 对象头：第一部分用于存储对象自身的运行时数据。对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不一定要经过对象本身，这点将在2.3.3节讨论。另外，如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是从数组的元数据中却无法确定数组的大小。 实例数据：是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。 对齐填充：并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说，就是对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或者2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。  对象的访问定位 ​\t建立对象是为了使用对象，我们的Java程序需要通过栈上的reference数据来操作堆上的具体对象。\n第3章 垃圾收集器与内存分配策略 ​\t程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭。因此这几个区域的内存分配和回收都具备确定性，在这几个区域内就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。而Java堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序处于运行期间时才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器所关注的是这部分内存。\n对象已死吗 ​\t在堆里面存放着Java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前，第一件事情就是要确定这些对象之中哪些还“存活”着，哪些已经“死去”（即不可能再被任何途径使用的对象）。\n引用计数算法 ​\t很多教科书判断对象是否存活的算法是这样的：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减1；任何时刻计数器为0的对象就是不可能再被使用的。但是，至少主流的Java虚拟机里面没有选用引用计数算法来管理内存，其中最主要的原因是它很难解决对象之间相互循环引用的问题。\n可达性分析算法 ​\t这个算法的基本思路就是通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（用图论的话来说，就是从GC Roots到这个对象不可达）时，则证明此对象是不可用的。在Java语言中，可作为GC Roots的对象包括下面几种：\n 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI（即一般说的Native方法）引用的对象。  再谈引用 ​\t在JDK 1.2以前，Java中的引用的定义很传统：如果reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块内存代表着一个引用。\n​\t在JDK 1.2之后，Java对引用的概念进行了扩充，将引用分为强引用（StrongReference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。\n 强引用就是指在程序代码之中普遍存在的，类似“Object obj = newObject()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK 1.2之后，提供了SoftReference类来实现软引用。 弱引用也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2之后，提供了WeakReference类来实现弱引用。 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2之后，提供了PhantomReference类来实现虚引用。  生存还是死亡 ​\t即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。\n​\t如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是，如果一个对象在finalize()方法中执行缓慢，或者发生了死循环（更极端的情况），将很可能会导致F-Queue队列中其他对象永久处于等待，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移除出“即将回收”的集合。如果对象这时候还没有逃脱，那基本上它就真的被回收了。任何一个对象的finalize()方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize()方法不会被再次执行。\n判断对象是否已经死亡：\n 对象在进行可达性分析后发现没有与GC Roots相连接的引用链. 判断此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过都已经没必要执行，此时被回收。 如果被判定有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queue的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。若没有自救则被回收。  回收方法区 ​\t永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。\n  回收废弃常量：其与回收Java堆中的对象非常类似，假如一个字符串“abc”已经进入了常量池中，但是当前系统没有任何一个String对象是叫做“abc”的，换句话说，就是没有任何String对象引用常量池中的“abc”常量，也没有其他地方引用了这个字面量，如果这时发生内存回收，而且必要的话，这个“abc”常量就会被系统清理出常量池。常量池中的其他类（接口）、方法、字段的符号引用也与此类似。\n  回收无用的类：类需要同时满足下面3个条件才能算是“无用的类”：\n 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。  虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样，不使用了就必然会回收。\n  垃圾收集算法 标记-清除算法 ​\t最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。\n​\t它的主要不足有两个：一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。\n复制算法 ​\t它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。\n现在的商业虚拟机都采用这种收集算法来回收新生代\n新生代中的对象98%是“朝生夕死”的，所以并不需要按照1∶1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor[插图]。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。\n标记-整理算法 ​\t复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。\n分代收集算法 ​\t当前商业虚拟机的垃圾收集都采用“分代收集”（GenerationalCollection）算法，一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记—清理”或者“标记—整理”算法来进行回收。\nHotSpot的算法实现 ​\t可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行——这里“一致性”的意思是指在整个分析期间整个执行系统看起来就像被冻结在某个时间点上，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果准确性就无法得到保证。这点是导致GC进行时必须停顿所有Java执行线程（Sun将这件事情称为“Stop The World”）的其中一个重要原因，即使是在号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。\n​\t后面这本书中讲了几种虚拟机对这些算法的实现，看不进去。。\n内存分配与回收策略 ​\tJava技术体系中所提倡的自动内存管理最终可以归结为自动化地解决了两个问题：给对象分配内存以及回收分配给对象的内存。\n​\t对象的内存分配，往大方向讲，就是在堆上分配（但也可能经过JIT编译后被拆散为标量类型并间接地栈上分配），对象主要分配在新生代的Eden区上，如果启动了本地线程分配缓冲，将按线程优先在TLAB上分配。\n大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。\n 新生代GC（Minor GC):指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 老年代GC（Major GC / Full GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。Major GC的速度一般会比Minor GC慢10倍以上。  大对象直接进入老年代，所谓的大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组\n长期存活的对象将进入老年代\n​\t既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这点，虚拟机给每个对象定义了一个对象年龄（Age）计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor区中每“熬过”一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就将会被晋升到老年代中。\n​\t为了能更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。\n空间分配担保 ​\t在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC。\n​\t新生代使用复制收集算法，但为了内存利用率，只使用其中一个Survivor空间来作为轮换备份，因此当出现大量对象在Minor GC后仍然存活的情况（最极端的情况就是内存回收后新生代中所有对象都存活），就需要老年代进行分配担保，把Survivor无法容纳的对象直接进入老年代。与生活中的贷款担保类似，老年代要进行这样的担保，前提是老年代本身还有容纳这些对象的剩余空间，一共有多少对象会活下来在实际完成内存回收之前是无法明确知道的，所以只好取之前每一次回收晋升到老年代对象容量的平均大小值作为经验值，与老年代的剩余空间进行比较，决定是否进行Full GC来让老年代腾出更多空间。\n取平均值进行比较其实仍然是一种动态概率的手段，也就是说，如果某次Minor GC存活后的对象突增，远远高于平均值的话，依然会导致担保失败（Handle Promotion Failure）。如果出现了HandlePromotionFailure失败，那就只好在失败后重新发起一次Full GC。\n第6章 类文件结构 ​\t代码编译的结果从本地机器码转变为字节码，是存储格式发展的一小步，却是编程语言发展的一大步。\n​\t计算机只认识0和1，所以我们写的程序需要经编译器翻译成由0和1构成的二进制格式才能由计算机执行，将我们编写的程序编译成二进制本地机器码（Native Code）已不再是唯一的选择，越来越多的程序语言选择了与操作系统和机器指令集无关的、平台中立的格式作为程序编译后的存储格式。\n无关性的基石 ​\t如果计算机的CPU指令集只有x86一种，操作系统也只有Windows一种，那也许Java语言就不会出现。Java在刚刚诞生之时曾经提出过一个非常著名的宣传口号：“一次编写，到处运行（Write Once，Run Anywhere）”，Sun公司以及其他虚拟机提供商发布了许多可以运行在各种不同平台上的虚拟机，这些虚拟机都可以载入和执行同一种平台无关的字节码，从而实现了程序的“一次编写，到处运行”。\n​\t各种不同平台的虚拟机与所有平台都统一使用的程序存储格式——字节码（ByteCode）是构成平台无关性的基石但本节标题中刻意省略了“平台”二字，那是因为笔者注意到虚拟机的另外一种中立特性——语言无关性正越来越被开发者所重视。\n实现语言无关性的基础仍然是虚拟机和字节码存储格式。Java虚拟机不和包括Java在内的任何语言绑定，它只与“Class文件”这种特定的二进制文件格式所关联。\n后面详细讲了class文件的结构这个就不仔细写了。\n第7章 虚拟机类加载机制 ​\t虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。与那些在编译时需要进行连接工作的语言不同，在Java语言里面，类型的加载、连接和初始化过程都是在程序运行期间完成的。\n​\t类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）7个阶段。其中验证、准备、解析3个部分统称为连接（Linking）。\n对于初始化阶段，虚拟机规范则是严格规定了有且只有5种情况必须立即对类进行“初始化”\n 遇到new、getstatic、putstatic或invokestatic这4条字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。生成这4条指令的最常见的Java代码场景是：使用new关键字实例化对象的时候、读取或设置一个类的静态字段（被final修饰、已在编译期把结果放入常量池的静态字段除外）的时候，以及调用一个类的静态方法的时候。 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父接口全部都完成了初始化，只有在真正使用到父接口的时候（如引用接口中定义的常量）才会初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类 当使用JDK 1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化。  这5种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。\npackage com.zt.javastudy.grammar;\rimport javafx.scene.control.Pagination;\r/**\r* @author zhengtao\r* @description jvm中对类的加载过程的学习\r* @date 2021/3/18\r*/\rpublic class JVMStudyTwo {\rstatic {\rSystem.out.println(\u0026quot;JVM Study\u0026quot;);\r}\r// 用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类,所以只要运行这个main方法肯定最先初始化这个类\rpublic static void main(String[] args) {\r// 对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化\rSystem.out.println(SubClass.value);\r// 数组定义来引用类，不会触发此类的初始化\rSuperClass[] superClasses = new SuperClass[10];\r// 常量在编译阶段已经存储到了本类的常量池中去了，其实就相当于自身常量池的引用\rSystem.out.println(SubClass.HELLOWORLD);\rSystem.out.println(SuperClass.HELLOWORLD == SubClass.HELLOWORLD);\r}\r}\rclass SubClass extends SuperClass{\rstatic {\rSystem.out.println(\u0026quot;子类初始化\u0026quot;);\r}\rpublic static final String HELLOWORLD = \u0026quot;hello world！\u0026quot;;\r}\rclass SuperClass{\rstatic {\rSystem.out.println(\u0026quot;父类初始化\u0026quot;);\r}\rpublic static int value = 123;\rpublic static final String HELLOWORLD = \u0026quot;hello world\u0026quot;;\r}\r 加载 在加载阶段，虚拟机需要完成以下3件事情：\n 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。  相对于类加载过程的其他阶段，一个非数组类的加载阶段（准确地说，是加载阶段中获取类的二进制字节流的动作）是开发人员可控性最强的，因为加载阶段既可以使用系统提供的引导类加载器来完成，也可以由用户自定义的类加载器去完成，开发人员可以通过定义自己的类加载器去控制字节流的获取方式（即重写一个类加载器的loadClass()方法）。对于数组类而言，情况就有所不同，数组类本身不通过类加载器创建，它是由Java虚拟机直接创建的。\n​\t加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区之中,然后在内存中实例化一个java.lang.Class类的对象（并没有明确规定是在Java堆中，对于HotSpot虚拟机而言，Class对象比较特殊，它虽然是对象，但是存放在方法区里面），这个对象将作为程序访问方法区中的这些类型数据的外部接口。\n验证 ​\t验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。从执行性能的角度上讲，验证阶段的工作量在虚拟机的类加载子系统中又占了相当大的一部分。\n准备 ​\t准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。首先，这时候进行内存分配的仅包括类变量（被static修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中。其次，这里所说的初始值“通常情况”下是数据类型的零值。\npublic static int value = 123;\r// 在准备阶段分配内存，并初始化为0\r 在“通常情况”下初始值是零值，那相对的会有一些“特殊情况”：如果类字段的字段属性表中存在ConstantValue属性，那在准备阶段变量value就会被初始化为ConstantValue属性所指定的值，也就是final修饰的常量。\npublic static int value = 123;\r// 由于是常量直接被初始化为123\r 解析 ​\t解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。\n 符号引用（Symbolic References）：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。 直接引用（Direct References）：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。  初始化 ​\t在准备阶段，变量已经赋过一次系统要求的初始值，而在初始化阶段，则根据程序员通过程序制定的主观计划去初始化类变量和其他资源，或者可以从另外一个角度来表达：初始化阶段是执行类构造器()方法的过程。\n ()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。 ()方法与类的构造函数（或者说实例构造器()方法）不同，它不需要显式地调用父类构造器，虚拟机会保证在子类的()方法执行之前，父类的()方法已经执行完毕。因此在虚拟机中第一个被执行的()方法的类肯定是java.lang.Object。 由于父类的()方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成()方法。但接口与类不同的是，执行接口的()方法不需要先执行父接口的()方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的()方法 虚拟机会保证一个类的()方法在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的()方法，其他线程都需要阻塞等待，直到活动线程执行()方法完毕。  类加载器 ​\t从Java虚拟机的角度来讲，只存在两种不同的类加载器：一种是启动类加载器（Bootstrap ClassLoader），这个类加载器使用C++语言实现，是虚拟机自身的一部分；另一种就是所有其他的类加载器，这些类加载器都由Java语言实现，独立于虚拟机外部，并且全都继承自抽象类java.lang.ClassLoader。\n  启动类加载器（Bootstrap ClassLoader）：前面已经介绍过，这个类将器负责将存放在\u0026lt;JAVA_HOME\u0026gt;\\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如rt.jar，名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机内存中。\n  扩展类加载器（Extension ClassLoader）：这个加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载\u0026lt;JAVA_HOME\u0026gt;\\lib\\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。\n  应用程序类加载器（Application ClassLoader）：这个类加载器由sun.misc.Launcher$App-ClassLoader实现。由于这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器。\n三个加载器的层级为启动类加载器为高层，接下来为扩展类加载器然后应用类加载器，最后是用户自定义的加载器。\n  双亲委派模型 ​\t双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。这里类加载器之间的父子关系一般不会以继承（Inheritance）的关系来实现，而是都使用组合（Composition）关系来复用父加载器的代码。\n​\t双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。\n​\t使用双亲委派模型来组织类加载器之间的关系，有一个显而易见的好处就是Java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar之中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。双亲委派模型对于保证Java程序的稳定运作很重要。\n","id":23,"section":"posts","summary":"深入理解Java虚拟机 第2章 Java内存区域与内存溢出异常 程序计数器 ​ 程序计数器（Program Counter Register）是一块较小的内存空间，它","tags":["JVM"],"title":"深入理解Java虚拟机","uri":"https://wzgl998877.github.io/2022/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%A6%E4%B9%A0/","year":"2022"},{"content":"知识点！！！ 分布式session Nginx 正向代理 正向代理代理的对象是客户端，正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。配置如下：\n1. server{\r# dns必须要\r2. resolver 8.8.8.8;\r3. resolver_timeout 30s;\r4. listen 82;\r5. location / {\r6. proxy_pass http://$http_host$request_uri;\r7. proxy_set_header Host $http_host;\r8. proxy_buffers 256 4k;\r9. proxy_max_temp_file_size 0;\r10. proxy_connect_timeout 30;\r11. proxy_cache_valid 200 302 10m;\r12. proxy_cache_valid 301 1h;\r13. proxy_cache_valid any 1m;\r14. }\r15. }\r 配置好后，重启nginx，以浏览器为例，要使用这个代理服务器，必须将浏览器代理设置为http://+服务器ip地址+:+82（82是刚刚设置的端口号）即可使用了，这样就相当于我们访问的是代理服务，代理服务器再去访问我们目标网站。\n反向代理 反向代理代理的对象是服务端。反向代理正好相反，对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容原本就是它自己的一样。反向代理基本配置：\n1. http {\r2. # 省略了前面一般的配置，直接从负载均衡这里开始\r3. # 设置地址池，后端3台服务器，负载均衡这里是\r4. upstream http_server_pool {\r5. server 192.168.1.2:8080 weight=2 max_fails=2 fail_timeout=30s;\r6. server 192.168.1.3:8080 weight=3 max_fails=2 fail_timeout=30s;\r7. server 192.168.1.4:8080 weight=4 max_fails=2 fail_timeout=30s;\r8. }\r9. # 一个虚拟主机，用来反向代理http_server_pool这组服务器\r10. server {\r11. listen 80;\r12. # 外网访问的域名\r13. server_name www.test.com;\r14. location / {\r15. # 后端服务器返回500 503 404错误，自动请求转发到upstream池中另一台服务器\r16. proxy_next_upstream error timeout invalid_header http_500 http_503 http_404;\r17. proxy_pass http://http_server_pool;\r18. proxy_set_header Host www.test.com;\r19. proxy_set_header X-Real-IP $remote_addr;\r20. proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r21. }\r22. access_log logs/www.test.com.access.log combined;\r23. }\r24. }\r 反向代理就是在请求时，请求这个服务器的请求转发到真正处理的服务器，其中可以加上一些负载均衡之类的，比如现在请求，http://www.test.com, 就会将这个请求重定向到http://http_server_pool ，然后这个是自己定义的，第4行代码，然后就根据权重轮询发送到对应的服务器。\n基本配置 博客 https://www.cnblogs.com/54chensongxia/p/12938929.html\n具体的location指令 博客 https://www.jianshu.com/p/b010c9302cd0\n生产遇到的问题，调支付宝下单接口总是返回\norg.apache.http.NoHttpResponseException: open-sea.alipay.com:8449 failed to respond\rat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:143)\rat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:57)\rat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:261)\rat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:165)\rat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:167)\rat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:272)\rat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:124)\rat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:271)\rat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\rat org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88)\rat org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)\rat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\rat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\rat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)\rat com.jlpay.commons.tools.http.HttpServiceImpl.execute(HttpServiceImpl.java:178)\rat com.taifung.qrcode.chn.alipay.service.trans.OfflinepayService.doOfflinepayCn(OfflinepayService.java:282)\rat com.taifung.qrcode.chn.alipay.service.trans.OfflinepayService.executeCommand(OfflinepayService.java:52)\rat com.taifung.qrcode.chn.alipay.service.trans.OfflinepayService.executeCommand(OfflinepayService.java:33)\rat com.taifung.qrcode.chn.alipay.service.trans.AlipayService.execute(AlipayService.java:28)\rat com.jlpay.commons.rpc.thrift.server.Dispatcher.invoke(Dispatcher.java:47)\rat com.jlpay.commons.rpc.thrift.server.RpcAdapterImpl.Invoke(RpcAdapterImpl.java:32)\rat com.jlpay.commons.rpc.thrift.server.RpcAdapter$Processor$Invoke.getResult(RpcAdapter.java:175)\rat com.jlpay.commons.rpc.thrift.server.RpcAdapter$Processor$Invoke.getResult(RpcAdapter.java:160)\rat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\rat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\rat org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)\rat org.apache.thrift.server.Invocation.run(Invocation.java:18)\rat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\rat java.util.concurrent.FutureTask.run(FutureTask.java:266)\rat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\rat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\rat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\rat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\rat java.lang.Thread.run(Thread.java:748)\r 观察日志，发现报这个错都是3秒后，报错，然后看ng配置\nserver {\rlisten 8449;\rproxy_connect_timeout 1s; // 默认值60s, nginx连接到后端服务器的连接超时时间\rproxy_timeout 3s; // 默认值为10分钟，nginx接收后端服务器的响应超时时间\rproxy_pass alipay;\r}\rupstream alipay {\rhash $remote_addr consistent;\rserver open.alipaymo.com:443 weight=5 max_fails=3 fail_timeout=30s; //max_fails默认值为1,fail_timeout默认值为10s,max_fails=0表示不做检查\r// max_fails和fail_timeout参数是配合使用的，按默认值，当某台upstream server挂了，表示在10s(fail_timeout)之内，有1(max_fails)个请求打到这台挂了的服务器，nginx就会把这台upstream server设为down机的状态，时长是10s，在这10s内如果又有请求进来，就不会被转到这台server上，过了10s重新认为这台server又恢复了配置的意义\r}\r 所以说，问题就是，调支付宝下单接口三秒后还没返回，所以ng就把连接断开了，解决办法将超时时间调成了5s，这个错误的次数也出现的少了。\nshell #!/bin/bash\r# 不可自行修改脚本，统一管理\r# 开启脚本调试\r#set -x\r#设置生效那个环境配置文件\r#---- start.sh stdout 为开启stdout.log\r# 目前有 dev kx.verify kx.prod 这几个环境，对应application-*.yml 的配置文件\r# 定义变量，\rPROFILES_ACTIVE=dev\r# 定义日志输出位置，如果没定义的话，在./logs目录 必须/结尾\rLOGPATH=./logs/\r#设置 Sentinel 控制台地址 -Dcsp.sentinel.dashboard.server=172.20.21.40:58080\r#SENTINEL_OPTS='-Dcsp.sentinel.dashboard.server=localhost:58080 -Dproject.name=openAccess -Dcsp.sentinel.log.use.pid=true'\rJAVA_OPTS=\u0026quot; -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true \u0026quot;\r# echo 用于字符串的输出，-e开启转义\r# 函数定义方法，函数定义必须放在调用前 \\033[032;1m$@\\033[0m加颜色用的\rgreen_echo () { echo -e \u0026quot;\\033[032;1m$@\\033[0m\u0026quot;; }\r# 定位到运行脚本的相对位置\rcd `dirname $0`\rBIN_DIR=`pwd`\rcd ..\rDEPLOY_DIR=`pwd`\rCONF_DIR=$DEPLOY_DIR/config\rLOGS_FILE=$DEPLOY_DIR/logs\r# -z 检测字符串长度是否为0，为0返回 true。\r# -n 检测字符串长度是否不为 0，不为 0 返回 true。\r# $ 检测字符串是否为空，不为空返回 true。\rif [ -z \u0026quot;$SERVER_NAME\u0026quot; ]; then\r# 主机名\rSERVER_NAME=`hostname`\rfi\rPIDS=`ps -f | grep java | grep \u0026quot;.jar\u0026quot; | grep \u0026quot;$CONF_DIR\u0026quot; |awk '{print $2}'`\rif [ -n \u0026quot;$PIDS\u0026quot; ]; then\recho \u0026quot;ERROR: The $SERVER_NAME already started!\u0026quot;\recho \u0026quot;PID: $PIDS\u0026quot;\r# exit 退出命令，0为成功，其他为失败情况\rexit 1\rfi\rif [ -n \u0026quot;$SERVER_PORT\u0026quot; ]; then\rSERVER_PORT_COUNT=`netstat -tln | grep $SERVER_PORT | wc -l`\rif [ $SERVER_PORT_COUNT -gt 0 ]; then\recho \u0026quot;ERROR: The $SERVER_NAME port $SERVER_PORT already used!\u0026quot;\rexit 1\rfi\rfi\r#LOGS_DIR=\u0026quot;\u0026quot;\r#if [ -n \u0026quot;$LOGS_FILE\u0026quot; ]; then\r# LOGS_DIR=`dirname $LOGS_FILE`\r#else\r# LOGS_DIR=$DEPLOY_DIR/logs\r#fi\rLOGS_DIR=$DEPLOY_DIR/logs\r# -d 检测文件是否是目录，如果是,则返回true。\rif [ ! -d $LOGS_DIR ]; then\rmkdir $LOGS_DIR\rfi\rSTDOUT_FILE=$LOGS_DIR/stdout.log\rLIB_DIR=$DEPLOY_DIR\rMAIN_JAR=`ls $LIB_DIR|grep .jar |awk '{print \u0026quot;'$LIB_DIR'/\u0026quot;$0}' `\recho $MAIN_JAR\r#LIB_JARS=`ls $LIB_DIR|grep .jar|awk '{print \u0026quot;'$LIB_DIR'/\u0026quot;$0}'|tr \u0026quot;\\n\u0026quot; \u0026quot;:\u0026quot;`\rJAVA_DEBUG_OPTS=\u0026quot;\u0026quot;\rif [ \u0026quot;$1\u0026quot; = \u0026quot;debug\u0026quot; ]; then\rJAVA_DEBUG_OPTS=\u0026quot; -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=n \u0026quot;\rfi\rJAVA_JMX_OPTS=\u0026quot;\u0026quot;\rif [ \u0026quot;$1\u0026quot; = \u0026quot;jmx\u0026quot; ]; then\rJAVA_JMX_OPTS=\u0026quot; -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false \u0026quot;\rfi\rJAVA_MEM_OPTS=\u0026quot;\u0026quot;\rBITS=`java -version 2\u0026gt;\u0026amp;1 | grep -i 64-bit`\rif [ -n \u0026quot;$BITS\u0026quot; ]; then\rJAVA_MEM_OPTS=\u0026quot; -server -Xmx1g -Xms1g -Xmn256m -XX:PermSize=128m -Xss256k -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:LargePageSizeInBytes=128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 \u0026quot;\relse\rJAVA_MEM_OPTS=\u0026quot; -server -Xms1g -Xmx1g -XX:PermSize=128m -XX:SurvivorRatio=2 -XX:+UseParallelGC \u0026quot;\rfi\recho -e \u0026quot;Starting the $SERVER_NAME ...\\c\u0026quot;\r# if语法，if condition1 then command1 else command(不能为空) fi\rif [ \u0026quot;$1\u0026quot; == \u0026quot;stdout\u0026quot; ]; then\r# 0 表示stdin标准输入；1 表示stdout标准输出；2 表示stderr标准错误。\r# nohup java .....(java 运行期间的配置) -jar $MAIN_JAR \u0026gt; $STDOUT_FILE 2\u0026gt;\u0026amp;1 \u0026amp; 意思就是运行jar文件，并把1标准输出，2 标准错误全部输出到$STDOUT_FILE中去 ，/dev/null这个就为空，代表不输出东西，$MAIN_JAR \u0026gt; $STDOUT_FILE \u0026gt; 表示清空然后重新输入，\u0026gt;\u0026gt;表示追加，所以重新运行，日志会被清空\rnohup java $JAVA_OPTS $JAVA_MEM_OPTS $JAVA_DEBUG_OPTS $JAVA_JMX_OPTS $SENTINEL_OPTS -Xbootclasspath/a:$CONF_DIR -DlogPath=$LOGPATH -Dspring.profiles.active=$PROFILES_ACTIVE -jar $MAIN_JAR \u0026gt; $STDOUT_FILE 2\u0026gt;\u0026amp;1 \u0026amp;\relse\rnohup java $JAVA_OPTS $JAVA_MEM_OPTS $JAVA_DEBUG_OPTS $JAVA_JMX_OPTS $SENTINEL_OPTS -Xbootclasspath/a:$CONF_DIR -DlogPath=$LOGPATH -Dspring.profiles.active=$PROFILES_ACTIVE -jar $MAIN_JAR \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\rfi\rCOUNT=0\r# -eq =，-ne !=,-gt \u0026gt;,-lt \u0026lt;,-ge \u0026gt;=,-le \u0026lt;=\rwhile [ $COUNT -lt 1 ]; do\recho -e \u0026quot;.\\c\u0026quot;\rsleep 1\rif [ -n \u0026quot;$SERVER_PORT\u0026quot; ]; then\rif [ \u0026quot;$SERVER_PROTOCOL\u0026quot; == \u0026quot;thrift\u0026quot; ]; then\r#COUNT=`echo status | nc -i 1 127.0.0.1 $SERVER_PORT | grep -c OK`\rCOUNT=`netstat -an | grep $SERVER_PORT | wc -l`\relse\rCOUNT=`netstat -an | grep $SERVER_PORT | wc -l`\rfi\relse\rCOUNT=`ps -f | grep java | grep \u0026quot;.jar\u0026quot; | grep \u0026quot;$DEPLOY_DIR\u0026quot; | awk '{print $2}' | wc -l`\rfi\rif [ $COUNT -gt 0 ]; then\rbreak\rfi\rdone\recho \u0026quot;OK!\u0026quot;\rPIDS=`ps -f | grep java | grep \u0026quot;.jar\u0026quot; | grep \u0026quot;$DEPLOY_DIR\u0026quot; | awk '{print $2}'`\rsleep 2\rgreen_echo \u0026quot;------ Prog info PLS check \u0026amp; dev(开发环境) kx.prod (科兴验证环境) kx.prod (科兴生产环境) ks.prod (金山生产环境) -----\u0026quot;\recho \u0026quot;PID: $PIDS\u0026quot;\recho \u0026quot;PROG_ENV: $PROFILES_ACTIVE\u0026quot;\recho \u0026quot;LOGS\u0026quot;: `ls -l /proc/$PIDS/fd | grep .log | awk '{print $NF}' | sort |uniq`\r 日志 目前java中日志框架主要有JDKLog、Log4J、LogBack、SLF4J 其中slf4j相当于是一个接口，log4j和logback相当于是具体实现。\n目前最流行slf4j+logback\n引入依赖，目前springboot默认就是这个组合的日志框架所以不需要额外引入依赖\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt;\r\u0026lt;project xmlns=\u0026quot;http://maven.apache.org/POM/4.0.0\u0026quot; xmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot;\rxsi:schemaLocation=\u0026quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026quot;\u0026gt;\r\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\r\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.4.0\u0026lt;/version\u0026gt;\r\u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;groupId\u0026gt;com.zt\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;java-study\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt;\r\u0026lt;name\u0026gt;java-study\u0026lt;/name\u0026gt;\r\u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt;\r\u0026lt;properties\u0026gt;\r\u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt;\r\u0026lt;/properties\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;fastjson\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;1.2.71\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt;\r\u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;build\u0026gt;\r\u0026lt;plugins\u0026gt;\r\u0026lt;plugin\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt;\r\u0026lt;/plugin\u0026gt;\r\u0026lt;/plugins\u0026gt;\r\u0026lt;/build\u0026gt;\r\u0026lt;/project\u0026gt;\r 加入配置文件,配置文件详解见\nhttps://www.cnblogs.com/chanshuyi/p/something_about_java_log_framework.html\nhttps://cloud.tencent.com/developer/article/1698344\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt;\r\u0026lt;configuration scan=\u0026quot;true\u0026quot; scanPeriod=\u0026quot;10 seconds\u0026quot;\u0026gt;\r\u0026lt;!--springProperty和property都可以用来表示全局变量但springProperty代表去配置文件去读配置\t--\u0026gt;\r\u0026lt;springProperty scope=\u0026quot;context\u0026quot; name=\u0026quot;springAppName\u0026quot; source=\u0026quot;spring.application.name\u0026quot; defaultValue=\u0026quot;manage-tax\u0026quot;/\u0026gt;\r\u0026lt;!--那么问题来了，这个logPath配置文件里就没有是怎么读取的呢，没读到就默认是最高目录--\u0026gt;\r\u0026lt;springProperty scope=\u0026quot;context\u0026quot; name=\u0026quot;logPath\u0026quot; source=\u0026quot;logPath\u0026quot;/\u0026gt;\r\u0026lt;property name=\u0026quot;logPath\u0026quot; value=\u0026quot;../logs/\u0026quot;/\u0026gt;\r\u0026lt;property name=\u0026quot;CONSOLE_LOG_PATTERN\u0026quot; value=\u0026quot;[%date][%thread][manage-tax][%logger:%line][%M][%-5level][%X{logid}][%X{X-B3-TraceId:-}] =\u0026gt;%msg%n\u0026quot;/\u0026gt;\r\u0026lt;!--生产环境去掉控制台输出--\u0026gt;\r\u0026lt;appender name=\u0026quot;stdout\u0026quot; class=\u0026quot;ch.qos.logback.core.ConsoleAppender\u0026quot;\u0026gt;\r\u0026lt;withJansi\u0026gt;true\u0026lt;/withJansi\u0026gt;\r\u0026lt;encoder\u0026gt;\r\u0026lt;pattern\u0026gt;${CONSOLE_LOG_PATTERN}\u0026lt;/pattern\u0026gt;\r\u0026lt;charset\u0026gt;utf8\u0026lt;/charset\u0026gt;\r\u0026lt;/encoder\u0026gt;\r\u0026lt;/appender\u0026gt;\r\u0026lt;appender name=\u0026quot;dailyRollingFileAppender\u0026quot; class=\u0026quot;ch.qos.logback.core.rolling.RollingFileAppender\u0026quot;\u0026gt;\r\u0026lt;File\u0026gt;${logPath}${springAppName}.log\u0026lt;/File\u0026gt;\r\u0026lt;rollingPolicy class=\u0026quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy\u0026quot;\u0026gt;\r\u0026lt;FileNamePattern\u0026gt;${springAppName}.%d{yyyy-MM-dd}-%i.log\u0026lt;/FileNamePattern\u0026gt;\r\u0026lt;maxHistory\u0026gt;30\u0026lt;/maxHistory\u0026gt;\r\u0026lt;timeBasedFileNamingAndTriggeringPolicy class=\u0026quot;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\u0026quot;\u0026gt;\r\u0026lt;maxFileSize\u0026gt;100MB\u0026lt;/maxFileSize\u0026gt;\r\u0026lt;/timeBasedFileNamingAndTriggeringPolicy\u0026gt;\r\u0026lt;/rollingPolicy\u0026gt;\r\u0026lt;encoder\u0026gt;\r\u0026lt;Pattern\u0026gt;${CONSOLE_LOG_PATTERN}\u0026lt;/Pattern\u0026gt;\r\u0026lt;/encoder\u0026gt;\r\u0026lt;filter class=\u0026quot;ch.qos.logback.classic.filter.ThresholdFilter\u0026quot;\u0026gt;\r\u0026lt;level\u0026gt;DEBUG\u0026lt;/level\u0026gt;\r\u0026lt;/filter\u0026gt;\r\u0026lt;/appender\u0026gt;\r\u0026lt;!--\t异步打日志AsyncAppender --\u0026gt;\r\u0026lt;appender name =\u0026quot;ASYNC_FILE\u0026quot; class= \u0026quot;ch.qos.logback.classic.AsyncAppender\u0026quot;\u0026gt;\r\u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt;\r\u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt;\r\u0026lt;includeCallerData\u0026gt;true\u0026lt;/includeCallerData\u0026gt;\r\u0026lt;appender-ref ref = \u0026quot;dailyRollingFileAppender\u0026quot;/\u0026gt;\r\u0026lt;/appender\u0026gt;\r\u0026lt;appender name =\u0026quot;ASYNC_STDOUT\u0026quot; class= \u0026quot;ch.qos.logback.classic.AsyncAppender\u0026quot;\u0026gt;\r\u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt;\r\u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt;\r\u0026lt;includeCallerData\u0026gt;true\u0026lt;/includeCallerData\u0026gt;\r\u0026lt;appender-ref ref = \u0026quot;stdout\u0026quot;/\u0026gt;\r\u0026lt;/appender\u0026gt;\r\u0026lt;!-- 多环境日志配置\t--\u0026gt;\r\u0026lt;springProfile name=\u0026quot;dev\u0026quot;\u0026gt;\r\u0026lt;logger name=\u0026quot;com.jlpay\u0026quot; level=\u0026quot;DEBUG\u0026quot;/\u0026gt;\r\u0026lt;root level=\u0026quot;info\u0026quot;\u0026gt;\r\u0026lt;appender-ref ref=\u0026quot;ASYNC_FILE\u0026quot;/\u0026gt;\r\u0026lt;appender-ref ref=\u0026quot;ASYNC_STDOUT\u0026quot;/\u0026gt;\r\u0026lt;/root\u0026gt;\r\u0026lt;/springProfile\u0026gt;\r\u0026lt;root level=\u0026quot;info\u0026quot;\u0026gt;\r\u0026lt;appender-ref ref=\u0026quot;stdout\u0026quot;/\u0026gt;\r\u0026lt;appender-ref ref=\u0026quot;ASYNC_FILE\u0026quot;/\u0026gt;\r\u0026lt;/root\u0026gt;\r\u0026lt;/configuration\u0026gt;\r 使用\npackage com.zt.javastudy;\rimport lombok.extern.slf4j.Slf4j;\rimport org.slf4j.Logger;\rimport org.slf4j.LoggerFactory;\rimport org.springframework.boot.SpringApplication;\rimport org.springframework.boot.autoconfigure.SpringBootApplication;\rimport org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration;\r/**\r* @author zhengtao\r*/\r@SpringBootApplication(exclude= {DataSourceAutoConfiguration.class})\r// 引入lombok注解，加上这个注释就可以使用log打印日志了\r@Slf4j\rpublic class JavaStudyApplication {\rpublic static final Logger logger = LoggerFactory.getLogger(JavaStudyApplication.class);\rpublic static void main(String[] args) {\rlogger.info(\u0026quot;java-study starting.....\u0026quot;);\rlog.error(\u0026quot;slf4j starting...\u0026quot;);\rSpringApplication.run(JavaStudyApplication.class, args);\rlogger.info(\u0026quot;java-study ok.....\u0026quot;);\rlog.error(\u0026quot;slf4j ok.........\u0026quot;);\r}\r}\r 如果是使用log4j只需要引入相应的配置，然后配置对应的配置文件，使用方法是一样的，例如\nlog4j.rootLogger=INFO,Console,CommonFile\rlog4j.appender.Console=org.apache.log4j.ConsoleAppender log4j.appender.Console.Target=System.out\rlog4j.appender.Console.Threshold=DEBUG\rlog4j.appender.Console.layout = org.apache.log4j.PatternLayout log4j.appender.Console.layout.ConversionPattern=[%d{yyyy-MM-dd HH\\:mm\\:ss SSSSSS}][%t][(QRCODE_MERCHANT_SERVER)(%C:%L)(%M)][%p][%X{logid}][%X{orderid}]%m%n\r#WARN INFO\rlog4j.appender.CommonFile=org.apache.log4j.RollingFileAppender\rlog4j.appender.CommonFile.MaxFileSize = 100MB\rlog4j.appender.CommonFile.File=../logs/qrcode_merchant.log\rlog4j.appender.CommonFile.MaxBackupIndex=100 log4j.appender.CommonFile.Threshold=INFO,ERROR,WARN\rlog4j.appender.CommonFile.layout=org.apache.log4j.PatternLayout log4j.appender.CommonFile.layout.ConversionPattern=[%d{yyyy-MM-dd HH\\:mm\\:ss SSSSSS}][%t][(QRCODE_MERCHANT_SERVER)(%C:%L)(%M)][%p][%X{logid}][%X{orderid}]%m%n\rlog4j.appender.CommonFile.Encoding=UTF-8\rlog4j.appender.CommonFile.BufferedIO=false\r 单元测试 一致性哈希 多线程和异步任务 @Transaction和@TransactionalEventListener file类 maven 公司项目配置文件读取过程\n 通过指定配置文件目录，来使得本地配置读取的为development  \u0026lt;resources\u0026gt;\r\u0026lt;resource\u0026gt;\r\u0026lt;directory\u0026gt;src/main/resources\u0026lt;/directory\u0026gt;\r\u0026lt;!-- 资源根目录排除各环境的配置，使用单独的资源目录来指定 --\u0026gt;\r\u0026lt;excludes\u0026gt;\r\u0026lt;exclude\u0026gt;test/*\u0026lt;/exclude\u0026gt;\r\u0026lt;exclude\u0026gt;mir/*\u0026lt;/exclude\u0026gt;\r\u0026lt;exclude\u0026gt;verify/*\u0026lt;/exclude\u0026gt;\r\u0026lt;exclude\u0026gt;production/*\u0026lt;/exclude\u0026gt;\r\u0026lt;exclude\u0026gt;development/*\u0026lt;/exclude\u0026gt;\r\u0026lt;/excludes\u0026gt;\r\u0026lt;/resource\u0026gt;\r\u0026lt;resource\u0026gt;\r\u0026lt;directory\u0026gt;src/main/resources/${profiles.active}\u0026lt;/directory\u0026gt;\r\u0026lt;/resource\u0026gt;\r\u0026lt;/resources\u0026gt;\r\u0026lt;profile\u0026gt;\r\u0026lt;!-- 本地开发环境 配置为真--\u0026gt;\r\u0026lt;id\u0026gt;development\u0026lt;/id\u0026gt;\r\u0026lt;properties\u0026gt;\r\u0026lt;profiles.active\u0026gt;development\u0026lt;/profiles.active\u0026gt;\r\u0026lt;/properties\u0026gt;\r\u0026lt;activation\u0026gt;\r\u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt;\r\u0026lt;/activation\u0026gt;\r\u0026lt;/profile\u0026gt;\r  将配置文件达到jar中\n\u0026lt;outputDirectory\u0026gt;\rtarget/classes\r\u0026lt;/outputDirectory\u0026gt;\r   有了前面两步配合classpath* 就可以读到配置文件中的内容了，而我们线上之所以配置文件不是本地开发的配置文件是因为线上加了这个代码，\n  \u0026lt;resources\u0026gt;\r\u0026lt;resource\u0026gt;\r\u0026lt;directory\u0026gt;src/main/resources\u0026lt;/directory\u0026gt;\r\u0026lt;!-- resources下的所有文件都不打到jar中 --\u0026gt;\r\u0026lt;excludes\u0026gt;\r\u0026lt;exclude\u0026gt;**/*.*\u0026lt;/exclude\u0026gt;\r\u0026lt;/excludes\u0026gt;\r\u0026lt;/resource\u0026gt;\r\u0026lt;/resources\u0026gt;\r 从而线上的就会去寻找配置文件。\n把maven配置认真看下。\n幂等 分布式事务 ","id":24,"section":"posts","summary":"知识点！！！ 分布式session Nginx 正向代理 正向代理代理的对象是客户端，正向代理 是一个位于客户端和原始服务器(origin server)之间的","tags":null,"title":"知识点","uri":"https://wzgl998877.github.io/2022/01/%E7%9F%A5%E8%AF%86%E7%82%B9/","year":"2022"},{"content":"设计模式 七大软件设计原则 1 . 开闭原则 ​\t软件实体应当对扩展开放，对修改关闭。当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下，可以扩展模块的功能，使其满足新的需求，实现方法：可以通过“抽象约束、封装变化”来实现开闭原则，即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，而将相同的可变因素封装在相同的具体实现类中。\n2. 里氏替换原则 ​\t继承必须确保超类所拥有的性质在子类中仍然成立。里氏替换原则是实现开闭原则的重要方式之一，子类可以扩展父类的功能，但不能改变父类原有的功能。也就是说：子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。\n 当子类的方法重载父类的方法时，方法的前置条件（即方法的输入参数）要比父类的方法更宽松 当子类的方法实现父类的方法时（重写/重载或实现抽象方法），方法的后置条件（即方法的的输出/返回值）要比父类的方法更严格或相等  3. 依赖倒置原则 ​\t高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象。其核心思想是：要面向接口编程，不要面向实现编程，依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。\n4. 单一职责原则 ​\t一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分。单一职责原则的核心就是控制类的粒度大小、将对象解耦、提高其内聚性。如果遵循单一职责原则将有以下优点。\n 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。  5. 接口隔离原则 ​\t客户端不应该被迫依赖于它不使用的方法，一个类对另一个类的依赖应该建立在最小的接口上。要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。\n​\t接口隔离原则和单一职责都是为了提高类的内聚性、降低它们之间的耦合性，体现了封装的思想，但两者是不同的：\n 单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。 单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要约束接口，主要针对抽象和程序整体框架的构建。  在具体应用接口隔离原则时，应该根据以下几个规则来衡量。\n 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。  6. 迪米特法则 ​\t迪米特法则（Law of Demeter，LoD）又叫作最少知识原则，迪米特法则的定义是：只与你的直接朋友交谈，不跟“陌生人”说话。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。\n从迪米特法则的定义和特点可知，它强调以下两点：\n 从依赖者的角度来说，只依赖应该依赖的对象。 从被依赖者的角度说，只暴露应该暴露的方法。  7. 合成复用原则 ​\t合成复用原则又叫组合/聚合复用原则。它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。\n七大软件设计原则总结 ​\t这 7 种设计原则是软件设计模式必须尽量遵循的原则，各种原则要求的侧重点不同。其中，开闭原则是总纲，它告诉我们要对扩展开放，对修改关闭；里氏替换原则告诉我们不要破坏继承体系；依赖倒置原则告诉我们要面向接口编程；单一职责原则告诉我们实现类要职责单一；接口隔离原则告诉我们在设计接口的时候要精简单一；迪米特法则告诉我们要降低耦合度；合成复用原则告诉我们要优先使用组合或者聚合关系复用，少用继承关系复用。\n设计模式的分类和功能 1. 根据目的来分 根据模式是用来完成什么工作来划分，这种方式可分为创建型模式、结构型模式和行为型模式 3 种。\n 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。  2. 根据作用范围来分 根据模式是主要用于类上还是主要用于对象上来分，这种方式可分为类模式和对象模式两种。\n 类模式：用于处理类与子类之间的关系，这些关系通过继承来建立，是静态的，在编译时刻便确定下来了。GoF中的工厂方法、（类）适配器、模板方法、解释器属于该模式。 对象模式：用于处理对象之间的关系，这些关系可以通过组合或聚合来实现，在运行时刻是可以变化的，更具动态性。GoF 中除了以上 4 种，其他的都是对象模式。  表 1 介绍了这 23 种设计模式的分类。\n   范围\\目的 创建型模式 结构型模式 行为型模式     类模式 工厂方法 (类）适配器 模板方法、解释器   对象模式 单例 原型 抽象工厂 建造者 代理 (对象）适配器 桥接 装饰 外观 享元 组合 策略 命令 职责链 状态 观察者 中介者 迭代器 访问者 备忘录    3. GoF的23种设计模式的功能 前面说明了 GoF 的 23 种设计模式的分类，现在对各个模式的功能进行介绍。\n 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。  结构型模式概述 ​\t结构型模式描述如何将类或对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者釆用组合或聚合来组合对象。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象结构型模式比类结构型模式具有更大的灵活性。\n代理模式 ​\t代理模式的定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。\n适配器模式 ​\t将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。\n桥接模式 ​\t将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。\n​\t桥接（Bridge）模式包含以下主要角色。\n 抽象化（Abstraction）角色：定义抽象类，并包含一个对实现化对象的引用。这里就相当于是一个桥将一个类与另一个类联系起来了。 扩展抽象化（Refined Abstraction）角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化（Implementor）角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化（Concrete Implementor）角色：给出实现化角色接口的具体实现。  客户去买包，有钱包和书包，钱包又分红色和白色，书包也分红色和白色，如果按照普通的设计，那么就是书包继承包，钱包继承包，然后红色的书包继承书包，白色的书包继承书包等等，而桥接模式就是，先把颜色抽象出来成为一个单独的类（接口），红色实现颜色，黄色实现颜色，包将颜色变成属性（桥），书包继承包，钱包继承包，然后书包通过继承包就可以选择响应的颜色。\nColor color = new Yellow();\rBag bag = new Wallet();\rbag.setColor(color);\r 桥接模式通常适用于以下场景。\n 当一个类存在两个独立变化的维度，且这两个维度都需要进行扩展时。// 包按使用来分分为钱包和书包，按颜色分为红色和黄色 当一个系统不希望使用继承或因为多层次继承导致系统类的个数急剧增加时。// 因为如果采用多继承的话，增加一个垮包，需要增加三个类，（一个挎包类，一个黄色的挎包类，一个红色的挎包类） 当一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性时。  装饰模式 ​\t指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。通常情况下，扩展一个类的功能会使用继承方式来实现。但继承具有静态特征，耦合度高，并且随着扩展功能的增多，子类会很膨胀。如果使用组合关系来创建一个包装对象（即装饰对象）来包裹真实对象，并在保持真实对象的类结构不变的前提下，为其提供额外的功能，这就是装饰模式的目标。下面来分析其基本结构和实现方法。装饰模式主要包含以下角色。\n  抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。// 需要增加功能的类，shape类，抽象出了一个画的方法\n  具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。// 具体的构件，Circle画圆，Rectangle画长方形，这两个构件需要增加功能，比如画红色的圆。\n  抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。// 一个可以将具体构件传入的类，ShapeDecorator\n  具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。// 通过实现抽象装饰，来扩展传入的构件，追加方法，这里追加了一个方法，红色的边框。\nShape circle = new Circle();// 画一个圆\rcircle.draw();//画一个圆\rShape redCircle = new RedShapeDecorator(new Circle());// 将圆传入具体装饰角色追加功能，这里是追加了一个红色边框\rredCircle.draw();//画出了一个红色边框的圆\r    装饰模式通常在以下几种情况使用。\r- 当需要给一个现有类添加附加职责，而又不能采用生成子类的方法进行扩充时。例如，该类被隐藏或者该类是终极类或者采用继承方式会产生大量的子类。\r- 当需要通过对现有的一组基本功能进行排列组合而产生非常多的功能时，采用继承关系很难实现，而采用装饰模式却很好实现。// 比如既要画红色的圆，又要画红色的长方形，\r- 当对象的功能要求可以动态地添加，也可以再动态地撤销时。\rJava I/o 标准库就是采用了装饰模式\r![InputStream部分类关系](http://image.laijianfeng.org/20180918InputStream.png)\r````Java\rDataInputStream in=new DataInputStream(new BufferedInputStream(new FileInputStream(\u0026quot;D:\\\\hello.txt\u0026quot;)));\r 外观模式 ​\t外观（Facade）模式又叫作门面模式，是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。\n外观（Facade）模式是“迪米特法则”的典型应用，它有以下主要优点。\n 降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。 对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。 降低了大型软件系统中的编译依赖性，简化了系统在不同平台之间的移植过程，因为编译一个子系统不会影响其他的子系统，也不会影响外观对象。  外观（Facade）模式的主要缺点如下。\n 不能很好地限制客户使用子系统类，很容易带来未知风险。 增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则”。  外观（Facade）模式包含以下主要角色。\n 外观（Facade）角色：为多个子系统对外提供一个共同的接口。 子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。 客户（Client）角色：通过一个外观角色访问各个子系统的功能。  其结构图如图 2 所示。\n享元模式 ​\t享元（Flyweight）模式的定义：运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。享元模式的定义提出了两个要求，细粒度和共享对象。因为要求细粒度，所以不可避免地会使对象数量多且性质相近，此时我们就将这些对象的信息分为两个部分：内部状态和外部状态。\n 内部状态指对象共享出来的信息，存储在享元信息内部，并且不会随环境的改变而改变； 外部状态指对象得以依赖的一个标记，随环境的改变而改变，不可共享。  享元模式的主要角色有如下。\n  抽象享元角色（Flyweight）：是所有的具体享元类的基类，为具体享元规范需要实现的公共接口，非享元的外部状态以参数的形式通过方法传入。\n  具体享元（Concrete Flyweight）角色：实现抽象享元角色中所规定的接口。\n  非享元（Unsharable Flyweight)角色：是不可以共享的外部状态，它以参数的形式注入具体享元的相关方法中。\n  享元工厂（Flyweight Factory）角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。\n我的理解，其实所谓的享元模式就是将可以共享的元素或者方法抽象出来成为一个类，然后通过一个统一的接口传入不可共享的元素，完成工作，共享的方法是在一个工厂中，若此对象不存在则创建，然后加入到map中，若存在则直接从map中取出。\n  组合模式 ​\t组合（Composite Pattern）模式的定义：有时又叫作整体-部分（Part-Whole）模式，它是一种将对象组合成树状的层次结构的模式，用来表示“整体-部分”的关系，使用户对单个对象和组合对象具有一致的访问性。组合模式一般用来描述整体与部分的关系，它将对象组织到树形结构中，顶层的节点被称为根节点，根节点下面可以包含树枝节点和叶子节点，树枝节点下面又可以包含树枝节点和叶子节点，树形结构图如下。\n组合模式包含以下主要角色。\n 抽象构件（Component）角色：它的主要作用是为树叶构件和树枝构件声明公共接口，并实现它们的默认行为。在透明式的组合模式中抽象构件还声明访问和管理子类的接口；在安全式的组合模式中不声明访问和管理子类的接口，管理工作由树枝构件完成。（总的抽象类或接口，定义一些通用的方法，比如新增、删除） 树叶构件（Leaf）角色：是组合中的叶节点对象，它没有子节点，用于继承或实现抽象构件。 树枝构件（Composite）角色 / 中间构件：是组合中的分支节点对象，它有子节点，用于继承和实现抽象构件。它的主要作用是存储和管理子部件，通常包含 Add()、Remove()、GetChild() 等方法  行为型模式 ​\t行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。\n模板方法模式 ​\t定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。它是一种类行为型模式。\n模板方法模式包含以下主要角色。\n1）抽象类/抽象模板（Abstract Class）\n抽象模板类，负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下。\n① 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。\n② 基本方法：是整个算法中的一个步骤，包含以下几种类型。\n 抽象方法：在抽象类中声明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。  2）具体子类/具体实现（Concrete Class）\n具体实现类，实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤。\n模板方法模式通常适用于以下场景。\n 算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。 当多个子类存在公共的行为时，可以将其提取出来并集中到一个公共父类中以避免代码重复。首先，要识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。 当需要控制子类的扩展时，模板方法只在特定点调用钩子操作，这样就只允许在这些点进行扩展。  所以平常写的代码到底是命令模式还是模板方法模式？我们平常的代码有点像命令模式但是没有实现者，具体命令类也没有实现者对象，调用者好像也没有，哈哈\n命令模式 ​\t将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。\n可以将系统中的相关操作抽象成命令，使调用者与实现者相关分离，其结构如下。\n模式的结构\n命令模式包含以下主要角色。\n 抽象命令类（Command）角色：声明执行命令的接口，拥有执行命令的抽象方法 execute()。 具体命令类（Concrete Command）角色：是抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作。 实现者/接收者（Receiver）角色：执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）角色：是请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者。  其结构图如图 1 所示。\n策略模式 ​\t该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。策略模式是准备一组算法，并将这组算法封装到一系列的策略类里面，作为一个抽象策略类的子类。策略模式的重心不是如何实现算法，而是如何组织这些算法，从而让程序结构更加灵活，具有更好的维护性和扩展性，现在我们来分析其基本结构和实现方法。\n策略模式的主要角色如下。\n 抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。 环境（Context）类：持有一个策略类的引用，最终给客户端调用。  责任链模式 ​\t为了避免请求发送者与多个请求处理者耦合在一起，于是将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。\n通常情况下，可以通过数据链表来实现职责链模式的数据结构。\n模式的结构\n​\t职责链模式主要包含以下角色。\n 抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。  模式的应用场景\n 多个对象可以处理一个请求，但具体由哪个对象处理该请求在运行时自动确定。// 有多个具体处理者，由传入的条件决定使用哪个处理者 可动态指定一组对象处理请求，或添加新的处理者。 需要在不明确指定请求处理者的情况下，向多个处理者中的一个提交请求。  职责链模式存在以下两种情况。\n 纯的职责链模式：一个请求必须被某一个处理者对象所接收，且一个具体处理者对某个请求的处理只能采用以下两种行为之一：自己处理（承担责任）；把责任推给下家处理。 不纯的职责链模式：允许出现某一个具体处理者对象在承担了请求的一部分责任后又将剩余的责任传给下家的情况，且一个请求可以最终不被任何接收端对象所接收。  状态模式 ​\t对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。状态模式把受环境改变的对象行为包装在不同的状态对象里，其意图是让一个对象在其内部状态改变的时候，其行为也随之改变。现在我们来分析其基本结构和实现方法。\n模式的结构\n状态模式包含以下主要角色。\n 环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。  其结构图如图 1 所示。\n多线程存在 5 种状态，分别为新建状态、就绪状态、运行状态、阻塞状态和死亡状态，各个状态当遇到相关方法调用或事件触发时会转换到其他状态，其状态转换规律如图 3 所示。\n图3 线程状态转换图\n现在先定义一个抽象状态类（TheadState），然后为图 3 所示的每个状态设计一个具体状态类，它们是新建状态（New）、就绪状态（Runnable ）、运行状态（Running）、阻塞状态（Blocked）和死亡状态（Dead），每个状态中有触发它们转变状态的方法，环境类（ThreadContext）中先生成一个初始状态（New），并提供相关触发方法，图 4 所示是线程状态转换程序的结构图。\n观察者模式 ​\t指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。实现观察者模式时要注意具体目标对象和具体观察者对象之间不能直接调用，否则将使两者之间紧密耦合起来，这违反了面向对象的设计原则。\n观察者模式的主要角色如下。\n 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。  中介者模式 ​\t定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。\n中介者模式实现的关键是找出“中介者”，下面对它的结构和实现进行分析。\n中介者模式包含以下主要角色。\n 抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者（Concrete Mediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。 抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。 具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。  前面分析了中介者模式的结构与特点，下面分析其以下应用场景。\n 当对象之间存在复杂的网状结构关系而导致依赖关系混乱且难以复用时。 当想创建一个运行于多个类之间的对象，又不想生成新的子类时。  迭代器模式 ​\t提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示\n迭代器模式是通过将聚合对象的遍历行为分离出来，抽象成迭代器类来实现的，其目的是在不暴露聚合对象的内部结构的情况下，让外部代码透明地访问聚合的内部数据。现在我们来分析其基本结构与实现方法。\n模式的结构\n迭代器模式主要包含以下角色。\n 抽象聚合（Aggregate）角色：定义存储、添加、删除聚合对象以及创建迭代器对象的接口。 具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、first()、next() 等方法。 具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。  不是很懂\n访问者模式 ​\t将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离，是行为类模式中最复杂的一种模式。\n访问者模式包含以下主要角色。\n 抽象访问者（Visitor）角色：定义一个访问具体元素的接口，为每个具体元素类对应一个访问操作 visit() ，该操作中的参数类型标识了被访问的具体元素。 具体访问者（ConcreteVisitor）角色：实现抽象访问者角色中声明的各个访问操作，确定访问者访问一个元素时该做什么。 抽象元素（Element）角色：声明一个包含接受操作 accept() 的接口，被接受的访问者对象作为 accept() 方法的参数。 具体元素（ConcreteElement）角色：实现抽象元素角色提供的 accept() 操作，其方法体通常都是 visitor.visit(this) ，另外具体元素中可能还包含本身业务逻辑的相关操作。 对象结构（Object Structure）角色：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。// 数据结构，需要包含元素  访问者模式的核心思想是为了访问比较复杂的数据结构，不去改变数据结构，而是把对数据的操作抽象出来，在“访问”的过程中以回调形式在访问者中处理操作逻辑。如果要新增一组操作，那么只需要增加一个新的访问者。\n备忘录模式 ​\t在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。备忘录模式的核心是设计备忘录类以及用于管理备忘录的管理者类，现在我们来学习其结构与实现。\n模式的结构\n备忘录模式的主要角色如下。\n 发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。  自己的理解，备忘录模式其实就是发起人将自己的状态通过自身的方法保存在备忘录中，而这个备忘录是管理者的属性，也就是说，发起人将自己的状态保存在管理者的备忘录属性中，当发起人状态改变，但还未保存想要回滚时就可以从管理者这里取出自己保存的状态然后更新自己的状态，这就达到了回滚的效果。\n下面介绍备忘录模式如何同原型模式混合使用。在备忘录模式中，通过定义“备忘录”来备份“发起人”的信息，而原型模式的 clone() 方法具有自备份功能，所以，如果让发起人实现 Cloneable 接口就有备份自己的功能，这时可以删除备忘录类。也就是说将管理者直接管理发起人，当发起人状态改变，并保存时就把自己通过原型模式也就是克隆一份给到管理者，当需要回滚时，只需要将管理者中永远的发起人的状态更新给真正的发起人即可。\n解释器模式 ","id":25,"section":"posts","summary":"设计模式 七大软件设计原则 1 . 开闭原则 ​ 软件实体应当对扩展开放，对修改关闭。当应用的需求改变时，在不修改软件实体的源代码或者二进制代码的前提下","tags":["设计模式"],"title":"设计模式","uri":"https://wzgl998877.github.io/2022/01/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","year":"2022"},{"content":"[TOC]\nLinux工作日常命令 jps 命令 jps 查看全部的jvm进程，可以带上grep命令一起使用 jps |grep -e ×××\nfind命令 基本格式：find path expression\n1. 按照文件名查找 　(1)find / -name httpd.conf　#在根目录下查找文件httpd.conf，表示在整个硬盘查找 (2)find /etc -name httpd.conf　#在/etc目录下文件httpd.conf (3)find /etc -name \u0026lsquo;srm\u0026rsquo;　#使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件 (4)find . -name \u0026lsquo;srm*\u0026rsquo; #表示当前目录下查找文件名开头是字符串‘srm’的文件\n2.按照文件特征查找  　(1)find / -amin -10 # 查找在系统中最后10分钟访问的文件(access time) (2)find / -atime -2　# 查找在系统中最后48小时访问的文件 (3)find / -empty # 查找在系统中为空的文件或者文件夹 (4)find / -group cat # 查找在系统中属于 group为cat的文件 (5)find / -mmin -5 # 查找在系统中最后5分钟里修改过的文件(modify time) (6)find / -mtime -1 #查找在系统中最后24小时里修改过的文件 (7)find / -user fred #查找在系统中属于fred这个用户的文件 (8)find / -size +10000c　#查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB) (9)find / -size -1000k #查找出小于1000KB的文件\n3.使用混合查找方式查找文件 　参数有： ！，-and(-a)，-or(-o)。\n　(1)find /tmp -size +10000c -and -mtime +2 #在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件 (2)find / -user fred -or -user george #在/目录下查找用户是fred或者george的文件文件 (3)find /tmp ! -user panda　#在/tmp目录中查找所有不属于panda用户的文件\n文件大小排序 du -sh * 可以将文件夹下按顺序排序\nmv account_confirm.log.2* /log/logs/tms 移动文件可以直接使用正则表达式\ntar czvf 25480446_2020.tar.gz 2020* 压缩多个文件到压缩包\ndf -h 查看磁盘使用情况\nfree -h 查看内存使用情况\nip命令 比较高深，没参透，常用的有 ip a\ngrep   在文件中搜索一个单词，命令会返回一个包含 “match_pattern” 的文本行\ngrep \u0026quot;match_pattern\u0026quot; file_name\r如：grep \u0026quot;taifungChangeAssignTaskRpc\u0026quot; manage_service_workflow.log\r匹配多个 grep \u0026quot;match_pattern\u0026quot; file_name1 file_name2\r 匹配同时满足多个条件的\n# | grep \u0026quot;bb\u0026quot; 同时满足多少就写多少\rgrep \u0026quot;aaa\u0026quot; log.log | grep \u0026quot;bb\u0026quot;\r 相反的\ngrep -v \u0026quot;match_pattern\u0026quot; file_name\r输出不匹配的所有行\r   正则匹配\ngrep -E \u0026quot;[1-9]+\u0026quot; file_name\reg: grep -E *.06151000064240 manage_service_workflow.log  忽略大小写\ngrep -i \u0026quot;match_pattern\u0026quot; file_name\reg:grep -i \u0026quot;tAIfungChangeAssignTaskRpc\u0026quot; manage_service_workflow.log    打印规则\n# 显示匹配某个结果之后的3行，使用 -A 选项：\rgrep \u0026quot;5\u0026quot; file_name -A 3\r# 显示匹配某个结果之前的3行，使用 -B 选项：\rgrep \u0026quot;5\u0026quot; file_name -B 3\r# 显示匹配某个结果的前三行和后三行，使用 -C 选项：\rgrep \u0026quot;5\u0026quot; file_name -C 3\r   统计包含指定字符的行数\ncat test.log | grep -c test\r   ","id":26,"section":"posts","summary":"[TOC] Linux工作日常命令 jps 命令 jps 查看全部的jvm进程，可以带上grep命令一起使用 jps |grep -e ××× find命令 基本格式：find path expression 1. 按照文件名","tags":null,"title":"","uri":"https://wzgl998877.github.io/1/01/linux%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%B8%B8%E5%91%BD%E4%BB%A4/","year":"0001"}],"tags":[{"title":"http","uri":"https://wzgl998877.github.io/tags/http/"},{"title":"Java基础","uri":"https://wzgl998877.github.io/tags/java%E5%9F%BA%E7%A1%80/"},{"title":"Java并发","uri":"https://wzgl998877.github.io/tags/java%E5%B9%B6%E5%8F%91/"},{"title":"JVM","uri":"https://wzgl998877.github.io/tags/jvm/"},{"title":"LeetCode","uri":"https://wzgl998877.github.io/tags/leetcode/"},{"title":"mybatis","uri":"https://wzgl998877.github.io/tags/mybatis/"},{"title":"mysql","uri":"https://wzgl998877.github.io/tags/mysql/"},{"title":"Netty","uri":"https://wzgl998877.github.io/tags/netty/"},{"title":"Redis","uri":"https://wzgl998877.github.io/tags/redis/"},{"title":"RPC","uri":"https://wzgl998877.github.io/tags/rpc/"},{"title":"spring","uri":"https://wzgl998877.github.io/tags/spring/"},{"title":"Websocket","uri":"https://wzgl998877.github.io/tags/websocket/"},{"title":"消息队列","uri":"https://wzgl998877.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"title":"设计模式","uri":"https://wzgl998877.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]}